1 --> Deficiencies in Current Practices of  Clinical Natural Language Processing  (CNLP) – White Paper  “Virtually no CNLP software is fit for purpose out-of-the-box and will  invariably require tuning, if not significant enhancement, to serve a useful  productive purpose to a high accuracy for a particular client.”   Jon Patrick 2019  We have recently assessed the accuracy of Clinical NLP software available  through either open source projects or commercial demonstration systems  at processing pathology reports. 


 ---- TOKENS ----

 ['Deficiencies', 'in', 'Current', 'Practices', 'of', 'Clinical', 'Natural', 'Language', 'Processing', '(', 'CNLP', ')', '–', 'White', 'Paper', '“', 'Virtually', 'no', 'CNLP', 'software', 'is', 'fit', 'for', 'purpose', 'out-of-the-box', 'and', 'will', 'invariably', 'require', 'tuning', ',', 'if', 'not', 'significant', 'enhancement', ',', 'to', 'serve', 'a', 'useful', 'productive', 'purpose', 'to', 'a', 'high', 'accuracy', 'for', 'a', 'particular', 'client.', '”', 'Jon', 'Patrick', '2019', 'We', 'have', 'recently', 'assessed', 'the', 'accuracy', 'of', 'Clinical', 'NLP', 'software', 'available', 'through', 'either', 'open', 'source', 'projects', 'or', 'commercial', 'demonstration', 'systems', 'at', 'processing', 'pathology', 'reports', '.'] 

 TOTAL TOKENS ==> 79

 ---- POST ----

 [('Deficiencies', 'NNS'), ('in', 'IN'), ('Current', 'NNP'), ('Practices', 'NNPS'), ('of', 'IN'), ('Clinical', 'NNP'), ('Natural', 'NNP'), ('Language', 'NNP'), ('Processing', 'NNP'), ('(', '('), ('CNLP', 'NNP'), (')', ')'), ('–', 'NNP'), ('White', 'NNP'), ('Paper', 'NNP'), ('“', 'NNP'), ('Virtually', 'NNP'), ('no', 'DT'), ('CNLP', 'NNP'), ('software', 'NN'), ('is', 'VBZ'), ('fit', 'JJ'), ('for', 'IN'), ('purpose', 'JJ'), ('out-of-the-box', 'JJ'), ('and', 'CC'), ('will', 'MD'), ('invariably', 'RB'), ('require', 'VB'), ('tuning', 'NN'), (',', ','), ('if', 'IN'), ('not', 'RB'), ('significant', 'JJ'), ('enhancement', 'NN'), (',', ','), ('to', 'TO'), ('serve', 'VB'), ('a', 'DT'), ('useful', 'JJ'), ('productive', 'JJ'), ('purpose', 'NN'), ('to', 'TO'), ('a', 'DT'), ('high', 'JJ'), ('accuracy', 'NN'), ('for', 'IN'), ('a', 'DT'), ('particular', 'JJ'), ('client.', 'NN'), ('”', 'NNP'), ('Jon', 'NNP'), ('Patrick', 'NNP'), ('2019', 'CD'), ('We', 'PRP'), ('have', 'VBP'), ('recently', 'RB'), ('assessed', 'VBN'), ('the', 'DT'), ('accuracy', 'NN'), ('of', 'IN'), ('Clinical', 'NNP'), ('NLP', 'NNP'), ('software', 'NN'), ('available', 'JJ'), ('through', 'IN'), ('either', 'DT'), ('open', 'JJ'), ('source', 'NN'), ('projects', 'NNS'), ('or', 'CC'), ('commercial', 'JJ'), ('demonstration', 'NN'), ('systems', 'NNS'), ('at', 'IN'), ('processing', 'VBG'), ('pathology', 'NN'), ('reports', 'NNS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Deficiencies', 'Current', 'Practices', 'Clinical', 'Natural', 'Language', 'Processing', '(', 'CNLP', ')', '–', 'White', 'Paper', '“', 'Virtually', 'CNLP', 'software', 'fit', 'purpose', 'out-of-the-box', 'invariably', 'require', 'tuning', ',', 'significant', 'enhancement', ',', 'serve', 'useful', 'productive', 'purpose', 'high', 'accuracy', 'particular', 'client.', '”', 'Jon', 'Patrick', '2019', 'recently', 'assessed', 'accuracy', 'Clinical', 'NLP', 'software', 'available', 'either', 'open', 'source', 'projects', 'commercial', 'demonstration', 'systems', 'processing', 'pathology', 'reports', '.']

 TOTAL FILTERED TOKENS ==>  57

 ---- POST FOR FILTERED TOKENS ----

 [('Deficiencies', 'NNS'), ('Current', 'NNP'), ('Practices', 'NNPS'), ('Clinical', 'NNP'), ('Natural', 'NNP'), ('Language', 'NNP'), ('Processing', 'NNP'), ('(', '('), ('CNLP', 'NNP'), (')', ')'), ('–', 'NNP'), ('White', 'NNP'), ('Paper', 'NNP'), ('“', 'NNP'), ('Virtually', 'NNP'), ('CNLP', 'NNP'), ('software', 'NN'), ('fit', 'JJ'), ('purpose', 'JJ'), ('out-of-the-box', 'JJ'), ('invariably', 'RB'), ('require', 'VBP'), ('tuning', 'NN'), (',', ','), ('significant', 'JJ'), ('enhancement', 'NN'), (',', ','), ('serve', 'VBP'), ('useful', 'JJ'), ('productive', 'JJ'), ('purpose', 'NN'), ('high', 'JJ'), ('accuracy', 'NN'), ('particular', 'JJ'), ('client.', 'NN'), ('”', 'NNP'), ('Jon', 'NNP'), ('Patrick', 'NNP'), ('2019', 'CD'), ('recently', 'RB'), ('assessed', 'VBD'), ('accuracy', 'NN'), ('Clinical', 'NNP'), ('NLP', 'NNP'), ('software', 'NN'), ('available', 'JJ'), ('either', 'CC'), ('open', 'JJ'), ('source', 'NN'), ('projects', 'NNS'), ('commercial', 'JJ'), ('demonstration', 'NN'), ('systems', 'NNS'), ('processing', 'VBG'), ('pathology', 'NN'), ('reports', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Deficiencies Current', 'Current Practices', 'Practices Clinical', 'Clinical Natural', 'Natural Language', 'Language Processing', 'Processing (', '( CNLP', 'CNLP )', ') –', '– White', 'White Paper', 'Paper “', '“ Virtually', 'Virtually CNLP', 'CNLP software', 'software fit', 'fit purpose', 'purpose out-of-the-box', 'out-of-the-box invariably', 'invariably require', 'require tuning', 'tuning ,', ', significant', 'significant enhancement', 'enhancement ,', ', serve', 'serve useful', 'useful productive', 'productive purpose', 'purpose high', 'high accuracy', 'accuracy particular', 'particular client.', 'client. ”', '” Jon', 'Jon Patrick', 'Patrick 2019', '2019 recently', 'recently assessed', 'assessed accuracy', 'accuracy Clinical', 'Clinical NLP', 'NLP software', 'software available', 'available either', 'either open', 'open source', 'source projects', 'projects commercial', 'commercial demonstration', 'demonstration systems', 'systems processing', 'processing pathology', 'pathology reports', 'reports .'] 

 TOTAL BIGRAMS --> 56 



 ---- TRI-GRAMS ---- 

 ['Deficiencies Current Practices', 'Current Practices Clinical', 'Practices Clinical Natural', 'Clinical Natural Language', 'Natural Language Processing', 'Language Processing (', 'Processing ( CNLP', '( CNLP )', 'CNLP ) –', ') – White', '– White Paper', 'White Paper “', 'Paper “ Virtually', '“ Virtually CNLP', 'Virtually CNLP software', 'CNLP software fit', 'software fit purpose', 'fit purpose out-of-the-box', 'purpose out-of-the-box invariably', 'out-of-the-box invariably require', 'invariably require tuning', 'require tuning ,', 'tuning , significant', ', significant enhancement', 'significant enhancement ,', 'enhancement , serve', ', serve useful', 'serve useful productive', 'useful productive purpose', 'productive purpose high', 'purpose high accuracy', 'high accuracy particular', 'accuracy particular client.', 'particular client. ”', 'client. ” Jon', '” Jon Patrick', 'Jon Patrick 2019', 'Patrick 2019 recently', '2019 recently assessed', 'recently assessed accuracy', 'assessed accuracy Clinical', 'accuracy Clinical NLP', 'Clinical NLP software', 'NLP software available', 'software available either', 'available either open', 'either open source', 'open source projects', 'source projects commercial', 'projects commercial demonstration', 'commercial demonstration systems', 'demonstration systems processing', 'systems processing pathology', 'processing pathology reports', 'pathology reports .'] 

 TOTAL TRIGRAMS --> 55 



 ---- NOUN PHRASES ---- 

 ['software', 'tuning', 'significant enhancement', 'useful productive purpose', 'high accuracy', 'particular client.', 'accuracy', 'software', 'open source', 'commercial demonstration', 'pathology'] 

 TOTAL NOUN PHRASES --> 11 



 ---- NER ----

 
 ORGANIZATION ---> ['Current Practices Clinical Natural Language']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> ['Jon Patrick', 'Clinical NLP']
 TOTAL PERSON ENTITY --> 2 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['defici', 'current', 'practic', 'clinic', 'natur', 'languag', 'process', '(', 'cnlp', ')', '–', 'white', 'paper', '“', 'virtual', 'cnlp', 'softwar', 'fit', 'purpos', 'out-of-the-box', 'invari', 'requir', 'tune', ',', 'signific', 'enhanc', ',', 'serv', 'use', 'product', 'purpos', 'high', 'accuraci', 'particular', 'client.', '”', 'jon', 'patrick', '2019', 'recent', 'assess', 'accuraci', 'clinic', 'nlp', 'softwar', 'avail', 'either', 'open', 'sourc', 'project', 'commerci', 'demonstr', 'system', 'process', 'patholog', 'report', '.']

 TOTAL PORTER STEM WORDS ==> 57



 ---- SNOWBALL STEMMING ----

['defici', 'current', 'practic', 'clinic', 'natur', 'languag', 'process', '(', 'cnlp', ')', '–', 'white', 'paper', '“', 'virtual', 'cnlp', 'softwar', 'fit', 'purpos', 'out-of-the-box', 'invari', 'requir', 'tune', ',', 'signific', 'enhanc', ',', 'serv', 'use', 'product', 'purpos', 'high', 'accuraci', 'particular', 'client.', '”', 'jon', 'patrick', '2019', 'recent', 'assess', 'accuraci', 'clinic', 'nlp', 'softwar', 'avail', 'either', 'open', 'sourc', 'project', 'commerci', 'demonstr', 'system', 'process', 'patholog', 'report', '.']

 TOTAL SNOWBALL STEM WORDS ==> 57



 ---- LEMMATIZATION ----

['Deficiencies', 'Current', 'Practices', 'Clinical', 'Natural', 'Language', 'Processing', '(', 'CNLP', ')', '–', 'White', 'Paper', '“', 'Virtually', 'CNLP', 'software', 'fit', 'purpose', 'out-of-the-box', 'invariably', 'require', 'tuning', ',', 'significant', 'enhancement', ',', 'serve', 'useful', 'productive', 'purpose', 'high', 'accuracy', 'particular', 'client.', '”', 'Jon', 'Patrick', '2019', 'recently', 'assessed', 'accuracy', 'Clinical', 'NLP', 'software', 'available', 'either', 'open', 'source', 'project', 'commercial', 'demonstration', 'system', 'processing', 'pathology', 'report', '.']

 TOTAL LEMMATIZE WORDS ==> 57

************************************************************************************************************************

2 --> This whitepaper discusses the twenty- eight deficiencies we observed in our testing of five different systems. 


 ---- TOKENS ----

 ['This', 'whitepaper', 'discusses', 'the', 'twenty-', 'eight', 'deficiencies', 'we', 'observed', 'in', 'our', 'testing', 'of', 'five', 'different', 'systems', '.'] 

 TOTAL TOKENS ==> 17

 ---- POST ----

 [('This', 'DT'), ('whitepaper', 'NN'), ('discusses', 'VBZ'), ('the', 'DT'), ('twenty-', 'JJ'), ('eight', 'CD'), ('deficiencies', 'NNS'), ('we', 'PRP'), ('observed', 'VBD'), ('in', 'IN'), ('our', 'PRP$'), ('testing', 'NN'), ('of', 'IN'), ('five', 'CD'), ('different', 'JJ'), ('systems', 'NNS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['whitepaper', 'discusses', 'twenty-', 'eight', 'deficiencies', 'observed', 'testing', 'five', 'different', 'systems', '.']

 TOTAL FILTERED TOKENS ==>  11

 ---- POST FOR FILTERED TOKENS ----

 [('whitepaper', 'JJ'), ('discusses', 'VBZ'), ('twenty-', 'JJ'), ('eight', 'CD'), ('deficiencies', 'NNS'), ('observed', 'VBD'), ('testing', 'VBG'), ('five', 'CD'), ('different', 'JJ'), ('systems', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['whitepaper discusses', 'discusses twenty-', 'twenty- eight', 'eight deficiencies', 'deficiencies observed', 'observed testing', 'testing five', 'five different', 'different systems', 'systems .'] 

 TOTAL BIGRAMS --> 10 



 ---- TRI-GRAMS ---- 

 ['whitepaper discusses twenty-', 'discusses twenty- eight', 'twenty- eight deficiencies', 'eight deficiencies observed', 'deficiencies observed testing', 'observed testing five', 'testing five different', 'five different systems', 'different systems .'] 

 TOTAL TRIGRAMS --> 9 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['whitepap', 'discuss', 'twenty-', 'eight', 'defici', 'observ', 'test', 'five', 'differ', 'system', '.']

 TOTAL PORTER STEM WORDS ==> 11



 ---- SNOWBALL STEMMING ----

['whitepap', 'discuss', 'twenty-', 'eight', 'defici', 'observ', 'test', 'five', 'differ', 'system', '.']

 TOTAL SNOWBALL STEM WORDS ==> 11



 ---- LEMMATIZATION ----

['whitepaper', 'discus', 'twenty-', 'eight', 'deficiency', 'observed', 'testing', 'five', 'different', 'system', '.']

 TOTAL LEMMATIZE WORDS ==> 11

************************************************************************************************************************

3 --> Our analysis is based on the need for industrial strength language  engineering that must cope with a greater variety of real-world problems  than that experienced by research solutions. 


 ---- TOKENS ----

 ['Our', 'analysis', 'is', 'based', 'on', 'the', 'need', 'for', 'industrial', 'strength', 'language', 'engineering', 'that', 'must', 'cope', 'with', 'a', 'greater', 'variety', 'of', 'real-world', 'problems', 'than', 'that', 'experienced', 'by', 'research', 'solutions', '.'] 

 TOTAL TOKENS ==> 29

 ---- POST ----

 [('Our', 'PRP$'), ('analysis', 'NN'), ('is', 'VBZ'), ('based', 'VBN'), ('on', 'IN'), ('the', 'DT'), ('need', 'NN'), ('for', 'IN'), ('industrial', 'JJ'), ('strength', 'NN'), ('language', 'NN'), ('engineering', 'NN'), ('that', 'WDT'), ('must', 'MD'), ('cope', 'VB'), ('with', 'IN'), ('a', 'DT'), ('greater', 'JJR'), ('variety', 'NN'), ('of', 'IN'), ('real-world', 'NN'), ('problems', 'NNS'), ('than', 'IN'), ('that', 'DT'), ('experienced', 'VBN'), ('by', 'IN'), ('research', 'NN'), ('solutions', 'NNS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['analysis', 'based', 'need', 'industrial', 'strength', 'language', 'engineering', 'must', 'cope', 'greater', 'variety', 'real-world', 'problems', 'experienced', 'research', 'solutions', '.']

 TOTAL FILTERED TOKENS ==>  17

 ---- POST FOR FILTERED TOKENS ----

 [('analysis', 'NN'), ('based', 'VBN'), ('need', 'VBP'), ('industrial', 'JJ'), ('strength', 'NN'), ('language', 'NN'), ('engineering', 'NN'), ('must', 'MD'), ('cope', 'VB'), ('greater', 'JJR'), ('variety', 'NN'), ('real-world', 'NN'), ('problems', 'NNS'), ('experienced', 'VBD'), ('research', 'NN'), ('solutions', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['analysis based', 'based need', 'need industrial', 'industrial strength', 'strength language', 'language engineering', 'engineering must', 'must cope', 'cope greater', 'greater variety', 'variety real-world', 'real-world problems', 'problems experienced', 'experienced research', 'research solutions', 'solutions .'] 

 TOTAL BIGRAMS --> 16 



 ---- TRI-GRAMS ---- 

 ['analysis based need', 'based need industrial', 'need industrial strength', 'industrial strength language', 'strength language engineering', 'language engineering must', 'engineering must cope', 'must cope greater', 'cope greater variety', 'greater variety real-world', 'variety real-world problems', 'real-world problems experienced', 'problems experienced research', 'experienced research solutions', 'research solutions .'] 

 TOTAL TRIGRAMS --> 15 



 ---- NOUN PHRASES ---- 

 ['analysis', 'industrial strength', 'language', 'engineering', 'variety', 'real-world', 'research'] 

 TOTAL NOUN PHRASES --> 7 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['analysi', 'base', 'need', 'industri', 'strength', 'languag', 'engin', 'must', 'cope', 'greater', 'varieti', 'real-world', 'problem', 'experienc', 'research', 'solut', '.']

 TOTAL PORTER STEM WORDS ==> 17



 ---- SNOWBALL STEMMING ----

['analysi', 'base', 'need', 'industri', 'strength', 'languag', 'engin', 'must', 'cope', 'greater', 'varieti', 'real-world', 'problem', 'experienc', 'research', 'solut', '.']

 TOTAL SNOWBALL STEM WORDS ==> 17



 ---- LEMMATIZATION ----

['analysis', 'based', 'need', 'industrial', 'strength', 'language', 'engineering', 'must', 'cope', 'greater', 'variety', 'real-world', 'problem', 'experienced', 'research', 'solution', '.']

 TOTAL LEMMATIZE WORDS ==> 17

************************************************************************************************************************

4 --> In a research setting, users  can tailor their data and pre-processing solutions to address the answer to  a very specific investigation question unlike real-world usage where there  is little, or no, control over input data. 


 ---- TOKENS ----

 ['In', 'a', 'research', 'setting', ',', 'users', 'can', 'tailor', 'their', 'data', 'and', 'pre-processing', 'solutions', 'to', 'address', 'the', 'answer', 'to', 'a', 'very', 'specific', 'investigation', 'question', 'unlike', 'real-world', 'usage', 'where', 'there', 'is', 'little', ',', 'or', 'no', ',', 'control', 'over', 'input', 'data', '.'] 

 TOTAL TOKENS ==> 39

 ---- POST ----

 [('In', 'IN'), ('a', 'DT'), ('research', 'NN'), ('setting', 'NN'), (',', ','), ('users', 'NNS'), ('can', 'MD'), ('tailor', 'VB'), ('their', 'PRP$'), ('data', 'NNS'), ('and', 'CC'), ('pre-processing', 'JJ'), ('solutions', 'NNS'), ('to', 'TO'), ('address', 'VB'), ('the', 'DT'), ('answer', 'NN'), ('to', 'TO'), ('a', 'DT'), ('very', 'RB'), ('specific', 'JJ'), ('investigation', 'NN'), ('question', 'NN'), ('unlike', 'IN'), ('real-world', 'NN'), ('usage', 'NN'), ('where', 'WRB'), ('there', 'EX'), ('is', 'VBZ'), ('little', 'JJ'), (',', ','), ('or', 'CC'), ('no', 'DT'), (',', ','), ('control', 'NN'), ('over', 'IN'), ('input', 'NN'), ('data', 'NNS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['research', 'setting', ',', 'users', 'tailor', 'data', 'pre-processing', 'solutions', 'address', 'answer', 'specific', 'investigation', 'question', 'unlike', 'real-world', 'usage', 'little', ',', ',', 'control', 'input', 'data', '.']

 TOTAL FILTERED TOKENS ==>  23

 ---- POST FOR FILTERED TOKENS ----

 [('research', 'NN'), ('setting', 'NN'), (',', ','), ('users', 'NNS'), ('tailor', 'VBP'), ('data', 'NNS'), ('pre-processing', 'JJ'), ('solutions', 'NNS'), ('address', 'NN'), ('answer', 'NN'), ('specific', 'JJ'), ('investigation', 'NN'), ('question', 'NN'), ('unlike', 'IN'), ('real-world', 'JJ'), ('usage', 'NN'), ('little', 'JJ'), (',', ','), (',', ','), ('control', 'NN'), ('input', 'NN'), ('data', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['research setting', 'setting ,', ', users', 'users tailor', 'tailor data', 'data pre-processing', 'pre-processing solutions', 'solutions address', 'address answer', 'answer specific', 'specific investigation', 'investigation question', 'question unlike', 'unlike real-world', 'real-world usage', 'usage little', 'little ,', ', ,', ', control', 'control input', 'input data', 'data .'] 

 TOTAL BIGRAMS --> 22 



 ---- TRI-GRAMS ---- 

 ['research setting ,', 'setting , users', ', users tailor', 'users tailor data', 'tailor data pre-processing', 'data pre-processing solutions', 'pre-processing solutions address', 'solutions address answer', 'address answer specific', 'answer specific investigation', 'specific investigation question', 'investigation question unlike', 'question unlike real-world', 'unlike real-world usage', 'real-world usage little', 'usage little ,', 'little , ,', ', , control', ', control input', 'control input data', 'input data .'] 

 TOTAL TRIGRAMS --> 21 



 ---- NOUN PHRASES ---- 

 ['research', 'setting', 'address', 'answer', 'specific investigation', 'question', 'real-world usage', 'control', 'input'] 

 TOTAL NOUN PHRASES --> 9 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['research', 'set', ',', 'user', 'tailor', 'data', 'pre-process', 'solut', 'address', 'answer', 'specif', 'investig', 'question', 'unlik', 'real-world', 'usag', 'littl', ',', ',', 'control', 'input', 'data', '.']

 TOTAL PORTER STEM WORDS ==> 23



 ---- SNOWBALL STEMMING ----

['research', 'set', ',', 'user', 'tailor', 'data', 'pre-process', 'solut', 'address', 'answer', 'specif', 'investig', 'question', 'unlik', 'real-world', 'usag', 'littl', ',', ',', 'control', 'input', 'data', '.']

 TOTAL SNOWBALL STEM WORDS ==> 23



 ---- LEMMATIZATION ----

['research', 'setting', ',', 'user', 'tailor', 'data', 'pre-processing', 'solution', 'address', 'answer', 'specific', 'investigation', 'question', 'unlike', 'real-world', 'usage', 'little', ',', ',', 'control', 'input', 'data', '.']

 TOTAL LEMMATIZE WORDS ==> 23

************************************************************************************************************************

5 --> As a simple example, in a language  engineering application the data could be delivered in a standard  messaging format, say HL7, that has to be processed no matter what  vagaries it embodies. 


 ---- TOKENS ----

 ['As', 'a', 'simple', 'example', ',', 'in', 'a', 'language', 'engineering', 'application', 'the', 'data', 'could', 'be', 'delivered', 'in', 'a', 'standard', 'messaging', 'format', ',', 'say', 'HL7', ',', 'that', 'has', 'to', 'be', 'processed', 'no', 'matter', 'what', 'vagaries', 'it', 'embodies', '.'] 

 TOTAL TOKENS ==> 36

 ---- POST ----

 [('As', 'IN'), ('a', 'DT'), ('simple', 'JJ'), ('example', 'NN'), (',', ','), ('in', 'IN'), ('a', 'DT'), ('language', 'NN'), ('engineering', 'NN'), ('application', 'NN'), ('the', 'DT'), ('data', 'NN'), ('could', 'MD'), ('be', 'VB'), ('delivered', 'VBN'), ('in', 'IN'), ('a', 'DT'), ('standard', 'JJ'), ('messaging', 'NN'), ('format', 'NN'), (',', ','), ('say', 'VBP'), ('HL7', 'NNP'), (',', ','), ('that', 'WDT'), ('has', 'VBZ'), ('to', 'TO'), ('be', 'VB'), ('processed', 'VBN'), ('no', 'DT'), ('matter', 'NN'), ('what', 'WP'), ('vagaries', 'VBZ'), ('it', 'PRP'), ('embodies', 'VBZ'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['simple', 'example', ',', 'language', 'engineering', 'application', 'data', 'could', 'delivered', 'standard', 'messaging', 'format', ',', 'say', 'HL7', ',', 'processed', 'matter', 'vagaries', 'embodies', '.']

 TOTAL FILTERED TOKENS ==>  21

 ---- POST FOR FILTERED TOKENS ----

 [('simple', 'JJ'), ('example', 'NN'), (',', ','), ('language', 'NN'), ('engineering', 'NN'), ('application', 'NN'), ('data', 'NNS'), ('could', 'MD'), ('delivered', 'VB'), ('standard', 'JJ'), ('messaging', 'VBG'), ('format', 'NN'), (',', ','), ('say', 'VBP'), ('HL7', 'NNP'), (',', ','), ('processed', 'VBD'), ('matter', 'NN'), ('vagaries', 'NNS'), ('embodies', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['simple example', 'example ,', ', language', 'language engineering', 'engineering application', 'application data', 'data could', 'could delivered', 'delivered standard', 'standard messaging', 'messaging format', 'format ,', ', say', 'say HL7', 'HL7 ,', ', processed', 'processed matter', 'matter vagaries', 'vagaries embodies', 'embodies .'] 

 TOTAL BIGRAMS --> 20 



 ---- TRI-GRAMS ---- 

 ['simple example ,', 'example , language', ', language engineering', 'language engineering application', 'engineering application data', 'application data could', 'data could delivered', 'could delivered standard', 'delivered standard messaging', 'standard messaging format', 'messaging format ,', 'format , say', ', say HL7', 'say HL7 ,', 'HL7 , processed', ', processed matter', 'processed matter vagaries', 'matter vagaries embodies', 'vagaries embodies .'] 

 TOTAL TRIGRAMS --> 19 



 ---- NOUN PHRASES ---- 

 ['simple example', 'language', 'engineering', 'application', 'format', 'matter'] 

 TOTAL NOUN PHRASES --> 6 



 ---- NER ----

 
 ORGANIZATION ---> ['HL7']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['simpl', 'exampl', ',', 'languag', 'engin', 'applic', 'data', 'could', 'deliv', 'standard', 'messag', 'format', ',', 'say', 'hl7', ',', 'process', 'matter', 'vagari', 'embodi', '.']

 TOTAL PORTER STEM WORDS ==> 21



 ---- SNOWBALL STEMMING ----

['simpl', 'exampl', ',', 'languag', 'engin', 'applic', 'data', 'could', 'deliv', 'standard', 'messag', 'format', ',', 'say', 'hl7', ',', 'process', 'matter', 'vagari', 'embodi', '.']

 TOTAL SNOWBALL STEM WORDS ==> 21



 ---- LEMMATIZATION ----

['simple', 'example', ',', 'language', 'engineering', 'application', 'data', 'could', 'delivered', 'standard', 'messaging', 'format', ',', 'say', 'HL7', ',', 'processed', 'matter', 'vagary', 'embodies', '.']

 TOTAL LEMMATIZE WORDS ==> 21

************************************************************************************************************************

6 --> In a research project that data could be curated to  overcome the uncertainties created by this delivery mechanism by  removing the HL7 components before the CNLP processing was invoked, a  fix not available in a standard clinical setting. 


 ---- TOKENS ----

 ['In', 'a', 'research', 'project', 'that', 'data', 'could', 'be', 'curated', 'to', 'overcome', 'the', 'uncertainties', 'created', 'by', 'this', 'delivery', 'mechanism', 'by', 'removing', 'the', 'HL7', 'components', 'before', 'the', 'CNLP', 'processing', 'was', 'invoked', ',', 'a', 'fix', 'not', 'available', 'in', 'a', 'standard', 'clinical', 'setting', '.'] 

 TOTAL TOKENS ==> 40

 ---- POST ----

 [('In', 'IN'), ('a', 'DT'), ('research', 'NN'), ('project', 'NN'), ('that', 'IN'), ('data', 'NNS'), ('could', 'MD'), ('be', 'VB'), ('curated', 'VBN'), ('to', 'TO'), ('overcome', 'VB'), ('the', 'DT'), ('uncertainties', 'NNS'), ('created', 'VBN'), ('by', 'IN'), ('this', 'DT'), ('delivery', 'NN'), ('mechanism', 'NN'), ('by', 'IN'), ('removing', 'VBG'), ('the', 'DT'), ('HL7', 'NNP'), ('components', 'NNS'), ('before', 'IN'), ('the', 'DT'), ('CNLP', 'NNP'), ('processing', 'NN'), ('was', 'VBD'), ('invoked', 'VBN'), (',', ','), ('a', 'DT'), ('fix', 'NN'), ('not', 'RB'), ('available', 'JJ'), ('in', 'IN'), ('a', 'DT'), ('standard', 'JJ'), ('clinical', 'JJ'), ('setting', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['research', 'project', 'data', 'could', 'curated', 'overcome', 'uncertainties', 'created', 'delivery', 'mechanism', 'removing', 'HL7', 'components', 'CNLP', 'processing', 'invoked', ',', 'fix', 'available', 'standard', 'clinical', 'setting', '.']

 TOTAL FILTERED TOKENS ==>  23

 ---- POST FOR FILTERED TOKENS ----

 [('research', 'NN'), ('project', 'NN'), ('data', 'NNS'), ('could', 'MD'), ('curated', 'VB'), ('overcome', 'JJ'), ('uncertainties', 'NNS'), ('created', 'VBN'), ('delivery', 'NN'), ('mechanism', 'NN'), ('removing', 'VBG'), ('HL7', 'NNP'), ('components', 'NNS'), ('CNLP', 'NNP'), ('processing', 'NN'), ('invoked', 'VBD'), (',', ','), ('fix', 'RBR'), ('available', 'JJ'), ('standard', 'JJ'), ('clinical', 'JJ'), ('setting', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['research project', 'project data', 'data could', 'could curated', 'curated overcome', 'overcome uncertainties', 'uncertainties created', 'created delivery', 'delivery mechanism', 'mechanism removing', 'removing HL7', 'HL7 components', 'components CNLP', 'CNLP processing', 'processing invoked', 'invoked ,', ', fix', 'fix available', 'available standard', 'standard clinical', 'clinical setting', 'setting .'] 

 TOTAL BIGRAMS --> 22 



 ---- TRI-GRAMS ---- 

 ['research project data', 'project data could', 'data could curated', 'could curated overcome', 'curated overcome uncertainties', 'overcome uncertainties created', 'uncertainties created delivery', 'created delivery mechanism', 'delivery mechanism removing', 'mechanism removing HL7', 'removing HL7 components', 'HL7 components CNLP', 'components CNLP processing', 'CNLP processing invoked', 'processing invoked ,', 'invoked , fix', ', fix available', 'fix available standard', 'available standard clinical', 'standard clinical setting', 'clinical setting .'] 

 TOTAL TRIGRAMS --> 21 



 ---- NOUN PHRASES ---- 

 ['research', 'project', 'delivery', 'mechanism', 'processing', 'available standard clinical setting'] 

 TOTAL NOUN PHRASES --> 6 



 ---- NER ----

 
 ORGANIZATION ---> ['HL7', 'CNLP']
 TOTAL ORGANIZATION ENTITY --> 2 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['research', 'project', 'data', 'could', 'curat', 'overcom', 'uncertainti', 'creat', 'deliveri', 'mechan', 'remov', 'hl7', 'compon', 'cnlp', 'process', 'invok', ',', 'fix', 'avail', 'standard', 'clinic', 'set', '.']

 TOTAL PORTER STEM WORDS ==> 23



 ---- SNOWBALL STEMMING ----

['research', 'project', 'data', 'could', 'curat', 'overcom', 'uncertainti', 'creat', 'deliveri', 'mechan', 'remov', 'hl7', 'compon', 'cnlp', 'process', 'invok', ',', 'fix', 'avail', 'standard', 'clinic', 'set', '.']

 TOTAL SNOWBALL STEM WORDS ==> 23



 ---- LEMMATIZATION ----

['research', 'project', 'data', 'could', 'curated', 'overcome', 'uncertainty', 'created', 'delivery', 'mechanism', 'removing', 'HL7', 'component', 'CNLP', 'processing', 'invoked', ',', 'fix', 'available', 'standard', 'clinical', 'setting', '.']

 TOTAL LEMMATIZE WORDS ==> 23

************************************************************************************************************************

7 --> When an organisation is intending to apply a CNLP system to their data  the topics discussed in this document need to be assessed for their  potential impact on their desired outcomes. 


 ---- TOKENS ----

 ['When', 'an', 'organisation', 'is', 'intending', 'to', 'apply', 'a', 'CNLP', 'system', 'to', 'their', 'data', 'the', 'topics', 'discussed', 'in', 'this', 'document', 'need', 'to', 'be', 'assessed', 'for', 'their', 'potential', 'impact', 'on', 'their', 'desired', 'outcomes', '.'] 

 TOTAL TOKENS ==> 32

 ---- POST ----

 [('When', 'WRB'), ('an', 'DT'), ('organisation', 'NN'), ('is', 'VBZ'), ('intending', 'VBG'), ('to', 'TO'), ('apply', 'VB'), ('a', 'DT'), ('CNLP', 'NNP'), ('system', 'NN'), ('to', 'TO'), ('their', 'PRP$'), ('data', 'NNS'), ('the', 'DT'), ('topics', 'NNS'), ('discussed', 'VBN'), ('in', 'IN'), ('this', 'DT'), ('document', 'NN'), ('need', 'NN'), ('to', 'TO'), ('be', 'VB'), ('assessed', 'VBN'), ('for', 'IN'), ('their', 'PRP$'), ('potential', 'JJ'), ('impact', 'NN'), ('on', 'IN'), ('their', 'PRP$'), ('desired', 'VBN'), ('outcomes', 'NNS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['organisation', 'intending', 'apply', 'CNLP', 'system', 'data', 'topics', 'discussed', 'document', 'need', 'assessed', 'potential', 'impact', 'desired', 'outcomes', '.']

 TOTAL FILTERED TOKENS ==>  16

 ---- POST FOR FILTERED TOKENS ----

 [('organisation', 'NN'), ('intending', 'VBG'), ('apply', 'VB'), ('CNLP', 'NNP'), ('system', 'NN'), ('data', 'NNS'), ('topics', 'NNS'), ('discussed', 'VBD'), ('document', 'NN'), ('need', 'NN'), ('assessed', 'VBD'), ('potential', 'JJ'), ('impact', 'NN'), ('desired', 'VBN'), ('outcomes', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['organisation intending', 'intending apply', 'apply CNLP', 'CNLP system', 'system data', 'data topics', 'topics discussed', 'discussed document', 'document need', 'need assessed', 'assessed potential', 'potential impact', 'impact desired', 'desired outcomes', 'outcomes .'] 

 TOTAL BIGRAMS --> 15 



 ---- TRI-GRAMS ---- 

 ['organisation intending apply', 'intending apply CNLP', 'apply CNLP system', 'CNLP system data', 'system data topics', 'data topics discussed', 'topics discussed document', 'discussed document need', 'document need assessed', 'need assessed potential', 'assessed potential impact', 'potential impact desired', 'impact desired outcomes', 'desired outcomes .'] 

 TOTAL TRIGRAMS --> 14 



 ---- NOUN PHRASES ---- 

 ['organisation', 'system', 'document', 'need', 'potential impact'] 

 TOTAL NOUN PHRASES --> 5 



 ---- NER ----

 
 ORGANIZATION ---> ['CNLP']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['organis', 'intend', 'appli', 'cnlp', 'system', 'data', 'topic', 'discuss', 'document', 'need', 'assess', 'potenti', 'impact', 'desir', 'outcom', '.']

 TOTAL PORTER STEM WORDS ==> 16



 ---- SNOWBALL STEMMING ----

['organis', 'intend', 'appli', 'cnlp', 'system', 'data', 'topic', 'discuss', 'document', 'need', 'assess', 'potenti', 'impact', 'desir', 'outcom', '.']

 TOTAL SNOWBALL STEM WORDS ==> 16



 ---- LEMMATIZATION ----

['organisation', 'intending', 'apply', 'CNLP', 'system', 'data', 'topic', 'discussed', 'document', 'need', 'assessed', 'potential', 'impact', 'desired', 'outcome', '.']

 TOTAL LEMMATIZE WORDS ==> 16

************************************************************************************************************************

8 --> The evaluations were based on two key principles:  • There is a primary function to be performed by CNLP, that is,  Clinical Entity Recognition (CER). 


 ---- TOKENS ----

 ['The', 'evaluations', 'were', 'based', 'on', 'two', 'key', 'principles', ':', '•', 'There', 'is', 'a', 'primary', 'function', 'to', 'be', 'performed', 'by', 'CNLP', ',', 'that', 'is', ',', 'Clinical', 'Entity', 'Recognition', '(', 'CER', ')', '.'] 

 TOTAL TOKENS ==> 31

 ---- POST ----

 [('The', 'DT'), ('evaluations', 'NNS'), ('were', 'VBD'), ('based', 'VBN'), ('on', 'IN'), ('two', 'CD'), ('key', 'JJ'), ('principles', 'NNS'), (':', ':'), ('•', 'NN'), ('There', 'EX'), ('is', 'VBZ'), ('a', 'DT'), ('primary', 'JJ'), ('function', 'NN'), ('to', 'TO'), ('be', 'VB'), ('performed', 'VBN'), ('by', 'IN'), ('CNLP', 'NNP'), (',', ','), ('that', 'WDT'), ('is', 'VBZ'), (',', ','), ('Clinical', 'JJ'), ('Entity', 'NNP'), ('Recognition', 'NNP'), ('(', '('), ('CER', 'NNP'), (')', ')'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['evaluations', 'based', 'two', 'key', 'principles', ':', '•', 'primary', 'function', 'performed', 'CNLP', ',', ',', 'Clinical', 'Entity', 'Recognition', '(', 'CER', ')', '.']

 TOTAL FILTERED TOKENS ==>  20

 ---- POST FOR FILTERED TOKENS ----

 [('evaluations', 'NNS'), ('based', 'VBN'), ('two', 'CD'), ('key', 'JJ'), ('principles', 'NNS'), (':', ':'), ('•', 'JJ'), ('primary', 'JJ'), ('function', 'NN'), ('performed', 'VBD'), ('CNLP', 'NNP'), (',', ','), (',', ','), ('Clinical', 'NNP'), ('Entity', 'NNP'), ('Recognition', 'NNP'), ('(', '('), ('CER', 'NNP'), (')', ')'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['evaluations based', 'based two', 'two key', 'key principles', 'principles :', ': •', '• primary', 'primary function', 'function performed', 'performed CNLP', 'CNLP ,', ', ,', ', Clinical', 'Clinical Entity', 'Entity Recognition', 'Recognition (', '( CER', 'CER )', ') .'] 

 TOTAL BIGRAMS --> 19 



 ---- TRI-GRAMS ---- 

 ['evaluations based two', 'based two key', 'two key principles', 'key principles :', 'principles : •', ': • primary', '• primary function', 'primary function performed', 'function performed CNLP', 'performed CNLP ,', 'CNLP , ,', ', , Clinical', ', Clinical Entity', 'Clinical Entity Recognition', 'Entity Recognition (', 'Recognition ( CER', '( CER )', 'CER ) .'] 

 TOTAL TRIGRAMS --> 18 



 ---- NOUN PHRASES ---- 

 ['• primary function'] 

 TOTAL NOUN PHRASES --> 1 



 ---- NER ----

 
 ORGANIZATION ---> ['CNLP']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> ['Clinical Entity']
 TOTAL PERSON ENTITY --> 1 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['evalu', 'base', 'two', 'key', 'principl', ':', '•', 'primari', 'function', 'perform', 'cnlp', ',', ',', 'clinic', 'entiti', 'recognit', '(', 'cer', ')', '.']

 TOTAL PORTER STEM WORDS ==> 20



 ---- SNOWBALL STEMMING ----

['evalu', 'base', 'two', 'key', 'principl', ':', '•', 'primari', 'function', 'perform', 'cnlp', ',', ',', 'clinic', 'entiti', 'recognit', '(', 'cer', ')', '.']

 TOTAL SNOWBALL STEM WORDS ==> 20



 ---- LEMMATIZATION ----

['evaluation', 'based', 'two', 'key', 'principle', ':', '•', 'primary', 'function', 'performed', 'CNLP', ',', ',', 'Clinical', 'Entity', 'Recognition', '(', 'CER', ')', '.']

 TOTAL LEMMATIZE WORDS ==> 20

************************************************************************************************************************

9 --> • There is one secondary function and that is Relationship  Identification. 


 ---- TOKENS ----

 ['•', 'There', 'is', 'one', 'secondary', 'function', 'and', 'that', 'is', 'Relationship', 'Identification', '.'] 

 TOTAL TOKENS ==> 12

 ---- POST ----

 [('•', 'NN'), ('There', 'EX'), ('is', 'VBZ'), ('one', 'CD'), ('secondary', 'JJ'), ('function', 'NN'), ('and', 'CC'), ('that', 'DT'), ('is', 'VBZ'), ('Relationship', 'JJ'), ('Identification', 'NNP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['•', 'one', 'secondary', 'function', 'Relationship', 'Identification', '.']

 TOTAL FILTERED TOKENS ==>  7

 ---- POST FOR FILTERED TOKENS ----

 [('•', 'VB'), ('one', 'CD'), ('secondary', 'JJ'), ('function', 'NN'), ('Relationship', 'NNP'), ('Identification', 'NNP'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['• one', 'one secondary', 'secondary function', 'function Relationship', 'Relationship Identification', 'Identification .'] 

 TOTAL BIGRAMS --> 6 



 ---- TRI-GRAMS ---- 

 ['• one secondary', 'one secondary function', 'secondary function Relationship', 'function Relationship Identification', 'Relationship Identification .'] 

 TOTAL TRIGRAMS --> 5 



 ---- NOUN PHRASES ---- 

 ['secondary function'] 

 TOTAL NOUN PHRASES --> 1 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> ['Relationship Identification']
 TOTAL PERSON ENTITY --> 1 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['•', 'one', 'secondari', 'function', 'relationship', 'identif', '.']

 TOTAL PORTER STEM WORDS ==> 7



 ---- SNOWBALL STEMMING ----

['•', 'one', 'secondari', 'function', 'relationship', 'identif', '.']

 TOTAL SNOWBALL STEM WORDS ==> 7



 ---- LEMMATIZATION ----

['•', 'one', 'secondary', 'function', 'Relationship', 'Identification', '.']

 TOTAL LEMMATIZE WORDS ==> 7

************************************************************************************************************************

10 --> Any other clinical NLP processing will rely on one or both of these primary  functions. 


 ---- TOKENS ----

 ['Any', 'other', 'clinical', 'NLP', 'processing', 'will', 'rely', 'on', 'one', 'or', 'both', 'of', 'these', 'primary', 'functions', '.'] 

 TOTAL TOKENS ==> 16

 ---- POST ----

 [('Any', 'DT'), ('other', 'JJ'), ('clinical', 'JJ'), ('NLP', 'NNP'), ('processing', 'NN'), ('will', 'MD'), ('rely', 'VB'), ('on', 'IN'), ('one', 'CD'), ('or', 'CC'), ('both', 'DT'), ('of', 'IN'), ('these', 'DT'), ('primary', 'JJ'), ('functions', 'NNS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['clinical', 'NLP', 'processing', 'rely', 'one', 'primary', 'functions', '.']

 TOTAL FILTERED TOKENS ==>  8

 ---- POST FOR FILTERED TOKENS ----

 [('clinical', 'JJ'), ('NLP', 'NNP'), ('processing', 'NN'), ('rely', 'RB'), ('one', 'CD'), ('primary', 'JJ'), ('functions', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['clinical NLP', 'NLP processing', 'processing rely', 'rely one', 'one primary', 'primary functions', 'functions .'] 

 TOTAL BIGRAMS --> 7 



 ---- TRI-GRAMS ---- 

 ['clinical NLP processing', 'NLP processing rely', 'processing rely one', 'rely one primary', 'one primary functions', 'primary functions .'] 

 TOTAL TRIGRAMS --> 6 



 ---- NOUN PHRASES ---- 

 ['processing'] 

 TOTAL NOUN PHRASES --> 1 



 ---- NER ----

 
 ORGANIZATION ---> ['NLP']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['clinic', 'nlp', 'process', 'reli', 'one', 'primari', 'function', '.']

 TOTAL PORTER STEM WORDS ==> 8



 ---- SNOWBALL STEMMING ----

['clinic', 'nlp', 'process', 'reli', 'one', 'primari', 'function', '.']

 TOTAL SNOWBALL STEM WORDS ==> 8



 ---- LEMMATIZATION ----

['clinical', 'NLP', 'processing', 'rely', 'one', 'primary', 'function', '.']

 TOTAL LEMMATIZE WORDS ==> 8

************************************************************************************************************************

11 --> For the purposes of this conversation we exclude “text mining”  which uses a “bag-of-words” approach to language analysis and is  woefully inadequate in a clinical setting. 


 ---- TOKENS ----

 ['For', 'the', 'purposes', 'of', 'this', 'conversation', 'we', 'exclude', '“', 'text', 'mining', '”', 'which', 'uses', 'a', '“', 'bag-of-words', '”', 'approach', 'to', 'language', 'analysis', 'and', 'is', 'woefully', 'inadequate', 'in', 'a', 'clinical', 'setting', '.'] 

 TOTAL TOKENS ==> 31

 ---- POST ----

 [('For', 'IN'), ('the', 'DT'), ('purposes', 'NNS'), ('of', 'IN'), ('this', 'DT'), ('conversation', 'NN'), ('we', 'PRP'), ('exclude', 'VBP'), ('“', 'JJ'), ('text', 'NN'), ('mining', 'NN'), ('”', 'NN'), ('which', 'WDT'), ('uses', 'VBZ'), ('a', 'DT'), ('“', 'JJ'), ('bag-of-words', 'NNS'), ('”', 'JJ'), ('approach', 'NN'), ('to', 'TO'), ('language', 'NN'), ('analysis', 'NN'), ('and', 'CC'), ('is', 'VBZ'), ('woefully', 'RB'), ('inadequate', 'JJ'), ('in', 'IN'), ('a', 'DT'), ('clinical', 'JJ'), ('setting', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['purposes', 'conversation', 'exclude', '“', 'text', 'mining', '”', 'uses', '“', 'bag-of-words', '”', 'approach', 'language', 'analysis', 'woefully', 'inadequate', 'clinical', 'setting', '.']

 TOTAL FILTERED TOKENS ==>  19

 ---- POST FOR FILTERED TOKENS ----

 [('purposes', 'NNS'), ('conversation', 'NN'), ('exclude', 'VBP'), ('“', 'NNP'), ('text', 'NN'), ('mining', 'NN'), ('”', 'JJ'), ('uses', 'VBZ'), ('“', 'JJ'), ('bag-of-words', 'NNS'), ('”', 'JJ'), ('approach', 'NN'), ('language', 'NN'), ('analysis', 'NN'), ('woefully', 'RB'), ('inadequate', 'JJ'), ('clinical', 'JJ'), ('setting', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['purposes conversation', 'conversation exclude', 'exclude “', '“ text', 'text mining', 'mining ”', '” uses', 'uses “', '“ bag-of-words', 'bag-of-words ”', '” approach', 'approach language', 'language analysis', 'analysis woefully', 'woefully inadequate', 'inadequate clinical', 'clinical setting', 'setting .'] 

 TOTAL BIGRAMS --> 18 



 ---- TRI-GRAMS ---- 

 ['purposes conversation exclude', 'conversation exclude “', 'exclude “ text', '“ text mining', 'text mining ”', 'mining ” uses', '” uses “', 'uses “ bag-of-words', '“ bag-of-words ”', 'bag-of-words ” approach', '” approach language', 'approach language analysis', 'language analysis woefully', 'analysis woefully inadequate', 'woefully inadequate clinical', 'inadequate clinical setting', 'clinical setting .'] 

 TOTAL TRIGRAMS --> 17 



 ---- NOUN PHRASES ---- 

 ['conversation', 'text', 'mining', '” approach', 'language', 'analysis', 'inadequate clinical setting'] 

 TOTAL NOUN PHRASES --> 7 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['purpos', 'convers', 'exclud', '“', 'text', 'mine', '”', 'use', '“', 'bag-of-word', '”', 'approach', 'languag', 'analysi', 'woefulli', 'inadequ', 'clinic', 'set', '.']

 TOTAL PORTER STEM WORDS ==> 19



 ---- SNOWBALL STEMMING ----

['purpos', 'convers', 'exclud', '“', 'text', 'mine', '”', 'use', '“', 'bag-of-word', '”', 'approach', 'languag', 'analysi', 'woefulli', 'inadequ', 'clinic', 'set', '.']

 TOTAL SNOWBALL STEM WORDS ==> 19



 ---- LEMMATIZATION ----

['purpose', 'conversation', 'exclude', '“', 'text', 'mining', '”', 'us', '“', 'bag-of-words', '”', 'approach', 'language', 'analysis', 'woefully', 'inadequate', 'clinical', 'setting', '.']

 TOTAL LEMMATIZE WORDS ==> 19

************************************************************************************************************************

12 --> Assessed Software:   • Amazon Concept Medical  • Stanford NLP + Metathesaurus  • OPenNLP + Metathesaurus  • GATE + Metathesaurus  • cTAKES  The systems have the listed deficiencies to a greater or lesser extent. 


 ---- TOKENS ----

 ['Assessed', 'Software', ':', '•', 'Amazon', 'Concept', 'Medical', '•', 'Stanford', 'NLP', '+', 'Metathesaurus', '•', 'OPenNLP', '+', 'Metathesaurus', '•', 'GATE', '+', 'Metathesaurus', '•', 'cTAKES', 'The', 'systems', 'have', 'the', 'listed', 'deficiencies', 'to', 'a', 'greater', 'or', 'lesser', 'extent', '.'] 

 TOTAL TOKENS ==> 35

 ---- POST ----

 [('Assessed', 'VBN'), ('Software', 'NN'), (':', ':'), ('•', 'NN'), ('Amazon', 'NNP'), ('Concept', 'NNP'), ('Medical', 'NNP'), ('•', 'NNP'), ('Stanford', 'NNP'), ('NLP', 'NNP'), ('+', 'NNP'), ('Metathesaurus', 'NNP'), ('•', 'NNP'), ('OPenNLP', 'NNP'), ('+', 'NNP'), ('Metathesaurus', 'NNP'), ('•', 'NNP'), ('GATE', 'NNP'), ('+', 'NNP'), ('Metathesaurus', 'NNP'), ('•', 'NNP'), ('cTAKES', 'VBD'), ('The', 'DT'), ('systems', 'NNS'), ('have', 'VBP'), ('the', 'DT'), ('listed', 'VBN'), ('deficiencies', 'NNS'), ('to', 'TO'), ('a', 'DT'), ('greater', 'JJR'), ('or', 'CC'), ('lesser', 'JJR'), ('extent', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Assessed', 'Software', ':', '•', 'Amazon', 'Concept', 'Medical', '•', 'Stanford', 'NLP', '+', 'Metathesaurus', '•', 'OPenNLP', '+', 'Metathesaurus', '•', 'GATE', '+', 'Metathesaurus', '•', 'cTAKES', 'systems', 'listed', 'deficiencies', 'greater', 'lesser', 'extent', '.']

 TOTAL FILTERED TOKENS ==>  29

 ---- POST FOR FILTERED TOKENS ----

 [('Assessed', 'VBN'), ('Software', 'NN'), (':', ':'), ('•', 'NN'), ('Amazon', 'NNP'), ('Concept', 'NNP'), ('Medical', 'NNP'), ('•', 'NNP'), ('Stanford', 'NNP'), ('NLP', 'NNP'), ('+', 'NNP'), ('Metathesaurus', 'NNP'), ('•', 'NNP'), ('OPenNLP', 'NNP'), ('+', 'NNP'), ('Metathesaurus', 'NNP'), ('•', 'NNP'), ('GATE', 'NNP'), ('+', 'NNP'), ('Metathesaurus', 'NNP'), ('•', 'NNP'), ('cTAKES', 'NN'), ('systems', 'NNS'), ('listed', 'VBN'), ('deficiencies', 'NNS'), ('greater', 'JJR'), ('lesser', 'JJR'), ('extent', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Assessed Software', 'Software :', ': •', '• Amazon', 'Amazon Concept', 'Concept Medical', 'Medical •', '• Stanford', 'Stanford NLP', 'NLP +', '+ Metathesaurus', 'Metathesaurus •', '• OPenNLP', 'OPenNLP +', '+ Metathesaurus', 'Metathesaurus •', '• GATE', 'GATE +', '+ Metathesaurus', 'Metathesaurus •', '• cTAKES', 'cTAKES systems', 'systems listed', 'listed deficiencies', 'deficiencies greater', 'greater lesser', 'lesser extent', 'extent .'] 

 TOTAL BIGRAMS --> 28 



 ---- TRI-GRAMS ---- 

 ['Assessed Software :', 'Software : •', ': • Amazon', '• Amazon Concept', 'Amazon Concept Medical', 'Concept Medical •', 'Medical • Stanford', '• Stanford NLP', 'Stanford NLP +', 'NLP + Metathesaurus', '+ Metathesaurus •', 'Metathesaurus • OPenNLP', '• OPenNLP +', 'OPenNLP + Metathesaurus', '+ Metathesaurus •', 'Metathesaurus • GATE', '• GATE +', 'GATE + Metathesaurus', '+ Metathesaurus •', 'Metathesaurus • cTAKES', '• cTAKES systems', 'cTAKES systems listed', 'systems listed deficiencies', 'listed deficiencies greater', 'deficiencies greater lesser', 'greater lesser extent', 'lesser extent .'] 

 TOTAL TRIGRAMS --> 27 



 ---- NOUN PHRASES ---- 

 ['Software', '•', 'cTAKES', 'extent'] 

 TOTAL NOUN PHRASES --> 4 



 ---- NER ----

 
 ORGANIZATION ---> ['Metathesaurus', 'Metathesaurus', 'GATE', 'Metathesaurus']
 TOTAL ORGANIZATION ENTITY --> 4 


 PERSON ---> ['Software', 'Amazon Concept Medical']
 TOTAL PERSON ENTITY --> 2 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['assess', 'softwar', ':', '•', 'amazon', 'concept', 'medic', '•', 'stanford', 'nlp', '+', 'metathesauru', '•', 'opennlp', '+', 'metathesauru', '•', 'gate', '+', 'metathesauru', '•', 'ctake', 'system', 'list', 'defici', 'greater', 'lesser', 'extent', '.']

 TOTAL PORTER STEM WORDS ==> 29



 ---- SNOWBALL STEMMING ----

['assess', 'softwar', ':', '•', 'amazon', 'concept', 'medic', '•', 'stanford', 'nlp', '+', 'metathesaurus', '•', 'opennlp', '+', 'metathesaurus', '•', 'gate', '+', 'metathesaurus', '•', 'ctake', 'system', 'list', 'defici', 'greater', 'lesser', 'extent', '.']

 TOTAL SNOWBALL STEM WORDS ==> 29



 ---- LEMMATIZATION ----

['Assessed', 'Software', ':', '•', 'Amazon', 'Concept', 'Medical', '•', 'Stanford', 'NLP', '+', 'Metathesaurus', '•', 'OPenNLP', '+', 'Metathesaurus', '•', 'GATE', '+', 'Metathesaurus', '•', 'cTAKES', 'system', 'listed', 'deficiency', 'greater', 'lesser', 'extent', '.']

 TOTAL LEMMATIZE WORDS ==> 29

************************************************************************************************************************

13 --> No  system has all these problems. 


 ---- TOKENS ----

 ['No', 'system', 'has', 'all', 'these', 'problems', '.'] 

 TOTAL TOKENS ==> 7

 ---- POST ----

 [('No', 'DT'), ('system', 'NN'), ('has', 'VBZ'), ('all', 'PDT'), ('these', 'DT'), ('problems', 'NNS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['system', 'problems', '.']

 TOTAL FILTERED TOKENS ==>  3

 ---- POST FOR FILTERED TOKENS ----

 [('system', 'NN'), ('problems', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['system problems', 'problems .'] 

 TOTAL BIGRAMS --> 2 



 ---- TRI-GRAMS ---- 

 ['system problems .'] 

 TOTAL TRIGRAMS --> 1 



 ---- NOUN PHRASES ---- 

 ['system'] 

 TOTAL NOUN PHRASES --> 1 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['system', 'problem', '.']

 TOTAL PORTER STEM WORDS ==> 3



 ---- SNOWBALL STEMMING ----

['system', 'problem', '.']

 TOTAL SNOWBALL STEM WORDS ==> 3



 ---- LEMMATIZATION ----

['system', 'problem', '.']

 TOTAL LEMMATIZE WORDS ==> 3

************************************************************************************************************************

14 --> The deficiencies discussed are compiled  across the 5 systems under the following headings:  • Deficiencies in Understanding Document Structure  • Deficiencies in Tokenisation  • Deficiencies in Grammatical Understanding  • Deficiencies in the CER Algorithms  • Deficiencies in Semantics and Interpreting Medical Terminology   Deficiencies in Understanding Document Structure   Missing Contextual Recognition  The first task for any system is to recognise the context of the text. 


 ---- TOKENS ----

 ['The', 'deficiencies', 'discussed', 'are', 'compiled', 'across', 'the', '5', 'systems', 'under', 'the', 'following', 'headings', ':', '•', 'Deficiencies', 'in', 'Understanding', 'Document', 'Structure', '•', 'Deficiencies', 'in', 'Tokenisation', '•', 'Deficiencies', 'in', 'Grammatical', 'Understanding', '•', 'Deficiencies', 'in', 'the', 'CER', 'Algorithms', '•', 'Deficiencies', 'in', 'Semantics', 'and', 'Interpreting', 'Medical', 'Terminology', 'Deficiencies', 'in', 'Understanding', 'Document', 'Structure', 'Missing', 'Contextual', 'Recognition', 'The', 'first', 'task', 'for', 'any', 'system', 'is', 'to', 'recognise', 'the', 'context', 'of', 'the', 'text', '.'] 

 TOTAL TOKENS ==> 66

 ---- POST ----

 [('The', 'DT'), ('deficiencies', 'NNS'), ('discussed', 'VBN'), ('are', 'VBP'), ('compiled', 'VBN'), ('across', 'IN'), ('the', 'DT'), ('5', 'CD'), ('systems', 'NNS'), ('under', 'IN'), ('the', 'DT'), ('following', 'JJ'), ('headings', 'NNS'), (':', ':'), ('•', 'NN'), ('Deficiencies', 'NNS'), ('in', 'IN'), ('Understanding', 'VBG'), ('Document', 'NNP'), ('Structure', 'NNP'), ('•', 'NNP'), ('Deficiencies', 'NNP'), ('in', 'IN'), ('Tokenisation', 'NNP'), ('•', 'NNP'), ('Deficiencies', 'NNP'), ('in', 'IN'), ('Grammatical', 'NNP'), ('Understanding', 'NNP'), ('•', 'NNP'), ('Deficiencies', 'NNP'), ('in', 'IN'), ('the', 'DT'), ('CER', 'NNP'), ('Algorithms', 'NNP'), ('•', 'NNP'), ('Deficiencies', 'NNP'), ('in', 'IN'), ('Semantics', 'NNP'), ('and', 'CC'), ('Interpreting', 'NNP'), ('Medical', 'NNP'), ('Terminology', 'NNP'), ('Deficiencies', 'NNP'), ('in', 'IN'), ('Understanding', 'NNP'), ('Document', 'NNP'), ('Structure', 'NNP'), ('Missing', 'NNP'), ('Contextual', 'NNP'), ('Recognition', 'NNP'), ('The', 'DT'), ('first', 'JJ'), ('task', 'NN'), ('for', 'IN'), ('any', 'DT'), ('system', 'NN'), ('is', 'VBZ'), ('to', 'TO'), ('recognise', 'VB'), ('the', 'DT'), ('context', 'NN'), ('of', 'IN'), ('the', 'DT'), ('text', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['deficiencies', 'discussed', 'compiled', 'across', '5', 'systems', 'following', 'headings', ':', '•', 'Deficiencies', 'Understanding', 'Document', 'Structure', '•', 'Deficiencies', 'Tokenisation', '•', 'Deficiencies', 'Grammatical', 'Understanding', '•', 'Deficiencies', 'CER', 'Algorithms', '•', 'Deficiencies', 'Semantics', 'Interpreting', 'Medical', 'Terminology', 'Deficiencies', 'Understanding', 'Document', 'Structure', 'Missing', 'Contextual', 'Recognition', 'first', 'task', 'system', 'recognise', 'context', 'text', '.']

 TOTAL FILTERED TOKENS ==>  45

 ---- POST FOR FILTERED TOKENS ----

 [('deficiencies', 'NNS'), ('discussed', 'VBD'), ('compiled', 'VBN'), ('across', 'IN'), ('5', 'CD'), ('systems', 'NNS'), ('following', 'VBG'), ('headings', 'NNS'), (':', ':'), ('•', 'NN'), ('Deficiencies', 'VBZ'), ('Understanding', 'NNP'), ('Document', 'NNP'), ('Structure', 'NNP'), ('•', 'NNP'), ('Deficiencies', 'NNP'), ('Tokenisation', 'NNP'), ('•', 'NNP'), ('Deficiencies', 'NNP'), ('Grammatical', 'NNP'), ('Understanding', 'NNP'), ('•', 'NNP'), ('Deficiencies', 'NNP'), ('CER', 'NNP'), ('Algorithms', 'NNP'), ('•', 'NNP'), ('Deficiencies', 'NNP'), ('Semantics', 'NNP'), ('Interpreting', 'NNP'), ('Medical', 'NNP'), ('Terminology', 'NNP'), ('Deficiencies', 'NNP'), ('Understanding', 'NNP'), ('Document', 'NNP'), ('Structure', 'NNP'), ('Missing', 'NNP'), ('Contextual', 'NNP'), ('Recognition', 'NNP'), ('first', 'RB'), ('task', 'NN'), ('system', 'NN'), ('recognise', 'VB'), ('context', 'JJ'), ('text', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['deficiencies discussed', 'discussed compiled', 'compiled across', 'across 5', '5 systems', 'systems following', 'following headings', 'headings :', ': •', '• Deficiencies', 'Deficiencies Understanding', 'Understanding Document', 'Document Structure', 'Structure •', '• Deficiencies', 'Deficiencies Tokenisation', 'Tokenisation •', '• Deficiencies', 'Deficiencies Grammatical', 'Grammatical Understanding', 'Understanding •', '• Deficiencies', 'Deficiencies CER', 'CER Algorithms', 'Algorithms •', '• Deficiencies', 'Deficiencies Semantics', 'Semantics Interpreting', 'Interpreting Medical', 'Medical Terminology', 'Terminology Deficiencies', 'Deficiencies Understanding', 'Understanding Document', 'Document Structure', 'Structure Missing', 'Missing Contextual', 'Contextual Recognition', 'Recognition first', 'first task', 'task system', 'system recognise', 'recognise context', 'context text', 'text .'] 

 TOTAL BIGRAMS --> 44 



 ---- TRI-GRAMS ---- 

 ['deficiencies discussed compiled', 'discussed compiled across', 'compiled across 5', 'across 5 systems', '5 systems following', 'systems following headings', 'following headings :', 'headings : •', ': • Deficiencies', '• Deficiencies Understanding', 'Deficiencies Understanding Document', 'Understanding Document Structure', 'Document Structure •', 'Structure • Deficiencies', '• Deficiencies Tokenisation', 'Deficiencies Tokenisation •', 'Tokenisation • Deficiencies', '• Deficiencies Grammatical', 'Deficiencies Grammatical Understanding', 'Grammatical Understanding •', 'Understanding • Deficiencies', '• Deficiencies CER', 'Deficiencies CER Algorithms', 'CER Algorithms •', 'Algorithms • Deficiencies', '• Deficiencies Semantics', 'Deficiencies Semantics Interpreting', 'Semantics Interpreting Medical', 'Interpreting Medical Terminology', 'Medical Terminology Deficiencies', 'Terminology Deficiencies Understanding', 'Deficiencies Understanding Document', 'Understanding Document Structure', 'Document Structure Missing', 'Structure Missing Contextual', 'Missing Contextual Recognition', 'Contextual Recognition first', 'Recognition first task', 'first task system', 'task system recognise', 'system recognise context', 'recognise context text', 'context text .'] 

 TOTAL TRIGRAMS --> 43 



 ---- NOUN PHRASES ---- 

 ['•', 'task', 'system', 'context text'] 

 TOTAL NOUN PHRASES --> 4 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['defici', 'discuss', 'compil', 'across', '5', 'system', 'follow', 'head', ':', '•', 'defici', 'understand', 'document', 'structur', '•', 'defici', 'tokenis', '•', 'defici', 'grammat', 'understand', '•', 'defici', 'cer', 'algorithm', '•', 'defici', 'semant', 'interpret', 'medic', 'terminolog', 'defici', 'understand', 'document', 'structur', 'miss', 'contextu', 'recognit', 'first', 'task', 'system', 'recognis', 'context', 'text', '.']

 TOTAL PORTER STEM WORDS ==> 45



 ---- SNOWBALL STEMMING ----

['defici', 'discuss', 'compil', 'across', '5', 'system', 'follow', 'head', ':', '•', 'defici', 'understand', 'document', 'structur', '•', 'defici', 'tokenis', '•', 'defici', 'grammat', 'understand', '•', 'defici', 'cer', 'algorithm', '•', 'defici', 'semant', 'interpret', 'medic', 'terminolog', 'defici', 'understand', 'document', 'structur', 'miss', 'contextu', 'recognit', 'first', 'task', 'system', 'recognis', 'context', 'text', '.']

 TOTAL SNOWBALL STEM WORDS ==> 45



 ---- LEMMATIZATION ----

['deficiency', 'discussed', 'compiled', 'across', '5', 'system', 'following', 'heading', ':', '•', 'Deficiencies', 'Understanding', 'Document', 'Structure', '•', 'Deficiencies', 'Tokenisation', '•', 'Deficiencies', 'Grammatical', 'Understanding', '•', 'Deficiencies', 'CER', 'Algorithms', '•', 'Deficiencies', 'Semantics', 'Interpreting', 'Medical', 'Terminology', 'Deficiencies', 'Understanding', 'Document', 'Structure', 'Missing', 'Contextual', 'Recognition', 'first', 'task', 'system', 'recognise', 'context', 'text', '.']

 TOTAL LEMMATIZE WORDS ==> 45

************************************************************************************************************************

15 --> This requires identifying the class of information in the document as a  whole although sometimes it is only manifest through the structure of the  document,    Inability to Recognise Headings. 


 ---- TOKENS ----

 ['This', 'requires', 'identifying', 'the', 'class', 'of', 'information', 'in', 'the', 'document', 'as', 'a', 'whole', 'although', 'sometimes', 'it', 'is', 'only', 'manifest', 'through', 'the', 'structure', 'of', 'the', 'document', ',', 'Inability', 'to', 'Recognise', 'Headings', '.'] 

 TOTAL TOKENS ==> 31

 ---- POST ----

 [('This', 'DT'), ('requires', 'VBZ'), ('identifying', 'VBG'), ('the', 'DT'), ('class', 'NN'), ('of', 'IN'), ('information', 'NN'), ('in', 'IN'), ('the', 'DT'), ('document', 'NN'), ('as', 'IN'), ('a', 'DT'), ('whole', 'JJ'), ('although', 'IN'), ('sometimes', 'RB'), ('it', 'PRP'), ('is', 'VBZ'), ('only', 'RB'), ('manifest', 'JJS'), ('through', 'IN'), ('the', 'DT'), ('structure', 'NN'), ('of', 'IN'), ('the', 'DT'), ('document', 'NN'), (',', ','), ('Inability', 'NNP'), ('to', 'TO'), ('Recognise', 'NNP'), ('Headings', 'NNP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['requires', 'identifying', 'class', 'information', 'document', 'whole', 'although', 'sometimes', 'manifest', 'structure', 'document', ',', 'Inability', 'Recognise', 'Headings', '.']

 TOTAL FILTERED TOKENS ==>  16

 ---- POST FOR FILTERED TOKENS ----

 [('requires', 'VBZ'), ('identifying', 'VBG'), ('class', 'NN'), ('information', 'NN'), ('document', 'NN'), ('whole', 'JJ'), ('although', 'IN'), ('sometimes', 'RB'), ('manifest', 'JJS'), ('structure', 'NN'), ('document', 'NN'), (',', ','), ('Inability', 'NNP'), ('Recognise', 'NNP'), ('Headings', 'NNP'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['requires identifying', 'identifying class', 'class information', 'information document', 'document whole', 'whole although', 'although sometimes', 'sometimes manifest', 'manifest structure', 'structure document', 'document ,', ', Inability', 'Inability Recognise', 'Recognise Headings', 'Headings .'] 

 TOTAL BIGRAMS --> 15 



 ---- TRI-GRAMS ---- 

 ['requires identifying class', 'identifying class information', 'class information document', 'information document whole', 'document whole although', 'whole although sometimes', 'although sometimes manifest', 'sometimes manifest structure', 'manifest structure document', 'structure document ,', 'document , Inability', ', Inability Recognise', 'Inability Recognise Headings', 'Recognise Headings .'] 

 TOTAL TRIGRAMS --> 14 



 ---- NOUN PHRASES ---- 

 ['class', 'information', 'document', 'structure', 'document'] 

 TOTAL NOUN PHRASES --> 5 



 ---- NER ----

 
 ORGANIZATION ---> ['Inability']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['requir', 'identifi', 'class', 'inform', 'document', 'whole', 'although', 'sometim', 'manifest', 'structur', 'document', ',', 'inabl', 'recognis', 'head', '.']

 TOTAL PORTER STEM WORDS ==> 16



 ---- SNOWBALL STEMMING ----

['requir', 'identifi', 'class', 'inform', 'document', 'whole', 'although', 'sometim', 'manifest', 'structur', 'document', ',', 'inabl', 'recognis', 'head', '.']

 TOTAL SNOWBALL STEM WORDS ==> 16



 ---- LEMMATIZATION ----

['requires', 'identifying', 'class', 'information', 'document', 'whole', 'although', 'sometimes', 'manifest', 'structure', 'document', ',', 'Inability', 'Recognise', 'Headings', '.']

 TOTAL LEMMATIZE WORDS ==> 16

************************************************************************************************************************

16 --> Headings can be presented in a report by visual layout of uppercase  or title case orthography and surrounding whitespace. 


 ---- TOKENS ----

 ['Headings', 'can', 'be', 'presented', 'in', 'a', 'report', 'by', 'visual', 'layout', 'of', 'uppercase', 'or', 'title', 'case', 'orthography', 'and', 'surrounding', 'whitespace', '.'] 

 TOTAL TOKENS ==> 20

 ---- POST ----

 [('Headings', 'NNS'), ('can', 'MD'), ('be', 'VB'), ('presented', 'VBN'), ('in', 'IN'), ('a', 'DT'), ('report', 'NN'), ('by', 'IN'), ('visual', 'JJ'), ('layout', 'NN'), ('of', 'IN'), ('uppercase', 'NN'), ('or', 'CC'), ('title', 'NN'), ('case', 'NN'), ('orthography', 'NN'), ('and', 'CC'), ('surrounding', 'VBG'), ('whitespace', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Headings', 'presented', 'report', 'visual', 'layout', 'uppercase', 'title', 'case', 'orthography', 'surrounding', 'whitespace', '.']

 TOTAL FILTERED TOKENS ==>  12

 ---- POST FOR FILTERED TOKENS ----

 [('Headings', 'NNS'), ('presented', 'VBN'), ('report', 'VBP'), ('visual', 'JJ'), ('layout', 'IN'), ('uppercase', 'JJ'), ('title', 'NN'), ('case', 'NN'), ('orthography', 'NN'), ('surrounding', 'VBG'), ('whitespace', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Headings presented', 'presented report', 'report visual', 'visual layout', 'layout uppercase', 'uppercase title', 'title case', 'case orthography', 'orthography surrounding', 'surrounding whitespace', 'whitespace .'] 

 TOTAL BIGRAMS --> 11 



 ---- TRI-GRAMS ---- 

 ['Headings presented report', 'presented report visual', 'report visual layout', 'visual layout uppercase', 'layout uppercase title', 'uppercase title case', 'title case orthography', 'case orthography surrounding', 'orthography surrounding whitespace', 'surrounding whitespace .'] 

 TOTAL TRIGRAMS --> 10 



 ---- NOUN PHRASES ---- 

 ['uppercase title', 'case', 'orthography', 'whitespace'] 

 TOTAL NOUN PHRASES --> 4 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['head', 'present', 'report', 'visual', 'layout', 'uppercas', 'titl', 'case', 'orthographi', 'surround', 'whitespac', '.']

 TOTAL PORTER STEM WORDS ==> 12



 ---- SNOWBALL STEMMING ----

['head', 'present', 'report', 'visual', 'layout', 'uppercas', 'titl', 'case', 'orthographi', 'surround', 'whitespac', '.']

 TOTAL SNOWBALL STEM WORDS ==> 12



 ---- LEMMATIZATION ----

['Headings', 'presented', 'report', 'visual', 'layout', 'uppercase', 'title', 'case', 'orthography', 'surrounding', 'whitespace', '.']

 TOTAL LEMMATIZE WORDS ==> 12

************************************************************************************************************************

17 --> However they can  also be provided by lables from a HL7 tagset. 


 ---- TOKENS ----

 ['However', 'they', 'can', 'also', 'be', 'provided', 'by', 'lables', 'from', 'a', 'HL7', 'tagset', '.'] 

 TOTAL TOKENS ==> 13

 ---- POST ----

 [('However', 'RB'), ('they', 'PRP'), ('can', 'MD'), ('also', 'RB'), ('be', 'VB'), ('provided', 'VBN'), ('by', 'IN'), ('lables', 'NNS'), ('from', 'IN'), ('a', 'DT'), ('HL7', 'NNP'), ('tagset', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['However', 'also', 'provided', 'lables', 'HL7', 'tagset', '.']

 TOTAL FILTERED TOKENS ==>  7

 ---- POST FOR FILTERED TOKENS ----

 [('However', 'RB'), ('also', 'RB'), ('provided', 'VBN'), ('lables', 'NNS'), ('HL7', 'NNP'), ('tagset', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['However also', 'also provided', 'provided lables', 'lables HL7', 'HL7 tagset', 'tagset .'] 

 TOTAL BIGRAMS --> 6 



 ---- TRI-GRAMS ---- 

 ['However also provided', 'also provided lables', 'provided lables HL7', 'lables HL7 tagset', 'HL7 tagset .'] 

 TOTAL TRIGRAMS --> 5 



 ---- NOUN PHRASES ---- 

 ['tagset'] 

 TOTAL NOUN PHRASES --> 1 



 ---- NER ----

 
 ORGANIZATION ---> ['HL7']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['howev', 'also', 'provid', 'labl', 'hl7', 'tagset', '.']

 TOTAL PORTER STEM WORDS ==> 7



 ---- SNOWBALL STEMMING ----

['howev', 'also', 'provid', 'labl', 'hl7', 'tagset', '.']

 TOTAL SNOWBALL STEM WORDS ==> 7



 ---- LEMMATIZATION ----

['However', 'also', 'provided', 'lables', 'HL7', 'tagset', '.']

 TOTAL LEMMATIZE WORDS ==> 7

************************************************************************************************************************

18 --> Headings provide key information on the shift in the type of content  to be expected and therefore warrant a different processing objective,  that is, key information components that represent major topic shifts. 


 ---- TOKENS ----

 ['Headings', 'provide', 'key', 'information', 'on', 'the', 'shift', 'in', 'the', 'type', 'of', 'content', 'to', 'be', 'expected', 'and', 'therefore', 'warrant', 'a', 'different', 'processing', 'objective', ',', 'that', 'is', ',', 'key', 'information', 'components', 'that', 'represent', 'major', 'topic', 'shifts', '.'] 

 TOTAL TOKENS ==> 35

 ---- POST ----

 [('Headings', 'NNS'), ('provide', 'VBP'), ('key', 'JJ'), ('information', 'NN'), ('on', 'IN'), ('the', 'DT'), ('shift', 'NN'), ('in', 'IN'), ('the', 'DT'), ('type', 'NN'), ('of', 'IN'), ('content', 'NN'), ('to', 'TO'), ('be', 'VB'), ('expected', 'VBN'), ('and', 'CC'), ('therefore', 'RB'), ('warrant', 'VBP'), ('a', 'DT'), ('different', 'JJ'), ('processing', 'NN'), ('objective', 'NN'), (',', ','), ('that', 'WDT'), ('is', 'VBZ'), (',', ','), ('key', 'JJ'), ('information', 'NN'), ('components', 'NNS'), ('that', 'WDT'), ('represent', 'VBP'), ('major', 'JJ'), ('topic', 'NN'), ('shifts', 'NNS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Headings', 'provide', 'key', 'information', 'shift', 'type', 'content', 'expected', 'therefore', 'warrant', 'different', 'processing', 'objective', ',', ',', 'key', 'information', 'components', 'represent', 'major', 'topic', 'shifts', '.']

 TOTAL FILTERED TOKENS ==>  23

 ---- POST FOR FILTERED TOKENS ----

 [('Headings', 'NNS'), ('provide', 'VBP'), ('key', 'JJ'), ('information', 'NN'), ('shift', 'NN'), ('type', 'NN'), ('content', 'NN'), ('expected', 'VBN'), ('therefore', 'RB'), ('warrant', 'JJ'), ('different', 'JJ'), ('processing', 'NN'), ('objective', 'NN'), (',', ','), (',', ','), ('key', 'JJ'), ('information', 'NN'), ('components', 'NNS'), ('represent', 'VBP'), ('major', 'JJ'), ('topic', 'NN'), ('shifts', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Headings provide', 'provide key', 'key information', 'information shift', 'shift type', 'type content', 'content expected', 'expected therefore', 'therefore warrant', 'warrant different', 'different processing', 'processing objective', 'objective ,', ', ,', ', key', 'key information', 'information components', 'components represent', 'represent major', 'major topic', 'topic shifts', 'shifts .'] 

 TOTAL BIGRAMS --> 22 



 ---- TRI-GRAMS ---- 

 ['Headings provide key', 'provide key information', 'key information shift', 'information shift type', 'shift type content', 'type content expected', 'content expected therefore', 'expected therefore warrant', 'therefore warrant different', 'warrant different processing', 'different processing objective', 'processing objective ,', 'objective , ,', ', , key', ', key information', 'key information components', 'information components represent', 'components represent major', 'represent major topic', 'major topic shifts', 'topic shifts .'] 

 TOTAL TRIGRAMS --> 21 



 ---- NOUN PHRASES ---- 

 ['key information', 'shift', 'type', 'content', 'warrant different processing', 'objective', 'key information', 'major topic'] 

 TOTAL NOUN PHRASES --> 8 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['head', 'provid', 'key', 'inform', 'shift', 'type', 'content', 'expect', 'therefor', 'warrant', 'differ', 'process', 'object', ',', ',', 'key', 'inform', 'compon', 'repres', 'major', 'topic', 'shift', '.']

 TOTAL PORTER STEM WORDS ==> 23



 ---- SNOWBALL STEMMING ----

['head', 'provid', 'key', 'inform', 'shift', 'type', 'content', 'expect', 'therefor', 'warrant', 'differ', 'process', 'object', ',', ',', 'key', 'inform', 'compon', 'repres', 'major', 'topic', 'shift', '.']

 TOTAL SNOWBALL STEM WORDS ==> 23



 ---- LEMMATIZATION ----

['Headings', 'provide', 'key', 'information', 'shift', 'type', 'content', 'expected', 'therefore', 'warrant', 'different', 'processing', 'objective', ',', ',', 'key', 'information', 'component', 'represent', 'major', 'topic', 'shift', '.']

 TOTAL LEMMATIZE WORDS ==> 23

************************************************************************************************************************

19 --> These are classically defined by section headers, but not always. 


 ---- TOKENS ----

 ['These', 'are', 'classically', 'defined', 'by', 'section', 'headers', ',', 'but', 'not', 'always', '.'] 

 TOTAL TOKENS ==> 12

 ---- POST ----

 [('These', 'DT'), ('are', 'VBP'), ('classically', 'RB'), ('defined', 'VBN'), ('by', 'IN'), ('section', 'NN'), ('headers', 'NNS'), (',', ','), ('but', 'CC'), ('not', 'RB'), ('always', 'RB'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['classically', 'defined', 'section', 'headers', ',', 'always', '.']

 TOTAL FILTERED TOKENS ==>  7

 ---- POST FOR FILTERED TOKENS ----

 [('classically', 'RB'), ('defined', 'VBN'), ('section', 'NN'), ('headers', 'NNS'), (',', ','), ('always', 'RB'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['classically defined', 'defined section', 'section headers', 'headers ,', ', always', 'always .'] 

 TOTAL BIGRAMS --> 6 



 ---- TRI-GRAMS ---- 

 ['classically defined section', 'defined section headers', 'section headers ,', 'headers , always', ', always .'] 

 TOTAL TRIGRAMS --> 5 



 ---- NOUN PHRASES ---- 

 ['section'] 

 TOTAL NOUN PHRASES --> 1 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['classic', 'defin', 'section', 'header', ',', 'alway', '.']

 TOTAL PORTER STEM WORDS ==> 7



 ---- SNOWBALL STEMMING ----

['classic', 'defin', 'section', 'header', ',', 'alway', '.']

 TOTAL SNOWBALL STEM WORDS ==> 7



 ---- LEMMATIZATION ----

['classically', 'defined', 'section', 'header', ',', 'always', '.']

 TOTAL LEMMATIZE WORDS ==> 7

************************************************************************************************************************

20 --> Recognising headerless topic shifts is crucial to high accuracy results. 


 ---- TOKENS ----

 ['Recognising', 'headerless', 'topic', 'shifts', 'is', 'crucial', 'to', 'high', 'accuracy', 'results', '.'] 

 TOTAL TOKENS ==> 11

 ---- POST ----

 [('Recognising', 'VBG'), ('headerless', 'NN'), ('topic', 'NN'), ('shifts', 'NNS'), ('is', 'VBZ'), ('crucial', 'JJ'), ('to', 'TO'), ('high', 'JJ'), ('accuracy', 'NN'), ('results', 'NNS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Recognising', 'headerless', 'topic', 'shifts', 'crucial', 'high', 'accuracy', 'results', '.']

 TOTAL FILTERED TOKENS ==>  9

 ---- POST FOR FILTERED TOKENS ----

 [('Recognising', 'VBG'), ('headerless', 'NN'), ('topic', 'NN'), ('shifts', 'NNS'), ('crucial', 'JJ'), ('high', 'JJ'), ('accuracy', 'NN'), ('results', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Recognising headerless', 'headerless topic', 'topic shifts', 'shifts crucial', 'crucial high', 'high accuracy', 'accuracy results', 'results .'] 

 TOTAL BIGRAMS --> 8 



 ---- TRI-GRAMS ---- 

 ['Recognising headerless topic', 'headerless topic shifts', 'topic shifts crucial', 'shifts crucial high', 'crucial high accuracy', 'high accuracy results', 'accuracy results .'] 

 TOTAL TRIGRAMS --> 7 



 ---- NOUN PHRASES ---- 

 ['headerless', 'topic', 'crucial high accuracy'] 

 TOTAL NOUN PHRASES --> 3 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['recognis', 'headerless', 'topic', 'shift', 'crucial', 'high', 'accuraci', 'result', '.']

 TOTAL PORTER STEM WORDS ==> 9



 ---- SNOWBALL STEMMING ----

['recognis', 'headerless', 'topic', 'shift', 'crucial', 'high', 'accuraci', 'result', '.']

 TOTAL SNOWBALL STEM WORDS ==> 9



 ---- LEMMATIZATION ----

['Recognising', 'headerless', 'topic', 'shift', 'crucial', 'high', 'accuracy', 'result', '.']

 TOTAL LEMMATIZE WORDS ==> 9

************************************************************************************************************************

21 --> Failure to recognise headings will lead to identification of incorrect entity  values or inhibit corroboration of correct entity identification, e.g.-  identifying the full specimen description under examination might only be  achieved by comparing content in the Final Diagnosis and the Nature of  Specimen sections of a pathology report. 


 ---- TOKENS ----

 ['Failure', 'to', 'recognise', 'headings', 'will', 'lead', 'to', 'identification', 'of', 'incorrect', 'entity', 'values', 'or', 'inhibit', 'corroboration', 'of', 'correct', 'entity', 'identification', ',', 'e.g.-', 'identifying', 'the', 'full', 'specimen', 'description', 'under', 'examination', 'might', 'only', 'be', 'achieved', 'by', 'comparing', 'content', 'in', 'the', 'Final', 'Diagnosis', 'and', 'the', 'Nature', 'of', 'Specimen', 'sections', 'of', 'a', 'pathology', 'report', '.'] 

 TOTAL TOKENS ==> 50

 ---- POST ----

 [('Failure', 'NN'), ('to', 'TO'), ('recognise', 'VB'), ('headings', 'NNS'), ('will', 'MD'), ('lead', 'VB'), ('to', 'TO'), ('identification', 'NN'), ('of', 'IN'), ('incorrect', 'JJ'), ('entity', 'NN'), ('values', 'NNS'), ('or', 'CC'), ('inhibit', 'VB'), ('corroboration', 'NN'), ('of', 'IN'), ('correct', 'JJ'), ('entity', 'NN'), ('identification', 'NN'), (',', ','), ('e.g.-', 'JJ'), ('identifying', 'VBG'), ('the', 'DT'), ('full', 'JJ'), ('specimen', 'NNS'), ('description', 'NN'), ('under', 'IN'), ('examination', 'NN'), ('might', 'MD'), ('only', 'RB'), ('be', 'VB'), ('achieved', 'VBN'), ('by', 'IN'), ('comparing', 'VBG'), ('content', 'NN'), ('in', 'IN'), ('the', 'DT'), ('Final', 'NNP'), ('Diagnosis', 'NNP'), ('and', 'CC'), ('the', 'DT'), ('Nature', 'NN'), ('of', 'IN'), ('Specimen', 'NNP'), ('sections', 'NNS'), ('of', 'IN'), ('a', 'DT'), ('pathology', 'NN'), ('report', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Failure', 'recognise', 'headings', 'lead', 'identification', 'incorrect', 'entity', 'values', 'inhibit', 'corroboration', 'correct', 'entity', 'identification', ',', 'e.g.-', 'identifying', 'full', 'specimen', 'description', 'examination', 'might', 'achieved', 'comparing', 'content', 'Final', 'Diagnosis', 'Nature', 'Specimen', 'sections', 'pathology', 'report', '.']

 TOTAL FILTERED TOKENS ==>  32

 ---- POST FOR FILTERED TOKENS ----

 [('Failure', 'NN'), ('recognise', 'NN'), ('headings', 'NNS'), ('lead', 'JJ'), ('identification', 'NN'), ('incorrect', 'JJ'), ('entity', 'NN'), ('values', 'NNS'), ('inhibit', 'VBP'), ('corroboration', 'NN'), ('correct', 'VBP'), ('entity', 'NN'), ('identification', 'NN'), (',', ','), ('e.g.-', 'JJ'), ('identifying', 'VBG'), ('full', 'JJ'), ('specimen', 'NNS'), ('description', 'NN'), ('examination', 'NN'), ('might', 'MD'), ('achieved', 'VB'), ('comparing', 'VBG'), ('content', 'JJ'), ('Final', 'NNP'), ('Diagnosis', 'NNP'), ('Nature', 'NNP'), ('Specimen', 'NNP'), ('sections', 'NNS'), ('pathology', 'VBP'), ('report', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Failure recognise', 'recognise headings', 'headings lead', 'lead identification', 'identification incorrect', 'incorrect entity', 'entity values', 'values inhibit', 'inhibit corroboration', 'corroboration correct', 'correct entity', 'entity identification', 'identification ,', ', e.g.-', 'e.g.- identifying', 'identifying full', 'full specimen', 'specimen description', 'description examination', 'examination might', 'might achieved', 'achieved comparing', 'comparing content', 'content Final', 'Final Diagnosis', 'Diagnosis Nature', 'Nature Specimen', 'Specimen sections', 'sections pathology', 'pathology report', 'report .'] 

 TOTAL BIGRAMS --> 31 



 ---- TRI-GRAMS ---- 

 ['Failure recognise headings', 'recognise headings lead', 'headings lead identification', 'lead identification incorrect', 'identification incorrect entity', 'incorrect entity values', 'entity values inhibit', 'values inhibit corroboration', 'inhibit corroboration correct', 'corroboration correct entity', 'correct entity identification', 'entity identification ,', 'identification , e.g.-', ', e.g.- identifying', 'e.g.- identifying full', 'identifying full specimen', 'full specimen description', 'specimen description examination', 'description examination might', 'examination might achieved', 'might achieved comparing', 'achieved comparing content', 'comparing content Final', 'content Final Diagnosis', 'Final Diagnosis Nature', 'Diagnosis Nature Specimen', 'Nature Specimen sections', 'Specimen sections pathology', 'sections pathology report', 'pathology report .'] 

 TOTAL TRIGRAMS --> 30 



 ---- NOUN PHRASES ---- 

 ['Failure', 'recognise', 'lead identification', 'incorrect entity', 'corroboration', 'entity', 'identification', 'description', 'examination', 'report'] 

 TOTAL NOUN PHRASES --> 10 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> ['Diagnosis Nature Specimen']
 TOTAL PERSON ENTITY --> 1 


 GPE ---> ['Failure']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['failur', 'recognis', 'head', 'lead', 'identif', 'incorrect', 'entiti', 'valu', 'inhibit', 'corrobor', 'correct', 'entiti', 'identif', ',', 'e.g.-', 'identifi', 'full', 'specimen', 'descript', 'examin', 'might', 'achiev', 'compar', 'content', 'final', 'diagnosi', 'natur', 'specimen', 'section', 'patholog', 'report', '.']

 TOTAL PORTER STEM WORDS ==> 32



 ---- SNOWBALL STEMMING ----

['failur', 'recognis', 'head', 'lead', 'identif', 'incorrect', 'entiti', 'valu', 'inhibit', 'corrobor', 'correct', 'entiti', 'identif', ',', 'e.g.-', 'identifi', 'full', 'specimen', 'descript', 'examin', 'might', 'achiev', 'compar', 'content', 'final', 'diagnosi', 'natur', 'specimen', 'section', 'patholog', 'report', '.']

 TOTAL SNOWBALL STEM WORDS ==> 32



 ---- LEMMATIZATION ----

['Failure', 'recognise', 'heading', 'lead', 'identification', 'incorrect', 'entity', 'value', 'inhibit', 'corroboration', 'correct', 'entity', 'identification', ',', 'e.g.-', 'identifying', 'full', 'specimen', 'description', 'examination', 'might', 'achieved', 'comparing', 'content', 'Final', 'Diagnosis', 'Nature', 'Specimen', 'section', 'pathology', 'report', '.']

 TOTAL LEMMATIZE WORDS ==> 32

************************************************************************************************************************

22 --> One system had difficulty recognising headings that where  concatenations of words embedded with full stops, e.g.-  Pathology.Report.Section due to their tokenizers behaviour. 


 ---- TOKENS ----

 ['One', 'system', 'had', 'difficulty', 'recognising', 'headings', 'that', 'where', 'concatenations', 'of', 'words', 'embedded', 'with', 'full', 'stops', ',', 'e.g.-', 'Pathology.Report.Section', 'due', 'to', 'their', 'tokenizers', 'behaviour', '.'] 

 TOTAL TOKENS ==> 24

 ---- POST ----

 [('One', 'CD'), ('system', 'NN'), ('had', 'VBD'), ('difficulty', 'NN'), ('recognising', 'VBG'), ('headings', 'NNS'), ('that', 'WDT'), ('where', 'WRB'), ('concatenations', 'NNS'), ('of', 'IN'), ('words', 'NNS'), ('embedded', 'VBN'), ('with', 'IN'), ('full', 'JJ'), ('stops', 'NNS'), (',', ','), ('e.g.-', 'JJ'), ('Pathology.Report.Section', 'NN'), ('due', 'JJ'), ('to', 'TO'), ('their', 'PRP$'), ('tokenizers', 'NNS'), ('behaviour', 'VBP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['One', 'system', 'difficulty', 'recognising', 'headings', 'concatenations', 'words', 'embedded', 'full', 'stops', ',', 'e.g.-', 'Pathology.Report.Section', 'due', 'tokenizers', 'behaviour', '.']

 TOTAL FILTERED TOKENS ==>  17

 ---- POST FOR FILTERED TOKENS ----

 [('One', 'CD'), ('system', 'NN'), ('difficulty', 'NN'), ('recognising', 'VBG'), ('headings', 'NNS'), ('concatenations', 'NNS'), ('words', 'NNS'), ('embedded', 'VBD'), ('full', 'JJ'), ('stops', 'NNS'), (',', ','), ('e.g.-', 'JJ'), ('Pathology.Report.Section', 'NNP'), ('due', 'JJ'), ('tokenizers', 'NNS'), ('behaviour', 'VBP'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['One system', 'system difficulty', 'difficulty recognising', 'recognising headings', 'headings concatenations', 'concatenations words', 'words embedded', 'embedded full', 'full stops', 'stops ,', ', e.g.-', 'e.g.- Pathology.Report.Section', 'Pathology.Report.Section due', 'due tokenizers', 'tokenizers behaviour', 'behaviour .'] 

 TOTAL BIGRAMS --> 16 



 ---- TRI-GRAMS ---- 

 ['One system difficulty', 'system difficulty recognising', 'difficulty recognising headings', 'recognising headings concatenations', 'headings concatenations words', 'concatenations words embedded', 'words embedded full', 'embedded full stops', 'full stops ,', 'stops , e.g.-', ', e.g.- Pathology.Report.Section', 'e.g.- Pathology.Report.Section due', 'Pathology.Report.Section due tokenizers', 'due tokenizers behaviour', 'tokenizers behaviour .'] 

 TOTAL TRIGRAMS --> 15 



 ---- NOUN PHRASES ---- 

 ['system', 'difficulty'] 

 TOTAL NOUN PHRASES --> 2 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['one', 'system', 'difficulti', 'recognis', 'head', 'concaten', 'word', 'embed', 'full', 'stop', ',', 'e.g.-', 'pathology.report.sect', 'due', 'token', 'behaviour', '.']

 TOTAL PORTER STEM WORDS ==> 17



 ---- SNOWBALL STEMMING ----

['one', 'system', 'difficulti', 'recognis', 'head', 'concaten', 'word', 'embed', 'full', 'stop', ',', 'e.g.-', 'pathology.report.sect', 'due', 'token', 'behaviour', '.']

 TOTAL SNOWBALL STEM WORDS ==> 17



 ---- LEMMATIZATION ----

['One', 'system', 'difficulty', 'recognising', 'heading', 'concatenation', 'word', 'embedded', 'full', 'stop', ',', 'e.g.-', 'Pathology.Report.Section', 'due', 'tokenizers', 'behaviour', '.']

 TOTAL LEMMATIZE WORDS ==> 17

************************************************************************************************************************

23 --> As headings  are important both for section boundary detection and context setting this  Deficiency threatens a great deal of later processing. 


 ---- TOKENS ----

 ['As', 'headings', 'are', 'important', 'both', 'for', 'section', 'boundary', 'detection', 'and', 'context', 'setting', 'this', 'Deficiency', 'threatens', 'a', 'great', 'deal', 'of', 'later', 'processing', '.'] 

 TOTAL TOKENS ==> 22

 ---- POST ----

 [('As', 'IN'), ('headings', 'NNS'), ('are', 'VBP'), ('important', 'JJ'), ('both', 'DT'), ('for', 'IN'), ('section', 'NN'), ('boundary', 'JJ'), ('detection', 'NN'), ('and', 'CC'), ('context', 'NN'), ('setting', 'NN'), ('this', 'DT'), ('Deficiency', 'NNP'), ('threatens', 'VBZ'), ('a', 'DT'), ('great', 'JJ'), ('deal', 'NN'), ('of', 'IN'), ('later', 'RB'), ('processing', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['headings', 'important', 'section', 'boundary', 'detection', 'context', 'setting', 'Deficiency', 'threatens', 'great', 'deal', 'later', 'processing', '.']

 TOTAL FILTERED TOKENS ==>  14

 ---- POST FOR FILTERED TOKENS ----

 [('headings', 'NNS'), ('important', 'JJ'), ('section', 'NN'), ('boundary', 'JJ'), ('detection', 'NN'), ('context', 'NN'), ('setting', 'VBG'), ('Deficiency', 'NNP'), ('threatens', 'VBZ'), ('great', 'JJ'), ('deal', 'NN'), ('later', 'RB'), ('processing', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['headings important', 'important section', 'section boundary', 'boundary detection', 'detection context', 'context setting', 'setting Deficiency', 'Deficiency threatens', 'threatens great', 'great deal', 'deal later', 'later processing', 'processing .'] 

 TOTAL BIGRAMS --> 13 



 ---- TRI-GRAMS ---- 

 ['headings important section', 'important section boundary', 'section boundary detection', 'boundary detection context', 'detection context setting', 'context setting Deficiency', 'setting Deficiency threatens', 'Deficiency threatens great', 'threatens great deal', 'great deal later', 'deal later processing', 'later processing .'] 

 TOTAL TRIGRAMS --> 12 



 ---- NOUN PHRASES ---- 

 ['important section', 'boundary detection', 'context', 'great deal', 'processing'] 

 TOTAL NOUN PHRASES --> 5 



 ---- NER ----

 
 ORGANIZATION ---> ['Deficiency']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['head', 'import', 'section', 'boundari', 'detect', 'context', 'set', 'defici', 'threaten', 'great', 'deal', 'later', 'process', '.']

 TOTAL PORTER STEM WORDS ==> 14



 ---- SNOWBALL STEMMING ----

['head', 'import', 'section', 'boundari', 'detect', 'context', 'set', 'defici', 'threaten', 'great', 'deal', 'later', 'process', '.']

 TOTAL SNOWBALL STEM WORDS ==> 14



 ---- LEMMATIZATION ----

['heading', 'important', 'section', 'boundary', 'detection', 'context', 'setting', 'Deficiency', 'threatens', 'great', 'deal', 'later', 'processing', '.']

 TOTAL LEMMATIZE WORDS ==> 14

************************************************************************************************************************

24 --> Inability to properly recognise specimen boundaries. 


 ---- TOKENS ----

 ['Inability', 'to', 'properly', 'recognise', 'specimen', 'boundaries', '.'] 

 TOTAL TOKENS ==> 7

 ---- POST ----

 [('Inability', 'NN'), ('to', 'TO'), ('properly', 'RB'), ('recognise', 'VB'), ('specimen', 'JJ'), ('boundaries', 'NNS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Inability', 'properly', 'recognise', 'specimen', 'boundaries', '.']

 TOTAL FILTERED TOKENS ==>  6

 ---- POST FOR FILTERED TOKENS ----

 [('Inability', 'NNP'), ('properly', 'RB'), ('recognise', 'VB'), ('specimen', 'JJ'), ('boundaries', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Inability properly', 'properly recognise', 'recognise specimen', 'specimen boundaries', 'boundaries .'] 

 TOTAL BIGRAMS --> 5 



 ---- TRI-GRAMS ---- 

 ['Inability properly recognise', 'properly recognise specimen', 'recognise specimen boundaries', 'specimen boundaries .'] 

 TOTAL TRIGRAMS --> 4 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> ['Inability']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['inabl', 'properli', 'recognis', 'specimen', 'boundari', '.']

 TOTAL PORTER STEM WORDS ==> 6



 ---- SNOWBALL STEMMING ----

['inabl', 'proper', 'recognis', 'specimen', 'boundari', '.']

 TOTAL SNOWBALL STEM WORDS ==> 6



 ---- LEMMATIZATION ----

['Inability', 'properly', 'recognise', 'specimen', 'boundary', '.']

 TOTAL LEMMATIZE WORDS ==> 6

************************************************************************************************************************

25 --> Separating specimens in a multi-specimen report is critical to  correct interpretation of the disease location. 


 ---- TOKENS ----

 ['Separating', 'specimens', 'in', 'a', 'multi-specimen', 'report', 'is', 'critical', 'to', 'correct', 'interpretation', 'of', 'the', 'disease', 'location', '.'] 

 TOTAL TOKENS ==> 16

 ---- POST ----

 [('Separating', 'VBG'), ('specimens', 'NNS'), ('in', 'IN'), ('a', 'DT'), ('multi-specimen', 'JJ'), ('report', 'NN'), ('is', 'VBZ'), ('critical', 'JJ'), ('to', 'TO'), ('correct', 'VB'), ('interpretation', 'NN'), ('of', 'IN'), ('the', 'DT'), ('disease', 'NN'), ('location', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Separating', 'specimens', 'multi-specimen', 'report', 'critical', 'correct', 'interpretation', 'disease', 'location', '.']

 TOTAL FILTERED TOKENS ==>  10

 ---- POST FOR FILTERED TOKENS ----

 [('Separating', 'VBG'), ('specimens', 'NNS'), ('multi-specimen', 'JJ'), ('report', 'NN'), ('critical', 'JJ'), ('correct', 'JJ'), ('interpretation', 'NN'), ('disease', 'NN'), ('location', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Separating specimens', 'specimens multi-specimen', 'multi-specimen report', 'report critical', 'critical correct', 'correct interpretation', 'interpretation disease', 'disease location', 'location .'] 

 TOTAL BIGRAMS --> 9 



 ---- TRI-GRAMS ---- 

 ['Separating specimens multi-specimen', 'specimens multi-specimen report', 'multi-specimen report critical', 'report critical correct', 'critical correct interpretation', 'correct interpretation disease', 'interpretation disease location', 'disease location .'] 

 TOTAL TRIGRAMS --> 8 



 ---- NOUN PHRASES ---- 

 ['multi-specimen report', 'critical correct interpretation', 'disease', 'location'] 

 TOTAL NOUN PHRASES --> 4 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['separ', 'specimen', 'multi-specimen', 'report', 'critic', 'correct', 'interpret', 'diseas', 'locat', '.']

 TOTAL PORTER STEM WORDS ==> 10



 ---- SNOWBALL STEMMING ----

['separ', 'specimen', 'multi-specimen', 'report', 'critic', 'correct', 'interpret', 'diseas', 'locat', '.']

 TOTAL SNOWBALL STEM WORDS ==> 10



 ---- LEMMATIZATION ----

['Separating', 'specimen', 'multi-specimen', 'report', 'critical', 'correct', 'interpretation', 'disease', 'location', '.']

 TOTAL LEMMATIZE WORDS ==> 10

************************************************************************************************************************

26 --> In some types of reports  many specimens may be described with only some containing disease so  incorrect identification of the boundary of the specimen description will  result in the wrong specimen being assigned the identified disease. 


 ---- TOKENS ----

 ['In', 'some', 'types', 'of', 'reports', 'many', 'specimens', 'may', 'be', 'described', 'with', 'only', 'some', 'containing', 'disease', 'so', 'incorrect', 'identification', 'of', 'the', 'boundary', 'of', 'the', 'specimen', 'description', 'will', 'result', 'in', 'the', 'wrong', 'specimen', 'being', 'assigned', 'the', 'identified', 'disease', '.'] 

 TOTAL TOKENS ==> 37

 ---- POST ----

 [('In', 'IN'), ('some', 'DT'), ('types', 'NNS'), ('of', 'IN'), ('reports', 'NNS'), ('many', 'JJ'), ('specimens', 'NNS'), ('may', 'MD'), ('be', 'VB'), ('described', 'VBN'), ('with', 'IN'), ('only', 'RB'), ('some', 'DT'), ('containing', 'VBG'), ('disease', 'NN'), ('so', 'RB'), ('incorrect', 'JJ'), ('identification', 'NN'), ('of', 'IN'), ('the', 'DT'), ('boundary', 'NN'), ('of', 'IN'), ('the', 'DT'), ('specimen', 'NNS'), ('description', 'NN'), ('will', 'MD'), ('result', 'VB'), ('in', 'IN'), ('the', 'DT'), ('wrong', 'JJ'), ('specimen', 'NNS'), ('being', 'VBG'), ('assigned', 'VBD'), ('the', 'DT'), ('identified', 'JJ'), ('disease', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['types', 'reports', 'many', 'specimens', 'may', 'described', 'containing', 'disease', 'incorrect', 'identification', 'boundary', 'specimen', 'description', 'result', 'wrong', 'specimen', 'assigned', 'identified', 'disease', '.']

 TOTAL FILTERED TOKENS ==>  20

 ---- POST FOR FILTERED TOKENS ----

 [('types', 'NNS'), ('reports', 'NNS'), ('many', 'JJ'), ('specimens', 'NNS'), ('may', 'MD'), ('described', 'VB'), ('containing', 'VBG'), ('disease', 'NN'), ('incorrect', 'JJ'), ('identification', 'NN'), ('boundary', 'JJ'), ('specimen', 'NNS'), ('description', 'NN'), ('result', 'NN'), ('wrong', 'JJ'), ('specimen', 'NNS'), ('assigned', 'VBD'), ('identified', 'JJ'), ('disease', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['types reports', 'reports many', 'many specimens', 'specimens may', 'may described', 'described containing', 'containing disease', 'disease incorrect', 'incorrect identification', 'identification boundary', 'boundary specimen', 'specimen description', 'description result', 'result wrong', 'wrong specimen', 'specimen assigned', 'assigned identified', 'identified disease', 'disease .'] 

 TOTAL BIGRAMS --> 19 



 ---- TRI-GRAMS ---- 

 ['types reports many', 'reports many specimens', 'many specimens may', 'specimens may described', 'may described containing', 'described containing disease', 'containing disease incorrect', 'disease incorrect identification', 'incorrect identification boundary', 'identification boundary specimen', 'boundary specimen description', 'specimen description result', 'description result wrong', 'result wrong specimen', 'wrong specimen assigned', 'specimen assigned identified', 'assigned identified disease', 'identified disease .'] 

 TOTAL TRIGRAMS --> 18 



 ---- NOUN PHRASES ---- 

 ['disease', 'incorrect identification', 'description', 'result', 'identified disease'] 

 TOTAL NOUN PHRASES --> 5 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['type', 'report', 'mani', 'specimen', 'may', 'describ', 'contain', 'diseas', 'incorrect', 'identif', 'boundari', 'specimen', 'descript', 'result', 'wrong', 'specimen', 'assign', 'identifi', 'diseas', '.']

 TOTAL PORTER STEM WORDS ==> 20



 ---- SNOWBALL STEMMING ----

['type', 'report', 'mani', 'specimen', 'may', 'describ', 'contain', 'diseas', 'incorrect', 'identif', 'boundari', 'specimen', 'descript', 'result', 'wrong', 'specimen', 'assign', 'identifi', 'diseas', '.']

 TOTAL SNOWBALL STEM WORDS ==> 20



 ---- LEMMATIZATION ----

['type', 'report', 'many', 'specimen', 'may', 'described', 'containing', 'disease', 'incorrect', 'identification', 'boundary', 'specimen', 'description', 'result', 'wrong', 'specimen', 'assigned', 'identified', 'disease', '.']

 TOTAL LEMMATIZE WORDS ==> 20

************************************************************************************************************************

27 --> Deficiencies in Tokenisation   Weaknesses in tokenisation   Tokens can be crudely defined as the strings between whitespace  and they take many forms. 


 ---- TOKENS ----

 ['Deficiencies', 'in', 'Tokenisation', 'Weaknesses', 'in', 'tokenisation', 'Tokens', 'can', 'be', 'crudely', 'defined', 'as', 'the', 'strings', 'between', 'whitespace', 'and', 'they', 'take', 'many', 'forms', '.'] 

 TOTAL TOKENS ==> 22

 ---- POST ----

 [('Deficiencies', 'NNS'), ('in', 'IN'), ('Tokenisation', 'NNP'), ('Weaknesses', 'NNP'), ('in', 'IN'), ('tokenisation', 'NN'), ('Tokens', 'NNS'), ('can', 'MD'), ('be', 'VB'), ('crudely', 'RB'), ('defined', 'VBN'), ('as', 'IN'), ('the', 'DT'), ('strings', 'NNS'), ('between', 'IN'), ('whitespace', 'NN'), ('and', 'CC'), ('they', 'PRP'), ('take', 'VBP'), ('many', 'JJ'), ('forms', 'NNS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Deficiencies', 'Tokenisation', 'Weaknesses', 'tokenisation', 'Tokens', 'crudely', 'defined', 'strings', 'whitespace', 'take', 'many', 'forms', '.']

 TOTAL FILTERED TOKENS ==>  13

 ---- POST FOR FILTERED TOKENS ----

 [('Deficiencies', 'NNS'), ('Tokenisation', 'NNP'), ('Weaknesses', 'VBZ'), ('tokenisation', 'NN'), ('Tokens', 'NNP'), ('crudely', 'RB'), ('defined', 'VBD'), ('strings', 'NNS'), ('whitespace', 'VBP'), ('take', 'VB'), ('many', 'JJ'), ('forms', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Deficiencies Tokenisation', 'Tokenisation Weaknesses', 'Weaknesses tokenisation', 'tokenisation Tokens', 'Tokens crudely', 'crudely defined', 'defined strings', 'strings whitespace', 'whitespace take', 'take many', 'many forms', 'forms .'] 

 TOTAL BIGRAMS --> 12 



 ---- TRI-GRAMS ---- 

 ['Deficiencies Tokenisation Weaknesses', 'Tokenisation Weaknesses tokenisation', 'Weaknesses tokenisation Tokens', 'tokenisation Tokens crudely', 'Tokens crudely defined', 'crudely defined strings', 'defined strings whitespace', 'strings whitespace take', 'whitespace take many', 'take many forms', 'many forms .'] 

 TOTAL TRIGRAMS --> 11 



 ---- NOUN PHRASES ---- 

 ['tokenisation'] 

 TOTAL NOUN PHRASES --> 1 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> ['Tokens']
 TOTAL PERSON ENTITY --> 1 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['defici', 'tokenis', 'weak', 'tokenis', 'token', 'crude', 'defin', 'string', 'whitespac', 'take', 'mani', 'form', '.']

 TOTAL PORTER STEM WORDS ==> 13



 ---- SNOWBALL STEMMING ----

['defici', 'tokenis', 'weak', 'tokenis', 'token', 'crude', 'defin', 'string', 'whitespac', 'take', 'mani', 'form', '.']

 TOTAL SNOWBALL STEM WORDS ==> 13



 ---- LEMMATIZATION ----

['Deficiencies', 'Tokenisation', 'Weaknesses', 'tokenisation', 'Tokens', 'crudely', 'defined', 'string', 'whitespace', 'take', 'many', 'form', '.']

 TOTAL LEMMATIZE WORDS ==> 13

************************************************************************************************************************

28 --> A large range of non-alphabetic keyboard  characters can be used for different purposes and in clinical texts the  slash “/” has many functions. 


 ---- TOKENS ----

 ['A', 'large', 'range', 'of', 'non-alphabetic', 'keyboard', 'characters', 'can', 'be', 'used', 'for', 'different', 'purposes', 'and', 'in', 'clinical', 'texts', 'the', 'slash', '“', '/', '”', 'has', 'many', 'functions', '.'] 

 TOTAL TOKENS ==> 26

 ---- POST ----

 [('A', 'DT'), ('large', 'JJ'), ('range', 'NN'), ('of', 'IN'), ('non-alphabetic', 'JJ'), ('keyboard', 'NN'), ('characters', 'NNS'), ('can', 'MD'), ('be', 'VB'), ('used', 'VBN'), ('for', 'IN'), ('different', 'JJ'), ('purposes', 'NNS'), ('and', 'CC'), ('in', 'IN'), ('clinical', 'JJ'), ('texts', 'NN'), ('the', 'DT'), ('slash', 'NN'), ('“', 'NNP'), ('/', 'NNP'), ('”', 'NNP'), ('has', 'VBZ'), ('many', 'JJ'), ('functions', 'NNS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['large', 'range', 'non-alphabetic', 'keyboard', 'characters', 'used', 'different', 'purposes', 'clinical', 'texts', 'slash', '“', '/', '”', 'many', 'functions', '.']

 TOTAL FILTERED TOKENS ==>  17

 ---- POST FOR FILTERED TOKENS ----

 [('large', 'JJ'), ('range', 'NN'), ('non-alphabetic', 'JJ'), ('keyboard', 'NN'), ('characters', 'NNS'), ('used', 'VBD'), ('different', 'JJ'), ('purposes', 'NNS'), ('clinical', 'JJ'), ('texts', 'NNS'), ('slash', 'VBP'), ('“', 'JJ'), ('/', 'NNP'), ('”', 'NNP'), ('many', 'JJ'), ('functions', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['large range', 'range non-alphabetic', 'non-alphabetic keyboard', 'keyboard characters', 'characters used', 'used different', 'different purposes', 'purposes clinical', 'clinical texts', 'texts slash', 'slash “', '“ /', '/ ”', '” many', 'many functions', 'functions .'] 

 TOTAL BIGRAMS --> 16 



 ---- TRI-GRAMS ---- 

 ['large range non-alphabetic', 'range non-alphabetic keyboard', 'non-alphabetic keyboard characters', 'keyboard characters used', 'characters used different', 'used different purposes', 'different purposes clinical', 'purposes clinical texts', 'clinical texts slash', 'texts slash “', 'slash “ /', '“ / ”', '/ ” many', '” many functions', 'many functions .'] 

 TOTAL TRIGRAMS --> 15 



 ---- NOUN PHRASES ---- 

 ['large range', 'non-alphabetic keyboard'] 

 TOTAL NOUN PHRASES --> 2 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['larg', 'rang', 'non-alphabet', 'keyboard', 'charact', 'use', 'differ', 'purpos', 'clinic', 'text', 'slash', '“', '/', '”', 'mani', 'function', '.']

 TOTAL PORTER STEM WORDS ==> 17



 ---- SNOWBALL STEMMING ----

['larg', 'rang', 'non-alphabet', 'keyboard', 'charact', 'use', 'differ', 'purpos', 'clinic', 'text', 'slash', '“', '/', '”', 'mani', 'function', '.']

 TOTAL SNOWBALL STEM WORDS ==> 17



 ---- LEMMATIZATION ----

['large', 'range', 'non-alphabetic', 'keyboard', 'character', 'used', 'different', 'purpose', 'clinical', 'text', 'slash', '“', '/', '”', 'many', 'function', '.']

 TOTAL LEMMATIZE WORDS ==> 17

************************************************************************************************************************

29 --> It can used to express a ratio but also to  signify time duration, date, a proportion of lymph nodes involved in a  malignant tumour, etc. 


 ---- TOKENS ----

 ['It', 'can', 'used', 'to', 'express', 'a', 'ratio', 'but', 'also', 'to', 'signify', 'time', 'duration', ',', 'date', ',', 'a', 'proportion', 'of', 'lymph', 'nodes', 'involved', 'in', 'a', 'malignant', 'tumour', ',', 'etc', '.'] 

 TOTAL TOKENS ==> 29

 ---- POST ----

 [('It', 'PRP'), ('can', 'MD'), ('used', 'VBN'), ('to', 'TO'), ('express', 'VB'), ('a', 'DT'), ('ratio', 'NN'), ('but', 'CC'), ('also', 'RB'), ('to', 'TO'), ('signify', 'VB'), ('time', 'NN'), ('duration', 'NN'), (',', ','), ('date', 'NN'), (',', ','), ('a', 'DT'), ('proportion', 'NN'), ('of', 'IN'), ('lymph', 'NN'), ('nodes', 'NNS'), ('involved', 'VBN'), ('in', 'IN'), ('a', 'DT'), ('malignant', 'JJ'), ('tumour', 'NN'), (',', ','), ('etc', 'FW'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['used', 'express', 'ratio', 'also', 'signify', 'time', 'duration', ',', 'date', ',', 'proportion', 'lymph', 'nodes', 'involved', 'malignant', 'tumour', ',', 'etc', '.']

 TOTAL FILTERED TOKENS ==>  19

 ---- POST FOR FILTERED TOKENS ----

 [('used', 'VBN'), ('express', 'NN'), ('ratio', 'NN'), ('also', 'RB'), ('signify', 'JJ'), ('time', 'NN'), ('duration', 'NN'), (',', ','), ('date', 'NN'), (',', ','), ('proportion', 'NN'), ('lymph', 'NN'), ('nodes', 'NNS'), ('involved', 'VBN'), ('malignant', 'JJ'), ('tumour', 'NN'), (',', ','), ('etc', 'FW'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['used express', 'express ratio', 'ratio also', 'also signify', 'signify time', 'time duration', 'duration ,', ', date', 'date ,', ', proportion', 'proportion lymph', 'lymph nodes', 'nodes involved', 'involved malignant', 'malignant tumour', 'tumour ,', ', etc', 'etc .'] 

 TOTAL BIGRAMS --> 18 



 ---- TRI-GRAMS ---- 

 ['used express ratio', 'express ratio also', 'ratio also signify', 'also signify time', 'signify time duration', 'time duration ,', 'duration , date', ', date ,', 'date , proportion', ', proportion lymph', 'proportion lymph nodes', 'lymph nodes involved', 'nodes involved malignant', 'involved malignant tumour', 'malignant tumour ,', 'tumour , etc', ', etc .'] 

 TOTAL TRIGRAMS --> 17 



 ---- NOUN PHRASES ---- 

 ['express', 'ratio', 'signify time', 'duration', 'date', 'proportion', 'lymph', 'malignant tumour'] 

 TOTAL NOUN PHRASES --> 8 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['use', 'express', 'ratio', 'also', 'signifi', 'time', 'durat', ',', 'date', ',', 'proport', 'lymph', 'node', 'involv', 'malign', 'tumour', ',', 'etc', '.']

 TOTAL PORTER STEM WORDS ==> 19



 ---- SNOWBALL STEMMING ----

['use', 'express', 'ratio', 'also', 'signifi', 'time', 'durat', ',', 'date', ',', 'proport', 'lymph', 'node', 'involv', 'malign', 'tumour', ',', 'etc', '.']

 TOTAL SNOWBALL STEM WORDS ==> 19



 ---- LEMMATIZATION ----

['used', 'express', 'ratio', 'also', 'signify', 'time', 'duration', ',', 'date', ',', 'proportion', 'lymph', 'node', 'involved', 'malignant', 'tumour', ',', 'etc', '.']

 TOTAL LEMMATIZE WORDS ==> 19

************************************************************************************************************************

30 --> Two tokenisers keep the tokens on each side of  the slash together while another separated them, so that each was correct  some of the time and incorrect at other times. 


 ---- TOKENS ----

 ['Two', 'tokenisers', 'keep', 'the', 'tokens', 'on', 'each', 'side', 'of', 'the', 'slash', 'together', 'while', 'another', 'separated', 'them', ',', 'so', 'that', 'each', 'was', 'correct', 'some', 'of', 'the', 'time', 'and', 'incorrect', 'at', 'other', 'times', '.'] 

 TOTAL TOKENS ==> 32

 ---- POST ----

 [('Two', 'CD'), ('tokenisers', 'NNS'), ('keep', 'VBP'), ('the', 'DT'), ('tokens', 'NNS'), ('on', 'IN'), ('each', 'DT'), ('side', 'NN'), ('of', 'IN'), ('the', 'DT'), ('slash', 'NN'), ('together', 'RB'), ('while', 'IN'), ('another', 'DT'), ('separated', 'VBD'), ('them', 'PRP'), (',', ','), ('so', 'RB'), ('that', 'IN'), ('each', 'DT'), ('was', 'VBD'), ('correct', 'JJ'), ('some', 'DT'), ('of', 'IN'), ('the', 'DT'), ('time', 'NN'), ('and', 'CC'), ('incorrect', 'NN'), ('at', 'IN'), ('other', 'JJ'), ('times', 'NNS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Two', 'tokenisers', 'keep', 'tokens', 'side', 'slash', 'together', 'another', 'separated', ',', 'correct', 'time', 'incorrect', 'times', '.']

 TOTAL FILTERED TOKENS ==>  15

 ---- POST FOR FILTERED TOKENS ----

 [('Two', 'CD'), ('tokenisers', 'NNS'), ('keep', 'VBP'), ('tokens', 'NNS'), ('side', 'JJ'), ('slash', 'NN'), ('together', 'RB'), ('another', 'DT'), ('separated', 'VBN'), (',', ','), ('correct', 'JJ'), ('time', 'NN'), ('incorrect', 'JJ'), ('times', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Two tokenisers', 'tokenisers keep', 'keep tokens', 'tokens side', 'side slash', 'slash together', 'together another', 'another separated', 'separated ,', ', correct', 'correct time', 'time incorrect', 'incorrect times', 'times .'] 

 TOTAL BIGRAMS --> 14 



 ---- TRI-GRAMS ---- 

 ['Two tokenisers keep', 'tokenisers keep tokens', 'keep tokens side', 'tokens side slash', 'side slash together', 'slash together another', 'together another separated', 'another separated ,', 'separated , correct', ', correct time', 'correct time incorrect', 'time incorrect times', 'incorrect times .'] 

 TOTAL TRIGRAMS --> 13 



 ---- NOUN PHRASES ---- 

 ['side slash', 'correct time'] 

 TOTAL NOUN PHRASES --> 2 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['two', 'tokenis', 'keep', 'token', 'side', 'slash', 'togeth', 'anoth', 'separ', ',', 'correct', 'time', 'incorrect', 'time', '.']

 TOTAL PORTER STEM WORDS ==> 15



 ---- SNOWBALL STEMMING ----

['two', 'tokenis', 'keep', 'token', 'side', 'slash', 'togeth', 'anoth', 'separ', ',', 'correct', 'time', 'incorrect', 'time', '.']

 TOTAL SNOWBALL STEM WORDS ==> 15



 ---- LEMMATIZATION ----

['Two', 'tokenisers', 'keep', 'token', 'side', 'slash', 'together', 'another', 'separated', ',', 'correct', 'time', 'incorrect', 'time', '.']

 TOTAL LEMMATIZE WORDS ==> 15

************************************************************************************************************************

31 --> This problem needs  stronger context identification to produce the correct analysis at a  consistently high accuracy level. 


 ---- TOKENS ----

 ['This', 'problem', 'needs', 'stronger', 'context', 'identification', 'to', 'produce', 'the', 'correct', 'analysis', 'at', 'a', 'consistently', 'high', 'accuracy', 'level', '.'] 

 TOTAL TOKENS ==> 18

 ---- POST ----

 [('This', 'DT'), ('problem', 'NN'), ('needs', 'VBZ'), ('stronger', 'JJR'), ('context', 'JJ'), ('identification', 'NN'), ('to', 'TO'), ('produce', 'VB'), ('the', 'DT'), ('correct', 'JJ'), ('analysis', 'NN'), ('at', 'IN'), ('a', 'DT'), ('consistently', 'RB'), ('high', 'JJ'), ('accuracy', 'NN'), ('level', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['problem', 'needs', 'stronger', 'context', 'identification', 'produce', 'correct', 'analysis', 'consistently', 'high', 'accuracy', 'level', '.']

 TOTAL FILTERED TOKENS ==>  13

 ---- POST FOR FILTERED TOKENS ----

 [('problem', 'NN'), ('needs', 'VBZ'), ('stronger', 'JJR'), ('context', 'JJ'), ('identification', 'NN'), ('produce', 'VBP'), ('correct', 'JJ'), ('analysis', 'NN'), ('consistently', 'RB'), ('high', 'JJ'), ('accuracy', 'NN'), ('level', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['problem needs', 'needs stronger', 'stronger context', 'context identification', 'identification produce', 'produce correct', 'correct analysis', 'analysis consistently', 'consistently high', 'high accuracy', 'accuracy level', 'level .'] 

 TOTAL BIGRAMS --> 12 



 ---- TRI-GRAMS ---- 

 ['problem needs stronger', 'needs stronger context', 'stronger context identification', 'context identification produce', 'identification produce correct', 'produce correct analysis', 'correct analysis consistently', 'analysis consistently high', 'consistently high accuracy', 'high accuracy level', 'accuracy level .'] 

 TOTAL TRIGRAMS --> 11 



 ---- NOUN PHRASES ---- 

 ['problem', 'context identification', 'correct analysis', 'high accuracy', 'level'] 

 TOTAL NOUN PHRASES --> 5 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['problem', 'need', 'stronger', 'context', 'identif', 'produc', 'correct', 'analysi', 'consist', 'high', 'accuraci', 'level', '.']

 TOTAL PORTER STEM WORDS ==> 13



 ---- SNOWBALL STEMMING ----

['problem', 'need', 'stronger', 'context', 'identif', 'produc', 'correct', 'analysi', 'consist', 'high', 'accuraci', 'level', '.']

 TOTAL SNOWBALL STEM WORDS ==> 13



 ---- LEMMATIZATION ----

['problem', 'need', 'stronger', 'context', 'identification', 'produce', 'correct', 'analysis', 'consistently', 'high', 'accuracy', 'level', '.']

 TOTAL LEMMATIZE WORDS ==> 13

************************************************************************************************************************

32 --> Deficiency to recognise alphanumeric entities  Many entities are described with a combination of characters and  digits, especially biochemical names. 


 ---- TOKENS ----

 ['Deficiency', 'to', 'recognise', 'alphanumeric', 'entities', 'Many', 'entities', 'are', 'described', 'with', 'a', 'combination', 'of', 'characters', 'and', 'digits', ',', 'especially', 'biochemical', 'names', '.'] 

 TOTAL TOKENS ==> 21

 ---- POST ----

 [('Deficiency', 'NN'), ('to', 'TO'), ('recognise', 'VB'), ('alphanumeric', 'JJ'), ('entities', 'NNS'), ('Many', 'JJ'), ('entities', 'NNS'), ('are', 'VBP'), ('described', 'VBN'), ('with', 'IN'), ('a', 'DT'), ('combination', 'NN'), ('of', 'IN'), ('characters', 'NNS'), ('and', 'CC'), ('digits', 'NNS'), (',', ','), ('especially', 'RB'), ('biochemical', 'JJ'), ('names', 'NNS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Deficiency', 'recognise', 'alphanumeric', 'entities', 'Many', 'entities', 'described', 'combination', 'characters', 'digits', ',', 'especially', 'biochemical', 'names', '.']

 TOTAL FILTERED TOKENS ==>  15

 ---- POST FOR FILTERED TOKENS ----

 [('Deficiency', 'NNP'), ('recognise', 'NN'), ('alphanumeric', 'JJ'), ('entities', 'NNS'), ('Many', 'JJ'), ('entities', 'NNS'), ('described', 'VBN'), ('combination', 'NN'), ('characters', 'NNS'), ('digits', 'NNS'), (',', ','), ('especially', 'RB'), ('biochemical', 'JJ'), ('names', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Deficiency recognise', 'recognise alphanumeric', 'alphanumeric entities', 'entities Many', 'Many entities', 'entities described', 'described combination', 'combination characters', 'characters digits', 'digits ,', ', especially', 'especially biochemical', 'biochemical names', 'names .'] 

 TOTAL BIGRAMS --> 14 



 ---- TRI-GRAMS ---- 

 ['Deficiency recognise alphanumeric', 'recognise alphanumeric entities', 'alphanumeric entities Many', 'entities Many entities', 'Many entities described', 'entities described combination', 'described combination characters', 'combination characters digits', 'characters digits ,', 'digits , especially', ', especially biochemical', 'especially biochemical names', 'biochemical names .'] 

 TOTAL TRIGRAMS --> 13 



 ---- NOUN PHRASES ---- 

 ['recognise', 'combination'] 

 TOTAL NOUN PHRASES --> 2 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> ['Deficiency']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['defici', 'recognis', 'alphanumer', 'entiti', 'mani', 'entiti', 'describ', 'combin', 'charact', 'digit', ',', 'especi', 'biochem', 'name', '.']

 TOTAL PORTER STEM WORDS ==> 15



 ---- SNOWBALL STEMMING ----

['defici', 'recognis', 'alphanumer', 'entiti', 'mani', 'entiti', 'describ', 'combin', 'charact', 'digit', ',', 'especi', 'biochem', 'name', '.']

 TOTAL SNOWBALL STEM WORDS ==> 15



 ---- LEMMATIZATION ----

['Deficiency', 'recognise', 'alphanumeric', 'entity', 'Many', 'entity', 'described', 'combination', 'character', 'digit', ',', 'especially', 'biochemical', 'name', '.']

 TOTAL LEMMATIZE WORDS ==> 15

************************************************************************************************************************

33 --> These can be written with and  without hyphens, e.g.- HER2 and HER-2. 


 ---- TOKENS ----

 ['These', 'can', 'be', 'written', 'with', 'and', 'without', 'hyphens', ',', 'e.g.-', 'HER2', 'and', 'HER-2', '.'] 

 TOTAL TOKENS ==> 14

 ---- POST ----

 [('These', 'DT'), ('can', 'MD'), ('be', 'VB'), ('written', 'VBN'), ('with', 'IN'), ('and', 'CC'), ('without', 'IN'), ('hyphens', 'NNS'), (',', ','), ('e.g.-', 'JJ'), ('HER2', 'NNP'), ('and', 'CC'), ('HER-2', 'NNP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['written', 'without', 'hyphens', ',', 'e.g.-', 'HER2', 'HER-2', '.']

 TOTAL FILTERED TOKENS ==>  8

 ---- POST FOR FILTERED TOKENS ----

 [('written', 'VBN'), ('without', 'IN'), ('hyphens', 'NNS'), (',', ','), ('e.g.-', 'JJ'), ('HER2', 'NNP'), ('HER-2', 'NNP'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['written without', 'without hyphens', 'hyphens ,', ', e.g.-', 'e.g.- HER2', 'HER2 HER-2', 'HER-2 .'] 

 TOTAL BIGRAMS --> 7 



 ---- TRI-GRAMS ---- 

 ['written without hyphens', 'without hyphens ,', 'hyphens , e.g.-', ', e.g.- HER2', 'e.g.- HER2 HER-2', 'HER2 HER-2 .'] 

 TOTAL TRIGRAMS --> 6 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> ['HER2']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['written', 'without', 'hyphen', ',', 'e.g.-', 'her2', 'her-2', '.']

 TOTAL PORTER STEM WORDS ==> 8



 ---- SNOWBALL STEMMING ----

['written', 'without', 'hyphen', ',', 'e.g.-', 'her2', 'her-2', '.']

 TOTAL SNOWBALL STEM WORDS ==> 8



 ---- LEMMATIZATION ----

['written', 'without', 'hyphen', ',', 'e.g.-', 'HER2', 'HER-2', '.']

 TOTAL LEMMATIZE WORDS ==> 8

************************************************************************************************************************

34 --> It is not uncommon to see the  numeric component treated incorrectly as the value of the entity in  question instead of being part of its name. 


 ---- TOKENS ----

 ['It', 'is', 'not', 'uncommon', 'to', 'see', 'the', 'numeric', 'component', 'treated', 'incorrectly', 'as', 'the', 'value', 'of', 'the', 'entity', 'in', 'question', 'instead', 'of', 'being', 'part', 'of', 'its', 'name', '.'] 

 TOTAL TOKENS ==> 27

 ---- POST ----

 [('It', 'PRP'), ('is', 'VBZ'), ('not', 'RB'), ('uncommon', 'JJ'), ('to', 'TO'), ('see', 'VB'), ('the', 'DT'), ('numeric', 'JJ'), ('component', 'NN'), ('treated', 'VBD'), ('incorrectly', 'RB'), ('as', 'IN'), ('the', 'DT'), ('value', 'NN'), ('of', 'IN'), ('the', 'DT'), ('entity', 'NN'), ('in', 'IN'), ('question', 'NN'), ('instead', 'RB'), ('of', 'IN'), ('being', 'VBG'), ('part', 'NN'), ('of', 'IN'), ('its', 'PRP$'), ('name', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['uncommon', 'see', 'numeric', 'component', 'treated', 'incorrectly', 'value', 'entity', 'question', 'instead', 'part', 'name', '.']

 TOTAL FILTERED TOKENS ==>  13

 ---- POST FOR FILTERED TOKENS ----

 [('uncommon', 'JJ'), ('see', 'NN'), ('numeric', 'JJ'), ('component', 'NN'), ('treated', 'VBD'), ('incorrectly', 'RB'), ('value', 'NN'), ('entity', 'NN'), ('question', 'NN'), ('instead', 'RB'), ('part', 'NN'), ('name', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['uncommon see', 'see numeric', 'numeric component', 'component treated', 'treated incorrectly', 'incorrectly value', 'value entity', 'entity question', 'question instead', 'instead part', 'part name', 'name .'] 

 TOTAL BIGRAMS --> 12 



 ---- TRI-GRAMS ---- 

 ['uncommon see numeric', 'see numeric component', 'numeric component treated', 'component treated incorrectly', 'treated incorrectly value', 'incorrectly value entity', 'value entity question', 'entity question instead', 'question instead part', 'instead part name', 'part name .'] 

 TOTAL TRIGRAMS --> 11 



 ---- NOUN PHRASES ---- 

 ['uncommon see', 'numeric component', 'value', 'entity', 'question', 'part', 'name'] 

 TOTAL NOUN PHRASES --> 7 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['uncommon', 'see', 'numer', 'compon', 'treat', 'incorrectli', 'valu', 'entiti', 'question', 'instead', 'part', 'name', '.']

 TOTAL PORTER STEM WORDS ==> 13



 ---- SNOWBALL STEMMING ----

['uncommon', 'see', 'numer', 'compon', 'treat', 'incorrect', 'valu', 'entiti', 'question', 'instead', 'part', 'name', '.']

 TOTAL SNOWBALL STEM WORDS ==> 13



 ---- LEMMATIZATION ----

['uncommon', 'see', 'numeric', 'component', 'treated', 'incorrectly', 'value', 'entity', 'question', 'instead', 'part', 'name', '.']

 TOTAL LEMMATIZE WORDS ==> 13

************************************************************************************************************************

35 --> Inability to exclude bullet point markers from any named entity  It is common to present content as a series of bullet points to make  for easier reading. 


 ---- TOKENS ----

 ['Inability', 'to', 'exclude', 'bullet', 'point', 'markers', 'from', 'any', 'named', 'entity', 'It', 'is', 'common', 'to', 'present', 'content', 'as', 'a', 'series', 'of', 'bullet', 'points', 'to', 'make', 'for', 'easier', 'reading', '.'] 

 TOTAL TOKENS ==> 28

 ---- POST ----

 [('Inability', 'NN'), ('to', 'TO'), ('exclude', 'VB'), ('bullet', 'NN'), ('point', 'NN'), ('markers', 'NNS'), ('from', 'IN'), ('any', 'DT'), ('named', 'VBN'), ('entity', 'NN'), ('It', 'PRP'), ('is', 'VBZ'), ('common', 'JJ'), ('to', 'TO'), ('present', 'JJ'), ('content', 'NN'), ('as', 'IN'), ('a', 'DT'), ('series', 'NN'), ('of', 'IN'), ('bullet', 'NN'), ('points', 'NNS'), ('to', 'TO'), ('make', 'VB'), ('for', 'IN'), ('easier', 'JJR'), ('reading', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Inability', 'exclude', 'bullet', 'point', 'markers', 'named', 'entity', 'common', 'present', 'content', 'series', 'bullet', 'points', 'make', 'easier', 'reading', '.']

 TOTAL FILTERED TOKENS ==>  17

 ---- POST FOR FILTERED TOKENS ----

 [('Inability', 'NNP'), ('exclude', 'VBP'), ('bullet', 'NN'), ('point', 'NN'), ('markers', 'NNS'), ('named', 'VBN'), ('entity', 'NN'), ('common', 'JJ'), ('present', 'JJ'), ('content', 'JJ'), ('series', 'NN'), ('bullet', 'NN'), ('points', 'NNS'), ('make', 'VBP'), ('easier', 'JJR'), ('reading', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Inability exclude', 'exclude bullet', 'bullet point', 'point markers', 'markers named', 'named entity', 'entity common', 'common present', 'present content', 'content series', 'series bullet', 'bullet points', 'points make', 'make easier', 'easier reading', 'reading .'] 

 TOTAL BIGRAMS --> 16 



 ---- TRI-GRAMS ---- 

 ['Inability exclude bullet', 'exclude bullet point', 'bullet point markers', 'point markers named', 'markers named entity', 'named entity common', 'entity common present', 'common present content', 'present content series', 'content series bullet', 'series bullet points', 'bullet points make', 'points make easier', 'make easier reading', 'easier reading .'] 

 TOTAL TRIGRAMS --> 15 



 ---- NOUN PHRASES ---- 

 ['bullet', 'point', 'entity', 'common present content series', 'bullet', 'reading'] 

 TOTAL NOUN PHRASES --> 6 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['inabl', 'exclud', 'bullet', 'point', 'marker', 'name', 'entiti', 'common', 'present', 'content', 'seri', 'bullet', 'point', 'make', 'easier', 'read', '.']

 TOTAL PORTER STEM WORDS ==> 17



 ---- SNOWBALL STEMMING ----

['inabl', 'exclud', 'bullet', 'point', 'marker', 'name', 'entiti', 'common', 'present', 'content', 'seri', 'bullet', 'point', 'make', 'easier', 'read', '.']

 TOTAL SNOWBALL STEM WORDS ==> 17



 ---- LEMMATIZATION ----

['Inability', 'exclude', 'bullet', 'point', 'marker', 'named', 'entity', 'common', 'present', 'content', 'series', 'bullet', 'point', 'make', 'easier', 'reading', '.']

 TOTAL LEMMATIZE WORDS ==> 17

************************************************************************************************************************

36 --> The bullet identifier can be of many different forms  including digits, Roman digits in upper and lower case, dots and hyphens. 


 ---- TOKENS ----

 ['The', 'bullet', 'identifier', 'can', 'be', 'of', 'many', 'different', 'forms', 'including', 'digits', ',', 'Roman', 'digits', 'in', 'upper', 'and', 'lower', 'case', ',', 'dots', 'and', 'hyphens', '.'] 

 TOTAL TOKENS ==> 24

 ---- POST ----

 [('The', 'DT'), ('bullet', 'NN'), ('identifier', 'NN'), ('can', 'MD'), ('be', 'VB'), ('of', 'IN'), ('many', 'JJ'), ('different', 'JJ'), ('forms', 'NNS'), ('including', 'VBG'), ('digits', 'NNS'), (',', ','), ('Roman', 'NNP'), ('digits', 'VBZ'), ('in', 'IN'), ('upper', 'JJ'), ('and', 'CC'), ('lower', 'JJR'), ('case', 'NN'), (',', ','), ('dots', 'NNS'), ('and', 'CC'), ('hyphens', 'NNS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['bullet', 'identifier', 'many', 'different', 'forms', 'including', 'digits', ',', 'Roman', 'digits', 'upper', 'lower', 'case', ',', 'dots', 'hyphens', '.']

 TOTAL FILTERED TOKENS ==>  17

 ---- POST FOR FILTERED TOKENS ----

 [('bullet', 'NN'), ('identifier', 'VB'), ('many', 'JJ'), ('different', 'JJ'), ('forms', 'NNS'), ('including', 'VBG'), ('digits', 'NNS'), (',', ','), ('Roman', 'NNP'), ('digits', 'VBZ'), ('upper', 'JJ'), ('lower', 'JJR'), ('case', 'NN'), (',', ','), ('dots', 'NNS'), ('hyphens', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['bullet identifier', 'identifier many', 'many different', 'different forms', 'forms including', 'including digits', 'digits ,', ', Roman', 'Roman digits', 'digits upper', 'upper lower', 'lower case', 'case ,', ', dots', 'dots hyphens', 'hyphens .'] 

 TOTAL BIGRAMS --> 16 



 ---- TRI-GRAMS ---- 

 ['bullet identifier many', 'identifier many different', 'many different forms', 'different forms including', 'forms including digits', 'including digits ,', 'digits , Roman', ', Roman digits', 'Roman digits upper', 'digits upper lower', 'upper lower case', 'lower case ,', 'case , dots', ', dots hyphens', 'dots hyphens .'] 

 TOTAL TRIGRAMS --> 15 



 ---- NOUN PHRASES ---- 

 ['bullet', 'case'] 

 TOTAL NOUN PHRASES --> 2 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> ['Roman']
 TOTAL PERSON ENTITY --> 1 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['bullet', 'identifi', 'mani', 'differ', 'form', 'includ', 'digit', ',', 'roman', 'digit', 'upper', 'lower', 'case', ',', 'dot', 'hyphen', '.']

 TOTAL PORTER STEM WORDS ==> 17



 ---- SNOWBALL STEMMING ----

['bullet', 'identifi', 'mani', 'differ', 'form', 'includ', 'digit', ',', 'roman', 'digit', 'upper', 'lower', 'case', ',', 'dot', 'hyphen', '.']

 TOTAL SNOWBALL STEM WORDS ==> 17



 ---- LEMMATIZATION ----

['bullet', 'identifier', 'many', 'different', 'form', 'including', 'digit', ',', 'Roman', 'digit', 'upper', 'lower', 'case', ',', 'dot', 'hyphen', '.']

 TOTAL LEMMATIZE WORDS ==> 17

************************************************************************************************************************

37 --> Incorrect tokenisation has incorporated this information into a clinical  entity, so that subsequently the entity could not be correctly semantically  identified. 


 ---- TOKENS ----

 ['Incorrect', 'tokenisation', 'has', 'incorporated', 'this', 'information', 'into', 'a', 'clinical', 'entity', ',', 'so', 'that', 'subsequently', 'the', 'entity', 'could', 'not', 'be', 'correctly', 'semantically', 'identified', '.'] 

 TOTAL TOKENS ==> 23

 ---- POST ----

 [('Incorrect', 'NNP'), ('tokenisation', 'NN'), ('has', 'VBZ'), ('incorporated', 'VBN'), ('this', 'DT'), ('information', 'NN'), ('into', 'IN'), ('a', 'DT'), ('clinical', 'JJ'), ('entity', 'NN'), (',', ','), ('so', 'IN'), ('that', 'DT'), ('subsequently', 'RB'), ('the', 'DT'), ('entity', 'NN'), ('could', 'MD'), ('not', 'RB'), ('be', 'VB'), ('correctly', 'RB'), ('semantically', 'RB'), ('identified', 'VBN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Incorrect', 'tokenisation', 'incorporated', 'information', 'clinical', 'entity', ',', 'subsequently', 'entity', 'could', 'correctly', 'semantically', 'identified', '.']

 TOTAL FILTERED TOKENS ==>  14

 ---- POST FOR FILTERED TOKENS ----

 [('Incorrect', 'NNP'), ('tokenisation', 'NN'), ('incorporated', 'VBN'), ('information', 'NN'), ('clinical', 'JJ'), ('entity', 'NN'), (',', ','), ('subsequently', 'RB'), ('entity', 'NN'), ('could', 'MD'), ('correctly', 'RB'), ('semantically', 'RB'), ('identified', 'VBN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Incorrect tokenisation', 'tokenisation incorporated', 'incorporated information', 'information clinical', 'clinical entity', 'entity ,', ', subsequently', 'subsequently entity', 'entity could', 'could correctly', 'correctly semantically', 'semantically identified', 'identified .'] 

 TOTAL BIGRAMS --> 13 



 ---- TRI-GRAMS ---- 

 ['Incorrect tokenisation incorporated', 'tokenisation incorporated information', 'incorporated information clinical', 'information clinical entity', 'clinical entity ,', 'entity , subsequently', ', subsequently entity', 'subsequently entity could', 'entity could correctly', 'could correctly semantically', 'correctly semantically identified', 'semantically identified .'] 

 TOTAL TRIGRAMS --> 12 



 ---- NOUN PHRASES ---- 

 ['tokenisation', 'information', 'clinical entity', 'entity'] 

 TOTAL NOUN PHRASES --> 4 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> ['Incorrect']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['incorrect', 'tokenis', 'incorpor', 'inform', 'clinic', 'entiti', ',', 'subsequ', 'entiti', 'could', 'correctli', 'semant', 'identifi', '.']

 TOTAL PORTER STEM WORDS ==> 14



 ---- SNOWBALL STEMMING ----

['incorrect', 'tokenis', 'incorpor', 'inform', 'clinic', 'entiti', ',', 'subsequ', 'entiti', 'could', 'correct', 'semant', 'identifi', '.']

 TOTAL SNOWBALL STEM WORDS ==> 14



 ---- LEMMATIZATION ----

['Incorrect', 'tokenisation', 'incorporated', 'information', 'clinical', 'entity', ',', 'subsequently', 'entity', 'could', 'correctly', 'semantically', 'identified', '.']

 TOTAL LEMMATIZE WORDS ==> 14

************************************************************************************************************************

38 --> Faulty \Newline tokenising  We notice that different tokenisers use different ways to deal with the  newline symbol ‘\n’. 


 ---- TOKENS ----

 ['Faulty', '\\Newline', 'tokenising', 'We', 'notice', 'that', 'different', 'tokenisers', 'use', 'different', 'ways', 'to', 'deal', 'with', 'the', 'newline', 'symbol', '‘', '\\n', '’', '.'] 

 TOTAL TOKENS ==> 21

 ---- POST ----

 [('Faulty', 'NNP'), ('\\Newline', 'NN'), ('tokenising', 'VBG'), ('We', 'PRP'), ('notice', 'VBP'), ('that', 'DT'), ('different', 'JJ'), ('tokenisers', 'NNS'), ('use', 'VBP'), ('different', 'JJ'), ('ways', 'NNS'), ('to', 'TO'), ('deal', 'VB'), ('with', 'IN'), ('the', 'DT'), ('newline', 'JJ'), ('symbol', 'NN'), ('‘', 'NNP'), ('\\n', 'NNP'), ('’', 'NNP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Faulty', '\\Newline', 'tokenising', 'notice', 'different', 'tokenisers', 'use', 'different', 'ways', 'deal', 'newline', 'symbol', '‘', '\\n', '’', '.']

 TOTAL FILTERED TOKENS ==>  16

 ---- POST FOR FILTERED TOKENS ----

 [('Faulty', 'NNP'), ('\\Newline', 'NN'), ('tokenising', 'VBG'), ('notice', 'JJ'), ('different', 'JJ'), ('tokenisers', 'NNS'), ('use', 'VBP'), ('different', 'JJ'), ('ways', 'NNS'), ('deal', 'VBP'), ('newline', 'JJ'), ('symbol', 'NN'), ('‘', 'NNP'), ('\\n', 'NNP'), ('’', 'NNP'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Faulty \\Newline', '\\Newline tokenising', 'tokenising notice', 'notice different', 'different tokenisers', 'tokenisers use', 'use different', 'different ways', 'ways deal', 'deal newline', 'newline symbol', 'symbol ‘', '‘ \\n', '\\n ’', '’ .'] 

 TOTAL BIGRAMS --> 15 



 ---- TRI-GRAMS ---- 

 ['Faulty \\Newline tokenising', '\\Newline tokenising notice', 'tokenising notice different', 'notice different tokenisers', 'different tokenisers use', 'tokenisers use different', 'use different ways', 'different ways deal', 'ways deal newline', 'deal newline symbol', 'newline symbol ‘', 'symbol ‘ \\n', '‘ \\n ’', '\\n ’ .'] 

 TOTAL TRIGRAMS --> 14 



 ---- NOUN PHRASES ---- 

 ['\\Newline', 'newline symbol'] 

 TOTAL NOUN PHRASES --> 2 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> ['Faulty']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['faulti', '\\newlin', 'tokenis', 'notic', 'differ', 'tokenis', 'use', 'differ', 'way', 'deal', 'newlin', 'symbol', '‘', '\\n', '’', '.']

 TOTAL PORTER STEM WORDS ==> 16



 ---- SNOWBALL STEMMING ----

['faulti', '\\newlin', 'tokenis', 'notic', 'differ', 'tokenis', 'use', 'differ', 'way', 'deal', 'newlin', 'symbol', '‘', '\\n', '’', '.']

 TOTAL SNOWBALL STEM WORDS ==> 16



 ---- LEMMATIZATION ----

['Faulty', '\\Newline', 'tokenising', 'notice', 'different', 'tokenisers', 'use', 'different', 'way', 'deal', 'newline', 'symbol', '‘', '\\n', '’', '.']

 TOTAL LEMMATIZE WORDS ==> 16

************************************************************************************************************************

39 --> Three tokenisers do not split the input string by the  newline symbol ‘\n’. 


 ---- TOKENS ----

 ['Three', 'tokenisers', 'do', 'not', 'split', 'the', 'input', 'string', 'by', 'the', 'newline', 'symbol', '‘', '\\n', '’', '.'] 

 TOTAL TOKENS ==> 16

 ---- POST ----

 [('Three', 'CD'), ('tokenisers', 'NNS'), ('do', 'VBP'), ('not', 'RB'), ('split', 'VB'), ('the', 'DT'), ('input', 'NN'), ('string', 'NN'), ('by', 'IN'), ('the', 'DT'), ('newline', 'JJ'), ('symbol', 'NN'), ('‘', 'NNP'), ('\\n', 'NNP'), ('’', 'NNP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Three', 'tokenisers', 'split', 'input', 'string', 'newline', 'symbol', '‘', '\\n', '’', '.']

 TOTAL FILTERED TOKENS ==>  11

 ---- POST FOR FILTERED TOKENS ----

 [('Three', 'CD'), ('tokenisers', 'NNS'), ('split', 'VBD'), ('input', 'RB'), ('string', 'VBG'), ('newline', 'JJ'), ('symbol', 'NN'), ('‘', 'NNP'), ('\\n', 'NNP'), ('’', 'NNP'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Three tokenisers', 'tokenisers split', 'split input', 'input string', 'string newline', 'newline symbol', 'symbol ‘', '‘ \\n', '\\n ’', '’ .'] 

 TOTAL BIGRAMS --> 10 



 ---- TRI-GRAMS ---- 

 ['Three tokenisers split', 'tokenisers split input', 'split input string', 'input string newline', 'string newline symbol', 'newline symbol ‘', 'symbol ‘ \\n', '‘ \\n ’', '\\n ’ .'] 

 TOTAL TRIGRAMS --> 9 



 ---- NOUN PHRASES ---- 

 ['newline symbol'] 

 TOTAL NOUN PHRASES --> 1 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['three', 'tokenis', 'split', 'input', 'string', 'newlin', 'symbol', '‘', '\\n', '’', '.']

 TOTAL PORTER STEM WORDS ==> 11



 ---- SNOWBALL STEMMING ----

['three', 'tokenis', 'split', 'input', 'string', 'newlin', 'symbol', '‘', '\\n', '’', '.']

 TOTAL SNOWBALL STEM WORDS ==> 11



 ---- LEMMATIZATION ----

['Three', 'tokenisers', 'split', 'input', 'string', 'newline', 'symbol', '‘', '\\n', '’', '.']

 TOTAL LEMMATIZE WORDS ==> 11

************************************************************************************************************************

40 --> Two Tokenisers separate the tokens by backslash ‘\’  and merge n into the next word. 


 ---- TOKENS ----

 ['Two', 'Tokenisers', 'separate', 'the', 'tokens', 'by', 'backslash', '‘', '\\', '’', 'and', 'merge', 'n', 'into', 'the', 'next', 'word', '.'] 

 TOTAL TOKENS ==> 18

 ---- POST ----

 [('Two', 'CD'), ('Tokenisers', 'NNS'), ('separate', 'VBP'), ('the', 'DT'), ('tokens', 'NNS'), ('by', 'IN'), ('backslash', 'NN'), ('‘', 'NNP'), ('\\', 'NNP'), ('’', 'NNP'), ('and', 'CC'), ('merge', 'VB'), ('n', 'RB'), ('into', 'IN'), ('the', 'DT'), ('next', 'JJ'), ('word', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Two', 'Tokenisers', 'separate', 'tokens', 'backslash', '‘', '\\', '’', 'merge', 'n', 'next', 'word', '.']

 TOTAL FILTERED TOKENS ==>  13

 ---- POST FOR FILTERED TOKENS ----

 [('Two', 'CD'), ('Tokenisers', 'NNS'), ('separate', 'JJ'), ('tokens', 'NNS'), ('backslash', 'VBP'), ('‘', 'JJ'), ('\\', 'NNP'), ('’', 'NNP'), ('merge', 'NN'), ('n', 'JJ'), ('next', 'JJ'), ('word', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Two Tokenisers', 'Tokenisers separate', 'separate tokens', 'tokens backslash', 'backslash ‘', '‘ \\', '\\ ’', '’ merge', 'merge n', 'n next', 'next word', 'word .'] 

 TOTAL BIGRAMS --> 12 



 ---- TRI-GRAMS ---- 

 ['Two Tokenisers separate', 'Tokenisers separate tokens', 'separate tokens backslash', 'tokens backslash ‘', 'backslash ‘ \\', '‘ \\ ’', '\\ ’ merge', '’ merge n', 'merge n next', 'n next word', 'next word .'] 

 TOTAL TRIGRAMS --> 11 



 ---- NOUN PHRASES ---- 

 ['merge', 'n next word'] 

 TOTAL NOUN PHRASES --> 2 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['two', 'tokenis', 'separ', 'token', 'backslash', '‘', '\\', '’', 'merg', 'n', 'next', 'word', '.']

 TOTAL PORTER STEM WORDS ==> 13



 ---- SNOWBALL STEMMING ----

['two', 'tokenis', 'separ', 'token', 'backslash', '‘', '\\', '’', 'merg', 'n', 'next', 'word', '.']

 TOTAL SNOWBALL STEM WORDS ==> 13



 ---- LEMMATIZATION ----

['Two', 'Tokenisers', 'separate', 'token', 'backslash', '‘', '\\', '’', 'merge', 'n', 'next', 'word', '.']

 TOTAL LEMMATIZE WORDS ==> 13

************************************************************************************************************************

41 --> The third tokeniser keeps the whole  newline symbol ‘\n’ together as a whole token concatenated with the next  word. 


 ---- TOKENS ----

 ['The', 'third', 'tokeniser', 'keeps', 'the', 'whole', 'newline', 'symbol', '‘', '\\n', '’', 'together', 'as', 'a', 'whole', 'token', 'concatenated', 'with', 'the', 'next', 'word', '.'] 

 TOTAL TOKENS ==> 22

 ---- POST ----

 [('The', 'DT'), ('third', 'JJ'), ('tokeniser', 'NN'), ('keeps', 'VBZ'), ('the', 'DT'), ('whole', 'JJ'), ('newline', 'JJ'), ('symbol', 'NN'), ('‘', 'NNP'), ('\\n', 'NNP'), ('’', 'NNP'), ('together', 'RB'), ('as', 'IN'), ('a', 'DT'), ('whole', 'JJ'), ('token', 'NN'), ('concatenated', 'VBN'), ('with', 'IN'), ('the', 'DT'), ('next', 'JJ'), ('word', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['third', 'tokeniser', 'keeps', 'whole', 'newline', 'symbol', '‘', '\\n', '’', 'together', 'whole', 'token', 'concatenated', 'next', 'word', '.']

 TOTAL FILTERED TOKENS ==>  16

 ---- POST FOR FILTERED TOKENS ----

 [('third', 'JJ'), ('tokeniser', 'NN'), ('keeps', 'NNS'), ('whole', 'JJ'), ('newline', 'JJ'), ('symbol', 'NN'), ('‘', 'NNP'), ('\\n', 'NNP'), ('’', 'NNP'), ('together', 'RB'), ('whole', 'VBD'), ('token', 'RB'), ('concatenated', 'VBN'), ('next', 'JJ'), ('word', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['third tokeniser', 'tokeniser keeps', 'keeps whole', 'whole newline', 'newline symbol', 'symbol ‘', '‘ \\n', '\\n ’', '’ together', 'together whole', 'whole token', 'token concatenated', 'concatenated next', 'next word', 'word .'] 

 TOTAL BIGRAMS --> 15 



 ---- TRI-GRAMS ---- 

 ['third tokeniser keeps', 'tokeniser keeps whole', 'keeps whole newline', 'whole newline symbol', 'newline symbol ‘', 'symbol ‘ \\n', '‘ \\n ’', '\\n ’ together', '’ together whole', 'together whole token', 'whole token concatenated', 'token concatenated next', 'concatenated next word', 'next word .'] 

 TOTAL TRIGRAMS --> 14 



 ---- NOUN PHRASES ---- 

 ['third tokeniser', 'whole newline symbol', 'next word'] 

 TOTAL NOUN PHRASES --> 3 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['third', 'tokenis', 'keep', 'whole', 'newlin', 'symbol', '‘', '\\n', '’', 'togeth', 'whole', 'token', 'concaten', 'next', 'word', '.']

 TOTAL PORTER STEM WORDS ==> 16



 ---- SNOWBALL STEMMING ----

['third', 'tokenis', 'keep', 'whole', 'newlin', 'symbol', '‘', '\\n', '’', 'togeth', 'whole', 'token', 'concaten', 'next', 'word', '.']

 TOTAL SNOWBALL STEM WORDS ==> 16



 ---- LEMMATIZATION ----

['third', 'tokeniser', 'keep', 'whole', 'newline', 'symbol', '‘', '\\n', '’', 'together', 'whole', 'token', 'concatenated', 'next', 'word', '.']

 TOTAL LEMMATIZE WORDS ==> 16

************************************************************************************************************************

42 --> Faulty Interpretation of ‘\’  One tokeniser for a reason we don’t entirely understand changed a ‘\’ to a  ‘\\’. 


 ---- TOKENS ----

 ['Faulty', 'Interpretation', 'of', '‘', '\\', '’', 'One', 'tokeniser', 'for', 'a', 'reason', 'we', 'don', '’', 't', 'entirely', 'understand', 'changed', 'a', '‘', '\\', '’', 'to', 'a', '‘', '\\\\', '’', '.'] 

 TOTAL TOKENS ==> 28

 ---- POST ----

 [('Faulty', 'NNP'), ('Interpretation', 'NNP'), ('of', 'IN'), ('‘', 'NNP'), ('\\', 'NNP'), ('’', 'NNP'), ('One', 'NNP'), ('tokeniser', 'NN'), ('for', 'IN'), ('a', 'DT'), ('reason', 'NN'), ('we', 'PRP'), ('don', 'VBP'), ('’', 'JJ'), ('t', 'NN'), ('entirely', 'RB'), ('understand', 'VBP'), ('changed', 'VBD'), ('a', 'DT'), ('‘', 'JJ'), ('\\', 'NN'), ('’', 'NN'), ('to', 'TO'), ('a', 'DT'), ('‘', 'JJ'), ('\\\\', 'NN'), ('’', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Faulty', 'Interpretation', '‘', '\\', '’', 'One', 'tokeniser', 'reason', '’', 'entirely', 'understand', 'changed', '‘', '\\', '’', '‘', '\\\\', '’', '.']

 TOTAL FILTERED TOKENS ==>  19

 ---- POST FOR FILTERED TOKENS ----

 [('Faulty', 'NNP'), ('Interpretation', 'NNP'), ('‘', 'NNP'), ('\\', 'NNP'), ('’', 'NNP'), ('One', 'NNP'), ('tokeniser', 'NN'), ('reason', 'NN'), ('’', 'NNP'), ('entirely', 'RB'), ('understand', 'VBP'), ('changed', 'VBN'), ('‘', 'NNP'), ('\\', 'NNP'), ('’', 'NNP'), ('‘', 'NNP'), ('\\\\', 'NNP'), ('’', 'NNP'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Faulty Interpretation', 'Interpretation ‘', '‘ \\', '\\ ’', '’ One', 'One tokeniser', 'tokeniser reason', 'reason ’', '’ entirely', 'entirely understand', 'understand changed', 'changed ‘', '‘ \\', '\\ ’', '’ ‘', '‘ \\\\', '\\\\ ’', '’ .'] 

 TOTAL BIGRAMS --> 18 



 ---- TRI-GRAMS ---- 

 ['Faulty Interpretation ‘', 'Interpretation ‘ \\', '‘ \\ ’', '\\ ’ One', '’ One tokeniser', 'One tokeniser reason', 'tokeniser reason ’', 'reason ’ entirely', '’ entirely understand', 'entirely understand changed', 'understand changed ‘', 'changed ‘ \\', '‘ \\ ’', '\\ ’ ‘', '’ ‘ \\\\', '‘ \\\\ ’', '\\\\ ’ .'] 

 TOTAL TRIGRAMS --> 17 



 ---- NOUN PHRASES ---- 

 ['tokeniser', 'reason'] 

 TOTAL NOUN PHRASES --> 2 



 ---- NER ----

 
 ORGANIZATION ---> ['Interpretation']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> ['Faulty']
 TOTAL PERSON ENTITY --> 1 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['faulti', 'interpret', '‘', '\\', '’', 'one', 'tokenis', 'reason', '’', 'entir', 'understand', 'chang', '‘', '\\', '’', '‘', '\\\\', '’', '.']

 TOTAL PORTER STEM WORDS ==> 19



 ---- SNOWBALL STEMMING ----

['faulti', 'interpret', '‘', '\\', '’', 'one', 'tokenis', 'reason', '’', 'entir', 'understand', 'chang', '‘', '\\', '’', '‘', '\\\\', '’', '.']

 TOTAL SNOWBALL STEM WORDS ==> 19



 ---- LEMMATIZATION ----

['Faulty', 'Interpretation', '‘', '\\', '’', 'One', 'tokeniser', 'reason', '’', 'entirely', 'understand', 'changed', '‘', '\\', '’', '‘', '\\\\', '’', '.']

 TOTAL LEMMATIZE WORDS ==> 19

************************************************************************************************************************

43 --> While their subsequent processing seemed to cope with this shift we  found that all our annotations were made incorrect as to their position due  to the introduction of new characters. 


 ---- TOKENS ----

 ['While', 'their', 'subsequent', 'processing', 'seemed', 'to', 'cope', 'with', 'this', 'shift', 'we', 'found', 'that', 'all', 'our', 'annotations', 'were', 'made', 'incorrect', 'as', 'to', 'their', 'position', 'due', 'to', 'the', 'introduction', 'of', 'new', 'characters', '.'] 

 TOTAL TOKENS ==> 31

 ---- POST ----

 [('While', 'IN'), ('their', 'PRP$'), ('subsequent', 'JJ'), ('processing', 'NN'), ('seemed', 'VBD'), ('to', 'TO'), ('cope', 'VB'), ('with', 'IN'), ('this', 'DT'), ('shift', 'NN'), ('we', 'PRP'), ('found', 'VBD'), ('that', 'IN'), ('all', 'DT'), ('our', 'PRP$'), ('annotations', 'NNS'), ('were', 'VBD'), ('made', 'VBN'), ('incorrect', 'NN'), ('as', 'IN'), ('to', 'TO'), ('their', 'PRP$'), ('position', 'NN'), ('due', 'JJ'), ('to', 'TO'), ('the', 'DT'), ('introduction', 'NN'), ('of', 'IN'), ('new', 'JJ'), ('characters', 'NNS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['subsequent', 'processing', 'seemed', 'cope', 'shift', 'found', 'annotations', 'made', 'incorrect', 'position', 'due', 'introduction', 'new', 'characters', '.']

 TOTAL FILTERED TOKENS ==>  15

 ---- POST FOR FILTERED TOKENS ----

 [('subsequent', 'JJ'), ('processing', 'NN'), ('seemed', 'VBD'), ('cope', 'JJ'), ('shift', 'NN'), ('found', 'VBD'), ('annotations', 'NNS'), ('made', 'VBN'), ('incorrect', 'JJ'), ('position', 'NN'), ('due', 'JJ'), ('introduction', 'NN'), ('new', 'JJ'), ('characters', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['subsequent processing', 'processing seemed', 'seemed cope', 'cope shift', 'shift found', 'found annotations', 'annotations made', 'made incorrect', 'incorrect position', 'position due', 'due introduction', 'introduction new', 'new characters', 'characters .'] 

 TOTAL BIGRAMS --> 14 



 ---- TRI-GRAMS ---- 

 ['subsequent processing seemed', 'processing seemed cope', 'seemed cope shift', 'cope shift found', 'shift found annotations', 'found annotations made', 'annotations made incorrect', 'made incorrect position', 'incorrect position due', 'position due introduction', 'due introduction new', 'introduction new characters', 'new characters .'] 

 TOTAL TRIGRAMS --> 13 



 ---- NOUN PHRASES ---- 

 ['subsequent processing', 'cope shift', 'incorrect position', 'due introduction'] 

 TOTAL NOUN PHRASES --> 4 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['subsequ', 'process', 'seem', 'cope', 'shift', 'found', 'annot', 'made', 'incorrect', 'posit', 'due', 'introduct', 'new', 'charact', '.']

 TOTAL PORTER STEM WORDS ==> 15



 ---- SNOWBALL STEMMING ----

['subsequ', 'process', 'seem', 'cope', 'shift', 'found', 'annot', 'made', 'incorrect', 'posit', 'due', 'introduct', 'new', 'charact', '.']

 TOTAL SNOWBALL STEM WORDS ==> 15



 ---- LEMMATIZATION ----

['subsequent', 'processing', 'seemed', 'cope', 'shift', 'found', 'annotation', 'made', 'incorrect', 'position', 'due', 'introduction', 'new', 'character', '.']

 TOTAL LEMMATIZE WORDS ==> 15

************************************************************************************************************************

44 --> Faulty Special Symbol Tokenising  One of the tokenisers did not recognise these symbols, {‘|’,’^’,’~’},  and treated them together with the neighbouring words giving faulty  outcomes. 


 ---- TOKENS ----

 ['Faulty', 'Special', 'Symbol', 'Tokenising', 'One', 'of', 'the', 'tokenisers', 'did', 'not', 'recognise', 'these', 'symbols', ',', '{', '‘', '|', '’', ',', '’', '^', '’', ',', '’', '~', '’', '}', ',', 'and', 'treated', 'them', 'together', 'with', 'the', 'neighbouring', 'words', 'giving', 'faulty', 'outcomes', '.'] 

 TOTAL TOKENS ==> 40

 ---- POST ----

 [('Faulty', 'NNP'), ('Special', 'NNP'), ('Symbol', 'NNP'), ('Tokenising', 'NNP'), ('One', 'CD'), ('of', 'IN'), ('the', 'DT'), ('tokenisers', 'NNS'), ('did', 'VBD'), ('not', 'RB'), ('recognise', 'VB'), ('these', 'DT'), ('symbols', 'NNS'), (',', ','), ('{', '('), ('‘', 'JJ'), ('|', 'NN'), ('’', 'NN'), (',', ','), ('’', 'NNP'), ('^', 'NNP'), ('’', 'NNP'), (',', ','), ('’', 'NNP'), ('~', 'NNP'), ('’', 'NNP'), ('}', ')'), (',', ','), ('and', 'CC'), ('treated', 'VBD'), ('them', 'PRP'), ('together', 'RB'), ('with', 'IN'), ('the', 'DT'), ('neighbouring', 'JJ'), ('words', 'NNS'), ('giving', 'VBG'), ('faulty', 'NN'), ('outcomes', 'NNS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Faulty', 'Special', 'Symbol', 'Tokenising', 'One', 'tokenisers', 'recognise', 'symbols', ',', '{', '‘', '|', '’', ',', '’', '^', '’', ',', '’', '~', '’', '}', ',', 'treated', 'together', 'neighbouring', 'words', 'giving', 'faulty', 'outcomes', '.']

 TOTAL FILTERED TOKENS ==>  31

 ---- POST FOR FILTERED TOKENS ----

 [('Faulty', 'NNP'), ('Special', 'NNP'), ('Symbol', 'NNP'), ('Tokenising', 'NNP'), ('One', 'NNP'), ('tokenisers', 'VBZ'), ('recognise', 'NN'), ('symbols', 'NNS'), (',', ','), ('{', '('), ('‘', 'JJ'), ('|', 'NN'), ('’', 'NN'), (',', ','), ('’', 'NNP'), ('^', 'NNP'), ('’', 'NNP'), (',', ','), ('’', 'NNP'), ('~', 'NNP'), ('’', 'NNP'), ('}', ')'), (',', ','), ('treated', 'VBD'), ('together', 'RB'), ('neighbouring', 'JJ'), ('words', 'NNS'), ('giving', 'VBG'), ('faulty', 'NN'), ('outcomes', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Faulty Special', 'Special Symbol', 'Symbol Tokenising', 'Tokenising One', 'One tokenisers', 'tokenisers recognise', 'recognise symbols', 'symbols ,', ', {', '{ ‘', '‘ |', '| ’', '’ ,', ', ’', '’ ^', '^ ’', '’ ,', ', ’', '’ ~', '~ ’', '’ }', '} ,', ', treated', 'treated together', 'together neighbouring', 'neighbouring words', 'words giving', 'giving faulty', 'faulty outcomes', 'outcomes .'] 

 TOTAL BIGRAMS --> 30 



 ---- TRI-GRAMS ---- 

 ['Faulty Special Symbol', 'Special Symbol Tokenising', 'Symbol Tokenising One', 'Tokenising One tokenisers', 'One tokenisers recognise', 'tokenisers recognise symbols', 'recognise symbols ,', 'symbols , {', ', { ‘', '{ ‘ |', '‘ | ’', '| ’ ,', '’ , ’', ', ’ ^', '’ ^ ’', '^ ’ ,', '’ , ’', ', ’ ~', '’ ~ ’', '~ ’ }', '’ } ,', '} , treated', ', treated together', 'treated together neighbouring', 'together neighbouring words', 'neighbouring words giving', 'words giving faulty', 'giving faulty outcomes', 'faulty outcomes .'] 

 TOTAL TRIGRAMS --> 29 



 ---- NOUN PHRASES ---- 

 ['recognise', 'faulty'] 

 TOTAL NOUN PHRASES --> 2 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> ['Faulty']
 TOTAL PERSON ENTITY --> 1 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['faulti', 'special', 'symbol', 'tokenis', 'one', 'tokenis', 'recognis', 'symbol', ',', '{', '‘', '|', '’', ',', '’', '^', '’', ',', '’', '~', '’', '}', ',', 'treat', 'togeth', 'neighbour', 'word', 'give', 'faulti', 'outcom', '.']

 TOTAL PORTER STEM WORDS ==> 31



 ---- SNOWBALL STEMMING ----

['faulti', 'special', 'symbol', 'tokenis', 'one', 'tokenis', 'recognis', 'symbol', ',', '{', '‘', '|', '’', ',', '’', '^', '’', ',', '’', '~', '’', '}', ',', 'treat', 'togeth', 'neighbour', 'word', 'give', 'faulti', 'outcom', '.']

 TOTAL SNOWBALL STEM WORDS ==> 31



 ---- LEMMATIZATION ----

['Faulty', 'Special', 'Symbol', 'Tokenising', 'One', 'tokenisers', 'recognise', 'symbol', ',', '{', '‘', '|', '’', ',', '’', '^', '’', ',', '’', '~', '’', '}', ',', 'treated', 'together', 'neighbouring', 'word', 'giving', 'faulty', 'outcome', '.']

 TOTAL LEMMATIZE WORDS ==> 31

************************************************************************************************************************

45 --> Problematic Interpretation of the Hyphen ‘-‘  The use of the hyphen is ambiguous for CNLP. 


 ---- TOKENS ----

 ['Problematic', 'Interpretation', 'of', 'the', 'Hyphen', '‘', '-', '‘', 'The', 'use', 'of', 'the', 'hyphen', 'is', 'ambiguous', 'for', 'CNLP', '.'] 

 TOTAL TOKENS ==> 18

 ---- POST ----

 [('Problematic', 'JJ'), ('Interpretation', 'NNP'), ('of', 'IN'), ('the', 'DT'), ('Hyphen', 'NNP'), ('‘', 'NNP'), ('-', ':'), ('‘', 'VBP'), ('The', 'DT'), ('use', 'NN'), ('of', 'IN'), ('the', 'DT'), ('hyphen', 'NN'), ('is', 'VBZ'), ('ambiguous', 'JJ'), ('for', 'IN'), ('CNLP', 'NNP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Problematic', 'Interpretation', 'Hyphen', '‘', '-', '‘', 'use', 'hyphen', 'ambiguous', 'CNLP', '.']

 TOTAL FILTERED TOKENS ==>  11

 ---- POST FOR FILTERED TOKENS ----

 [('Problematic', 'JJ'), ('Interpretation', 'NNP'), ('Hyphen', 'NNP'), ('‘', 'NNP'), ('-', ':'), ('‘', 'NN'), ('use', 'NN'), ('hyphen', 'NN'), ('ambiguous', 'JJ'), ('CNLP', 'NNP'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Problematic Interpretation', 'Interpretation Hyphen', 'Hyphen ‘', '‘ -', '- ‘', '‘ use', 'use hyphen', 'hyphen ambiguous', 'ambiguous CNLP', 'CNLP .'] 

 TOTAL BIGRAMS --> 10 



 ---- TRI-GRAMS ---- 

 ['Problematic Interpretation Hyphen', 'Interpretation Hyphen ‘', 'Hyphen ‘ -', '‘ - ‘', '- ‘ use', '‘ use hyphen', 'use hyphen ambiguous', 'hyphen ambiguous CNLP', 'ambiguous CNLP .'] 

 TOTAL TRIGRAMS --> 9 



 ---- NOUN PHRASES ---- 

 ['‘', 'use', 'hyphen'] 

 TOTAL NOUN PHRASES --> 3 



 ---- NER ----

 
 ORGANIZATION ---> ['Interpretation Hyphen', 'CNLP']
 TOTAL ORGANIZATION ENTITY --> 2 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['problemat', 'interpret', 'hyphen', '‘', '-', '‘', 'use', 'hyphen', 'ambigu', 'cnlp', '.']

 TOTAL PORTER STEM WORDS ==> 11



 ---- SNOWBALL STEMMING ----

['problemat', 'interpret', 'hyphen', '‘', '-', '‘', 'use', 'hyphen', 'ambigu', 'cnlp', '.']

 TOTAL SNOWBALL STEM WORDS ==> 11



 ---- LEMMATIZATION ----

['Problematic', 'Interpretation', 'Hyphen', '‘', '-', '‘', 'use', 'hyphen', 'ambiguous', 'CNLP', '.']

 TOTAL LEMMATIZE WORDS ==> 11

************************************************************************************************************************

46 --> It can used to join two  concepts together and to separate two discrete concepts from each other. 


 ---- TOKENS ----

 ['It', 'can', 'used', 'to', 'join', 'two', 'concepts', 'together', 'and', 'to', 'separate', 'two', 'discrete', 'concepts', 'from', 'each', 'other', '.'] 

 TOTAL TOKENS ==> 18

 ---- POST ----

 [('It', 'PRP'), ('can', 'MD'), ('used', 'VBN'), ('to', 'TO'), ('join', 'VB'), ('two', 'CD'), ('concepts', 'NNS'), ('together', 'RB'), ('and', 'CC'), ('to', 'TO'), ('separate', 'VB'), ('two', 'CD'), ('discrete', 'JJ'), ('concepts', 'NNS'), ('from', 'IN'), ('each', 'DT'), ('other', 'JJ'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['used', 'join', 'two', 'concepts', 'together', 'separate', 'two', 'discrete', 'concepts', '.']

 TOTAL FILTERED TOKENS ==>  10

 ---- POST FOR FILTERED TOKENS ----

 [('used', 'VBN'), ('join', 'NN'), ('two', 'CD'), ('concepts', 'NNS'), ('together', 'RB'), ('separate', 'VBP'), ('two', 'CD'), ('discrete', 'JJ'), ('concepts', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['used join', 'join two', 'two concepts', 'concepts together', 'together separate', 'separate two', 'two discrete', 'discrete concepts', 'concepts .'] 

 TOTAL BIGRAMS --> 9 



 ---- TRI-GRAMS ---- 

 ['used join two', 'join two concepts', 'two concepts together', 'concepts together separate', 'together separate two', 'separate two discrete', 'two discrete concepts', 'discrete concepts .'] 

 TOTAL TRIGRAMS --> 8 



 ---- NOUN PHRASES ---- 

 ['join'] 

 TOTAL NOUN PHRASES --> 1 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['use', 'join', 'two', 'concept', 'togeth', 'separ', 'two', 'discret', 'concept', '.']

 TOTAL PORTER STEM WORDS ==> 10



 ---- SNOWBALL STEMMING ----

['use', 'join', 'two', 'concept', 'togeth', 'separ', 'two', 'discret', 'concept', '.']

 TOTAL SNOWBALL STEM WORDS ==> 10



 ---- LEMMATIZATION ----

['used', 'join', 'two', 'concept', 'together', 'separate', 'two', 'discrete', 'concept', '.']

 TOTAL LEMMATIZE WORDS ==> 10

************************************************************************************************************************

47 --> Our policy is to separate lexical elements either side of a hyphen and  interpret each individually. 


 ---- TOKENS ----

 ['Our', 'policy', 'is', 'to', 'separate', 'lexical', 'elements', 'either', 'side', 'of', 'a', 'hyphen', 'and', 'interpret', 'each', 'individually', '.'] 

 TOTAL TOKENS ==> 17

 ---- POST ----

 [('Our', 'PRP$'), ('policy', 'NN'), ('is', 'VBZ'), ('to', 'TO'), ('separate', 'VB'), ('lexical', 'JJ'), ('elements', 'NNS'), ('either', 'DT'), ('side', 'NN'), ('of', 'IN'), ('a', 'DT'), ('hyphen', 'NN'), ('and', 'CC'), ('interpret', 'VB'), ('each', 'DT'), ('individually', 'RB'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['policy', 'separate', 'lexical', 'elements', 'either', 'side', 'hyphen', 'interpret', 'individually', '.']

 TOTAL FILTERED TOKENS ==>  10

 ---- POST FOR FILTERED TOKENS ----

 [('policy', 'NN'), ('separate', 'JJ'), ('lexical', 'JJ'), ('elements', 'NNS'), ('either', 'CC'), ('side', 'NN'), ('hyphen', 'NN'), ('interpret', 'NN'), ('individually', 'RB'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['policy separate', 'separate lexical', 'lexical elements', 'elements either', 'either side', 'side hyphen', 'hyphen interpret', 'interpret individually', 'individually .'] 

 TOTAL BIGRAMS --> 9 



 ---- TRI-GRAMS ---- 

 ['policy separate lexical', 'separate lexical elements', 'lexical elements either', 'elements either side', 'either side hyphen', 'side hyphen interpret', 'hyphen interpret individually', 'interpret individually .'] 

 TOTAL TRIGRAMS --> 8 



 ---- NOUN PHRASES ---- 

 ['policy', 'side', 'hyphen', 'interpret'] 

 TOTAL NOUN PHRASES --> 4 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['polici', 'separ', 'lexic', 'element', 'either', 'side', 'hyphen', 'interpret', 'individu', '.']

 TOTAL PORTER STEM WORDS ==> 10



 ---- SNOWBALL STEMMING ----

['polici', 'separ', 'lexic', 'element', 'either', 'side', 'hyphen', 'interpret', 'individu', '.']

 TOTAL SNOWBALL STEM WORDS ==> 10



 ---- LEMMATIZATION ----

['policy', 'separate', 'lexical', 'element', 'either', 'side', 'hyphen', 'interpret', 'individually', '.']

 TOTAL LEMMATIZE WORDS ==> 10

************************************************************************************************************************

48 --> However, the three examined tokenisers do  not split by hyphen and treat the whole combination as one single token. 


 ---- TOKENS ----

 ['However', ',', 'the', 'three', 'examined', 'tokenisers', 'do', 'not', 'split', 'by', 'hyphen', 'and', 'treat', 'the', 'whole', 'combination', 'as', 'one', 'single', 'token', '.'] 

 TOTAL TOKENS ==> 21

 ---- POST ----

 [('However', 'RB'), (',', ','), ('the', 'DT'), ('three', 'CD'), ('examined', 'VBD'), ('tokenisers', 'NNS'), ('do', 'VBP'), ('not', 'RB'), ('split', 'VB'), ('by', 'IN'), ('hyphen', 'NN'), ('and', 'CC'), ('treat', 'VB'), ('the', 'DT'), ('whole', 'JJ'), ('combination', 'NN'), ('as', 'IN'), ('one', 'CD'), ('single', 'JJ'), ('token', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['However', ',', 'three', 'examined', 'tokenisers', 'split', 'hyphen', 'treat', 'whole', 'combination', 'one', 'single', 'token', '.']

 TOTAL FILTERED TOKENS ==>  14

 ---- POST FOR FILTERED TOKENS ----

 [('However', 'RB'), (',', ','), ('three', 'CD'), ('examined', 'VBD'), ('tokenisers', 'NNS'), ('split', 'VBD'), ('hyphen', 'RB'), ('treat', 'JJ'), ('whole', 'JJ'), ('combination', 'NN'), ('one', 'CD'), ('single', 'JJ'), ('token', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['However ,', ', three', 'three examined', 'examined tokenisers', 'tokenisers split', 'split hyphen', 'hyphen treat', 'treat whole', 'whole combination', 'combination one', 'one single', 'single token', 'token .'] 

 TOTAL BIGRAMS --> 13 



 ---- TRI-GRAMS ---- 

 ['However , three', ', three examined', 'three examined tokenisers', 'examined tokenisers split', 'tokenisers split hyphen', 'split hyphen treat', 'hyphen treat whole', 'treat whole combination', 'whole combination one', 'combination one single', 'one single token', 'single token .'] 

 TOTAL TRIGRAMS --> 12 



 ---- NOUN PHRASES ---- 

 ['treat whole combination', 'single token'] 

 TOTAL NOUN PHRASES --> 2 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['howev', ',', 'three', 'examin', 'tokenis', 'split', 'hyphen', 'treat', 'whole', 'combin', 'one', 'singl', 'token', '.']

 TOTAL PORTER STEM WORDS ==> 14



 ---- SNOWBALL STEMMING ----

['howev', ',', 'three', 'examin', 'tokenis', 'split', 'hyphen', 'treat', 'whole', 'combin', 'one', 'singl', 'token', '.']

 TOTAL SNOWBALL STEM WORDS ==> 14



 ---- LEMMATIZATION ----

['However', ',', 'three', 'examined', 'tokenisers', 'split', 'hyphen', 'treat', 'whole', 'combination', 'one', 'single', 'token', '.']

 TOTAL LEMMATIZE WORDS ==> 14

************************************************************************************************************************

49 --> This is bad practice. 


 ---- TOKENS ----

 ['This', 'is', 'bad', 'practice', '.'] 

 TOTAL TOKENS ==> 5

 ---- POST ----

 [('This', 'DT'), ('is', 'VBZ'), ('bad', 'JJ'), ('practice', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['bad', 'practice', '.']

 TOTAL FILTERED TOKENS ==>  3

 ---- POST FOR FILTERED TOKENS ----

 [('bad', 'JJ'), ('practice', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['bad practice', 'practice .'] 

 TOTAL BIGRAMS --> 2 



 ---- TRI-GRAMS ---- 

 ['bad practice .'] 

 TOTAL TRIGRAMS --> 1 



 ---- NOUN PHRASES ---- 

 ['bad practice'] 

 TOTAL NOUN PHRASES --> 1 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['bad', 'practic', '.']

 TOTAL PORTER STEM WORDS ==> 3



 ---- SNOWBALL STEMMING ----

['bad', 'practic', '.']

 TOTAL SNOWBALL STEM WORDS ==> 3



 ---- LEMMATIZATION ----

['bad', 'practice', '.']

 TOTAL LEMMATIZE WORDS ==> 3

************************************************************************************************************************

50 --> Faulty Alphanumeric String Processing  Alphanumeric strings should usually be kept intact for clinical  processing as they most often represent a unit record identifier of some  sort. 


 ---- TOKENS ----

 ['Faulty', 'Alphanumeric', 'String', 'Processing', 'Alphanumeric', 'strings', 'should', 'usually', 'be', 'kept', 'intact', 'for', 'clinical', 'processing', 'as', 'they', 'most', 'often', 'represent', 'a', 'unit', 'record', 'identifier', 'of', 'some', 'sort', '.'] 

 TOTAL TOKENS ==> 27

 ---- POST ----

 [('Faulty', 'NNP'), ('Alphanumeric', 'NNP'), ('String', 'NNP'), ('Processing', 'NNP'), ('Alphanumeric', 'NNP'), ('strings', 'NNS'), ('should', 'MD'), ('usually', 'RB'), ('be', 'VB'), ('kept', 'VBN'), ('intact', 'JJ'), ('for', 'IN'), ('clinical', 'JJ'), ('processing', 'NN'), ('as', 'IN'), ('they', 'PRP'), ('most', 'RBS'), ('often', 'RB'), ('represent', 'VBP'), ('a', 'DT'), ('unit', 'NN'), ('record', 'NN'), ('identifier', 'NN'), ('of', 'IN'), ('some', 'DT'), ('sort', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Faulty', 'Alphanumeric', 'String', 'Processing', 'Alphanumeric', 'strings', 'usually', 'kept', 'intact', 'clinical', 'processing', 'often', 'represent', 'unit', 'record', 'identifier', 'sort', '.']

 TOTAL FILTERED TOKENS ==>  18

 ---- POST FOR FILTERED TOKENS ----

 [('Faulty', 'NNP'), ('Alphanumeric', 'NNP'), ('String', 'NNP'), ('Processing', 'NNP'), ('Alphanumeric', 'NNP'), ('strings', 'NNS'), ('usually', 'RB'), ('kept', 'VBD'), ('intact', 'JJ'), ('clinical', 'JJ'), ('processing', 'NN'), ('often', 'RB'), ('represent', 'JJ'), ('unit', 'NN'), ('record', 'NN'), ('identifier', 'NN'), ('sort', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Faulty Alphanumeric', 'Alphanumeric String', 'String Processing', 'Processing Alphanumeric', 'Alphanumeric strings', 'strings usually', 'usually kept', 'kept intact', 'intact clinical', 'clinical processing', 'processing often', 'often represent', 'represent unit', 'unit record', 'record identifier', 'identifier sort', 'sort .'] 

 TOTAL BIGRAMS --> 17 



 ---- TRI-GRAMS ---- 

 ['Faulty Alphanumeric String', 'Alphanumeric String Processing', 'String Processing Alphanumeric', 'Processing Alphanumeric strings', 'Alphanumeric strings usually', 'strings usually kept', 'usually kept intact', 'kept intact clinical', 'intact clinical processing', 'clinical processing often', 'processing often represent', 'often represent unit', 'represent unit record', 'unit record identifier', 'record identifier sort', 'identifier sort .'] 

 TOTAL TRIGRAMS --> 16 



 ---- NOUN PHRASES ---- 

 ['intact clinical processing', 'represent unit', 'record', 'identifier', 'sort'] 

 TOTAL NOUN PHRASES --> 5 



 ---- NER ----

 
 ORGANIZATION ---> ['Alphanumeric']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> ['Faulty']
 TOTAL PERSON ENTITY --> 1 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['faulti', 'alphanumer', 'string', 'process', 'alphanumer', 'string', 'usual', 'kept', 'intact', 'clinic', 'process', 'often', 'repres', 'unit', 'record', 'identifi', 'sort', '.']

 TOTAL PORTER STEM WORDS ==> 18



 ---- SNOWBALL STEMMING ----

['faulti', 'alphanumer', 'string', 'process', 'alphanumer', 'string', 'usual', 'kept', 'intact', 'clinic', 'process', 'often', 'repres', 'unit', 'record', 'identifi', 'sort', '.']

 TOTAL SNOWBALL STEM WORDS ==> 18



 ---- LEMMATIZATION ----

['Faulty', 'Alphanumeric', 'String', 'Processing', 'Alphanumeric', 'string', 'usually', 'kept', 'intact', 'clinical', 'processing', 'often', 'represent', 'unit', 'record', 'identifier', 'sort', '.']

 TOTAL LEMMATIZE WORDS ==> 18

************************************************************************************************************************

51 --> One tokeniser split the string at character-type boundaries resulting  in false identification. 


 ---- TOKENS ----

 ['One', 'tokeniser', 'split', 'the', 'string', 'at', 'character-type', 'boundaries', 'resulting', 'in', 'false', 'identification', '.'] 

 TOTAL TOKENS ==> 13

 ---- POST ----

 [('One', 'CD'), ('tokeniser', 'NN'), ('split', 'VBD'), ('the', 'DT'), ('string', 'NN'), ('at', 'IN'), ('character-type', 'JJ'), ('boundaries', 'NNS'), ('resulting', 'VBG'), ('in', 'IN'), ('false', 'JJ'), ('identification', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['One', 'tokeniser', 'split', 'string', 'character-type', 'boundaries', 'resulting', 'false', 'identification', '.']

 TOTAL FILTERED TOKENS ==>  10

 ---- POST FOR FILTERED TOKENS ----

 [('One', 'CD'), ('tokeniser', 'NN'), ('split', 'NN'), ('string', 'VBG'), ('character-type', 'JJ'), ('boundaries', 'NNS'), ('resulting', 'VBG'), ('false', 'JJ'), ('identification', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['One tokeniser', 'tokeniser split', 'split string', 'string character-type', 'character-type boundaries', 'boundaries resulting', 'resulting false', 'false identification', 'identification .'] 

 TOTAL BIGRAMS --> 9 



 ---- TRI-GRAMS ---- 

 ['One tokeniser split', 'tokeniser split string', 'split string character-type', 'string character-type boundaries', 'character-type boundaries resulting', 'boundaries resulting false', 'resulting false identification', 'false identification .'] 

 TOTAL TRIGRAMS --> 8 



 ---- NOUN PHRASES ---- 

 ['tokeniser', 'split', 'false identification'] 

 TOTAL NOUN PHRASES --> 3 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['one', 'tokenis', 'split', 'string', 'character-typ', 'boundari', 'result', 'fals', 'identif', '.']

 TOTAL PORTER STEM WORDS ==> 10



 ---- SNOWBALL STEMMING ----

['one', 'tokenis', 'split', 'string', 'character-typ', 'boundari', 'result', 'fals', 'identif', '.']

 TOTAL SNOWBALL STEM WORDS ==> 10



 ---- LEMMATIZATION ----

['One', 'tokeniser', 'split', 'string', 'character-type', 'boundary', 'resulting', 'false', 'identification', '.']

 TOTAL LEMMATIZE WORDS ==> 10

************************************************************************************************************************

52 --> Faulty tokenisation of real valued numbers  One tokeniser would split real numbers on the decimal point so as to  create 3 tokens. 


 ---- TOKENS ----

 ['Faulty', 'tokenisation', 'of', 'real', 'valued', 'numbers', 'One', 'tokeniser', 'would', 'split', 'real', 'numbers', 'on', 'the', 'decimal', 'point', 'so', 'as', 'to', 'create', '3', 'tokens', '.'] 

 TOTAL TOKENS ==> 23

 ---- POST ----

 [('Faulty', 'NNP'), ('tokenisation', 'NN'), ('of', 'IN'), ('real', 'JJ'), ('valued', 'VBN'), ('numbers', 'NNS'), ('One', 'CD'), ('tokeniser', 'NN'), ('would', 'MD'), ('split', 'VB'), ('real', 'JJ'), ('numbers', 'NNS'), ('on', 'IN'), ('the', 'DT'), ('decimal', 'JJ'), ('point', 'NN'), ('so', 'RB'), ('as', 'IN'), ('to', 'TO'), ('create', 'VB'), ('3', 'CD'), ('tokens', 'NNS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Faulty', 'tokenisation', 'real', 'valued', 'numbers', 'One', 'tokeniser', 'would', 'split', 'real', 'numbers', 'decimal', 'point', 'create', '3', 'tokens', '.']

 TOTAL FILTERED TOKENS ==>  17

 ---- POST FOR FILTERED TOKENS ----

 [('Faulty', 'NNP'), ('tokenisation', 'NN'), ('real', 'JJ'), ('valued', 'VBN'), ('numbers', 'NNS'), ('One', 'CD'), ('tokeniser', 'NN'), ('would', 'MD'), ('split', 'VB'), ('real', 'JJ'), ('numbers', 'NNS'), ('decimal', 'VBP'), ('point', 'NN'), ('create', 'NN'), ('3', 'CD'), ('tokens', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Faulty tokenisation', 'tokenisation real', 'real valued', 'valued numbers', 'numbers One', 'One tokeniser', 'tokeniser would', 'would split', 'split real', 'real numbers', 'numbers decimal', 'decimal point', 'point create', 'create 3', '3 tokens', 'tokens .'] 

 TOTAL BIGRAMS --> 16 



 ---- TRI-GRAMS ---- 

 ['Faulty tokenisation real', 'tokenisation real valued', 'real valued numbers', 'valued numbers One', 'numbers One tokeniser', 'One tokeniser would', 'tokeniser would split', 'would split real', 'split real numbers', 'real numbers decimal', 'numbers decimal point', 'decimal point create', 'point create 3', 'create 3 tokens', '3 tokens .'] 

 TOTAL TRIGRAMS --> 15 



 ---- NOUN PHRASES ---- 

 ['tokenisation', 'tokeniser', 'point', 'create'] 

 TOTAL NOUN PHRASES --> 4 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> ['Faulty']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['faulti', 'tokenis', 'real', 'valu', 'number', 'one', 'tokenis', 'would', 'split', 'real', 'number', 'decim', 'point', 'creat', '3', 'token', '.']

 TOTAL PORTER STEM WORDS ==> 17



 ---- SNOWBALL STEMMING ----

['faulti', 'tokenis', 'real', 'valu', 'number', 'one', 'tokenis', 'would', 'split', 'real', 'number', 'decim', 'point', 'creat', '3', 'token', '.']

 TOTAL SNOWBALL STEM WORDS ==> 17



 ---- LEMMATIZATION ----

['Faulty', 'tokenisation', 'real', 'valued', 'number', 'One', 'tokeniser', 'would', 'split', 'real', 'number', 'decimal', 'point', 'create', '3', 'token', '.']

 TOTAL LEMMATIZE WORDS ==> 17

************************************************************************************************************************

53 --> This destroys the value of any decimal numeric values  attached to attributes for example. 


 ---- TOKENS ----

 ['This', 'destroys', 'the', 'value', 'of', 'any', 'decimal', 'numeric', 'values', 'attached', 'to', 'attributes', 'for', 'example', '.'] 

 TOTAL TOKENS ==> 15

 ---- POST ----

 [('This', 'DT'), ('destroys', 'VBZ'), ('the', 'DT'), ('value', 'NN'), ('of', 'IN'), ('any', 'DT'), ('decimal', 'JJ'), ('numeric', 'JJ'), ('values', 'NNS'), ('attached', 'VBN'), ('to', 'TO'), ('attributes', 'NNS'), ('for', 'IN'), ('example', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['destroys', 'value', 'decimal', 'numeric', 'values', 'attached', 'attributes', 'example', '.']

 TOTAL FILTERED TOKENS ==>  9

 ---- POST FOR FILTERED TOKENS ----

 [('destroys', 'NNS'), ('value', 'NN'), ('decimal', 'JJ'), ('numeric', 'JJ'), ('values', 'NNS'), ('attached', 'VBN'), ('attributes', 'VBZ'), ('example', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['destroys value', 'value decimal', 'decimal numeric', 'numeric values', 'values attached', 'attached attributes', 'attributes example', 'example .'] 

 TOTAL BIGRAMS --> 8 



 ---- TRI-GRAMS ---- 

 ['destroys value decimal', 'value decimal numeric', 'decimal numeric values', 'numeric values attached', 'values attached attributes', 'attached attributes example', 'attributes example .'] 

 TOTAL TRIGRAMS --> 7 



 ---- NOUN PHRASES ---- 

 ['value', 'example'] 

 TOTAL NOUN PHRASES --> 2 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['destroy', 'valu', 'decim', 'numer', 'valu', 'attach', 'attribut', 'exampl', '.']

 TOTAL PORTER STEM WORDS ==> 9



 ---- SNOWBALL STEMMING ----

['destroy', 'valu', 'decim', 'numer', 'valu', 'attach', 'attribut', 'exampl', '.']

 TOTAL SNOWBALL STEM WORDS ==> 9



 ---- LEMMATIZATION ----

['destroys', 'value', 'decimal', 'numeric', 'value', 'attached', 'attribute', 'example', '.']

 TOTAL LEMMATIZE WORDS ==> 9

************************************************************************************************************************

54 --> Deficiencies in Grammatical Understanding    Missing Acronym Association with Expanded Name  • Lack of association of acronyms with their full names. 


 ---- TOKENS ----

 ['Deficiencies', 'in', 'Grammatical', 'Understanding', 'Missing', 'Acronym', 'Association', 'with', 'Expanded', 'Name', '•', 'Lack', 'of', 'association', 'of', 'acronyms', 'with', 'their', 'full', 'names', '.'] 

 TOTAL TOKENS ==> 21

 ---- POST ----

 [('Deficiencies', 'NNS'), ('in', 'IN'), ('Grammatical', 'NNP'), ('Understanding', 'NNP'), ('Missing', 'NNP'), ('Acronym', 'NNP'), ('Association', 'NNP'), ('with', 'IN'), ('Expanded', 'NNP'), ('Name', 'NNP'), ('•', 'NNP'), ('Lack', 'NNP'), ('of', 'IN'), ('association', 'NN'), ('of', 'IN'), ('acronyms', 'NN'), ('with', 'IN'), ('their', 'PRP$'), ('full', 'JJ'), ('names', 'NNS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Deficiencies', 'Grammatical', 'Understanding', 'Missing', 'Acronym', 'Association', 'Expanded', 'Name', '•', 'Lack', 'association', 'acronyms', 'full', 'names', '.']

 TOTAL FILTERED TOKENS ==>  15

 ---- POST FOR FILTERED TOKENS ----

 [('Deficiencies', 'NNS'), ('Grammatical', 'NNP'), ('Understanding', 'NNP'), ('Missing', 'NNP'), ('Acronym', 'NNP'), ('Association', 'NNP'), ('Expanded', 'VBD'), ('Name', 'NNP'), ('•', 'NNP'), ('Lack', 'NNP'), ('association', 'NN'), ('acronyms', 'VBZ'), ('full', 'JJ'), ('names', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Deficiencies Grammatical', 'Grammatical Understanding', 'Understanding Missing', 'Missing Acronym', 'Acronym Association', 'Association Expanded', 'Expanded Name', 'Name •', '• Lack', 'Lack association', 'association acronyms', 'acronyms full', 'full names', 'names .'] 

 TOTAL BIGRAMS --> 14 



 ---- TRI-GRAMS ---- 

 ['Deficiencies Grammatical Understanding', 'Grammatical Understanding Missing', 'Understanding Missing Acronym', 'Missing Acronym Association', 'Acronym Association Expanded', 'Association Expanded Name', 'Expanded Name •', 'Name • Lack', '• Lack association', 'Lack association acronyms', 'association acronyms full', 'acronyms full names', 'full names .'] 

 TOTAL TRIGRAMS --> 13 



 ---- NOUN PHRASES ---- 

 ['association'] 

 TOTAL NOUN PHRASES --> 1 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> ['Grammatical Understanding Missing Acronym Association', 'Name']
 TOTAL PERSON ENTITY --> 2 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['defici', 'grammat', 'understand', 'miss', 'acronym', 'associ', 'expand', 'name', '•', 'lack', 'associ', 'acronym', 'full', 'name', '.']

 TOTAL PORTER STEM WORDS ==> 15



 ---- SNOWBALL STEMMING ----

['defici', 'grammat', 'understand', 'miss', 'acronym', 'associ', 'expand', 'name', '•', 'lack', 'associ', 'acronym', 'full', 'name', '.']

 TOTAL SNOWBALL STEM WORDS ==> 15



 ---- LEMMATIZATION ----

['Deficiencies', 'Grammatical', 'Understanding', 'Missing', 'Acronym', 'Association', 'Expanded', 'Name', '•', 'Lack', 'association', 'acronym', 'full', 'name', '.']

 TOTAL LEMMATIZE WORDS ==> 15

************************************************************************************************************************

55 --> Clinical  reports are replete with acronyms and their accurate interpretation  is important. 


 ---- TOKENS ----

 ['Clinical', 'reports', 'are', 'replete', 'with', 'acronyms', 'and', 'their', 'accurate', 'interpretation', 'is', 'important', '.'] 

 TOTAL TOKENS ==> 13

 ---- POST ----

 [('Clinical', 'JJ'), ('reports', 'NNS'), ('are', 'VBP'), ('replete', 'JJ'), ('with', 'IN'), ('acronyms', 'NN'), ('and', 'CC'), ('their', 'PRP$'), ('accurate', 'JJ'), ('interpretation', 'NN'), ('is', 'VBZ'), ('important', 'JJ'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Clinical', 'reports', 'replete', 'acronyms', 'accurate', 'interpretation', 'important', '.']

 TOTAL FILTERED TOKENS ==>  8

 ---- POST FOR FILTERED TOKENS ----

 [('Clinical', 'JJ'), ('reports', 'NNS'), ('replete', 'JJ'), ('acronyms', 'JJ'), ('accurate', 'NN'), ('interpretation', 'NN'), ('important', 'JJ'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Clinical reports', 'reports replete', 'replete acronyms', 'acronyms accurate', 'accurate interpretation', 'interpretation important', 'important .'] 

 TOTAL BIGRAMS --> 7 



 ---- TRI-GRAMS ---- 

 ['Clinical reports replete', 'reports replete acronyms', 'replete acronyms accurate', 'acronyms accurate interpretation', 'accurate interpretation important', 'interpretation important .'] 

 TOTAL TRIGRAMS --> 6 



 ---- NOUN PHRASES ---- 

 ['replete acronyms accurate', 'interpretation'] 

 TOTAL NOUN PHRASES --> 2 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> ['Clinical']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['clinic', 'report', 'replet', 'acronym', 'accur', 'interpret', 'import', '.']

 TOTAL PORTER STEM WORDS ==> 8



 ---- SNOWBALL STEMMING ----

['clinic', 'report', 'replet', 'acronym', 'accur', 'interpret', 'import', '.']

 TOTAL SNOWBALL STEM WORDS ==> 8



 ---- LEMMATIZATION ----

['Clinical', 'report', 'replete', 'acronym', 'accurate', 'interpretation', 'important', '.']

 TOTAL LEMMATIZE WORDS ==> 8

************************************************************************************************************************

56 --> Where they are presented along with their expanded  name the two should be correctly attached to each whereas we  have observed them being treated as separate entity references. 


 ---- TOKENS ----

 ['Where', 'they', 'are', 'presented', 'along', 'with', 'their', 'expanded', 'name', 'the', 'two', 'should', 'be', 'correctly', 'attached', 'to', 'each', 'whereas', 'we', 'have', 'observed', 'them', 'being', 'treated', 'as', 'separate', 'entity', 'references', '.'] 

 TOTAL TOKENS ==> 29

 ---- POST ----

 [('Where', 'WRB'), ('they', 'PRP'), ('are', 'VBP'), ('presented', 'VBN'), ('along', 'IN'), ('with', 'IN'), ('their', 'PRP$'), ('expanded', 'VBN'), ('name', 'NN'), ('the', 'DT'), ('two', 'CD'), ('should', 'MD'), ('be', 'VB'), ('correctly', 'RB'), ('attached', 'VBN'), ('to', 'TO'), ('each', 'DT'), ('whereas', 'NN'), ('we', 'PRP'), ('have', 'VBP'), ('observed', 'VBN'), ('them', 'PRP'), ('being', 'VBG'), ('treated', 'VBN'), ('as', 'IN'), ('separate', 'JJ'), ('entity', 'NN'), ('references', 'NNS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['presented', 'along', 'expanded', 'name', 'two', 'correctly', 'attached', 'whereas', 'observed', 'treated', 'separate', 'entity', 'references', '.']

 TOTAL FILTERED TOKENS ==>  14

 ---- POST FOR FILTERED TOKENS ----

 [('presented', 'VBN'), ('along', 'IN'), ('expanded', 'VBN'), ('name', 'NN'), ('two', 'CD'), ('correctly', 'RB'), ('attached', 'VBN'), ('whereas', 'NNS'), ('observed', 'VBD'), ('treated', 'JJ'), ('separate', 'JJ'), ('entity', 'NN'), ('references', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['presented along', 'along expanded', 'expanded name', 'name two', 'two correctly', 'correctly attached', 'attached whereas', 'whereas observed', 'observed treated', 'treated separate', 'separate entity', 'entity references', 'references .'] 

 TOTAL BIGRAMS --> 13 



 ---- TRI-GRAMS ---- 

 ['presented along expanded', 'along expanded name', 'expanded name two', 'name two correctly', 'two correctly attached', 'correctly attached whereas', 'attached whereas observed', 'whereas observed treated', 'observed treated separate', 'treated separate entity', 'separate entity references', 'entity references .'] 

 TOTAL TRIGRAMS --> 12 



 ---- NOUN PHRASES ---- 

 ['name', 'treated separate entity'] 

 TOTAL NOUN PHRASES --> 2 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['present', 'along', 'expand', 'name', 'two', 'correctli', 'attach', 'wherea', 'observ', 'treat', 'separ', 'entiti', 'refer', '.']

 TOTAL PORTER STEM WORDS ==> 14



 ---- SNOWBALL STEMMING ----

['present', 'along', 'expand', 'name', 'two', 'correct', 'attach', 'wherea', 'observ', 'treat', 'separ', 'entiti', 'refer', '.']

 TOTAL SNOWBALL STEM WORDS ==> 14



 ---- LEMMATIZATION ----

['presented', 'along', 'expanded', 'name', 'two', 'correctly', 'attached', 'whereas', 'observed', 'treated', 'separate', 'entity', 'reference', '.']

 TOTAL LEMMATIZE WORDS ==> 14

************************************************************************************************************************

57 --> Context inconsistencies  • A weakness at identifying the same content in different contexts. 


 ---- TOKENS ----

 ['Context', 'inconsistencies', '•', 'A', 'weakness', 'at', 'identifying', 'the', 'same', 'content', 'in', 'different', 'contexts', '.'] 

 TOTAL TOKENS ==> 14

 ---- POST ----

 [('Context', 'NNP'), ('inconsistencies', 'NNS'), ('•', 'VBP'), ('A', 'DT'), ('weakness', 'NN'), ('at', 'IN'), ('identifying', 'VBG'), ('the', 'DT'), ('same', 'JJ'), ('content', 'NN'), ('in', 'IN'), ('different', 'JJ'), ('contexts', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Context', 'inconsistencies', '•', 'weakness', 'identifying', 'content', 'different', 'contexts', '.']

 TOTAL FILTERED TOKENS ==>  9

 ---- POST FOR FILTERED TOKENS ----

 [('Context', 'NNP'), ('inconsistencies', 'NNS'), ('•', 'NNP'), ('weakness', 'NN'), ('identifying', 'VBG'), ('content', 'JJ'), ('different', 'JJ'), ('contexts', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Context inconsistencies', 'inconsistencies •', '• weakness', 'weakness identifying', 'identifying content', 'content different', 'different contexts', 'contexts .'] 

 TOTAL BIGRAMS --> 8 



 ---- TRI-GRAMS ---- 

 ['Context inconsistencies •', 'inconsistencies • weakness', '• weakness identifying', 'weakness identifying content', 'identifying content different', 'content different contexts', 'different contexts .'] 

 TOTAL TRIGRAMS --> 7 



 ---- NOUN PHRASES ---- 

 ['weakness', 'content different contexts'] 

 TOTAL NOUN PHRASES --> 2 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['context', 'inconsist', '•', 'weak', 'identifi', 'content', 'differ', 'context', '.']

 TOTAL PORTER STEM WORDS ==> 9



 ---- SNOWBALL STEMMING ----

['context', 'inconsist', '•', 'weak', 'identifi', 'content', 'differ', 'context', '.']

 TOTAL SNOWBALL STEM WORDS ==> 9



 ---- LEMMATIZATION ----

['Context', 'inconsistency', '•', 'weakness', 'identifying', 'content', 'different', 'context', '.']

 TOTAL LEMMATIZE WORDS ==> 9

************************************************************************************************************************

58 --> Some systems are inconsistent in that they will identify a given  entity correctly in one context but fail to identify the same entity in  a different context. 


 ---- TOKENS ----

 ['Some', 'systems', 'are', 'inconsistent', 'in', 'that', 'they', 'will', 'identify', 'a', 'given', 'entity', 'correctly', 'in', 'one', 'context', 'but', 'fail', 'to', 'identify', 'the', 'same', 'entity', 'in', 'a', 'different', 'context', '.'] 

 TOTAL TOKENS ==> 28

 ---- POST ----

 [('Some', 'DT'), ('systems', 'NNS'), ('are', 'VBP'), ('inconsistent', 'JJ'), ('in', 'IN'), ('that', 'DT'), ('they', 'PRP'), ('will', 'MD'), ('identify', 'VB'), ('a', 'DT'), ('given', 'VBN'), ('entity', 'NN'), ('correctly', 'RB'), ('in', 'IN'), ('one', 'CD'), ('context', 'NN'), ('but', 'CC'), ('fail', 'VBP'), ('to', 'TO'), ('identify', 'VB'), ('the', 'DT'), ('same', 'JJ'), ('entity', 'NN'), ('in', 'IN'), ('a', 'DT'), ('different', 'JJ'), ('context', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['systems', 'inconsistent', 'identify', 'given', 'entity', 'correctly', 'one', 'context', 'fail', 'identify', 'entity', 'different', 'context', '.']

 TOTAL FILTERED TOKENS ==>  14

 ---- POST FOR FILTERED TOKENS ----

 [('systems', 'NNS'), ('inconsistent', 'JJ'), ('identify', 'VB'), ('given', 'VBN'), ('entity', 'NN'), ('correctly', 'RB'), ('one', 'CD'), ('context', 'NN'), ('fail', 'NN'), ('identify', 'VBP'), ('entity', 'NN'), ('different', 'JJ'), ('context', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['systems inconsistent', 'inconsistent identify', 'identify given', 'given entity', 'entity correctly', 'correctly one', 'one context', 'context fail', 'fail identify', 'identify entity', 'entity different', 'different context', 'context .'] 

 TOTAL BIGRAMS --> 13 



 ---- TRI-GRAMS ---- 

 ['systems inconsistent identify', 'inconsistent identify given', 'identify given entity', 'given entity correctly', 'entity correctly one', 'correctly one context', 'one context fail', 'context fail identify', 'fail identify entity', 'identify entity different', 'entity different context', 'different context .'] 

 TOTAL TRIGRAMS --> 12 



 ---- NOUN PHRASES ---- 

 ['entity', 'context', 'fail', 'entity', 'different context'] 

 TOTAL NOUN PHRASES --> 5 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['system', 'inconsist', 'identifi', 'given', 'entiti', 'correctli', 'one', 'context', 'fail', 'identifi', 'entiti', 'differ', 'context', '.']

 TOTAL PORTER STEM WORDS ==> 14



 ---- SNOWBALL STEMMING ----

['system', 'inconsist', 'identifi', 'given', 'entiti', 'correct', 'one', 'context', 'fail', 'identifi', 'entiti', 'differ', 'context', '.']

 TOTAL SNOWBALL STEM WORDS ==> 14



 ---- LEMMATIZATION ----

['system', 'inconsistent', 'identify', 'given', 'entity', 'correctly', 'one', 'context', 'fail', 'identify', 'entity', 'different', 'context', '.']

 TOTAL LEMMATIZE WORDS ==> 14

************************************************************************************************************************

59 --> This is particularly surprising and indicates a  lack of generalisation in their entity recognition function. 


 ---- TOKENS ----

 ['This', 'is', 'particularly', 'surprising', 'and', 'indicates', 'a', 'lack', 'of', 'generalisation', 'in', 'their', 'entity', 'recognition', 'function', '.'] 

 TOTAL TOKENS ==> 16

 ---- POST ----

 [('This', 'DT'), ('is', 'VBZ'), ('particularly', 'RB'), ('surprising', 'JJ'), ('and', 'CC'), ('indicates', 'VBZ'), ('a', 'DT'), ('lack', 'NN'), ('of', 'IN'), ('generalisation', 'NN'), ('in', 'IN'), ('their', 'PRP$'), ('entity', 'NN'), ('recognition', 'NN'), ('function', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['particularly', 'surprising', 'indicates', 'lack', 'generalisation', 'entity', 'recognition', 'function', '.']

 TOTAL FILTERED TOKENS ==>  9

 ---- POST FOR FILTERED TOKENS ----

 [('particularly', 'RB'), ('surprising', 'JJ'), ('indicates', 'NNS'), ('lack', 'VBP'), ('generalisation', 'NN'), ('entity', 'NN'), ('recognition', 'NN'), ('function', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['particularly surprising', 'surprising indicates', 'indicates lack', 'lack generalisation', 'generalisation entity', 'entity recognition', 'recognition function', 'function .'] 

 TOTAL BIGRAMS --> 8 



 ---- TRI-GRAMS ---- 

 ['particularly surprising indicates', 'surprising indicates lack', 'indicates lack generalisation', 'lack generalisation entity', 'generalisation entity recognition', 'entity recognition function', 'recognition function .'] 

 TOTAL TRIGRAMS --> 7 



 ---- NOUN PHRASES ---- 

 ['generalisation', 'entity', 'recognition', 'function'] 

 TOTAL NOUN PHRASES --> 4 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['particularli', 'surpris', 'indic', 'lack', 'generalis', 'entiti', 'recognit', 'function', '.']

 TOTAL PORTER STEM WORDS ==> 9



 ---- SNOWBALL STEMMING ----

['particular', 'surpris', 'indic', 'lack', 'generalis', 'entiti', 'recognit', 'function', '.']

 TOTAL SNOWBALL STEM WORDS ==> 9



 ---- LEMMATIZATION ----

['particularly', 'surprising', 'indicates', 'lack', 'generalisation', 'entity', 'recognition', 'function', '.']

 TOTAL LEMMATIZE WORDS ==> 9

************************************************************************************************************************

60 --> Inability to recognise the same entity with the same words  expressed in a different word order. 


 ---- TOKENS ----

 ['Inability', 'to', 'recognise', 'the', 'same', 'entity', 'with', 'the', 'same', 'words', 'expressed', 'in', 'a', 'different', 'word', 'order', '.'] 

 TOTAL TOKENS ==> 17

 ---- POST ----

 [('Inability', 'NN'), ('to', 'TO'), ('recognise', 'VB'), ('the', 'DT'), ('same', 'JJ'), ('entity', 'NN'), ('with', 'IN'), ('the', 'DT'), ('same', 'JJ'), ('words', 'NNS'), ('expressed', 'VBN'), ('in', 'IN'), ('a', 'DT'), ('different', 'JJ'), ('word', 'NN'), ('order', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Inability', 'recognise', 'entity', 'words', 'expressed', 'different', 'word', 'order', '.']

 TOTAL FILTERED TOKENS ==>  9

 ---- POST FOR FILTERED TOKENS ----

 [('Inability', 'NNP'), ('recognise', 'VBP'), ('entity', 'NN'), ('words', 'NNS'), ('expressed', 'VBN'), ('different', 'JJ'), ('word', 'NN'), ('order', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Inability recognise', 'recognise entity', 'entity words', 'words expressed', 'expressed different', 'different word', 'word order', 'order .'] 

 TOTAL BIGRAMS --> 8 



 ---- TRI-GRAMS ---- 

 ['Inability recognise entity', 'recognise entity words', 'entity words expressed', 'words expressed different', 'expressed different word', 'different word order', 'word order .'] 

 TOTAL TRIGRAMS --> 7 



 ---- NOUN PHRASES ---- 

 ['entity', 'different word', 'order'] 

 TOTAL NOUN PHRASES --> 3 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['inabl', 'recognis', 'entiti', 'word', 'express', 'differ', 'word', 'order', '.']

 TOTAL PORTER STEM WORDS ==> 9



 ---- SNOWBALL STEMMING ----

['inabl', 'recognis', 'entiti', 'word', 'express', 'differ', 'word', 'order', '.']

 TOTAL SNOWBALL STEM WORDS ==> 9



 ---- LEMMATIZATION ----

['Inability', 'recognise', 'entity', 'word', 'expressed', 'different', 'word', 'order', '.']

 TOTAL LEMMATIZE WORDS ==> 9

************************************************************************************************************************

61 --> A critical aspect of entity recognition is being able to recognise the  same content with variable word order e.g.- “high grade serous  carcinoma” versus “Serous high grade carcinoma”. 


 ---- TOKENS ----

 ['A', 'critical', 'aspect', 'of', 'entity', 'recognition', 'is', 'being', 'able', 'to', 'recognise', 'the', 'same', 'content', 'with', 'variable', 'word', 'order', 'e.g.-', '“', 'high', 'grade', 'serous', 'carcinoma', '”', 'versus', '“', 'Serous', 'high', 'grade', 'carcinoma', '”', '.'] 

 TOTAL TOKENS ==> 33

 ---- POST ----

 [('A', 'DT'), ('critical', 'JJ'), ('aspect', 'NN'), ('of', 'IN'), ('entity', 'NN'), ('recognition', 'NN'), ('is', 'VBZ'), ('being', 'VBG'), ('able', 'JJ'), ('to', 'TO'), ('recognise', 'VB'), ('the', 'DT'), ('same', 'JJ'), ('content', 'NN'), ('with', 'IN'), ('variable', 'JJ'), ('word', 'NN'), ('order', 'NN'), ('e.g.-', 'JJ'), ('“', 'NNP'), ('high', 'JJ'), ('grade', 'NN'), ('serous', 'JJ'), ('carcinoma', 'NN'), ('”', 'NNP'), ('versus', 'NN'), ('“', 'NNP'), ('Serous', 'NNP'), ('high', 'JJ'), ('grade', 'NN'), ('carcinoma', 'NN'), ('”', 'NNP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['critical', 'aspect', 'entity', 'recognition', 'able', 'recognise', 'content', 'variable', 'word', 'order', 'e.g.-', '“', 'high', 'grade', 'serous', 'carcinoma', '”', 'versus', '“', 'Serous', 'high', 'grade', 'carcinoma', '”', '.']

 TOTAL FILTERED TOKENS ==>  25

 ---- POST FOR FILTERED TOKENS ----

 [('critical', 'JJ'), ('aspect', 'NN'), ('entity', 'NN'), ('recognition', 'NN'), ('able', 'JJ'), ('recognise', 'NN'), ('content', 'NN'), ('variable', 'JJ'), ('word', 'NN'), ('order', 'NN'), ('e.g.-', 'JJ'), ('“', 'NNP'), ('high', 'JJ'), ('grade', 'NN'), ('serous', 'JJ'), ('carcinoma', 'NN'), ('”', 'NNP'), ('versus', 'NN'), ('“', 'NNP'), ('Serous', 'NNP'), ('high', 'JJ'), ('grade', 'NN'), ('carcinoma', 'NN'), ('”', 'NNP'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['critical aspect', 'aspect entity', 'entity recognition', 'recognition able', 'able recognise', 'recognise content', 'content variable', 'variable word', 'word order', 'order e.g.-', 'e.g.- “', '“ high', 'high grade', 'grade serous', 'serous carcinoma', 'carcinoma ”', '” versus', 'versus “', '“ Serous', 'Serous high', 'high grade', 'grade carcinoma', 'carcinoma ”', '” .'] 

 TOTAL BIGRAMS --> 24 



 ---- TRI-GRAMS ---- 

 ['critical aspect entity', 'aspect entity recognition', 'entity recognition able', 'recognition able recognise', 'able recognise content', 'recognise content variable', 'content variable word', 'variable word order', 'word order e.g.-', 'order e.g.- “', 'e.g.- “ high', '“ high grade', 'high grade serous', 'grade serous carcinoma', 'serous carcinoma ”', 'carcinoma ” versus', '” versus “', 'versus “ Serous', '“ Serous high', 'Serous high grade', 'high grade carcinoma', 'grade carcinoma ”', 'carcinoma ” .'] 

 TOTAL TRIGRAMS --> 23 



 ---- NOUN PHRASES ---- 

 ['critical aspect', 'entity', 'recognition', 'able recognise', 'content', 'variable word', 'order', 'high grade', 'serous carcinoma', 'versus', 'high grade', 'carcinoma'] 

 TOTAL NOUN PHRASES --> 12 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['critic', 'aspect', 'entiti', 'recognit', 'abl', 'recognis', 'content', 'variabl', 'word', 'order', 'e.g.-', '“', 'high', 'grade', 'serou', 'carcinoma', '”', 'versu', '“', 'serou', 'high', 'grade', 'carcinoma', '”', '.']

 TOTAL PORTER STEM WORDS ==> 25



 ---- SNOWBALL STEMMING ----

['critic', 'aspect', 'entiti', 'recognit', 'abl', 'recognis', 'content', 'variabl', 'word', 'order', 'e.g.-', '“', 'high', 'grade', 'serous', 'carcinoma', '”', 'versus', '“', 'serous', 'high', 'grade', 'carcinoma', '”', '.']

 TOTAL SNOWBALL STEM WORDS ==> 25



 ---- LEMMATIZATION ----

['critical', 'aspect', 'entity', 'recognition', 'able', 'recognise', 'content', 'variable', 'word', 'order', 'e.g.-', '“', 'high', 'grade', 'serous', 'carcinoma', '”', 'versus', '“', 'Serous', 'high', 'grade', 'carcinoma', '”', '.']

 TOTAL LEMMATIZE WORDS ==> 25

************************************************************************************************************************

62 --> Simple CER  methods that use rule based approaches will have a serious difficulty  with this common problem. 


 ---- TOKENS ----

 ['Simple', 'CER', 'methods', 'that', 'use', 'rule', 'based', 'approaches', 'will', 'have', 'a', 'serious', 'difficulty', 'with', 'this', 'common', 'problem', '.'] 

 TOTAL TOKENS ==> 18

 ---- POST ----

 [('Simple', 'JJ'), ('CER', 'NNP'), ('methods', 'NNS'), ('that', 'WDT'), ('use', 'VBP'), ('rule', 'NN'), ('based', 'VBN'), ('approaches', 'NNS'), ('will', 'MD'), ('have', 'VB'), ('a', 'DT'), ('serious', 'JJ'), ('difficulty', 'NN'), ('with', 'IN'), ('this', 'DT'), ('common', 'JJ'), ('problem', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Simple', 'CER', 'methods', 'use', 'rule', 'based', 'approaches', 'serious', 'difficulty', 'common', 'problem', '.']

 TOTAL FILTERED TOKENS ==>  12

 ---- POST FOR FILTERED TOKENS ----

 [('Simple', 'JJ'), ('CER', 'NNP'), ('methods', 'NNS'), ('use', 'VBP'), ('rule', 'NN'), ('based', 'VBN'), ('approaches', 'RB'), ('serious', 'JJ'), ('difficulty', 'NN'), ('common', 'JJ'), ('problem', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Simple CER', 'CER methods', 'methods use', 'use rule', 'rule based', 'based approaches', 'approaches serious', 'serious difficulty', 'difficulty common', 'common problem', 'problem .'] 

 TOTAL BIGRAMS --> 11 



 ---- TRI-GRAMS ---- 

 ['Simple CER methods', 'CER methods use', 'methods use rule', 'use rule based', 'rule based approaches', 'based approaches serious', 'approaches serious difficulty', 'serious difficulty common', 'difficulty common problem', 'common problem .'] 

 TOTAL TRIGRAMS --> 10 



 ---- NOUN PHRASES ---- 

 ['rule', 'serious difficulty', 'common problem'] 

 TOTAL NOUN PHRASES --> 3 



 ---- NER ----

 
 ORGANIZATION ---> ['CER']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> ['Simple']
 TOTAL PERSON ENTITY --> 1 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['simpl', 'cer', 'method', 'use', 'rule', 'base', 'approach', 'seriou', 'difficulti', 'common', 'problem', '.']

 TOTAL PORTER STEM WORDS ==> 12



 ---- SNOWBALL STEMMING ----

['simpl', 'cer', 'method', 'use', 'rule', 'base', 'approach', 'serious', 'difficulti', 'common', 'problem', '.']

 TOTAL SNOWBALL STEM WORDS ==> 12



 ---- LEMMATIZATION ----

['Simple', 'CER', 'method', 'use', 'rule', 'based', 'approach', 'serious', 'difficulty', 'common', 'problem', '.']

 TOTAL LEMMATIZE WORDS ==> 12

************************************************************************************************************************

63 --> Statistical machine learning methods are  required to circumvent this by treating it as a generalised problem. 


 ---- TOKENS ----

 ['Statistical', 'machine', 'learning', 'methods', 'are', 'required', 'to', 'circumvent', 'this', 'by', 'treating', 'it', 'as', 'a', 'generalised', 'problem', '.'] 

 TOTAL TOKENS ==> 17

 ---- POST ----

 [('Statistical', 'JJ'), ('machine', 'NN'), ('learning', 'NN'), ('methods', 'NNS'), ('are', 'VBP'), ('required', 'VBN'), ('to', 'TO'), ('circumvent', 'VB'), ('this', 'DT'), ('by', 'IN'), ('treating', 'VBG'), ('it', 'PRP'), ('as', 'IN'), ('a', 'DT'), ('generalised', 'JJ'), ('problem', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Statistical', 'machine', 'learning', 'methods', 'required', 'circumvent', 'treating', 'generalised', 'problem', '.']

 TOTAL FILTERED TOKENS ==>  10

 ---- POST FOR FILTERED TOKENS ----

 [('Statistical', 'JJ'), ('machine', 'NN'), ('learning', 'VBG'), ('methods', 'NNS'), ('required', 'VBN'), ('circumvent', 'NN'), ('treating', 'VBG'), ('generalised', 'VBN'), ('problem', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Statistical machine', 'machine learning', 'learning methods', 'methods required', 'required circumvent', 'circumvent treating', 'treating generalised', 'generalised problem', 'problem .'] 

 TOTAL BIGRAMS --> 9 



 ---- TRI-GRAMS ---- 

 ['Statistical machine learning', 'machine learning methods', 'learning methods required', 'methods required circumvent', 'required circumvent treating', 'circumvent treating generalised', 'treating generalised problem', 'generalised problem .'] 

 TOTAL TRIGRAMS --> 8 



 ---- NOUN PHRASES ---- 

 ['Statistical machine', 'circumvent', 'problem'] 

 TOTAL NOUN PHRASES --> 3 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['statist', 'machin', 'learn', 'method', 'requir', 'circumv', 'treat', 'generalis', 'problem', '.']

 TOTAL PORTER STEM WORDS ==> 10



 ---- SNOWBALL STEMMING ----

['statist', 'machin', 'learn', 'method', 'requir', 'circumv', 'treat', 'generalis', 'problem', '.']

 TOTAL SNOWBALL STEM WORDS ==> 10



 ---- LEMMATIZATION ----

['Statistical', 'machine', 'learning', 'method', 'required', 'circumvent', 'treating', 'generalised', 'problem', '.']

 TOTAL LEMMATIZE WORDS ==> 10

************************************************************************************************************************

64 --> Failure to recognise Morphology of words  • We have observed an inability to identify entities when the same  word is rendered in a different lexical morphology. 


 ---- TOKENS ----

 ['Failure', 'to', 'recognise', 'Morphology', 'of', 'words', '•', 'We', 'have', 'observed', 'an', 'inability', 'to', 'identify', 'entities', 'when', 'the', 'same', 'word', 'is', 'rendered', 'in', 'a', 'different', 'lexical', 'morphology', '.'] 

 TOTAL TOKENS ==> 27

 ---- POST ----

 [('Failure', 'NN'), ('to', 'TO'), ('recognise', 'VB'), ('Morphology', 'NNP'), ('of', 'IN'), ('words', 'NNS'), ('•', 'IN'), ('We', 'PRP'), ('have', 'VBP'), ('observed', 'VBN'), ('an', 'DT'), ('inability', 'NN'), ('to', 'TO'), ('identify', 'VB'), ('entities', 'NNS'), ('when', 'WRB'), ('the', 'DT'), ('same', 'JJ'), ('word', 'NN'), ('is', 'VBZ'), ('rendered', 'VBN'), ('in', 'IN'), ('a', 'DT'), ('different', 'JJ'), ('lexical', 'JJ'), ('morphology', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Failure', 'recognise', 'Morphology', 'words', '•', 'observed', 'inability', 'identify', 'entities', 'word', 'rendered', 'different', 'lexical', 'morphology', '.']

 TOTAL FILTERED TOKENS ==>  15

 ---- POST FOR FILTERED TOKENS ----

 [('Failure', 'NN'), ('recognise', 'NN'), ('Morphology', 'NNP'), ('words', 'NNS'), ('•', 'NNP'), ('observed', 'VBD'), ('inability', 'NN'), ('identify', 'NN'), ('entities', 'NNS'), ('word', 'NN'), ('rendered', 'VBN'), ('different', 'JJ'), ('lexical', 'JJ'), ('morphology', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Failure recognise', 'recognise Morphology', 'Morphology words', 'words •', '• observed', 'observed inability', 'inability identify', 'identify entities', 'entities word', 'word rendered', 'rendered different', 'different lexical', 'lexical morphology', 'morphology .'] 

 TOTAL BIGRAMS --> 14 



 ---- TRI-GRAMS ---- 

 ['Failure recognise Morphology', 'recognise Morphology words', 'Morphology words •', 'words • observed', '• observed inability', 'observed inability identify', 'inability identify entities', 'identify entities word', 'entities word rendered', 'word rendered different', 'rendered different lexical', 'different lexical morphology', 'lexical morphology .'] 

 TOTAL TRIGRAMS --> 13 



 ---- NOUN PHRASES ---- 

 ['Failure', 'recognise', 'inability', 'identify', 'word', 'different lexical morphology'] 

 TOTAL NOUN PHRASES --> 6 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> ['Failure']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['failur', 'recognis', 'morpholog', 'word', '•', 'observ', 'inabl', 'identifi', 'entiti', 'word', 'render', 'differ', 'lexic', 'morpholog', '.']

 TOTAL PORTER STEM WORDS ==> 15



 ---- SNOWBALL STEMMING ----

['failur', 'recognis', 'morpholog', 'word', '•', 'observ', 'inabl', 'identifi', 'entiti', 'word', 'render', 'differ', 'lexic', 'morpholog', '.']

 TOTAL SNOWBALL STEM WORDS ==> 15



 ---- LEMMATIZATION ----

['Failure', 'recognise', 'Morphology', 'word', '•', 'observed', 'inability', 'identify', 'entity', 'word', 'rendered', 'different', 'lexical', 'morphology', '.']

 TOTAL LEMMATIZE WORDS ==> 15

************************************************************************************************************************

65 --> Many words  have the same general meaning but change form when used in  different grammatical roles e.g.- “malignant” and “malignancy”. 


 ---- TOKENS ----

 ['Many', 'words', 'have', 'the', 'same', 'general', 'meaning', 'but', 'change', 'form', 'when', 'used', 'in', 'different', 'grammatical', 'roles', 'e.g.-', '“', 'malignant', '”', 'and', '“', 'malignancy', '”', '.'] 

 TOTAL TOKENS ==> 25

 ---- POST ----

 [('Many', 'JJ'), ('words', 'NNS'), ('have', 'VBP'), ('the', 'DT'), ('same', 'JJ'), ('general', 'JJ'), ('meaning', 'NN'), ('but', 'CC'), ('change', 'NN'), ('form', 'NN'), ('when', 'WRB'), ('used', 'VBN'), ('in', 'IN'), ('different', 'JJ'), ('grammatical', 'JJ'), ('roles', 'NNS'), ('e.g.-', 'JJ'), ('“', 'NNP'), ('malignant', 'NN'), ('”', 'NN'), ('and', 'CC'), ('“', 'JJ'), ('malignancy', 'NN'), ('”', 'NNP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Many', 'words', 'general', 'meaning', 'change', 'form', 'used', 'different', 'grammatical', 'roles', 'e.g.-', '“', 'malignant', '”', '“', 'malignancy', '”', '.']

 TOTAL FILTERED TOKENS ==>  18

 ---- POST FOR FILTERED TOKENS ----

 [('Many', 'JJ'), ('words', 'NNS'), ('general', 'JJ'), ('meaning', 'VBG'), ('change', 'NN'), ('form', 'NN'), ('used', 'VBN'), ('different', 'JJ'), ('grammatical', 'JJ'), ('roles', 'NNS'), ('e.g.-', 'JJ'), ('“', 'NNP'), ('malignant', 'NN'), ('”', 'NNP'), ('“', 'NNP'), ('malignancy', 'NN'), ('”', 'NNP'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Many words', 'words general', 'general meaning', 'meaning change', 'change form', 'form used', 'used different', 'different grammatical', 'grammatical roles', 'roles e.g.-', 'e.g.- “', '“ malignant', 'malignant ”', '” “', '“ malignancy', 'malignancy ”', '” .'] 

 TOTAL BIGRAMS --> 17 



 ---- TRI-GRAMS ---- 

 ['Many words general', 'words general meaning', 'general meaning change', 'meaning change form', 'change form used', 'form used different', 'used different grammatical', 'different grammatical roles', 'grammatical roles e.g.-', 'roles e.g.- “', 'e.g.- “ malignant', '“ malignant ”', 'malignant ” “', '” “ malignancy', '“ malignancy ”', 'malignancy ” .'] 

 TOTAL TRIGRAMS --> 16 



 ---- NOUN PHRASES ---- 

 ['change', 'form', 'malignant', 'malignancy'] 

 TOTAL NOUN PHRASES --> 4 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['mani', 'word', 'gener', 'mean', 'chang', 'form', 'use', 'differ', 'grammat', 'role', 'e.g.-', '“', 'malign', '”', '“', 'malign', '”', '.']

 TOTAL PORTER STEM WORDS ==> 18



 ---- SNOWBALL STEMMING ----

['mani', 'word', 'general', 'mean', 'chang', 'form', 'use', 'differ', 'grammat', 'role', 'e.g.-', '“', 'malign', '”', '“', 'malign', '”', '.']

 TOTAL SNOWBALL STEM WORDS ==> 18



 ---- LEMMATIZATION ----

['Many', 'word', 'general', 'meaning', 'change', 'form', 'used', 'different', 'grammatical', 'role', 'e.g.-', '“', 'malignant', '”', '“', 'malignancy', '”', '.']

 TOTAL LEMMATIZE WORDS ==> 18

************************************************************************************************************************

66 --> However at times the different morphology can also carry different  meanings which often needs to be discriminated, so “malignant  cells” is a description of a behaviour, whereas “malignancy” is a  statement of a disorder. 


 ---- TOKENS ----

 ['However', 'at', 'times', 'the', 'different', 'morphology', 'can', 'also', 'carry', 'different', 'meanings', 'which', 'often', 'needs', 'to', 'be', 'discriminated', ',', 'so', '“', 'malignant', 'cells', '”', 'is', 'a', 'description', 'of', 'a', 'behaviour', ',', 'whereas', '“', 'malignancy', '”', 'is', 'a', 'statement', 'of', 'a', 'disorder', '.'] 

 TOTAL TOKENS ==> 41

 ---- POST ----

 [('However', 'RB'), ('at', 'IN'), ('times', 'NNS'), ('the', 'DT'), ('different', 'JJ'), ('morphology', 'NN'), ('can', 'MD'), ('also', 'RB'), ('carry', 'VB'), ('different', 'JJ'), ('meanings', 'NNS'), ('which', 'WDT'), ('often', 'RB'), ('needs', 'VBZ'), ('to', 'TO'), ('be', 'VB'), ('discriminated', 'VBN'), (',', ','), ('so', 'RB'), ('“', 'JJ'), ('malignant', 'JJ'), ('cells', 'NNS'), ('”', 'FW'), ('is', 'VBZ'), ('a', 'DT'), ('description', 'NN'), ('of', 'IN'), ('a', 'DT'), ('behaviour', 'NN'), (',', ','), ('whereas', 'WP'), ('“', 'JJ'), ('malignancy', 'NN'), ('”', 'NNP'), ('is', 'VBZ'), ('a', 'DT'), ('statement', 'NN'), ('of', 'IN'), ('a', 'DT'), ('disorder', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['However', 'times', 'different', 'morphology', 'also', 'carry', 'different', 'meanings', 'often', 'needs', 'discriminated', ',', '“', 'malignant', 'cells', '”', 'description', 'behaviour', ',', 'whereas', '“', 'malignancy', '”', 'statement', 'disorder', '.']

 TOTAL FILTERED TOKENS ==>  26

 ---- POST FOR FILTERED TOKENS ----

 [('However', 'RB'), ('times', 'NNS'), ('different', 'JJ'), ('morphology', 'NN'), ('also', 'RB'), ('carry', 'VBP'), ('different', 'JJ'), ('meanings', 'NNS'), ('often', 'RB'), ('needs', 'VBZ'), ('discriminated', 'VBN'), (',', ','), ('“', 'JJ'), ('malignant', 'JJ'), ('cells', 'NNS'), ('”', 'JJ'), ('description', 'NN'), ('behaviour', 'NN'), (',', ','), ('whereas', 'WP'), ('“', 'JJ'), ('malignancy', 'NN'), ('”', 'NNP'), ('statement', 'NN'), ('disorder', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['However times', 'times different', 'different morphology', 'morphology also', 'also carry', 'carry different', 'different meanings', 'meanings often', 'often needs', 'needs discriminated', 'discriminated ,', ', “', '“ malignant', 'malignant cells', 'cells ”', '” description', 'description behaviour', 'behaviour ,', ', whereas', 'whereas “', '“ malignancy', 'malignancy ”', '” statement', 'statement disorder', 'disorder .'] 

 TOTAL BIGRAMS --> 25 



 ---- TRI-GRAMS ---- 

 ['However times different', 'times different morphology', 'different morphology also', 'morphology also carry', 'also carry different', 'carry different meanings', 'different meanings often', 'meanings often needs', 'often needs discriminated', 'needs discriminated ,', 'discriminated , “', ', “ malignant', '“ malignant cells', 'malignant cells ”', 'cells ” description', '” description behaviour', 'description behaviour ,', 'behaviour , whereas', ', whereas “', 'whereas “ malignancy', '“ malignancy ”', 'malignancy ” statement', '” statement disorder', 'statement disorder .'] 

 TOTAL TRIGRAMS --> 24 



 ---- NOUN PHRASES ---- 

 ['different morphology', '” description', 'behaviour', '“ malignancy', 'statement', 'disorder'] 

 TOTAL NOUN PHRASES --> 6 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['howev', 'time', 'differ', 'morpholog', 'also', 'carri', 'differ', 'mean', 'often', 'need', 'discrimin', ',', '“', 'malign', 'cell', '”', 'descript', 'behaviour', ',', 'wherea', '“', 'malign', '”', 'statement', 'disord', '.']

 TOTAL PORTER STEM WORDS ==> 26



 ---- SNOWBALL STEMMING ----

['howev', 'time', 'differ', 'morpholog', 'also', 'carri', 'differ', 'mean', 'often', 'need', 'discrimin', ',', '“', 'malign', 'cell', '”', 'descript', 'behaviour', ',', 'wherea', '“', 'malign', '”', 'statement', 'disord', '.']

 TOTAL SNOWBALL STEM WORDS ==> 26



 ---- LEMMATIZATION ----

['However', 'time', 'different', 'morphology', 'also', 'carry', 'different', 'meaning', 'often', 'need', 'discriminated', ',', '“', 'malignant', 'cell', '”', 'description', 'behaviour', ',', 'whereas', '“', 'malignancy', '”', 'statement', 'disorder', '.']

 TOTAL LEMMATIZE WORDS ==> 26

************************************************************************************************************************

67 --> Incomplete Negation Recognition  • Negation in clinical texts is of vital importance but is also  complicated because of its four-way between the semantics of  negative meaning and the grammar of negation representation,  such as {normal, abnormal, not normal, not abnormal}. 


 ---- TOKENS ----

 ['Incomplete', 'Negation', 'Recognition', '•', 'Negation', 'in', 'clinical', 'texts', 'is', 'of', 'vital', 'importance', 'but', 'is', 'also', 'complicated', 'because', 'of', 'its', 'four-way', 'between', 'the', 'semantics', 'of', 'negative', 'meaning', 'and', 'the', 'grammar', 'of', 'negation', 'representation', ',', 'such', 'as', '{', 'normal', ',', 'abnormal', ',', 'not', 'normal', ',', 'not', 'abnormal', '}', '.'] 

 TOTAL TOKENS ==> 47

 ---- POST ----

 [('Incomplete', 'NNP'), ('Negation', 'NNP'), ('Recognition', 'NNP'), ('•', 'NNP'), ('Negation', 'NNP'), ('in', 'IN'), ('clinical', 'JJ'), ('texts', 'NN'), ('is', 'VBZ'), ('of', 'IN'), ('vital', 'JJ'), ('importance', 'NN'), ('but', 'CC'), ('is', 'VBZ'), ('also', 'RB'), ('complicated', 'VBN'), ('because', 'IN'), ('of', 'IN'), ('its', 'PRP$'), ('four-way', 'NN'), ('between', 'IN'), ('the', 'DT'), ('semantics', 'NNS'), ('of', 'IN'), ('negative', 'JJ'), ('meaning', 'NN'), ('and', 'CC'), ('the', 'DT'), ('grammar', 'NN'), ('of', 'IN'), ('negation', 'NN'), ('representation', 'NN'), (',', ','), ('such', 'JJ'), ('as', 'IN'), ('{', '('), ('normal', 'JJ'), (',', ','), ('abnormal', 'JJ'), (',', ','), ('not', 'RB'), ('normal', 'JJ'), (',', ','), ('not', 'RB'), ('abnormal', 'JJ'), ('}', ')'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Incomplete', 'Negation', 'Recognition', '•', 'Negation', 'clinical', 'texts', 'vital', 'importance', 'also', 'complicated', 'four-way', 'semantics', 'negative', 'meaning', 'grammar', 'negation', 'representation', ',', '{', 'normal', ',', 'abnormal', ',', 'normal', ',', 'abnormal', '}', '.']

 TOTAL FILTERED TOKENS ==>  29

 ---- POST FOR FILTERED TOKENS ----

 [('Incomplete', 'NNP'), ('Negation', 'NNP'), ('Recognition', 'NNP'), ('•', 'NNP'), ('Negation', 'NNP'), ('clinical', 'JJ'), ('texts', 'NN'), ('vital', 'JJ'), ('importance', 'NN'), ('also', 'RB'), ('complicated', 'VBD'), ('four-way', 'JJ'), ('semantics', 'NNS'), ('negative', 'JJ'), ('meaning', 'NN'), ('grammar', 'NN'), ('negation', 'NN'), ('representation', 'NN'), (',', ','), ('{', '('), ('normal', 'JJ'), (',', ','), ('abnormal', 'JJ'), (',', ','), ('normal', 'JJ'), (',', ','), ('abnormal', 'JJ'), ('}', ')'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Incomplete Negation', 'Negation Recognition', 'Recognition •', '• Negation', 'Negation clinical', 'clinical texts', 'texts vital', 'vital importance', 'importance also', 'also complicated', 'complicated four-way', 'four-way semantics', 'semantics negative', 'negative meaning', 'meaning grammar', 'grammar negation', 'negation representation', 'representation ,', ', {', '{ normal', 'normal ,', ', abnormal', 'abnormal ,', ', normal', 'normal ,', ', abnormal', 'abnormal }', '} .'] 

 TOTAL BIGRAMS --> 28 



 ---- TRI-GRAMS ---- 

 ['Incomplete Negation Recognition', 'Negation Recognition •', 'Recognition • Negation', '• Negation clinical', 'Negation clinical texts', 'clinical texts vital', 'texts vital importance', 'vital importance also', 'importance also complicated', 'also complicated four-way', 'complicated four-way semantics', 'four-way semantics negative', 'semantics negative meaning', 'negative meaning grammar', 'meaning grammar negation', 'grammar negation representation', 'negation representation ,', 'representation , {', ', { normal', '{ normal ,', 'normal , abnormal', ', abnormal ,', 'abnormal , normal', ', normal ,', 'normal , abnormal', ', abnormal }', 'abnormal } .'] 

 TOTAL TRIGRAMS --> 27 



 ---- NOUN PHRASES ---- 

 ['clinical texts', 'vital importance', 'negative meaning', 'grammar', 'negation', 'representation'] 

 TOTAL NOUN PHRASES --> 6 



 ---- NER ----

 
 ORGANIZATION ---> ['Negation']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> ['Incomplete']
 TOTAL PERSON ENTITY --> 1 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['incomplet', 'negat', 'recognit', '•', 'negat', 'clinic', 'text', 'vital', 'import', 'also', 'complic', 'four-way', 'semant', 'neg', 'mean', 'grammar', 'negat', 'represent', ',', '{', 'normal', ',', 'abnorm', ',', 'normal', ',', 'abnorm', '}', '.']

 TOTAL PORTER STEM WORDS ==> 29



 ---- SNOWBALL STEMMING ----

['incomplet', 'negat', 'recognit', '•', 'negat', 'clinic', 'text', 'vital', 'import', 'also', 'complic', 'four-way', 'semant', 'negat', 'mean', 'grammar', 'negat', 'represent', ',', '{', 'normal', ',', 'abnorm', ',', 'normal', ',', 'abnorm', '}', '.']

 TOTAL SNOWBALL STEM WORDS ==> 29



 ---- LEMMATIZATION ----

['Incomplete', 'Negation', 'Recognition', '•', 'Negation', 'clinical', 'text', 'vital', 'importance', 'also', 'complicated', 'four-way', 'semantics', 'negative', 'meaning', 'grammar', 'negation', 'representation', ',', '{', 'normal', ',', 'abnormal', ',', 'normal', ',', 'abnormal', '}', '.']

 TOTAL LEMMATIZE WORDS ==> 29

************************************************************************************************************************

68 --> While  some systems do recognise grammatical negation many do not  control for the positive/negative aspect of the semantics of  individual medical lexical items. 


 ---- TOKENS ----

 ['While', 'some', 'systems', 'do', 'recognise', 'grammatical', 'negation', 'many', 'do', 'not', 'control', 'for', 'the', 'positive/negative', 'aspect', 'of', 'the', 'semantics', 'of', 'individual', 'medical', 'lexical', 'items', '.'] 

 TOTAL TOKENS ==> 24

 ---- POST ----

 [('While', 'IN'), ('some', 'DT'), ('systems', 'NNS'), ('do', 'VBP'), ('recognise', 'VB'), ('grammatical', 'JJ'), ('negation', 'NN'), ('many', 'JJ'), ('do', 'VBP'), ('not', 'RB'), ('control', 'VB'), ('for', 'IN'), ('the', 'DT'), ('positive/negative', 'JJ'), ('aspect', 'NN'), ('of', 'IN'), ('the', 'DT'), ('semantics', 'NNS'), ('of', 'IN'), ('individual', 'JJ'), ('medical', 'JJ'), ('lexical', 'JJ'), ('items', 'NNS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['systems', 'recognise', 'grammatical', 'negation', 'many', 'control', 'positive/negative', 'aspect', 'semantics', 'individual', 'medical', 'lexical', 'items', '.']

 TOTAL FILTERED TOKENS ==>  14

 ---- POST FOR FILTERED TOKENS ----

 [('systems', 'NNS'), ('recognise', 'VBP'), ('grammatical', 'JJ'), ('negation', 'NN'), ('many', 'JJ'), ('control', 'NN'), ('positive/negative', 'JJ'), ('aspect', 'NN'), ('semantics', 'NNS'), ('individual', 'JJ'), ('medical', 'JJ'), ('lexical', 'JJ'), ('items', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['systems recognise', 'recognise grammatical', 'grammatical negation', 'negation many', 'many control', 'control positive/negative', 'positive/negative aspect', 'aspect semantics', 'semantics individual', 'individual medical', 'medical lexical', 'lexical items', 'items .'] 

 TOTAL BIGRAMS --> 13 



 ---- TRI-GRAMS ---- 

 ['systems recognise grammatical', 'recognise grammatical negation', 'grammatical negation many', 'negation many control', 'many control positive/negative', 'control positive/negative aspect', 'positive/negative aspect semantics', 'aspect semantics individual', 'semantics individual medical', 'individual medical lexical', 'medical lexical items', 'lexical items .'] 

 TOTAL TRIGRAMS --> 12 



 ---- NOUN PHRASES ---- 

 ['grammatical negation', 'many control', 'positive aspect'] 

 TOTAL NOUN PHRASES --> 3 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['system', 'recognis', 'grammat', 'negat', 'mani', 'control', 'positive/neg', 'aspect', 'semant', 'individu', 'medic', 'lexic', 'item', '.']

 TOTAL PORTER STEM WORDS ==> 14



 ---- SNOWBALL STEMMING ----

['system', 'recognis', 'grammat', 'negat', 'mani', 'control', 'positive/neg', 'aspect', 'semant', 'individu', 'medic', 'lexic', 'item', '.']

 TOTAL SNOWBALL STEM WORDS ==> 14



 ---- LEMMATIZATION ----

['system', 'recognise', 'grammatical', 'negation', 'many', 'control', 'positive/negative', 'aspect', 'semantics', 'individual', 'medical', 'lexical', 'item', '.']

 TOTAL LEMMATIZE WORDS ==> 14

************************************************************************************************************************

69 --> This failure can lead to either false  positives or false negatives in the processed outcome. 


 ---- TOKENS ----

 ['This', 'failure', 'can', 'lead', 'to', 'either', 'false', 'positives', 'or', 'false', 'negatives', 'in', 'the', 'processed', 'outcome', '.'] 

 TOTAL TOKENS ==> 16

 ---- POST ----

 [('This', 'DT'), ('failure', 'NN'), ('can', 'MD'), ('lead', 'VB'), ('to', 'TO'), ('either', 'DT'), ('false', 'JJ'), ('positives', 'NNS'), ('or', 'CC'), ('false', 'JJ'), ('negatives', 'NNS'), ('in', 'IN'), ('the', 'DT'), ('processed', 'JJ'), ('outcome', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['failure', 'lead', 'either', 'false', 'positives', 'false', 'negatives', 'processed', 'outcome', '.']

 TOTAL FILTERED TOKENS ==>  10

 ---- POST FOR FILTERED TOKENS ----

 [('failure', 'NN'), ('lead', 'NN'), ('either', 'CC'), ('false', 'JJ'), ('positives', 'NNS'), ('false', 'JJ'), ('negatives', 'NNS'), ('processed', 'VBD'), ('outcome', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['failure lead', 'lead either', 'either false', 'false positives', 'positives false', 'false negatives', 'negatives processed', 'processed outcome', 'outcome .'] 

 TOTAL BIGRAMS --> 9 



 ---- TRI-GRAMS ---- 

 ['failure lead either', 'lead either false', 'either false positives', 'false positives false', 'positives false negatives', 'false negatives processed', 'negatives processed outcome', 'processed outcome .'] 

 TOTAL TRIGRAMS --> 8 



 ---- NOUN PHRASES ---- 

 ['failure', 'lead', 'outcome'] 

 TOTAL NOUN PHRASES --> 3 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['failur', 'lead', 'either', 'fals', 'posit', 'fals', 'neg', 'process', 'outcom', '.']

 TOTAL PORTER STEM WORDS ==> 10



 ---- SNOWBALL STEMMING ----

['failur', 'lead', 'either', 'fals', 'posit', 'fals', 'negat', 'process', 'outcom', '.']

 TOTAL SNOWBALL STEM WORDS ==> 10



 ---- LEMMATIZATION ----

['failure', 'lead', 'either', 'false', 'positive', 'false', 'negative', 'processed', 'outcome', '.']

 TOTAL LEMMATIZE WORDS ==> 10

************************************************************************************************************************

70 --> Part-of-Speech (POS) Identification Invalid  Two common mis-categorisations of POS is the assignment of nouns  as adjectives and incorrect identification of Proper Nouns. 


 ---- TOKENS ----

 ['Part-of-Speech', '(', 'POS', ')', 'Identification', 'Invalid', 'Two', 'common', 'mis-categorisations', 'of', 'POS', 'is', 'the', 'assignment', 'of', 'nouns', 'as', 'adjectives', 'and', 'incorrect', 'identification', 'of', 'Proper', 'Nouns', '.'] 

 TOTAL TOKENS ==> 25

 ---- POST ----

 [('Part-of-Speech', 'NNP'), ('(', '('), ('POS', 'NNP'), (')', ')'), ('Identification', 'NNP'), ('Invalid', 'NNP'), ('Two', 'CD'), ('common', 'JJ'), ('mis-categorisations', 'NNS'), ('of', 'IN'), ('POS', 'NNP'), ('is', 'VBZ'), ('the', 'DT'), ('assignment', 'NN'), ('of', 'IN'), ('nouns', 'NNS'), ('as', 'IN'), ('adjectives', 'NNS'), ('and', 'CC'), ('incorrect', 'JJ'), ('identification', 'NN'), ('of', 'IN'), ('Proper', 'NNP'), ('Nouns', 'NNP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Part-of-Speech', '(', 'POS', ')', 'Identification', 'Invalid', 'Two', 'common', 'mis-categorisations', 'POS', 'assignment', 'nouns', 'adjectives', 'incorrect', 'identification', 'Proper', 'Nouns', '.']

 TOTAL FILTERED TOKENS ==>  18

 ---- POST FOR FILTERED TOKENS ----

 [('Part-of-Speech', 'NNP'), ('(', '('), ('POS', 'NNP'), (')', ')'), ('Identification', 'NNP'), ('Invalid', 'NNP'), ('Two', 'CD'), ('common', 'JJ'), ('mis-categorisations', 'NNS'), ('POS', 'NNP'), ('assignment', 'NN'), ('nouns', 'VBZ'), ('adjectives', 'NNS'), ('incorrect', 'JJ'), ('identification', 'NN'), ('Proper', 'NNP'), ('Nouns', 'NNP'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Part-of-Speech (', '( POS', 'POS )', ') Identification', 'Identification Invalid', 'Invalid Two', 'Two common', 'common mis-categorisations', 'mis-categorisations POS', 'POS assignment', 'assignment nouns', 'nouns adjectives', 'adjectives incorrect', 'incorrect identification', 'identification Proper', 'Proper Nouns', 'Nouns .'] 

 TOTAL BIGRAMS --> 17 



 ---- TRI-GRAMS ---- 

 ['Part-of-Speech ( POS', '( POS )', 'POS ) Identification', ') Identification Invalid', 'Identification Invalid Two', 'Invalid Two common', 'Two common mis-categorisations', 'common mis-categorisations POS', 'mis-categorisations POS assignment', 'POS assignment nouns', 'assignment nouns adjectives', 'nouns adjectives incorrect', 'adjectives incorrect identification', 'incorrect identification Proper', 'identification Proper Nouns', 'Proper Nouns .'] 

 TOTAL TRIGRAMS --> 16 



 ---- NOUN PHRASES ---- 

 ['assignment', 'incorrect identification'] 

 TOTAL NOUN PHRASES --> 2 



 ---- NER ----

 
 ORGANIZATION ---> ['POS']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> ['Proper Nouns']
 TOTAL PERSON ENTITY --> 1 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['part-of-speech', '(', 'po', ')', 'identif', 'invalid', 'two', 'common', 'mis-categoris', 'po', 'assign', 'noun', 'adject', 'incorrect', 'identif', 'proper', 'noun', '.']

 TOTAL PORTER STEM WORDS ==> 18



 ---- SNOWBALL STEMMING ----

['part-of-speech', '(', 'pos', ')', 'identif', 'invalid', 'two', 'common', 'mis-categoris', 'pos', 'assign', 'noun', 'adject', 'incorrect', 'identif', 'proper', 'noun', '.']

 TOTAL SNOWBALL STEM WORDS ==> 18



 ---- LEMMATIZATION ----

['Part-of-Speech', '(', 'POS', ')', 'Identification', 'Invalid', 'Two', 'common', 'mis-categorisations', 'POS', 'assignment', 'noun', 'adjective', 'incorrect', 'identification', 'Proper', 'Nouns', '.']

 TOTAL LEMMATIZE WORDS ==> 18

************************************************************************************************************************

71 --> Erroneous Sentence boundary Detection  • Sentence Boundary detection was often faulty due to the tokenising  of the newline character. 


 ---- TOKENS ----

 ['Erroneous', 'Sentence', 'boundary', 'Detection', '•', 'Sentence', 'Boundary', 'detection', 'was', 'often', 'faulty', 'due', 'to', 'the', 'tokenising', 'of', 'the', 'newline', 'character', '.'] 

 TOTAL TOKENS ==> 20

 ---- POST ----

 [('Erroneous', 'JJ'), ('Sentence', 'NNP'), ('boundary', 'JJ'), ('Detection', 'NNP'), ('•', 'NNP'), ('Sentence', 'NNP'), ('Boundary', 'NNP'), ('detection', 'NN'), ('was', 'VBD'), ('often', 'RB'), ('faulty', 'JJ'), ('due', 'JJ'), ('to', 'TO'), ('the', 'DT'), ('tokenising', 'NN'), ('of', 'IN'), ('the', 'DT'), ('newline', 'JJ'), ('character', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Erroneous', 'Sentence', 'boundary', 'Detection', '•', 'Sentence', 'Boundary', 'detection', 'often', 'faulty', 'due', 'tokenising', 'newline', 'character', '.']

 TOTAL FILTERED TOKENS ==>  15

 ---- POST FOR FILTERED TOKENS ----

 [('Erroneous', 'JJ'), ('Sentence', 'NNP'), ('boundary', 'JJ'), ('Detection', 'NNP'), ('•', 'NNP'), ('Sentence', 'NNP'), ('Boundary', 'NNP'), ('detection', 'NN'), ('often', 'RB'), ('faulty', 'JJ'), ('due', 'JJ'), ('tokenising', 'VBG'), ('newline', 'JJ'), ('character', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Erroneous Sentence', 'Sentence boundary', 'boundary Detection', 'Detection •', '• Sentence', 'Sentence Boundary', 'Boundary detection', 'detection often', 'often faulty', 'faulty due', 'due tokenising', 'tokenising newline', 'newline character', 'character .'] 

 TOTAL BIGRAMS --> 14 



 ---- TRI-GRAMS ---- 

 ['Erroneous Sentence boundary', 'Sentence boundary Detection', 'boundary Detection •', 'Detection • Sentence', '• Sentence Boundary', 'Sentence Boundary detection', 'Boundary detection often', 'detection often faulty', 'often faulty due', 'faulty due tokenising', 'due tokenising newline', 'tokenising newline character', 'newline character .'] 

 TOTAL TRIGRAMS --> 13 



 ---- NOUN PHRASES ---- 

 ['detection', 'newline character'] 

 TOTAL NOUN PHRASES --> 2 



 ---- NER ----

 
 ORGANIZATION ---> ['Sentence']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> ['Erroneous']
 TOTAL PERSON ENTITY --> 1 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['erron', 'sentenc', 'boundari', 'detect', '•', 'sentenc', 'boundari', 'detect', 'often', 'faulti', 'due', 'tokenis', 'newlin', 'charact', '.']

 TOTAL PORTER STEM WORDS ==> 15



 ---- SNOWBALL STEMMING ----

['erron', 'sentenc', 'boundari', 'detect', '•', 'sentenc', 'boundari', 'detect', 'often', 'faulti', 'due', 'tokenis', 'newlin', 'charact', '.']

 TOTAL SNOWBALL STEM WORDS ==> 15



 ---- LEMMATIZATION ----

['Erroneous', 'Sentence', 'boundary', 'Detection', '•', 'Sentence', 'Boundary', 'detection', 'often', 'faulty', 'due', 'tokenising', 'newline', 'character', '.']

 TOTAL LEMMATIZE WORDS ==> 15

************************************************************************************************************************

72 --> When the “n character was concatenated  with the following word often section names would become  unidentifiable e.g.- “nDiagnosis”. 


 ---- TOKENS ----

 ['When', 'the', '“', 'n', 'character', 'was', 'concatenated', 'with', 'the', 'following', 'word', 'often', 'section', 'names', 'would', 'become', 'unidentifiable', 'e.g.-', '“', 'nDiagnosis', '”', '.'] 

 TOTAL TOKENS ==> 22

 ---- POST ----

 [('When', 'WRB'), ('the', 'DT'), ('“', 'NNP'), ('n', 'JJ'), ('character', 'NN'), ('was', 'VBD'), ('concatenated', 'VBN'), ('with', 'IN'), ('the', 'DT'), ('following', 'JJ'), ('word', 'NN'), ('often', 'RB'), ('section', 'NN'), ('names', 'NNS'), ('would', 'MD'), ('become', 'VB'), ('unidentifiable', 'JJ'), ('e.g.-', 'JJ'), ('“', 'NN'), ('nDiagnosis', 'NN'), ('”', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['“', 'n', 'character', 'concatenated', 'following', 'word', 'often', 'section', 'names', 'would', 'become', 'unidentifiable', 'e.g.-', '“', 'nDiagnosis', '”', '.']

 TOTAL FILTERED TOKENS ==>  17

 ---- POST FOR FILTERED TOKENS ----

 [('“', 'NN'), ('n', 'JJ'), ('character', 'NN'), ('concatenated', 'VBD'), ('following', 'VBG'), ('word', 'NN'), ('often', 'RB'), ('section', 'NN'), ('names', 'NNS'), ('would', 'MD'), ('become', 'VB'), ('unidentifiable', 'JJ'), ('e.g.-', 'JJ'), ('“', 'NN'), ('nDiagnosis', 'NN'), ('”', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['“ n', 'n character', 'character concatenated', 'concatenated following', 'following word', 'word often', 'often section', 'section names', 'names would', 'would become', 'become unidentifiable', 'unidentifiable e.g.-', 'e.g.- “', '“ nDiagnosis', 'nDiagnosis ”', '” .'] 

 TOTAL BIGRAMS --> 16 



 ---- TRI-GRAMS ---- 

 ['“ n character', 'n character concatenated', 'character concatenated following', 'concatenated following word', 'following word often', 'word often section', 'often section names', 'section names would', 'names would become', 'would become unidentifiable', 'become unidentifiable e.g.-', 'unidentifiable e.g.- “', 'e.g.- “ nDiagnosis', '“ nDiagnosis ”', 'nDiagnosis ” .'] 

 TOTAL TRIGRAMS --> 15 



 ---- NOUN PHRASES ---- 

 ['“', 'n character', 'word', 'section', 'unidentifiable e.g.- “', 'nDiagnosis', '”'] 

 TOTAL NOUN PHRASES --> 7 



 ---- NER ----

 
 ORGANIZATION ---> ['nDiagnosis']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['“', 'n', 'charact', 'concaten', 'follow', 'word', 'often', 'section', 'name', 'would', 'becom', 'unidentifi', 'e.g.-', '“', 'ndiagnosi', '”', '.']

 TOTAL PORTER STEM WORDS ==> 17



 ---- SNOWBALL STEMMING ----

['“', 'n', 'charact', 'concaten', 'follow', 'word', 'often', 'section', 'name', 'would', 'becom', 'unidentifi', 'e.g.-', '“', 'ndiagnosi', '”', '.']

 TOTAL SNOWBALL STEM WORDS ==> 17



 ---- LEMMATIZATION ----

['“', 'n', 'character', 'concatenated', 'following', 'word', 'often', 'section', 'name', 'would', 'become', 'unidentifiable', 'e.g.-', '“', 'nDiagnosis', '”', '.']

 TOTAL LEMMATIZE WORDS ==> 17

************************************************************************************************************************

73 --> We regard this as a major failure  because of the cascading effect in correctly processing the  document. 


 ---- TOKENS ----

 ['We', 'regard', 'this', 'as', 'a', 'major', 'failure', 'because', 'of', 'the', 'cascading', 'effect', 'in', 'correctly', 'processing', 'the', 'document', '.'] 

 TOTAL TOKENS ==> 18

 ---- POST ----

 [('We', 'PRP'), ('regard', 'VBP'), ('this', 'DT'), ('as', 'IN'), ('a', 'DT'), ('major', 'JJ'), ('failure', 'NN'), ('because', 'IN'), ('of', 'IN'), ('the', 'DT'), ('cascading', 'VBG'), ('effect', 'NN'), ('in', 'IN'), ('correctly', 'RB'), ('processing', 'VBG'), ('the', 'DT'), ('document', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['regard', 'major', 'failure', 'cascading', 'effect', 'correctly', 'processing', 'document', '.']

 TOTAL FILTERED TOKENS ==>  9

 ---- POST FOR FILTERED TOKENS ----

 [('regard', 'NN'), ('major', 'JJ'), ('failure', 'NN'), ('cascading', 'VBG'), ('effect', 'NN'), ('correctly', 'RB'), ('processing', 'VBG'), ('document', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['regard major', 'major failure', 'failure cascading', 'cascading effect', 'effect correctly', 'correctly processing', 'processing document', 'document .'] 

 TOTAL BIGRAMS --> 8 



 ---- TRI-GRAMS ---- 

 ['regard major failure', 'major failure cascading', 'failure cascading effect', 'cascading effect correctly', 'effect correctly processing', 'correctly processing document', 'processing document .'] 

 TOTAL TRIGRAMS --> 7 



 ---- NOUN PHRASES ---- 

 ['regard', 'major failure', 'effect', 'document'] 

 TOTAL NOUN PHRASES --> 4 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['regard', 'major', 'failur', 'cascad', 'effect', 'correctli', 'process', 'document', '.']

 TOTAL PORTER STEM WORDS ==> 9



 ---- SNOWBALL STEMMING ----

['regard', 'major', 'failur', 'cascad', 'effect', 'correct', 'process', 'document', '.']

 TOTAL SNOWBALL STEM WORDS ==> 9



 ---- LEMMATIZATION ----

['regard', 'major', 'failure', 'cascading', 'effect', 'correctly', 'processing', 'document', '.']

 TOTAL LEMMATIZE WORDS ==> 9

************************************************************************************************************************

74 --> Deficiencies in the CER Algorithms   Inconsistent Relationship linking  • Identifying relationships between entities to a high accuracy is very  difficult and still very much a research topic. 


 ---- TOKENS ----

 ['Deficiencies', 'in', 'the', 'CER', 'Algorithms', 'Inconsistent', 'Relationship', 'linking', '•', 'Identifying', 'relationships', 'between', 'entities', 'to', 'a', 'high', 'accuracy', 'is', 'very', 'difficult', 'and', 'still', 'very', 'much', 'a', 'research', 'topic', '.'] 

 TOTAL TOKENS ==> 28

 ---- POST ----

 [('Deficiencies', 'NNS'), ('in', 'IN'), ('the', 'DT'), ('CER', 'NNP'), ('Algorithms', 'NNP'), ('Inconsistent', 'NNP'), ('Relationship', 'NNP'), ('linking', 'VBG'), ('•', 'NNP'), ('Identifying', 'VBG'), ('relationships', 'NNS'), ('between', 'IN'), ('entities', 'NNS'), ('to', 'TO'), ('a', 'DT'), ('high', 'JJ'), ('accuracy', 'NN'), ('is', 'VBZ'), ('very', 'RB'), ('difficult', 'JJ'), ('and', 'CC'), ('still', 'RB'), ('very', 'RB'), ('much', 'RB'), ('a', 'DT'), ('research', 'NN'), ('topic', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Deficiencies', 'CER', 'Algorithms', 'Inconsistent', 'Relationship', 'linking', '•', 'Identifying', 'relationships', 'entities', 'high', 'accuracy', 'difficult', 'still', 'much', 'research', 'topic', '.']

 TOTAL FILTERED TOKENS ==>  18

 ---- POST FOR FILTERED TOKENS ----

 [('Deficiencies', 'NNS'), ('CER', 'NNP'), ('Algorithms', 'NNP'), ('Inconsistent', 'NNP'), ('Relationship', 'NNP'), ('linking', 'VBG'), ('•', 'NNP'), ('Identifying', 'NNP'), ('relationships', 'NNS'), ('entities', 'NNS'), ('high', 'JJ'), ('accuracy', 'NN'), ('difficult', 'JJ'), ('still', 'RB'), ('much', 'JJ'), ('research', 'NN'), ('topic', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Deficiencies CER', 'CER Algorithms', 'Algorithms Inconsistent', 'Inconsistent Relationship', 'Relationship linking', 'linking •', '• Identifying', 'Identifying relationships', 'relationships entities', 'entities high', 'high accuracy', 'accuracy difficult', 'difficult still', 'still much', 'much research', 'research topic', 'topic .'] 

 TOTAL BIGRAMS --> 17 



 ---- TRI-GRAMS ---- 

 ['Deficiencies CER Algorithms', 'CER Algorithms Inconsistent', 'Algorithms Inconsistent Relationship', 'Inconsistent Relationship linking', 'Relationship linking •', 'linking • Identifying', '• Identifying relationships', 'Identifying relationships entities', 'relationships entities high', 'entities high accuracy', 'high accuracy difficult', 'accuracy difficult still', 'difficult still much', 'still much research', 'much research topic', 'research topic .'] 

 TOTAL TRIGRAMS --> 16 



 ---- NOUN PHRASES ---- 

 ['high accuracy', 'much research', 'topic'] 

 TOTAL NOUN PHRASES --> 3 



 ---- NER ----

 
 ORGANIZATION ---> ['CER Algorithms Inconsistent']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['defici', 'cer', 'algorithm', 'inconsist', 'relationship', 'link', '•', 'identifi', 'relationship', 'entiti', 'high', 'accuraci', 'difficult', 'still', 'much', 'research', 'topic', '.']

 TOTAL PORTER STEM WORDS ==> 18



 ---- SNOWBALL STEMMING ----

['defici', 'cer', 'algorithm', 'inconsist', 'relationship', 'link', '•', 'identifi', 'relationship', 'entiti', 'high', 'accuraci', 'difficult', 'still', 'much', 'research', 'topic', '.']

 TOTAL SNOWBALL STEM WORDS ==> 18



 ---- LEMMATIZATION ----

['Deficiencies', 'CER', 'Algorithms', 'Inconsistent', 'Relationship', 'linking', '•', 'Identifying', 'relationship', 'entity', 'high', 'accuracy', 'difficult', 'still', 'much', 'research', 'topic', '.']

 TOTAL LEMMATIZE WORDS ==> 18

************************************************************************************************************************

75 --> Systems that do  identify relationships need to be very careful but at least should be  consistent in its pairings which, from our observations, commonly  they are not. 


 ---- TOKENS ----

 ['Systems', 'that', 'do', 'identify', 'relationships', 'need', 'to', 'be', 'very', 'careful', 'but', 'at', 'least', 'should', 'be', 'consistent', 'in', 'its', 'pairings', 'which', ',', 'from', 'our', 'observations', ',', 'commonly', 'they', 'are', 'not', '.'] 

 TOTAL TOKENS ==> 30

 ---- POST ----

 [('Systems', 'NNS'), ('that', 'WDT'), ('do', 'VBP'), ('identify', 'VB'), ('relationships', 'NNS'), ('need', 'VB'), ('to', 'TO'), ('be', 'VB'), ('very', 'RB'), ('careful', 'JJ'), ('but', 'CC'), ('at', 'IN'), ('least', 'JJS'), ('should', 'MD'), ('be', 'VB'), ('consistent', 'JJ'), ('in', 'IN'), ('its', 'PRP$'), ('pairings', 'NNS'), ('which', 'WDT'), (',', ','), ('from', 'IN'), ('our', 'PRP$'), ('observations', 'NNS'), (',', ','), ('commonly', 'RB'), ('they', 'PRP'), ('are', 'VBP'), ('not', 'RB'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Systems', 'identify', 'relationships', 'need', 'careful', 'least', 'consistent', 'pairings', ',', 'observations', ',', 'commonly', '.']

 TOTAL FILTERED TOKENS ==>  13

 ---- POST FOR FILTERED TOKENS ----

 [('Systems', 'NNP'), ('identify', 'VBZ'), ('relationships', 'NNS'), ('need', 'VBP'), ('careful', 'JJ'), ('least', 'RBS'), ('consistent', 'JJ'), ('pairings', 'NNS'), (',', ','), ('observations', 'NNS'), (',', ','), ('commonly', 'RB'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Systems identify', 'identify relationships', 'relationships need', 'need careful', 'careful least', 'least consistent', 'consistent pairings', 'pairings ,', ', observations', 'observations ,', ', commonly', 'commonly .'] 

 TOTAL BIGRAMS --> 12 



 ---- TRI-GRAMS ---- 

 ['Systems identify relationships', 'identify relationships need', 'relationships need careful', 'need careful least', 'careful least consistent', 'least consistent pairings', 'consistent pairings ,', 'pairings , observations', ', observations ,', 'observations , commonly', ', commonly .'] 

 TOTAL TRIGRAMS --> 11 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> ['Systems']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['system', 'identifi', 'relationship', 'need', 'care', 'least', 'consist', 'pair', ',', 'observ', ',', 'commonli', '.']

 TOTAL PORTER STEM WORDS ==> 13



 ---- SNOWBALL STEMMING ----

['system', 'identifi', 'relationship', 'need', 'care', 'least', 'consist', 'pair', ',', 'observ', ',', 'common', '.']

 TOTAL SNOWBALL STEM WORDS ==> 13



 ---- LEMMATIZATION ----

['Systems', 'identify', 'relationship', 'need', 'careful', 'least', 'consistent', 'pairing', ',', 'observation', ',', 'commonly', '.']

 TOTAL LEMMATIZE WORDS ==> 13

************************************************************************************************************************

76 --> Mistakes in Graphical Representation of Relationships   • Drawing lines to connect related entities and labeling graphically is  helpful in interpreting the computational constructions, but they  need to connect the correct entities, and not create false  relationships. 


 ---- TOKENS ----

 ['Mistakes', 'in', 'Graphical', 'Representation', 'of', 'Relationships', '•', 'Drawing', 'lines', 'to', 'connect', 'related', 'entities', 'and', 'labeling', 'graphically', 'is', 'helpful', 'in', 'interpreting', 'the', 'computational', 'constructions', ',', 'but', 'they', 'need', 'to', 'connect', 'the', 'correct', 'entities', ',', 'and', 'not', 'create', 'false', 'relationships', '.'] 

 TOTAL TOKENS ==> 39

 ---- POST ----

 [('Mistakes', 'NNS'), ('in', 'IN'), ('Graphical', 'NNP'), ('Representation', 'NNP'), ('of', 'IN'), ('Relationships', 'NNP'), ('•', 'NNP'), ('Drawing', 'NNP'), ('lines', 'NNS'), ('to', 'TO'), ('connect', 'VB'), ('related', 'JJ'), ('entities', 'NNS'), ('and', 'CC'), ('labeling', 'VBG'), ('graphically', 'RB'), ('is', 'VBZ'), ('helpful', 'JJ'), ('in', 'IN'), ('interpreting', 'VBG'), ('the', 'DT'), ('computational', 'JJ'), ('constructions', 'NNS'), (',', ','), ('but', 'CC'), ('they', 'PRP'), ('need', 'VBP'), ('to', 'TO'), ('connect', 'VB'), ('the', 'DT'), ('correct', 'JJ'), ('entities', 'NNS'), (',', ','), ('and', 'CC'), ('not', 'RB'), ('create', 'VB'), ('false', 'JJ'), ('relationships', 'NNS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Mistakes', 'Graphical', 'Representation', 'Relationships', '•', 'Drawing', 'lines', 'connect', 'related', 'entities', 'labeling', 'graphically', 'helpful', 'interpreting', 'computational', 'constructions', ',', 'need', 'connect', 'correct', 'entities', ',', 'create', 'false', 'relationships', '.']

 TOTAL FILTERED TOKENS ==>  26

 ---- POST FOR FILTERED TOKENS ----

 [('Mistakes', 'NNP'), ('Graphical', 'NNP'), ('Representation', 'NNP'), ('Relationships', 'NNP'), ('•', 'NNP'), ('Drawing', 'NNP'), ('lines', 'NNS'), ('connect', 'VBP'), ('related', 'JJ'), ('entities', 'NNS'), ('labeling', 'VBG'), ('graphically', 'RB'), ('helpful', 'JJ'), ('interpreting', 'VBG'), ('computational', 'JJ'), ('constructions', 'NNS'), (',', ','), ('need', 'VBP'), ('connect', 'JJ'), ('correct', 'JJ'), ('entities', 'NNS'), (',', ','), ('create', 'NN'), ('false', 'JJ'), ('relationships', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Mistakes Graphical', 'Graphical Representation', 'Representation Relationships', 'Relationships •', '• Drawing', 'Drawing lines', 'lines connect', 'connect related', 'related entities', 'entities labeling', 'labeling graphically', 'graphically helpful', 'helpful interpreting', 'interpreting computational', 'computational constructions', 'constructions ,', ', need', 'need connect', 'connect correct', 'correct entities', 'entities ,', ', create', 'create false', 'false relationships', 'relationships .'] 

 TOTAL BIGRAMS --> 25 



 ---- TRI-GRAMS ---- 

 ['Mistakes Graphical Representation', 'Graphical Representation Relationships', 'Representation Relationships •', 'Relationships • Drawing', '• Drawing lines', 'Drawing lines connect', 'lines connect related', 'connect related entities', 'related entities labeling', 'entities labeling graphically', 'labeling graphically helpful', 'graphically helpful interpreting', 'helpful interpreting computational', 'interpreting computational constructions', 'computational constructions ,', 'constructions , need', ', need connect', 'need connect correct', 'connect correct entities', 'correct entities ,', 'entities , create', ', create false', 'create false relationships', 'false relationships .'] 

 TOTAL TRIGRAMS --> 24 



 ---- NOUN PHRASES ---- 

 ['create'] 

 TOTAL NOUN PHRASES --> 1 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> ['Mistakes', 'Graphical']
 TOTAL PERSON ENTITY --> 2 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['mistak', 'graphic', 'represent', 'relationship', '•', 'draw', 'line', 'connect', 'relat', 'entiti', 'label', 'graphic', 'help', 'interpret', 'comput', 'construct', ',', 'need', 'connect', 'correct', 'entiti', ',', 'creat', 'fals', 'relationship', '.']

 TOTAL PORTER STEM WORDS ==> 26



 ---- SNOWBALL STEMMING ----

['mistak', 'graphic', 'represent', 'relationship', '•', 'draw', 'line', 'connect', 'relat', 'entiti', 'label', 'graphic', 'help', 'interpret', 'comput', 'construct', ',', 'need', 'connect', 'correct', 'entiti', ',', 'creat', 'fals', 'relationship', '.']

 TOTAL SNOWBALL STEM WORDS ==> 26



 ---- LEMMATIZATION ----

['Mistakes', 'Graphical', 'Representation', 'Relationships', '•', 'Drawing', 'line', 'connect', 'related', 'entity', 'labeling', 'graphically', 'helpful', 'interpreting', 'computational', 'construction', ',', 'need', 'connect', 'correct', 'entity', ',', 'create', 'false', 'relationship', '.']

 TOTAL LEMMATIZE WORDS ==> 26

************************************************************************************************************************

77 --> The danger here is that a visually appealing graphical  representation carries a lot of weight and errors are therefore easily  accepted. 


 ---- TOKENS ----

 ['The', 'danger', 'here', 'is', 'that', 'a', 'visually', 'appealing', 'graphical', 'representation', 'carries', 'a', 'lot', 'of', 'weight', 'and', 'errors', 'are', 'therefore', 'easily', 'accepted', '.'] 

 TOTAL TOKENS ==> 22

 ---- POST ----

 [('The', 'DT'), ('danger', 'NN'), ('here', 'RB'), ('is', 'VBZ'), ('that', 'IN'), ('a', 'DT'), ('visually', 'RB'), ('appealing', 'VBG'), ('graphical', 'JJ'), ('representation', 'NN'), ('carries', 'VBZ'), ('a', 'DT'), ('lot', 'NN'), ('of', 'IN'), ('weight', 'NN'), ('and', 'CC'), ('errors', 'NNS'), ('are', 'VBP'), ('therefore', 'RB'), ('easily', 'RB'), ('accepted', 'VBN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['danger', 'visually', 'appealing', 'graphical', 'representation', 'carries', 'lot', 'weight', 'errors', 'therefore', 'easily', 'accepted', '.']

 TOTAL FILTERED TOKENS ==>  13

 ---- POST FOR FILTERED TOKENS ----

 [('danger', 'NN'), ('visually', 'RB'), ('appealing', 'VBG'), ('graphical', 'JJ'), ('representation', 'NN'), ('carries', 'VBZ'), ('lot', 'NN'), ('weight', 'JJ'), ('errors', 'NNS'), ('therefore', 'VBP'), ('easily', 'RB'), ('accepted', 'VBN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['danger visually', 'visually appealing', 'appealing graphical', 'graphical representation', 'representation carries', 'carries lot', 'lot weight', 'weight errors', 'errors therefore', 'therefore easily', 'easily accepted', 'accepted .'] 

 TOTAL BIGRAMS --> 12 



 ---- TRI-GRAMS ---- 

 ['danger visually appealing', 'visually appealing graphical', 'appealing graphical representation', 'graphical representation carries', 'representation carries lot', 'carries lot weight', 'lot weight errors', 'weight errors therefore', 'errors therefore easily', 'therefore easily accepted', 'easily accepted .'] 

 TOTAL TRIGRAMS --> 11 



 ---- NOUN PHRASES ---- 

 ['danger', 'graphical representation', 'lot'] 

 TOTAL NOUN PHRASES --> 3 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['danger', 'visual', 'appeal', 'graphic', 'represent', 'carri', 'lot', 'weight', 'error', 'therefor', 'easili', 'accept', '.']

 TOTAL PORTER STEM WORDS ==> 13



 ---- SNOWBALL STEMMING ----

['danger', 'visual', 'appeal', 'graphic', 'represent', 'carri', 'lot', 'weight', 'error', 'therefor', 'easili', 'accept', '.']

 TOTAL SNOWBALL STEM WORDS ==> 13



 ---- LEMMATIZATION ----

['danger', 'visually', 'appealing', 'graphical', 'representation', 'carry', 'lot', 'weight', 'error', 'therefore', 'easily', 'accepted', '.']

 TOTAL LEMMATIZE WORDS ==> 13

************************************************************************************************************************

78 --> Intrusive Newlines  • Interference in recognising the correct extent of an entity can be  due to newline characters. 


 ---- TOKENS ----

 ['Intrusive', 'Newlines', '•', 'Interference', 'in', 'recognising', 'the', 'correct', 'extent', 'of', 'an', 'entity', 'can', 'be', 'due', 'to', 'newline', 'characters', '.'] 

 TOTAL TOKENS ==> 19

 ---- POST ----

 [('Intrusive', 'JJ'), ('Newlines', 'NNP'), ('•', 'NNP'), ('Interference', 'NNP'), ('in', 'IN'), ('recognising', 'VBG'), ('the', 'DT'), ('correct', 'JJ'), ('extent', 'NN'), ('of', 'IN'), ('an', 'DT'), ('entity', 'NN'), ('can', 'MD'), ('be', 'VB'), ('due', 'JJ'), ('to', 'TO'), ('newline', 'JJ'), ('characters', 'NNS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Intrusive', 'Newlines', '•', 'Interference', 'recognising', 'correct', 'extent', 'entity', 'due', 'newline', 'characters', '.']

 TOTAL FILTERED TOKENS ==>  12

 ---- POST FOR FILTERED TOKENS ----

 [('Intrusive', 'JJ'), ('Newlines', 'NNP'), ('•', 'NNP'), ('Interference', 'NNP'), ('recognising', 'VBG'), ('correct', 'JJ'), ('extent', 'NN'), ('entity', 'NN'), ('due', 'JJ'), ('newline', 'NN'), ('characters', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Intrusive Newlines', 'Newlines •', '• Interference', 'Interference recognising', 'recognising correct', 'correct extent', 'extent entity', 'entity due', 'due newline', 'newline characters', 'characters .'] 

 TOTAL BIGRAMS --> 11 



 ---- TRI-GRAMS ---- 

 ['Intrusive Newlines •', 'Newlines • Interference', '• Interference recognising', 'Interference recognising correct', 'recognising correct extent', 'correct extent entity', 'extent entity due', 'entity due newline', 'due newline characters', 'newline characters .'] 

 TOTAL TRIGRAMS --> 10 



 ---- NOUN PHRASES ---- 

 ['correct extent', 'entity', 'due newline'] 

 TOTAL NOUN PHRASES --> 3 



 ---- NER ----

 
 ORGANIZATION ---> ['Newlines']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['intrus', 'newlin', '•', 'interfer', 'recognis', 'correct', 'extent', 'entiti', 'due', 'newlin', 'charact', '.']

 TOTAL PORTER STEM WORDS ==> 12



 ---- SNOWBALL STEMMING ----

['intrus', 'newlin', '•', 'interfer', 'recognis', 'correct', 'extent', 'entiti', 'due', 'newlin', 'charact', '.']

 TOTAL SNOWBALL STEM WORDS ==> 12



 ---- LEMMATIZATION ----

['Intrusive', 'Newlines', '•', 'Interference', 'recognising', 'correct', 'extent', 'entity', 'due', 'newline', 'character', '.']

 TOTAL LEMMATIZE WORDS ==> 12

************************************************************************************************************************

79 --> Entity recognition can be seriously  imperiled by newline characters distributed throughout the text as  is very commonplace in pathology reports. 


 ---- TOKENS ----

 ['Entity', 'recognition', 'can', 'be', 'seriously', 'imperiled', 'by', 'newline', 'characters', 'distributed', 'throughout', 'the', 'text', 'as', 'is', 'very', 'commonplace', 'in', 'pathology', 'reports', '.'] 

 TOTAL TOKENS ==> 21

 ---- POST ----

 [('Entity', 'NNP'), ('recognition', 'NN'), ('can', 'MD'), ('be', 'VB'), ('seriously', 'RB'), ('imperiled', 'VBN'), ('by', 'IN'), ('newline', 'JJ'), ('characters', 'NNS'), ('distributed', 'VBD'), ('throughout', 'IN'), ('the', 'DT'), ('text', 'NN'), ('as', 'IN'), ('is', 'VBZ'), ('very', 'RB'), ('commonplace', 'JJ'), ('in', 'IN'), ('pathology', 'NN'), ('reports', 'NNS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Entity', 'recognition', 'seriously', 'imperiled', 'newline', 'characters', 'distributed', 'throughout', 'text', 'commonplace', 'pathology', 'reports', '.']

 TOTAL FILTERED TOKENS ==>  13

 ---- POST FOR FILTERED TOKENS ----

 [('Entity', 'NNP'), ('recognition', 'NN'), ('seriously', 'RB'), ('imperiled', 'VBD'), ('newline', 'JJ'), ('characters', 'NNS'), ('distributed', 'VBD'), ('throughout', 'IN'), ('text', 'JJ'), ('commonplace', 'NN'), ('pathology', 'NN'), ('reports', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Entity recognition', 'recognition seriously', 'seriously imperiled', 'imperiled newline', 'newline characters', 'characters distributed', 'distributed throughout', 'throughout text', 'text commonplace', 'commonplace pathology', 'pathology reports', 'reports .'] 

 TOTAL BIGRAMS --> 12 



 ---- TRI-GRAMS ---- 

 ['Entity recognition seriously', 'recognition seriously imperiled', 'seriously imperiled newline', 'imperiled newline characters', 'newline characters distributed', 'characters distributed throughout', 'distributed throughout text', 'throughout text commonplace', 'text commonplace pathology', 'commonplace pathology reports', 'pathology reports .'] 

 TOTAL TRIGRAMS --> 11 



 ---- NOUN PHRASES ---- 

 ['recognition', 'text commonplace', 'pathology'] 

 TOTAL NOUN PHRASES --> 3 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> ['Entity']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['entiti', 'recognit', 'serious', 'imperil', 'newlin', 'charact', 'distribut', 'throughout', 'text', 'commonplac', 'patholog', 'report', '.']

 TOTAL PORTER STEM WORDS ==> 13



 ---- SNOWBALL STEMMING ----

['entiti', 'recognit', 'serious', 'imperil', 'newlin', 'charact', 'distribut', 'throughout', 'text', 'commonplac', 'patholog', 'report', '.']

 TOTAL SNOWBALL STEM WORDS ==> 13



 ---- LEMMATIZATION ----

['Entity', 'recognition', 'seriously', 'imperiled', 'newline', 'character', 'distributed', 'throughout', 'text', 'commonplace', 'pathology', 'report', '.']

 TOTAL LEMMATIZE WORDS ==> 13

************************************************************************************************************************

80 --> It is important to use a  pre-processor cleaning mechanisms to remove these extraneous  characters so that an entity is properly recognised even if a non- printable character is buried within its extent. 


 ---- TOKENS ----

 ['It', 'is', 'important', 'to', 'use', 'a', 'pre-processor', 'cleaning', 'mechanisms', 'to', 'remove', 'these', 'extraneous', 'characters', 'so', 'that', 'an', 'entity', 'is', 'properly', 'recognised', 'even', 'if', 'a', 'non-', 'printable', 'character', 'is', 'buried', 'within', 'its', 'extent', '.'] 

 TOTAL TOKENS ==> 33

 ---- POST ----

 [('It', 'PRP'), ('is', 'VBZ'), ('important', 'JJ'), ('to', 'TO'), ('use', 'VB'), ('a', 'DT'), ('pre-processor', 'JJ'), ('cleaning', 'NN'), ('mechanisms', 'NNS'), ('to', 'TO'), ('remove', 'VB'), ('these', 'DT'), ('extraneous', 'JJ'), ('characters', 'NNS'), ('so', 'RB'), ('that', 'IN'), ('an', 'DT'), ('entity', 'NN'), ('is', 'VBZ'), ('properly', 'RB'), ('recognised', 'VBN'), ('even', 'RB'), ('if', 'IN'), ('a', 'DT'), ('non-', 'JJ'), ('printable', 'JJ'), ('character', 'NN'), ('is', 'VBZ'), ('buried', 'VBN'), ('within', 'IN'), ('its', 'PRP$'), ('extent', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['important', 'use', 'pre-processor', 'cleaning', 'mechanisms', 'remove', 'extraneous', 'characters', 'entity', 'properly', 'recognised', 'even', 'non-', 'printable', 'character', 'buried', 'within', 'extent', '.']

 TOTAL FILTERED TOKENS ==>  19

 ---- POST FOR FILTERED TOKENS ----

 [('important', 'JJ'), ('use', 'NN'), ('pre-processor', 'JJ'), ('cleaning', 'NN'), ('mechanisms', 'NNS'), ('remove', 'VBP'), ('extraneous', 'JJ'), ('characters', 'NNS'), ('entity', 'VBP'), ('properly', 'RB'), ('recognised', 'VBN'), ('even', 'RB'), ('non-', 'JJ'), ('printable', 'JJ'), ('character', 'NN'), ('buried', 'VBN'), ('within', 'IN'), ('extent', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['important use', 'use pre-processor', 'pre-processor cleaning', 'cleaning mechanisms', 'mechanisms remove', 'remove extraneous', 'extraneous characters', 'characters entity', 'entity properly', 'properly recognised', 'recognised even', 'even non-', 'non- printable', 'printable character', 'character buried', 'buried within', 'within extent', 'extent .'] 

 TOTAL BIGRAMS --> 18 



 ---- TRI-GRAMS ---- 

 ['important use pre-processor', 'use pre-processor cleaning', 'pre-processor cleaning mechanisms', 'cleaning mechanisms remove', 'mechanisms remove extraneous', 'remove extraneous characters', 'extraneous characters entity', 'characters entity properly', 'entity properly recognised', 'properly recognised even', 'recognised even non-', 'even non- printable', 'non- printable character', 'printable character buried', 'character buried within', 'buried within extent', 'within extent .'] 

 TOTAL TRIGRAMS --> 17 



 ---- NOUN PHRASES ---- 

 ['important use', 'pre-processor cleaning', 'non- printable character', 'extent'] 

 TOTAL NOUN PHRASES --> 4 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['import', 'use', 'pre-processor', 'clean', 'mechan', 'remov', 'extran', 'charact', 'entiti', 'properli', 'recognis', 'even', 'non-', 'printabl', 'charact', 'buri', 'within', 'extent', '.']

 TOTAL PORTER STEM WORDS ==> 19



 ---- SNOWBALL STEMMING ----

['import', 'use', 'pre-processor', 'clean', 'mechan', 'remov', 'extran', 'charact', 'entiti', 'proper', 'recognis', 'even', 'non-', 'printabl', 'charact', 'buri', 'within', 'extent', '.']

 TOTAL SNOWBALL STEM WORDS ==> 19



 ---- LEMMATIZATION ----

['important', 'use', 'pre-processor', 'cleaning', 'mechanism', 'remove', 'extraneous', 'character', 'entity', 'properly', 'recognised', 'even', 'non-', 'printable', 'character', 'buried', 'within', 'extent', '.']

 TOTAL LEMMATIZE WORDS ==> 19

************************************************************************************************************************

81 --> Deficiencies in Semantics and Understanding Medical  Terminology   Unawareness of Anatomical Hierarchy  • Unawareness of conventional anatomical hierarchy. 


 ---- TOKENS ----

 ['Deficiencies', 'in', 'Semantics', 'and', 'Understanding', 'Medical', 'Terminology', 'Unawareness', 'of', 'Anatomical', 'Hierarchy', '•', 'Unawareness', 'of', 'conventional', 'anatomical', 'hierarchy', '.'] 

 TOTAL TOKENS ==> 18

 ---- POST ----

 [('Deficiencies', 'NNS'), ('in', 'IN'), ('Semantics', 'NNP'), ('and', 'CC'), ('Understanding', 'NNP'), ('Medical', 'NNP'), ('Terminology', 'NNP'), ('Unawareness', 'NNP'), ('of', 'IN'), ('Anatomical', 'NNP'), ('Hierarchy', 'NNP'), ('•', 'NNP'), ('Unawareness', 'NNP'), ('of', 'IN'), ('conventional', 'JJ'), ('anatomical', 'JJ'), ('hierarchy', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Deficiencies', 'Semantics', 'Understanding', 'Medical', 'Terminology', 'Unawareness', 'Anatomical', 'Hierarchy', '•', 'Unawareness', 'conventional', 'anatomical', 'hierarchy', '.']

 TOTAL FILTERED TOKENS ==>  14

 ---- POST FOR FILTERED TOKENS ----

 [('Deficiencies', 'NNS'), ('Semantics', 'NNPS'), ('Understanding', 'NNP'), ('Medical', 'NNP'), ('Terminology', 'NNP'), ('Unawareness', 'NNP'), ('Anatomical', 'NNP'), ('Hierarchy', 'NNP'), ('•', 'NNP'), ('Unawareness', 'NNP'), ('conventional', 'JJ'), ('anatomical', 'JJ'), ('hierarchy', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Deficiencies Semantics', 'Semantics Understanding', 'Understanding Medical', 'Medical Terminology', 'Terminology Unawareness', 'Unawareness Anatomical', 'Anatomical Hierarchy', 'Hierarchy •', '• Unawareness', 'Unawareness conventional', 'conventional anatomical', 'anatomical hierarchy', 'hierarchy .'] 

 TOTAL BIGRAMS --> 13 



 ---- TRI-GRAMS ---- 

 ['Deficiencies Semantics Understanding', 'Semantics Understanding Medical', 'Understanding Medical Terminology', 'Medical Terminology Unawareness', 'Terminology Unawareness Anatomical', 'Unawareness Anatomical Hierarchy', 'Anatomical Hierarchy •', 'Hierarchy • Unawareness', '• Unawareness conventional', 'Unawareness conventional anatomical', 'conventional anatomical hierarchy', 'anatomical hierarchy .'] 

 TOTAL TRIGRAMS --> 12 



 ---- NOUN PHRASES ---- 

 ['conventional anatomical hierarchy'] 

 TOTAL NOUN PHRASES --> 1 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> ['Semantics Understanding Medical Terminology Unawareness Anatomical Hierarchy']
 TOTAL PERSON ENTITY --> 1 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['defici', 'semant', 'understand', 'medic', 'terminolog', 'unawar', 'anatom', 'hierarchi', '•', 'unawar', 'convent', 'anatom', 'hierarchi', '.']

 TOTAL PORTER STEM WORDS ==> 14



 ---- SNOWBALL STEMMING ----

['defici', 'semant', 'understand', 'medic', 'terminolog', 'unawar', 'anatom', 'hierarchi', '•', 'unawar', 'convent', 'anatom', 'hierarchi', '.']

 TOTAL SNOWBALL STEM WORDS ==> 14



 ---- LEMMATIZATION ----

['Deficiencies', 'Semantics', 'Understanding', 'Medical', 'Terminology', 'Unawareness', 'Anatomical', 'Hierarchy', '•', 'Unawareness', 'conventional', 'anatomical', 'hierarchy', '.']

 TOTAL LEMMATIZE WORDS ==> 14

************************************************************************************************************************

82 --> Some  processing shows a lack of awareness of the general anatomical  hierarchy, e.g.-  cell components mislabeled as anatomical class. 


 ---- TOKENS ----

 ['Some', 'processing', 'shows', 'a', 'lack', 'of', 'awareness', 'of', 'the', 'general', 'anatomical', 'hierarchy', ',', 'e.g.-', 'cell', 'components', 'mislabeled', 'as', 'anatomical', 'class', '.'] 

 TOTAL TOKENS ==> 21

 ---- POST ----

 [('Some', 'DT'), ('processing', 'NN'), ('shows', 'VBZ'), ('a', 'DT'), ('lack', 'NN'), ('of', 'IN'), ('awareness', 'NN'), ('of', 'IN'), ('the', 'DT'), ('general', 'JJ'), ('anatomical', 'JJ'), ('hierarchy', 'NN'), (',', ','), ('e.g.-', 'JJ'), ('cell', 'NN'), ('components', 'NNS'), ('mislabeled', 'VBD'), ('as', 'IN'), ('anatomical', 'JJ'), ('class', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['processing', 'shows', 'lack', 'awareness', 'general', 'anatomical', 'hierarchy', ',', 'e.g.-', 'cell', 'components', 'mislabeled', 'anatomical', 'class', '.']

 TOTAL FILTERED TOKENS ==>  15

 ---- POST FOR FILTERED TOKENS ----

 [('processing', 'NN'), ('shows', 'NNS'), ('lack', 'VBP'), ('awareness', 'JJ'), ('general', 'JJ'), ('anatomical', 'JJ'), ('hierarchy', 'NN'), (',', ','), ('e.g.-', 'JJ'), ('cell', 'NN'), ('components', 'NNS'), ('mislabeled', 'VBD'), ('anatomical', 'JJ'), ('class', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['processing shows', 'shows lack', 'lack awareness', 'awareness general', 'general anatomical', 'anatomical hierarchy', 'hierarchy ,', ', e.g.-', 'e.g.- cell', 'cell components', 'components mislabeled', 'mislabeled anatomical', 'anatomical class', 'class .'] 

 TOTAL BIGRAMS --> 14 



 ---- TRI-GRAMS ---- 

 ['processing shows lack', 'shows lack awareness', 'lack awareness general', 'awareness general anatomical', 'general anatomical hierarchy', 'anatomical hierarchy ,', 'hierarchy , e.g.-', ', e.g.- cell', 'e.g.- cell components', 'cell components mislabeled', 'components mislabeled anatomical', 'mislabeled anatomical class', 'anatomical class .'] 

 TOTAL TRIGRAMS --> 13 



 ---- NOUN PHRASES ---- 

 ['processing', 'awareness general anatomical hierarchy', 'e.g.- cell', 'anatomical class'] 

 TOTAL NOUN PHRASES --> 4 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['process', 'show', 'lack', 'awar', 'gener', 'anatom', 'hierarchi', ',', 'e.g.-', 'cell', 'compon', 'mislabel', 'anatom', 'class', '.']

 TOTAL PORTER STEM WORDS ==> 15



 ---- SNOWBALL STEMMING ----

['process', 'show', 'lack', 'awar', 'general', 'anatom', 'hierarchi', ',', 'e.g.-', 'cell', 'compon', 'mislabel', 'anatom', 'class', '.']

 TOTAL SNOWBALL STEM WORDS ==> 15



 ---- LEMMATIZATION ----

['processing', 'show', 'lack', 'awareness', 'general', 'anatomical', 'hierarchy', ',', 'e.g.-', 'cell', 'component', 'mislabeled', 'anatomical', 'class', '.']

 TOTAL LEMMATIZE WORDS ==> 15

************************************************************************************************************************

83 --> • Poor utilisation of body structure ontology. 


 ---- TOKENS ----

 ['•', 'Poor', 'utilisation', 'of', 'body', 'structure', 'ontology', '.'] 

 TOTAL TOKENS ==> 8

 ---- POST ----

 [('•', 'JJ'), ('Poor', 'NNP'), ('utilisation', 'NN'), ('of', 'IN'), ('body', 'NN'), ('structure', 'NN'), ('ontology', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['•', 'Poor', 'utilisation', 'body', 'structure', 'ontology', '.']

 TOTAL FILTERED TOKENS ==>  7

 ---- POST FOR FILTERED TOKENS ----

 [('•', 'JJ'), ('Poor', 'NNP'), ('utilisation', 'NN'), ('body', 'NN'), ('structure', 'NN'), ('ontology', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['• Poor', 'Poor utilisation', 'utilisation body', 'body structure', 'structure ontology', 'ontology .'] 

 TOTAL BIGRAMS --> 6 



 ---- TRI-GRAMS ---- 

 ['• Poor utilisation', 'Poor utilisation body', 'utilisation body structure', 'body structure ontology', 'structure ontology .'] 

 TOTAL TRIGRAMS --> 5 



 ---- NOUN PHRASES ---- 

 ['utilisation', 'body', 'structure', 'ontology'] 

 TOTAL NOUN PHRASES --> 4 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['•', 'poor', 'utilis', 'bodi', 'structur', 'ontolog', '.']

 TOTAL PORTER STEM WORDS ==> 7



 ---- SNOWBALL STEMMING ----

['•', 'poor', 'utilis', 'bodi', 'structur', 'ontolog', '.']

 TOTAL SNOWBALL STEM WORDS ==> 7



 ---- LEMMATIZATION ----

['•', 'Poor', 'utilisation', 'body', 'structure', 'ontology', '.']

 TOTAL LEMMATIZE WORDS ==> 7

************************************************************************************************************************

84 --> Lack of a comprehensive medical vocabulary. 


 ---- TOKENS ----

 ['Lack', 'of', 'a', 'comprehensive', 'medical', 'vocabulary', '.'] 

 TOTAL TOKENS ==> 7

 ---- POST ----

 [('Lack', 'NN'), ('of', 'IN'), ('a', 'DT'), ('comprehensive', 'JJ'), ('medical', 'JJ'), ('vocabulary', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Lack', 'comprehensive', 'medical', 'vocabulary', '.']

 TOTAL FILTERED TOKENS ==>  5

 ---- POST FOR FILTERED TOKENS ----

 [('Lack', 'NNP'), ('comprehensive', 'JJ'), ('medical', 'JJ'), ('vocabulary', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Lack comprehensive', 'comprehensive medical', 'medical vocabulary', 'vocabulary .'] 

 TOTAL BIGRAMS --> 4 



 ---- TRI-GRAMS ---- 

 ['Lack comprehensive medical', 'comprehensive medical vocabulary', 'medical vocabulary .'] 

 TOTAL TRIGRAMS --> 3 



 ---- NOUN PHRASES ---- 

 ['comprehensive medical vocabulary'] 

 TOTAL NOUN PHRASES --> 1 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> ['Lack']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['lack', 'comprehens', 'medic', 'vocabulari', '.']

 TOTAL PORTER STEM WORDS ==> 5



 ---- SNOWBALL STEMMING ----

['lack', 'comprehens', 'medic', 'vocabulari', '.']

 TOTAL SNOWBALL STEM WORDS ==> 5



 ---- LEMMATIZATION ----

['Lack', 'comprehensive', 'medical', 'vocabulary', '.']

 TOTAL LEMMATIZE WORDS ==> 5

************************************************************************************************************************

85 --> •  Common medical words that should readily match to a known  name go unrecognised, e.g.- oophorectomy. 


 ---- TOKENS ----

 ['•', 'Common', 'medical', 'words', 'that', 'should', 'readily', 'match', 'to', 'a', 'known', 'name', 'go', 'unrecognised', ',', 'e.g.-', 'oophorectomy', '.'] 

 TOTAL TOKENS ==> 18

 ---- POST ----

 [('•', 'JJ'), ('Common', 'NNP'), ('medical', 'JJ'), ('words', 'NNS'), ('that', 'WDT'), ('should', 'MD'), ('readily', 'RB'), ('match', 'VB'), ('to', 'TO'), ('a', 'DT'), ('known', 'VBN'), ('name', 'NN'), ('go', 'NN'), ('unrecognised', 'JJ'), (',', ','), ('e.g.-', 'JJ'), ('oophorectomy', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['•', 'Common', 'medical', 'words', 'readily', 'match', 'known', 'name', 'go', 'unrecognised', ',', 'e.g.-', 'oophorectomy', '.']

 TOTAL FILTERED TOKENS ==>  14

 ---- POST FOR FILTERED TOKENS ----

 [('•', 'JJ'), ('Common', 'NNP'), ('medical', 'JJ'), ('words', 'NNS'), ('readily', 'RB'), ('match', 'VB'), ('known', 'NN'), ('name', 'NN'), ('go', 'VBP'), ('unrecognised', 'JJ'), (',', ','), ('e.g.-', 'JJ'), ('oophorectomy', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['• Common', 'Common medical', 'medical words', 'words readily', 'readily match', 'match known', 'known name', 'name go', 'go unrecognised', 'unrecognised ,', ', e.g.-', 'e.g.- oophorectomy', 'oophorectomy .'] 

 TOTAL BIGRAMS --> 13 



 ---- TRI-GRAMS ---- 

 ['• Common medical', 'Common medical words', 'medical words readily', 'words readily match', 'readily match known', 'match known name', 'known name go', 'name go unrecognised', 'go unrecognised ,', 'unrecognised , e.g.-', ', e.g.- oophorectomy', 'e.g.- oophorectomy .'] 

 TOTAL TRIGRAMS --> 12 



 ---- NOUN PHRASES ---- 

 ['known', 'name', 'e.g.- oophorectomy'] 

 TOTAL NOUN PHRASES --> 3 



 ---- NER ----

 
 ORGANIZATION ---> ['Common']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['•', 'common', 'medic', 'word', 'readili', 'match', 'known', 'name', 'go', 'unrecognis', ',', 'e.g.-', 'oophorectomi', '.']

 TOTAL PORTER STEM WORDS ==> 14



 ---- SNOWBALL STEMMING ----

['•', 'common', 'medic', 'word', 'readili', 'match', 'known', 'name', 'go', 'unrecognis', ',', 'e.g.-', 'oophorectomi', '.']

 TOTAL SNOWBALL STEM WORDS ==> 14



 ---- LEMMATIZATION ----

['•', 'Common', 'medical', 'word', 'readily', 'match', 'known', 'name', 'go', 'unrecognised', ',', 'e.g.-', 'oophorectomy', '.']

 TOTAL LEMMATIZE WORDS ==> 14

************************************************************************************************************************

86 --> Non-discrimination of Meta-information  • Lack of discrimination between meta-information and patient  specific information. 


 ---- TOKENS ----

 ['Non-discrimination', 'of', 'Meta-information', '•', 'Lack', 'of', 'discrimination', 'between', 'meta-information', 'and', 'patient', 'specific', 'information', '.'] 

 TOTAL TOKENS ==> 14

 ---- POST ----

 [('Non-discrimination', 'NN'), ('of', 'IN'), ('Meta-information', 'NNP'), ('•', 'NNP'), ('Lack', 'NNP'), ('of', 'IN'), ('discrimination', 'NN'), ('between', 'IN'), ('meta-information', 'NN'), ('and', 'CC'), ('patient', 'JJ'), ('specific', 'JJ'), ('information', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Non-discrimination', 'Meta-information', '•', 'Lack', 'discrimination', 'meta-information', 'patient', 'specific', 'information', '.']

 TOTAL FILTERED TOKENS ==>  10

 ---- POST FOR FILTERED TOKENS ----

 [('Non-discrimination', 'JJ'), ('Meta-information', 'NNP'), ('•', 'NNP'), ('Lack', 'NNP'), ('discrimination', 'NN'), ('meta-information', 'NN'), ('patient', 'NN'), ('specific', 'JJ'), ('information', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Non-discrimination Meta-information', 'Meta-information •', '• Lack', 'Lack discrimination', 'discrimination meta-information', 'meta-information patient', 'patient specific', 'specific information', 'information .'] 

 TOTAL BIGRAMS --> 9 



 ---- TRI-GRAMS ---- 

 ['Non-discrimination Meta-information •', 'Meta-information • Lack', '• Lack discrimination', 'Lack discrimination meta-information', 'discrimination meta-information patient', 'meta-information patient specific', 'patient specific information', 'specific information .'] 

 TOTAL TRIGRAMS --> 8 



 ---- NOUN PHRASES ---- 

 ['discrimination', 'meta-information', 'patient', 'specific information'] 

 TOTAL NOUN PHRASES --> 4 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['non-discrimin', 'meta-inform', '•', 'lack', 'discrimin', 'meta-inform', 'patient', 'specif', 'inform', '.']

 TOTAL PORTER STEM WORDS ==> 10



 ---- SNOWBALL STEMMING ----

['non-discrimin', 'meta-inform', '•', 'lack', 'discrimin', 'meta-inform', 'patient', 'specif', 'inform', '.']

 TOTAL SNOWBALL STEM WORDS ==> 10



 ---- LEMMATIZATION ----

['Non-discrimination', 'Meta-information', '•', 'Lack', 'discrimination', 'meta-information', 'patient', 'specific', 'information', '.']

 TOTAL LEMMATIZE WORDS ==> 10

************************************************************************************************************************

87 --> Pathology reports always contain general  information from the body of knowledge of the discipline and  specific content describing the patient’s disease state e.g.-  descriptions of the criteria for selecting various values for a grade  value, are not the grade of the sample examined. 


 ---- TOKENS ----

 ['Pathology', 'reports', 'always', 'contain', 'general', 'information', 'from', 'the', 'body', 'of', 'knowledge', 'of', 'the', 'discipline', 'and', 'specific', 'content', 'describing', 'the', 'patient', '’', 's', 'disease', 'state', 'e.g.-', 'descriptions', 'of', 'the', 'criteria', 'for', 'selecting', 'various', 'values', 'for', 'a', 'grade', 'value', ',', 'are', 'not', 'the', 'grade', 'of', 'the', 'sample', 'examined', '.'] 

 TOTAL TOKENS ==> 47

 ---- POST ----

 [('Pathology', 'NNP'), ('reports', 'NNS'), ('always', 'RB'), ('contain', 'VBP'), ('general', 'JJ'), ('information', 'NN'), ('from', 'IN'), ('the', 'DT'), ('body', 'NN'), ('of', 'IN'), ('knowledge', 'NN'), ('of', 'IN'), ('the', 'DT'), ('discipline', 'NN'), ('and', 'CC'), ('specific', 'JJ'), ('content', 'NN'), ('describing', 'VBG'), ('the', 'DT'), ('patient', 'NN'), ('’', 'NNP'), ('s', 'NN'), ('disease', 'NN'), ('state', 'NN'), ('e.g.-', 'JJ'), ('descriptions', 'NNS'), ('of', 'IN'), ('the', 'DT'), ('criteria', 'NNS'), ('for', 'IN'), ('selecting', 'VBG'), ('various', 'JJ'), ('values', 'NNS'), ('for', 'IN'), ('a', 'DT'), ('grade', 'NN'), ('value', 'NN'), (',', ','), ('are', 'VBP'), ('not', 'RB'), ('the', 'DT'), ('grade', 'NN'), ('of', 'IN'), ('the', 'DT'), ('sample', 'NN'), ('examined', 'VBD'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Pathology', 'reports', 'always', 'contain', 'general', 'information', 'body', 'knowledge', 'discipline', 'specific', 'content', 'describing', 'patient', '’', 'disease', 'state', 'e.g.-', 'descriptions', 'criteria', 'selecting', 'various', 'values', 'grade', 'value', ',', 'grade', 'sample', 'examined', '.']

 TOTAL FILTERED TOKENS ==>  29

 ---- POST FOR FILTERED TOKENS ----

 [('Pathology', 'NNP'), ('reports', 'NNS'), ('always', 'RB'), ('contain', 'VBP'), ('general', 'JJ'), ('information', 'NN'), ('body', 'NN'), ('knowledge', 'NN'), ('discipline', 'NN'), ('specific', 'JJ'), ('content', 'NN'), ('describing', 'VBG'), ('patient', 'JJ'), ('’', 'JJ'), ('disease', 'NN'), ('state', 'NN'), ('e.g.-', 'JJ'), ('descriptions', 'NNS'), ('criteria', 'NNS'), ('selecting', 'VBG'), ('various', 'JJ'), ('values', 'NNS'), ('grade', 'VBP'), ('value', 'NN'), (',', ','), ('grade', 'JJ'), ('sample', 'NN'), ('examined', 'VBD'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Pathology reports', 'reports always', 'always contain', 'contain general', 'general information', 'information body', 'body knowledge', 'knowledge discipline', 'discipline specific', 'specific content', 'content describing', 'describing patient', 'patient ’', '’ disease', 'disease state', 'state e.g.-', 'e.g.- descriptions', 'descriptions criteria', 'criteria selecting', 'selecting various', 'various values', 'values grade', 'grade value', 'value ,', ', grade', 'grade sample', 'sample examined', 'examined .'] 

 TOTAL BIGRAMS --> 28 



 ---- TRI-GRAMS ---- 

 ['Pathology reports always', 'reports always contain', 'always contain general', 'contain general information', 'general information body', 'information body knowledge', 'body knowledge discipline', 'knowledge discipline specific', 'discipline specific content', 'specific content describing', 'content describing patient', 'describing patient ’', 'patient ’ disease', '’ disease state', 'disease state e.g.-', 'state e.g.- descriptions', 'e.g.- descriptions criteria', 'descriptions criteria selecting', 'criteria selecting various', 'selecting various values', 'various values grade', 'values grade value', 'grade value ,', 'value , grade', ', grade sample', 'grade sample examined', 'sample examined .'] 

 TOTAL TRIGRAMS --> 27 



 ---- NOUN PHRASES ---- 

 ['general information', 'body', 'knowledge', 'discipline', 'specific content', 'patient ’ disease', 'state', 'value', 'grade sample'] 

 TOTAL NOUN PHRASES --> 9 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> ['Pathology']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['patholog', 'report', 'alway', 'contain', 'gener', 'inform', 'bodi', 'knowledg', 'disciplin', 'specif', 'content', 'describ', 'patient', '’', 'diseas', 'state', 'e.g.-', 'descript', 'criteria', 'select', 'variou', 'valu', 'grade', 'valu', ',', 'grade', 'sampl', 'examin', '.']

 TOTAL PORTER STEM WORDS ==> 29



 ---- SNOWBALL STEMMING ----

['patholog', 'report', 'alway', 'contain', 'general', 'inform', 'bodi', 'knowledg', 'disciplin', 'specif', 'content', 'describ', 'patient', '’', 'diseas', 'state', 'e.g.-', 'descript', 'criteria', 'select', 'various', 'valu', 'grade', 'valu', ',', 'grade', 'sampl', 'examin', '.']

 TOTAL SNOWBALL STEM WORDS ==> 29



 ---- LEMMATIZATION ----

['Pathology', 'report', 'always', 'contain', 'general', 'information', 'body', 'knowledge', 'discipline', 'specific', 'content', 'describing', 'patient', '’', 'disease', 'state', 'e.g.-', 'description', 'criterion', 'selecting', 'various', 'value', 'grade', 'value', ',', 'grade', 'sample', 'examined', '.']

 TOTAL LEMMATIZE WORDS ==> 29

************************************************************************************************************************

88 --> Inability to identify Attribute-Value Pairs  • Inability to identify the difference between the name of an attribute  and a description of the patient’s actual condition. 


 ---- TOKENS ----

 ['Inability', 'to', 'identify', 'Attribute-Value', 'Pairs', '•', 'Inability', 'to', 'identify', 'the', 'difference', 'between', 'the', 'name', 'of', 'an', 'attribute', 'and', 'a', 'description', 'of', 'the', 'patient', '’', 's', 'actual', 'condition', '.'] 

 TOTAL TOKENS ==> 28

 ---- POST ----

 [('Inability', 'NN'), ('to', 'TO'), ('identify', 'VB'), ('Attribute-Value', 'NNP'), ('Pairs', 'NNP'), ('•', 'NNP'), ('Inability', 'NNP'), ('to', 'TO'), ('identify', 'VB'), ('the', 'DT'), ('difference', 'NN'), ('between', 'IN'), ('the', 'DT'), ('name', 'NN'), ('of', 'IN'), ('an', 'DT'), ('attribute', 'NN'), ('and', 'CC'), ('a', 'DT'), ('description', 'NN'), ('of', 'IN'), ('the', 'DT'), ('patient', 'NN'), ('’', 'NNP'), ('s', 'VBD'), ('actual', 'JJ'), ('condition', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Inability', 'identify', 'Attribute-Value', 'Pairs', '•', 'Inability', 'identify', 'difference', 'name', 'attribute', 'description', 'patient', '’', 'actual', 'condition', '.']

 TOTAL FILTERED TOKENS ==>  16

 ---- POST FOR FILTERED TOKENS ----

 [('Inability', 'NNP'), ('identify', 'VB'), ('Attribute-Value', 'NNP'), ('Pairs', 'NNP'), ('•', 'NNP'), ('Inability', 'NNP'), ('identify', 'VB'), ('difference', 'NN'), ('name', 'NN'), ('attribute', 'JJ'), ('description', 'NN'), ('patient', 'NN'), ('’', 'NNP'), ('actual', 'JJ'), ('condition', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Inability identify', 'identify Attribute-Value', 'Attribute-Value Pairs', 'Pairs •', '• Inability', 'Inability identify', 'identify difference', 'difference name', 'name attribute', 'attribute description', 'description patient', 'patient ’', '’ actual', 'actual condition', 'condition .'] 

 TOTAL BIGRAMS --> 15 



 ---- TRI-GRAMS ---- 

 ['Inability identify Attribute-Value', 'identify Attribute-Value Pairs', 'Attribute-Value Pairs •', 'Pairs • Inability', '• Inability identify', 'Inability identify difference', 'identify difference name', 'difference name attribute', 'name attribute description', 'attribute description patient', 'description patient ’', 'patient ’ actual', '’ actual condition', 'actual condition .'] 

 TOTAL TRIGRAMS --> 14 



 ---- NOUN PHRASES ---- 

 ['difference', 'name', 'attribute description', 'patient', 'actual condition'] 

 TOTAL NOUN PHRASES --> 5 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> ['Inability']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['inabl', 'identifi', 'attribute-valu', 'pair', '•', 'inabl', 'identifi', 'differ', 'name', 'attribut', 'descript', 'patient', '’', 'actual', 'condit', '.']

 TOTAL PORTER STEM WORDS ==> 16



 ---- SNOWBALL STEMMING ----

['inabl', 'identifi', 'attribute-valu', 'pair', '•', 'inabl', 'identifi', 'differ', 'name', 'attribut', 'descript', 'patient', '’', 'actual', 'condit', '.']

 TOTAL SNOWBALL STEM WORDS ==> 16



 ---- LEMMATIZATION ----

['Inability', 'identify', 'Attribute-Value', 'Pairs', '•', 'Inability', 'identify', 'difference', 'name', 'attribute', 'description', 'patient', '’', 'actual', 'condition', '.']

 TOTAL LEMMATIZE WORDS ==> 16

************************************************************************************************************************

89 --> Synoptic reports  are designed to lay out the pertinent case information in a structure  of Attribute-Value pairs. 


 ---- TOKENS ----

 ['Synoptic', 'reports', 'are', 'designed', 'to', 'lay', 'out', 'the', 'pertinent', 'case', 'information', 'in', 'a', 'structure', 'of', 'Attribute-Value', 'pairs', '.'] 

 TOTAL TOKENS ==> 18

 ---- POST ----

 [('Synoptic', 'JJ'), ('reports', 'NNS'), ('are', 'VBP'), ('designed', 'VBN'), ('to', 'TO'), ('lay', 'VB'), ('out', 'RP'), ('the', 'DT'), ('pertinent', 'JJ'), ('case', 'NN'), ('information', 'NN'), ('in', 'IN'), ('a', 'DT'), ('structure', 'NN'), ('of', 'IN'), ('Attribute-Value', 'NNP'), ('pairs', 'NNS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Synoptic', 'reports', 'designed', 'lay', 'pertinent', 'case', 'information', 'structure', 'Attribute-Value', 'pairs', '.']

 TOTAL FILTERED TOKENS ==>  11

 ---- POST FOR FILTERED TOKENS ----

 [('Synoptic', 'JJ'), ('reports', 'NNS'), ('designed', 'VBN'), ('lay', 'JJ'), ('pertinent', 'NN'), ('case', 'NN'), ('information', 'NN'), ('structure', 'NN'), ('Attribute-Value', 'NNP'), ('pairs', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Synoptic reports', 'reports designed', 'designed lay', 'lay pertinent', 'pertinent case', 'case information', 'information structure', 'structure Attribute-Value', 'Attribute-Value pairs', 'pairs .'] 

 TOTAL BIGRAMS --> 10 



 ---- TRI-GRAMS ---- 

 ['Synoptic reports designed', 'reports designed lay', 'designed lay pertinent', 'lay pertinent case', 'pertinent case information', 'case information structure', 'information structure Attribute-Value', 'structure Attribute-Value pairs', 'Attribute-Value pairs .'] 

 TOTAL TRIGRAMS --> 9 



 ---- NOUN PHRASES ---- 

 ['lay pertinent', 'case', 'information', 'structure', 'pairs'] 

 TOTAL NOUN PHRASES --> 5 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> ['Synoptic']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['synopt', 'report', 'design', 'lay', 'pertin', 'case', 'inform', 'structur', 'attribute-valu', 'pair', '.']

 TOTAL PORTER STEM WORDS ==> 11



 ---- SNOWBALL STEMMING ----

['synopt', 'report', 'design', 'lay', 'pertin', 'case', 'inform', 'structur', 'attribute-valu', 'pair', '.']

 TOTAL SNOWBALL STEM WORDS ==> 11



 ---- LEMMATIZATION ----

['Synoptic', 'report', 'designed', 'lay', 'pertinent', 'case', 'information', 'structure', 'Attribute-Value', 'pair', '.']

 TOTAL LEMMATIZE WORDS ==> 11

************************************************************************************************************************

90 --> The attribute names do not represent an  identified characteristic of the patient’s health but rather are a label  that can only be interpreted in conjunction with the value e.g.-  Lymphovascular invasion: Absent, is NOT a statement about the  presence of this type of tumour invasion. 


 ---- TOKENS ----

 ['The', 'attribute', 'names', 'do', 'not', 'represent', 'an', 'identified', 'characteristic', 'of', 'the', 'patient', '’', 's', 'health', 'but', 'rather', 'are', 'a', 'label', 'that', 'can', 'only', 'be', 'interpreted', 'in', 'conjunction', 'with', 'the', 'value', 'e.g.-', 'Lymphovascular', 'invasion', ':', 'Absent', ',', 'is', 'NOT', 'a', 'statement', 'about', 'the', 'presence', 'of', 'this', 'type', 'of', 'tumour', 'invasion', '.'] 

 TOTAL TOKENS ==> 50

 ---- POST ----

 [('The', 'DT'), ('attribute', 'NN'), ('names', 'NNS'), ('do', 'VBP'), ('not', 'RB'), ('represent', 'VB'), ('an', 'DT'), ('identified', 'JJ'), ('characteristic', 'NN'), ('of', 'IN'), ('the', 'DT'), ('patient', 'NN'), ('’', 'NNP'), ('s', 'VBZ'), ('health', 'NN'), ('but', 'CC'), ('rather', 'RB'), ('are', 'VBP'), ('a', 'DT'), ('label', 'NN'), ('that', 'WDT'), ('can', 'MD'), ('only', 'RB'), ('be', 'VB'), ('interpreted', 'VBN'), ('in', 'IN'), ('conjunction', 'NN'), ('with', 'IN'), ('the', 'DT'), ('value', 'NN'), ('e.g.-', 'JJ'), ('Lymphovascular', 'JJ'), ('invasion', 'NN'), (':', ':'), ('Absent', 'NN'), (',', ','), ('is', 'VBZ'), ('NOT', 'NNP'), ('a', 'DT'), ('statement', 'NN'), ('about', 'IN'), ('the', 'DT'), ('presence', 'NN'), ('of', 'IN'), ('this', 'DT'), ('type', 'NN'), ('of', 'IN'), ('tumour', 'JJ'), ('invasion', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['attribute', 'names', 'represent', 'identified', 'characteristic', 'patient', '’', 'health', 'rather', 'label', 'interpreted', 'conjunction', 'value', 'e.g.-', 'Lymphovascular', 'invasion', ':', 'Absent', ',', 'statement', 'presence', 'type', 'tumour', 'invasion', '.']

 TOTAL FILTERED TOKENS ==>  25

 ---- POST FOR FILTERED TOKENS ----

 [('attribute', 'NN'), ('names', 'NNS'), ('represent', 'VBP'), ('identified', 'VBN'), ('characteristic', 'JJ'), ('patient', 'NN'), ('’', 'NNP'), ('health', 'NN'), ('rather', 'RB'), ('label', 'RB'), ('interpreted', 'VBN'), ('conjunction', 'NN'), ('value', 'NN'), ('e.g.-', 'JJ'), ('Lymphovascular', 'JJ'), ('invasion', 'NN'), (':', ':'), ('Absent', 'NN'), (',', ','), ('statement', 'NN'), ('presence', 'NN'), ('type', 'VBD'), ('tumour', 'JJ'), ('invasion', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['attribute names', 'names represent', 'represent identified', 'identified characteristic', 'characteristic patient', 'patient ’', '’ health', 'health rather', 'rather label', 'label interpreted', 'interpreted conjunction', 'conjunction value', 'value e.g.-', 'e.g.- Lymphovascular', 'Lymphovascular invasion', 'invasion :', ': Absent', 'Absent ,', ', statement', 'statement presence', 'presence type', 'type tumour', 'tumour invasion', 'invasion .'] 

 TOTAL BIGRAMS --> 24 



 ---- TRI-GRAMS ---- 

 ['attribute names represent', 'names represent identified', 'represent identified characteristic', 'identified characteristic patient', 'characteristic patient ’', 'patient ’ health', '’ health rather', 'health rather label', 'rather label interpreted', 'label interpreted conjunction', 'interpreted conjunction value', 'conjunction value e.g.-', 'value e.g.- Lymphovascular', 'e.g.- Lymphovascular invasion', 'Lymphovascular invasion :', 'invasion : Absent', ': Absent ,', 'Absent , statement', ', statement presence', 'statement presence type', 'presence type tumour', 'type tumour invasion', 'tumour invasion .'] 

 TOTAL TRIGRAMS --> 23 



 ---- NOUN PHRASES ---- 

 ['attribute', 'characteristic patient', 'health', 'conjunction', 'value', 'e.g.- Lymphovascular invasion', 'Absent', 'statement', 'presence', 'tumour invasion'] 

 TOTAL NOUN PHRASES --> 10 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['attribut', 'name', 'repres', 'identifi', 'characterist', 'patient', '’', 'health', 'rather', 'label', 'interpret', 'conjunct', 'valu', 'e.g.-', 'lymphovascular', 'invas', ':', 'absent', ',', 'statement', 'presenc', 'type', 'tumour', 'invas', '.']

 TOTAL PORTER STEM WORDS ==> 25



 ---- SNOWBALL STEMMING ----

['attribut', 'name', 'repres', 'identifi', 'characterist', 'patient', '’', 'health', 'rather', 'label', 'interpret', 'conjunct', 'valu', 'e.g.-', 'lymphovascular', 'invas', ':', 'absent', ',', 'statement', 'presenc', 'type', 'tumour', 'invas', '.']

 TOTAL SNOWBALL STEM WORDS ==> 25



 ---- LEMMATIZATION ----

['attribute', 'name', 'represent', 'identified', 'characteristic', 'patient', '’', 'health', 'rather', 'label', 'interpreted', 'conjunction', 'value', 'e.g.-', 'Lymphovascular', 'invasion', ':', 'Absent', ',', 'statement', 'presence', 'type', 'tumour', 'invasion', '.']

 TOTAL LEMMATIZE WORDS ==> 25

************************************************************************************************************************

91 --> Some systems ignore the  structure and use the mention of the condition as validation that it  exists for the patient. 


 ---- TOKENS ----

 ['Some', 'systems', 'ignore', 'the', 'structure', 'and', 'use', 'the', 'mention', 'of', 'the', 'condition', 'as', 'validation', 'that', 'it', 'exists', 'for', 'the', 'patient', '.'] 

 TOTAL TOKENS ==> 21

 ---- POST ----

 [('Some', 'DT'), ('systems', 'NNS'), ('ignore', 'VBP'), ('the', 'DT'), ('structure', 'NN'), ('and', 'CC'), ('use', 'VB'), ('the', 'DT'), ('mention', 'NN'), ('of', 'IN'), ('the', 'DT'), ('condition', 'NN'), ('as', 'IN'), ('validation', 'NN'), ('that', 'IN'), ('it', 'PRP'), ('exists', 'VBZ'), ('for', 'IN'), ('the', 'DT'), ('patient', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['systems', 'ignore', 'structure', 'use', 'mention', 'condition', 'validation', 'exists', 'patient', '.']

 TOTAL FILTERED TOKENS ==>  10

 ---- POST FOR FILTERED TOKENS ----

 [('systems', 'NNS'), ('ignore', 'VBP'), ('structure', 'NN'), ('use', 'NN'), ('mention', 'NN'), ('condition', 'NN'), ('validation', 'NN'), ('exists', 'VBZ'), ('patient', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['systems ignore', 'ignore structure', 'structure use', 'use mention', 'mention condition', 'condition validation', 'validation exists', 'exists patient', 'patient .'] 

 TOTAL BIGRAMS --> 9 



 ---- TRI-GRAMS ---- 

 ['systems ignore structure', 'ignore structure use', 'structure use mention', 'use mention condition', 'mention condition validation', 'condition validation exists', 'validation exists patient', 'exists patient .'] 

 TOTAL TRIGRAMS --> 8 



 ---- NOUN PHRASES ---- 

 ['structure', 'use', 'mention', 'condition', 'validation', 'patient'] 

 TOTAL NOUN PHRASES --> 6 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['system', 'ignor', 'structur', 'use', 'mention', 'condit', 'valid', 'exist', 'patient', '.']

 TOTAL PORTER STEM WORDS ==> 10



 ---- SNOWBALL STEMMING ----

['system', 'ignor', 'structur', 'use', 'mention', 'condit', 'valid', 'exist', 'patient', '.']

 TOTAL SNOWBALL STEM WORDS ==> 10



 ---- LEMMATIZATION ----

['system', 'ignore', 'structure', 'use', 'mention', 'condition', 'validation', 'exists', 'patient', '.']

 TOTAL LEMMATIZE WORDS ==> 10

************************************************************************************************************************

92 --> This of course leads to incorrect output. 


 ---- TOKENS ----

 ['This', 'of', 'course', 'leads', 'to', 'incorrect', 'output', '.'] 

 TOTAL TOKENS ==> 8

 ---- POST ----

 [('This', 'DT'), ('of', 'IN'), ('course', 'NN'), ('leads', 'NNS'), ('to', 'TO'), ('incorrect', 'VB'), ('output', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['course', 'leads', 'incorrect', 'output', '.']

 TOTAL FILTERED TOKENS ==>  5

 ---- POST FOR FILTERED TOKENS ----

 [('course', 'NN'), ('leads', 'VBZ'), ('incorrect', 'JJ'), ('output', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['course leads', 'leads incorrect', 'incorrect output', 'output .'] 

 TOTAL BIGRAMS --> 4 



 ---- TRI-GRAMS ---- 

 ['course leads incorrect', 'leads incorrect output', 'incorrect output .'] 

 TOTAL TRIGRAMS --> 3 



 ---- NOUN PHRASES ---- 

 ['course', 'incorrect output'] 

 TOTAL NOUN PHRASES --> 2 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['cours', 'lead', 'incorrect', 'output', '.']

 TOTAL PORTER STEM WORDS ==> 5



 ---- SNOWBALL STEMMING ----

['cours', 'lead', 'incorrect', 'output', '.']

 TOTAL SNOWBALL STEM WORDS ==> 5



 ---- LEMMATIZATION ----

['course', 'lead', 'incorrect', 'output', '.']

 TOTAL LEMMATIZE WORDS ==> 5

************************************************************************************************************************

93 --> Mis-labelling Unknown strings  • Automatically labeling any string it can’t recognise as a Test Name  entity. 


 ---- TOKENS ----

 ['Mis-labelling', 'Unknown', 'strings', '•', 'Automatically', 'labeling', 'any', 'string', 'it', 'can', '’', 't', 'recognise', 'as', 'a', 'Test', 'Name', 'entity', '.'] 

 TOTAL TOKENS ==> 19

 ---- POST ----

 [('Mis-labelling', 'JJ'), ('Unknown', 'NNP'), ('strings', 'NNS'), ('•', 'NNP'), ('Automatically', 'RB'), ('labeling', 'VBG'), ('any', 'DT'), ('string', 'VBG'), ('it', 'PRP'), ('can', 'MD'), ('’', 'VB'), ('t', 'JJ'), ('recognise', 'NN'), ('as', 'IN'), ('a', 'DT'), ('Test', 'NNP'), ('Name', 'NN'), ('entity', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Mis-labelling', 'Unknown', 'strings', '•', 'Automatically', 'labeling', 'string', '’', 'recognise', 'Test', 'Name', 'entity', '.']

 TOTAL FILTERED TOKENS ==>  13

 ---- POST FOR FILTERED TOKENS ----

 [('Mis-labelling', 'JJ'), ('Unknown', 'NNP'), ('strings', 'NNS'), ('•', 'NNP'), ('Automatically', 'RB'), ('labeling', 'VBG'), ('string', 'VBG'), ('’', 'JJ'), ('recognise', 'NN'), ('Test', 'NNP'), ('Name', 'NNP'), ('entity', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Mis-labelling Unknown', 'Unknown strings', 'strings •', '• Automatically', 'Automatically labeling', 'labeling string', 'string ’', '’ recognise', 'recognise Test', 'Test Name', 'Name entity', 'entity .'] 

 TOTAL BIGRAMS --> 12 



 ---- TRI-GRAMS ---- 

 ['Mis-labelling Unknown strings', 'Unknown strings •', 'strings • Automatically', '• Automatically labeling', 'Automatically labeling string', 'labeling string ’', 'string ’ recognise', '’ recognise Test', 'recognise Test Name', 'Test Name entity', 'Name entity .'] 

 TOTAL TRIGRAMS --> 11 



 ---- NOUN PHRASES ---- 

 ['’ recognise', 'entity'] 

 TOTAL NOUN PHRASES --> 2 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> ['Test Name']
 TOTAL PERSON ENTITY --> 1 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['mis-label', 'unknown', 'string', '•', 'automat', 'label', 'string', '’', 'recognis', 'test', 'name', 'entiti', '.']

 TOTAL PORTER STEM WORDS ==> 13



 ---- SNOWBALL STEMMING ----

['mis-label', 'unknown', 'string', '•', 'automat', 'label', 'string', '’', 'recognis', 'test', 'name', 'entiti', '.']

 TOTAL SNOWBALL STEM WORDS ==> 13



 ---- LEMMATIZATION ----

['Mis-labelling', 'Unknown', 'string', '•', 'Automatically', 'labeling', 'string', '’', 'recognise', 'Test', 'Name', 'entity', '.']

 TOTAL LEMMATIZE WORDS ==> 13

************************************************************************************************************************

94 --> In any NLP system the processing of unrecognizable words  needs to be very robust. 


 ---- TOKENS ----

 ['In', 'any', 'NLP', 'system', 'the', 'processing', 'of', 'unrecognizable', 'words', 'needs', 'to', 'be', 'very', 'robust', '.'] 

 TOTAL TOKENS ==> 15

 ---- POST ----

 [('In', 'IN'), ('any', 'DT'), ('NLP', 'NNP'), ('system', 'NN'), ('the', 'DT'), ('processing', 'NN'), ('of', 'IN'), ('unrecognizable', 'JJ'), ('words', 'NNS'), ('needs', 'NNS'), ('to', 'TO'), ('be', 'VB'), ('very', 'RB'), ('robust', 'JJ'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['NLP', 'system', 'processing', 'unrecognizable', 'words', 'needs', 'robust', '.']

 TOTAL FILTERED TOKENS ==>  8

 ---- POST FOR FILTERED TOKENS ----

 [('NLP', 'NNP'), ('system', 'NN'), ('processing', 'VBG'), ('unrecognizable', 'JJ'), ('words', 'NNS'), ('needs', 'VBZ'), ('robust', 'JJ'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['NLP system', 'system processing', 'processing unrecognizable', 'unrecognizable words', 'words needs', 'needs robust', 'robust .'] 

 TOTAL BIGRAMS --> 7 



 ---- TRI-GRAMS ---- 

 ['NLP system processing', 'system processing unrecognizable', 'processing unrecognizable words', 'unrecognizable words needs', 'words needs robust', 'needs robust .'] 

 TOTAL TRIGRAMS --> 6 



 ---- NOUN PHRASES ---- 

 ['system'] 

 TOTAL NOUN PHRASES --> 1 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['nlp', 'system', 'process', 'unrecogniz', 'word', 'need', 'robust', '.']

 TOTAL PORTER STEM WORDS ==> 8



 ---- SNOWBALL STEMMING ----

['nlp', 'system', 'process', 'unrecogniz', 'word', 'need', 'robust', '.']

 TOTAL SNOWBALL STEM WORDS ==> 8



 ---- LEMMATIZATION ----

['NLP', 'system', 'processing', 'unrecognizable', 'word', 'need', 'robust', '.']

 TOTAL LEMMATIZE WORDS ==> 8

************************************************************************************************************************

95 --> We have observed the assignment of  sematic categories to these items which is both dangerous and  needless in most cases. 


 ---- TOKENS ----

 ['We', 'have', 'observed', 'the', 'assignment', 'of', 'sematic', 'categories', 'to', 'these', 'items', 'which', 'is', 'both', 'dangerous', 'and', 'needless', 'in', 'most', 'cases', '.'] 

 TOTAL TOKENS ==> 21

 ---- POST ----

 [('We', 'PRP'), ('have', 'VBP'), ('observed', 'VBN'), ('the', 'DT'), ('assignment', 'NN'), ('of', 'IN'), ('sematic', 'JJ'), ('categories', 'NNS'), ('to', 'TO'), ('these', 'DT'), ('items', 'NNS'), ('which', 'WDT'), ('is', 'VBZ'), ('both', 'DT'), ('dangerous', 'JJ'), ('and', 'CC'), ('needless', 'JJ'), ('in', 'IN'), ('most', 'JJS'), ('cases', 'NNS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['observed', 'assignment', 'sematic', 'categories', 'items', 'dangerous', 'needless', 'cases', '.']

 TOTAL FILTERED TOKENS ==>  9

 ---- POST FOR FILTERED TOKENS ----

 [('observed', 'JJ'), ('assignment', 'NN'), ('sematic', 'JJ'), ('categories', 'NNS'), ('items', 'NNS'), ('dangerous', 'JJ'), ('needless', 'JJ'), ('cases', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['observed assignment', 'assignment sematic', 'sematic categories', 'categories items', 'items dangerous', 'dangerous needless', 'needless cases', 'cases .'] 

 TOTAL BIGRAMS --> 8 



 ---- TRI-GRAMS ---- 

 ['observed assignment sematic', 'assignment sematic categories', 'sematic categories items', 'categories items dangerous', 'items dangerous needless', 'dangerous needless cases', 'needless cases .'] 

 TOTAL TRIGRAMS --> 7 



 ---- NOUN PHRASES ---- 

 ['observed assignment'] 

 TOTAL NOUN PHRASES --> 1 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['observ', 'assign', 'semat', 'categori', 'item', 'danger', 'needless', 'case', '.']

 TOTAL PORTER STEM WORDS ==> 9



 ---- SNOWBALL STEMMING ----

['observ', 'assign', 'semat', 'categori', 'item', 'danger', 'needless', 'case', '.']

 TOTAL SNOWBALL STEM WORDS ==> 9



 ---- LEMMATIZATION ----

['observed', 'assignment', 'sematic', 'category', 'item', 'dangerous', 'needle', 'case', '.']

 TOTAL LEMMATIZE WORDS ==> 9

************************************************************************************************************************

96 --> Ambiguity in acronyms cannot be resolved correctly. 


 ---- TOKENS ----

 ['Ambiguity', 'in', 'acronyms', 'can', 'not', 'be', 'resolved', 'correctly', '.'] 

 TOTAL TOKENS ==> 9

 ---- POST ----

 [('Ambiguity', 'NN'), ('in', 'IN'), ('acronyms', 'NN'), ('can', 'MD'), ('not', 'RB'), ('be', 'VB'), ('resolved', 'VBN'), ('correctly', 'RB'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Ambiguity', 'acronyms', 'resolved', 'correctly', '.']

 TOTAL FILTERED TOKENS ==>  5

 ---- POST FOR FILTERED TOKENS ----

 [('Ambiguity', 'NNP'), ('acronyms', 'VBZ'), ('resolved', 'VBN'), ('correctly', 'RB'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Ambiguity acronyms', 'acronyms resolved', 'resolved correctly', 'correctly .'] 

 TOTAL BIGRAMS --> 4 



 ---- TRI-GRAMS ---- 

 ['Ambiguity acronyms resolved', 'acronyms resolved correctly', 'resolved correctly .'] 

 TOTAL TRIGRAMS --> 3 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> ['Ambiguity']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['ambigu', 'acronym', 'resolv', 'correctli', '.']

 TOTAL PORTER STEM WORDS ==> 5



 ---- SNOWBALL STEMMING ----

['ambigu', 'acronym', 'resolv', 'correct', '.']

 TOTAL SNOWBALL STEM WORDS ==> 5



 ---- LEMMATIZATION ----

['Ambiguity', 'acronym', 'resolved', 'correctly', '.']

 TOTAL LEMMATIZE WORDS ==> 5

************************************************************************************************************************

97 --> Acronyms, whilst highly useful in their own context, create confusion  when they have alternative interpretations, e.g.- MM can be either  millimetres or Malignant Melanoma. 


 ---- TOKENS ----

 ['Acronyms', ',', 'whilst', 'highly', 'useful', 'in', 'their', 'own', 'context', ',', 'create', 'confusion', 'when', 'they', 'have', 'alternative', 'interpretations', ',', 'e.g.-', 'MM', 'can', 'be', 'either', 'millimetres', 'or', 'Malignant', 'Melanoma', '.'] 

 TOTAL TOKENS ==> 28

 ---- POST ----

 [('Acronyms', 'NNP'), (',', ','), ('whilst', 'VBZ'), ('highly', 'RB'), ('useful', 'JJ'), ('in', 'IN'), ('their', 'PRP$'), ('own', 'JJ'), ('context', 'NN'), (',', ','), ('create', 'VBP'), ('confusion', 'NN'), ('when', 'WRB'), ('they', 'PRP'), ('have', 'VBP'), ('alternative', 'JJ'), ('interpretations', 'NNS'), (',', ','), ('e.g.-', 'JJ'), ('MM', 'NNP'), ('can', 'MD'), ('be', 'VB'), ('either', 'CC'), ('millimetres', 'NNS'), ('or', 'CC'), ('Malignant', 'JJ'), ('Melanoma', 'NNP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Acronyms', ',', 'whilst', 'highly', 'useful', 'context', ',', 'create', 'confusion', 'alternative', 'interpretations', ',', 'e.g.-', 'MM', 'either', 'millimetres', 'Malignant', 'Melanoma', '.']

 TOTAL FILTERED TOKENS ==>  19

 ---- POST FOR FILTERED TOKENS ----

 [('Acronyms', 'NNP'), (',', ','), ('whilst', 'VBZ'), ('highly', 'RB'), ('useful', 'JJ'), ('context', 'NN'), (',', ','), ('create', 'JJ'), ('confusion', 'NN'), ('alternative', 'JJ'), ('interpretations', 'NNS'), (',', ','), ('e.g.-', 'JJ'), ('MM', 'NNP'), ('either', 'CC'), ('millimetres', 'NNS'), ('Malignant', 'NNP'), ('Melanoma', 'NNP'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Acronyms ,', ', whilst', 'whilst highly', 'highly useful', 'useful context', 'context ,', ', create', 'create confusion', 'confusion alternative', 'alternative interpretations', 'interpretations ,', ', e.g.-', 'e.g.- MM', 'MM either', 'either millimetres', 'millimetres Malignant', 'Malignant Melanoma', 'Melanoma .'] 

 TOTAL BIGRAMS --> 18 



 ---- TRI-GRAMS ---- 

 ['Acronyms , whilst', ', whilst highly', 'whilst highly useful', 'highly useful context', 'useful context ,', 'context , create', ', create confusion', 'create confusion alternative', 'confusion alternative interpretations', 'alternative interpretations ,', 'interpretations , e.g.-', ', e.g.- MM', 'e.g.- MM either', 'MM either millimetres', 'either millimetres Malignant', 'millimetres Malignant Melanoma', 'Malignant Melanoma .'] 

 TOTAL TRIGRAMS --> 17 



 ---- NOUN PHRASES ---- 

 ['useful context', 'create confusion'] 

 TOTAL NOUN PHRASES --> 2 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> ['Malignant Melanoma']
 TOTAL PERSON ENTITY --> 1 


 GPE ---> ['Acronyms']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['acronym', ',', 'whilst', 'highli', 'use', 'context', ',', 'creat', 'confus', 'altern', 'interpret', ',', 'e.g.-', 'mm', 'either', 'millimetr', 'malign', 'melanoma', '.']

 TOTAL PORTER STEM WORDS ==> 19



 ---- SNOWBALL STEMMING ----

['acronym', ',', 'whilst', 'high', 'use', 'context', ',', 'creat', 'confus', 'altern', 'interpret', ',', 'e.g.-', 'mm', 'either', 'millimetr', 'malign', 'melanoma', '.']

 TOTAL SNOWBALL STEM WORDS ==> 19



 ---- LEMMATIZATION ----

['Acronyms', ',', 'whilst', 'highly', 'useful', 'context', ',', 'create', 'confusion', 'alternative', 'interpretation', ',', 'e.g.-', 'MM', 'either', 'millimetre', 'Malignant', 'Melanoma', '.']

 TOTAL LEMMATIZE WORDS ==> 19

************************************************************************************************************************

98 --> Without proper identification of  the context using statistical processing the correct interpretation  cannot be made. 


 ---- TOKENS ----

 ['Without', 'proper', 'identification', 'of', 'the', 'context', 'using', 'statistical', 'processing', 'the', 'correct', 'interpretation', 'can', 'not', 'be', 'made', '.'] 

 TOTAL TOKENS ==> 17

 ---- POST ----

 [('Without', 'IN'), ('proper', 'JJ'), ('identification', 'NN'), ('of', 'IN'), ('the', 'DT'), ('context', 'NN'), ('using', 'VBG'), ('statistical', 'JJ'), ('processing', 'VBG'), ('the', 'DT'), ('correct', 'JJ'), ('interpretation', 'NN'), ('can', 'MD'), ('not', 'RB'), ('be', 'VB'), ('made', 'VBN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Without', 'proper', 'identification', 'context', 'using', 'statistical', 'processing', 'correct', 'interpretation', 'made', '.']

 TOTAL FILTERED TOKENS ==>  11

 ---- POST FOR FILTERED TOKENS ----

 [('Without', 'IN'), ('proper', 'JJ'), ('identification', 'NN'), ('context', 'NN'), ('using', 'VBG'), ('statistical', 'JJ'), ('processing', 'NN'), ('correct', 'NN'), ('interpretation', 'NN'), ('made', 'VBN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Without proper', 'proper identification', 'identification context', 'context using', 'using statistical', 'statistical processing', 'processing correct', 'correct interpretation', 'interpretation made', 'made .'] 

 TOTAL BIGRAMS --> 10 



 ---- TRI-GRAMS ---- 

 ['Without proper identification', 'proper identification context', 'identification context using', 'context using statistical', 'using statistical processing', 'statistical processing correct', 'processing correct interpretation', 'correct interpretation made', 'interpretation made .'] 

 TOTAL TRIGRAMS --> 9 



 ---- NOUN PHRASES ---- 

 ['proper identification', 'context', 'statistical processing', 'correct', 'interpretation'] 

 TOTAL NOUN PHRASES --> 5 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['without', 'proper', 'identif', 'context', 'use', 'statist', 'process', 'correct', 'interpret', 'made', '.']

 TOTAL PORTER STEM WORDS ==> 11



 ---- SNOWBALL STEMMING ----

['without', 'proper', 'identif', 'context', 'use', 'statist', 'process', 'correct', 'interpret', 'made', '.']

 TOTAL SNOWBALL STEM WORDS ==> 11



 ---- LEMMATIZATION ----

['Without', 'proper', 'identification', 'context', 'using', 'statistical', 'processing', 'correct', 'interpretation', 'made', '.']

 TOTAL LEMMATIZE WORDS ==> 11

************************************************************************************************************************

