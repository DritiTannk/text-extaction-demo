1 --> Making Content Services Smarter Using NLP  WHITE PAPER MAKING CONTENT SERVICES  SMARTER USING NLP Abstract There is an exponential growth in unstructured content due to  digitization, IoT, regulatory compliance, etc. 


 ---- TOKENS ----

 ['Making', 'Content', 'Services', 'Smarter', 'Using', 'NLP', 'WHITE', 'PAPER', 'MAKING', 'CONTENT', 'SERVICES', 'SMARTER', 'USING', 'NLP', 'Abstract', 'There', 'is', 'an', 'exponential', 'growth', 'in', 'unstructured', 'content', 'due', 'to', 'digitization', ',', 'IoT', ',', 'regulatory', 'compliance', ',', 'etc', '.'] 

 TOTAL TOKENS ==> 34

 ---- POST ----

 [('Making', 'VBG'), ('Content', 'NNP'), ('Services', 'NNPS'), ('Smarter', 'NNP'), ('Using', 'NNP'), ('NLP', 'NNP'), ('WHITE', 'NNP'), ('PAPER', 'NNP'), ('MAKING', 'NNP'), ('CONTENT', 'NNP'), ('SERVICES', 'NNP'), ('SMARTER', 'NNP'), ('USING', 'NNP'), ('NLP', 'NNP'), ('Abstract', 'NNP'), ('There', 'EX'), ('is', 'VBZ'), ('an', 'DT'), ('exponential', 'JJ'), ('growth', 'NN'), ('in', 'IN'), ('unstructured', 'JJ'), ('content', 'NN'), ('due', 'JJ'), ('to', 'TO'), ('digitization', 'NN'), (',', ','), ('IoT', 'NNP'), (',', ','), ('regulatory', 'JJ'), ('compliance', 'NN'), (',', ','), ('etc', 'FW'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Making', 'Content', 'Services', 'Smarter', 'Using', 'NLP', 'WHITE', 'PAPER', 'MAKING', 'CONTENT', 'SERVICES', 'SMARTER', 'USING', 'NLP', 'Abstract', 'exponential', 'growth', 'unstructured', 'content', 'due', 'digitization', ',', 'IoT', ',', 'regulatory', 'compliance', ',', 'etc', '.']

 TOTAL FILTERED TOKENS ==>  29

 ---- POST FOR FILTERED TOKENS ----

 [('Making', 'VBG'), ('Content', 'NNP'), ('Services', 'NNPS'), ('Smarter', 'NNP'), ('Using', 'NNP'), ('NLP', 'NNP'), ('WHITE', 'NNP'), ('PAPER', 'NNP'), ('MAKING', 'NNP'), ('CONTENT', 'NNP'), ('SERVICES', 'NNP'), ('SMARTER', 'NNP'), ('USING', 'NNP'), ('NLP', 'NNP'), ('Abstract', 'NNP'), ('exponential', 'JJ'), ('growth', 'NN'), ('unstructured', 'JJ'), ('content', 'NN'), ('due', 'JJ'), ('digitization', 'NN'), (',', ','), ('IoT', 'NNP'), (',', ','), ('regulatory', 'JJ'), ('compliance', 'NN'), (',', ','), ('etc', 'FW'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Making Content', 'Content Services', 'Services Smarter', 'Smarter Using', 'Using NLP', 'NLP WHITE', 'WHITE PAPER', 'PAPER MAKING', 'MAKING CONTENT', 'CONTENT SERVICES', 'SERVICES SMARTER', 'SMARTER USING', 'USING NLP', 'NLP Abstract', 'Abstract exponential', 'exponential growth', 'growth unstructured', 'unstructured content', 'content due', 'due digitization', 'digitization ,', ', IoT', 'IoT ,', ', regulatory', 'regulatory compliance', 'compliance ,', ', etc', 'etc .'] 

 TOTAL BIGRAMS --> 28 



 ---- TRI-GRAMS ---- 

 ['Making Content Services', 'Content Services Smarter', 'Services Smarter Using', 'Smarter Using NLP', 'Using NLP WHITE', 'NLP WHITE PAPER', 'WHITE PAPER MAKING', 'PAPER MAKING CONTENT', 'MAKING CONTENT SERVICES', 'CONTENT SERVICES SMARTER', 'SERVICES SMARTER USING', 'SMARTER USING NLP', 'USING NLP Abstract', 'NLP Abstract exponential', 'Abstract exponential growth', 'exponential growth unstructured', 'growth unstructured content', 'unstructured content due', 'content due digitization', 'due digitization ,', 'digitization , IoT', ', IoT ,', 'IoT , regulatory', ', regulatory compliance', 'regulatory compliance ,', 'compliance , etc', ', etc .'] 

 TOTAL TRIGRAMS --> 27 



 ---- NOUN PHRASES ---- 

 ['exponential growth', 'unstructured content', 'due digitization', 'regulatory compliance'] 

 TOTAL NOUN PHRASES --> 4 



 ---- NER ----

 
 ORGANIZATION ---> ['Content Services Smarter', 'WHITE', 'CONTENT', 'IoT']
 TOTAL ORGANIZATION ENTITY --> 4 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['make', 'content', 'servic', 'smarter', 'use', 'nlp', 'white', 'paper', 'make', 'content', 'servic', 'smarter', 'use', 'nlp', 'abstract', 'exponenti', 'growth', 'unstructur', 'content', 'due', 'digit', ',', 'iot', ',', 'regulatori', 'complianc', ',', 'etc', '.']

 TOTAL PORTER STEM WORDS ==> 29



 ---- SNOWBALL STEMMING ----

['make', 'content', 'servic', 'smarter', 'use', 'nlp', 'white', 'paper', 'make', 'content', 'servic', 'smarter', 'use', 'nlp', 'abstract', 'exponenti', 'growth', 'unstructur', 'content', 'due', 'digit', ',', 'iot', ',', 'regulatori', 'complianc', ',', 'etc', '.']

 TOTAL SNOWBALL STEM WORDS ==> 29



 ---- LEMMATIZATION ----

['Making', 'Content', 'Services', 'Smarter', 'Using', 'NLP', 'WHITE', 'PAPER', 'MAKING', 'CONTENT', 'SERVICES', 'SMARTER', 'USING', 'NLP', 'Abstract', 'exponential', 'growth', 'unstructured', 'content', 'due', 'digitization', ',', 'IoT', ',', 'regulatory', 'compliance', ',', 'etc', '.']

 TOTAL LEMMATIZE WORDS ==> 29

************************************************************************************************************************

2 --> Because of this growth,  organizations are facing challenges, on how to capture, ingest and  manage huge volume of unstructured content effectively in Content  Services platform. 


 ---- TOKENS ----

 ['Because', 'of', 'this', 'growth', ',', 'organizations', 'are', 'facing', 'challenges', ',', 'on', 'how', 'to', 'capture', ',', 'ingest', 'and', 'manage', 'huge', 'volume', 'of', 'unstructured', 'content', 'effectively', 'in', 'Content', 'Services', 'platform', '.'] 

 TOTAL TOKENS ==> 29

 ---- POST ----

 [('Because', 'IN'), ('of', 'IN'), ('this', 'DT'), ('growth', 'NN'), (',', ','), ('organizations', 'NNS'), ('are', 'VBP'), ('facing', 'VBG'), ('challenges', 'NNS'), (',', ','), ('on', 'IN'), ('how', 'WRB'), ('to', 'TO'), ('capture', 'VB'), (',', ','), ('ingest', 'VB'), ('and', 'CC'), ('manage', 'VB'), ('huge', 'JJ'), ('volume', 'NN'), ('of', 'IN'), ('unstructured', 'JJ'), ('content', 'NN'), ('effectively', 'RB'), ('in', 'IN'), ('Content', 'NNP'), ('Services', 'NNPS'), ('platform', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['growth', ',', 'organizations', 'facing', 'challenges', ',', 'capture', ',', 'ingest', 'manage', 'huge', 'volume', 'unstructured', 'content', 'effectively', 'Content', 'Services', 'platform', '.']

 TOTAL FILTERED TOKENS ==>  19

 ---- POST FOR FILTERED TOKENS ----

 [('growth', 'NN'), (',', ','), ('organizations', 'NNS'), ('facing', 'VBG'), ('challenges', 'NNS'), (',', ','), ('capture', 'NN'), (',', ','), ('ingest', 'JJ'), ('manage', 'NN'), ('huge', 'JJ'), ('volume', 'NN'), ('unstructured', 'VBD'), ('content', 'JJ'), ('effectively', 'RB'), ('Content', 'NNP'), ('Services', 'NNPS'), ('platform', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['growth ,', ', organizations', 'organizations facing', 'facing challenges', 'challenges ,', ', capture', 'capture ,', ', ingest', 'ingest manage', 'manage huge', 'huge volume', 'volume unstructured', 'unstructured content', 'content effectively', 'effectively Content', 'Content Services', 'Services platform', 'platform .'] 

 TOTAL BIGRAMS --> 18 



 ---- TRI-GRAMS ---- 

 ['growth , organizations', ', organizations facing', 'organizations facing challenges', 'facing challenges ,', 'challenges , capture', ', capture ,', 'capture , ingest', ', ingest manage', 'ingest manage huge', 'manage huge volume', 'huge volume unstructured', 'volume unstructured content', 'unstructured content effectively', 'content effectively Content', 'effectively Content Services', 'Content Services platform', 'Services platform .'] 

 TOTAL TRIGRAMS --> 17 



 ---- NOUN PHRASES ---- 

 ['growth', 'capture', 'ingest manage', 'huge volume', 'platform'] 

 TOTAL NOUN PHRASES --> 5 



 ---- NER ----

 
 ORGANIZATION ---> ['Content Services']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['growth', ',', 'organ', 'face', 'challeng', ',', 'captur', ',', 'ingest', 'manag', 'huge', 'volum', 'unstructur', 'content', 'effect', 'content', 'servic', 'platform', '.']

 TOTAL PORTER STEM WORDS ==> 19



 ---- SNOWBALL STEMMING ----

['growth', ',', 'organ', 'face', 'challeng', ',', 'captur', ',', 'ingest', 'manag', 'huge', 'volum', 'unstructur', 'content', 'effect', 'content', 'servic', 'platform', '.']

 TOTAL SNOWBALL STEM WORDS ==> 19



 ---- LEMMATIZATION ----

['growth', ',', 'organization', 'facing', 'challenge', ',', 'capture', ',', 'ingest', 'manage', 'huge', 'volume', 'unstructured', 'content', 'effectively', 'Content', 'Services', 'platform', '.']

 TOTAL LEMMATIZE WORDS ==> 19

************************************************************************************************************************

3 --> Traditional approaches are becoming ineffective. 


 ---- TOKENS ----

 ['Traditional', 'approaches', 'are', 'becoming', 'ineffective', '.'] 

 TOTAL TOKENS ==> 6

 ---- POST ----

 [('Traditional', 'JJ'), ('approaches', 'NNS'), ('are', 'VBP'), ('becoming', 'VBG'), ('ineffective', 'JJ'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Traditional', 'approaches', 'becoming', 'ineffective', '.']

 TOTAL FILTERED TOKENS ==>  5

 ---- POST FOR FILTERED TOKENS ----

 [('Traditional', 'JJ'), ('approaches', 'NNS'), ('becoming', 'VBG'), ('ineffective', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Traditional approaches', 'approaches becoming', 'becoming ineffective', 'ineffective .'] 

 TOTAL BIGRAMS --> 4 



 ---- TRI-GRAMS ---- 

 ['Traditional approaches becoming', 'approaches becoming ineffective', 'becoming ineffective .'] 

 TOTAL TRIGRAMS --> 3 



 ---- NOUN PHRASES ---- 

 ['ineffective'] 

 TOTAL NOUN PHRASES --> 1 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['tradit', 'approach', 'becom', 'ineffect', '.']

 TOTAL PORTER STEM WORDS ==> 5



 ---- SNOWBALL STEMMING ----

['tradit', 'approach', 'becom', 'ineffect', '.']

 TOTAL SNOWBALL STEM WORDS ==> 5



 ---- LEMMATIZATION ----

['Traditional', 'approach', 'becoming', 'ineffective', '.']

 TOTAL LEMMATIZE WORDS ==> 5

************************************************************************************************************************

4 --> Need of the hour is to address these challenges differently and bring  some “intelligence”, so that Content Services platforms can manage  the unstructured content smartly. 


 ---- TOKENS ----

 ['Need', 'of', 'the', 'hour', 'is', 'to', 'address', 'these', 'challenges', 'differently', 'and', 'bring', 'some', '“', 'intelligence', '”', ',', 'so', 'that', 'Content', 'Services', 'platforms', 'can', 'manage', 'the', 'unstructured', 'content', 'smartly', '.'] 

 TOTAL TOKENS ==> 29

 ---- POST ----

 [('Need', 'NN'), ('of', 'IN'), ('the', 'DT'), ('hour', 'NN'), ('is', 'VBZ'), ('to', 'TO'), ('address', 'VB'), ('these', 'DT'), ('challenges', 'NNS'), ('differently', 'RB'), ('and', 'CC'), ('bring', 'VB'), ('some', 'DT'), ('“', 'JJ'), ('intelligence', 'NN'), ('”', 'NN'), (',', ','), ('so', 'IN'), ('that', 'DT'), ('Content', 'NNP'), ('Services', 'NNPS'), ('platforms', 'NNS'), ('can', 'MD'), ('manage', 'VB'), ('the', 'DT'), ('unstructured', 'JJ'), ('content', 'NN'), ('smartly', 'RB'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Need', 'hour', 'address', 'challenges', 'differently', 'bring', '“', 'intelligence', '”', ',', 'Content', 'Services', 'platforms', 'manage', 'unstructured', 'content', 'smartly', '.']

 TOTAL FILTERED TOKENS ==>  18

 ---- POST FOR FILTERED TOKENS ----

 [('Need', 'VB'), ('hour', 'NN'), ('address', 'NN'), ('challenges', 'NNS'), ('differently', 'RB'), ('bring', 'VBP'), ('“', 'JJ'), ('intelligence', 'NN'), ('”', 'NNP'), (',', ','), ('Content', 'NNP'), ('Services', 'NNPS'), ('platforms', 'VBZ'), ('manage', 'NN'), ('unstructured', 'JJ'), ('content', 'NN'), ('smartly', 'RB'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Need hour', 'hour address', 'address challenges', 'challenges differently', 'differently bring', 'bring “', '“ intelligence', 'intelligence ”', '” ,', ', Content', 'Content Services', 'Services platforms', 'platforms manage', 'manage unstructured', 'unstructured content', 'content smartly', 'smartly .'] 

 TOTAL BIGRAMS --> 17 



 ---- TRI-GRAMS ---- 

 ['Need hour address', 'hour address challenges', 'address challenges differently', 'challenges differently bring', 'differently bring “', 'bring “ intelligence', '“ intelligence ”', 'intelligence ” ,', '” , Content', ', Content Services', 'Content Services platforms', 'Services platforms manage', 'platforms manage unstructured', 'manage unstructured content', 'unstructured content smartly', 'content smartly .'] 

 TOTAL TRIGRAMS --> 16 



 ---- NOUN PHRASES ---- 

 ['hour', 'address', '“ intelligence', 'manage', 'unstructured content'] 

 TOTAL NOUN PHRASES --> 5 



 ---- NER ----

 
 ORGANIZATION ---> ['Content Services']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['need', 'hour', 'address', 'challeng', 'differ', 'bring', '“', 'intellig', '”', ',', 'content', 'servic', 'platform', 'manag', 'unstructur', 'content', 'smartli', '.']

 TOTAL PORTER STEM WORDS ==> 18



 ---- SNOWBALL STEMMING ----

['need', 'hour', 'address', 'challeng', 'differ', 'bring', '“', 'intellig', '”', ',', 'content', 'servic', 'platform', 'manag', 'unstructur', 'content', 'smart', '.']

 TOTAL SNOWBALL STEM WORDS ==> 18



 ---- LEMMATIZATION ----

['Need', 'hour', 'address', 'challenge', 'differently', 'bring', '“', 'intelligence', '”', ',', 'Content', 'Services', 'platform', 'manage', 'unstructured', 'content', 'smartly', '.']

 TOTAL LEMMATIZE WORDS ==> 18

************************************************************************************************************************

5 --> This paper discusses how Natural Language Processing (NLP) can  make a difference in Content Services by addressing challenges  which are being faced. 


 ---- TOKENS ----

 ['This', 'paper', 'discusses', 'how', 'Natural', 'Language', 'Processing', '(', 'NLP', ')', 'can', 'make', 'a', 'difference', 'in', 'Content', 'Services', 'by', 'addressing', 'challenges', 'which', 'are', 'being', 'faced', '.'] 

 TOTAL TOKENS ==> 25

 ---- POST ----

 [('This', 'DT'), ('paper', 'NN'), ('discusses', 'VBZ'), ('how', 'WRB'), ('Natural', 'JJ'), ('Language', 'NN'), ('Processing', 'NNP'), ('(', '('), ('NLP', 'NNP'), (')', ')'), ('can', 'MD'), ('make', 'VB'), ('a', 'DT'), ('difference', 'NN'), ('in', 'IN'), ('Content', 'NNP'), ('Services', 'NNPS'), ('by', 'IN'), ('addressing', 'VBG'), ('challenges', 'NNS'), ('which', 'WDT'), ('are', 'VBP'), ('being', 'VBG'), ('faced', 'VBN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['paper', 'discusses', 'Natural', 'Language', 'Processing', '(', 'NLP', ')', 'make', 'difference', 'Content', 'Services', 'addressing', 'challenges', 'faced', '.']

 TOTAL FILTERED TOKENS ==>  16

 ---- POST FOR FILTERED TOKENS ----

 [('paper', 'NN'), ('discusses', 'VBZ'), ('Natural', 'NNP'), ('Language', 'NNP'), ('Processing', 'NNP'), ('(', '('), ('NLP', 'NNP'), (')', ')'), ('make', 'VBP'), ('difference', 'NN'), ('Content', 'NNP'), ('Services', 'NNPS'), ('addressing', 'VBG'), ('challenges', 'NNS'), ('faced', 'VBN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['paper discusses', 'discusses Natural', 'Natural Language', 'Language Processing', 'Processing (', '( NLP', 'NLP )', ') make', 'make difference', 'difference Content', 'Content Services', 'Services addressing', 'addressing challenges', 'challenges faced', 'faced .'] 

 TOTAL BIGRAMS --> 15 



 ---- TRI-GRAMS ---- 

 ['paper discusses Natural', 'discusses Natural Language', 'Natural Language Processing', 'Language Processing (', 'Processing ( NLP', '( NLP )', 'NLP ) make', ') make difference', 'make difference Content', 'difference Content Services', 'Content Services addressing', 'Services addressing challenges', 'addressing challenges faced', 'challenges faced .'] 

 TOTAL TRIGRAMS --> 14 



 ---- NOUN PHRASES ---- 

 ['paper', 'difference'] 

 TOTAL NOUN PHRASES --> 2 



 ---- NER ----

 
 ORGANIZATION ---> ['Natural Language', 'Content Services']
 TOTAL ORGANIZATION ENTITY --> 2 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['paper', 'discuss', 'natur', 'languag', 'process', '(', 'nlp', ')', 'make', 'differ', 'content', 'servic', 'address', 'challeng', 'face', '.']

 TOTAL PORTER STEM WORDS ==> 16



 ---- SNOWBALL STEMMING ----

['paper', 'discuss', 'natur', 'languag', 'process', '(', 'nlp', ')', 'make', 'differ', 'content', 'servic', 'address', 'challeng', 'face', '.']

 TOTAL SNOWBALL STEM WORDS ==> 16



 ---- LEMMATIZATION ----

['paper', 'discus', 'Natural', 'Language', 'Processing', '(', 'NLP', ')', 'make', 'difference', 'Content', 'Services', 'addressing', 'challenge', 'faced', '.']

 TOTAL LEMMATIZE WORDS ==> 16

************************************************************************************************************************

6 --> Here, we will first introduce NLP and then  briefly describe how NLP can be used in Content Services while  capturing, ingesting and managing. 


 ---- TOKENS ----

 ['Here', ',', 'we', 'will', 'first', 'introduce', 'NLP', 'and', 'then', 'briefly', 'describe', 'how', 'NLP', 'can', 'be', 'used', 'in', 'Content', 'Services', 'while', 'capturing', ',', 'ingesting', 'and', 'managing', '.'] 

 TOTAL TOKENS ==> 26

 ---- POST ----

 [('Here', 'RB'), (',', ','), ('we', 'PRP'), ('will', 'MD'), ('first', 'RB'), ('introduce', 'VB'), ('NLP', 'NNP'), ('and', 'CC'), ('then', 'RB'), ('briefly', 'VB'), ('describe', 'VB'), ('how', 'WRB'), ('NLP', 'NNP'), ('can', 'MD'), ('be', 'VB'), ('used', 'VBN'), ('in', 'IN'), ('Content', 'NNP'), ('Services', 'NNPS'), ('while', 'IN'), ('capturing', 'VBG'), (',', ','), ('ingesting', 'VBG'), ('and', 'CC'), ('managing', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 [',', 'first', 'introduce', 'NLP', 'briefly', 'describe', 'NLP', 'used', 'Content', 'Services', 'capturing', ',', 'ingesting', 'managing', '.']

 TOTAL FILTERED TOKENS ==>  15

 ---- POST FOR FILTERED TOKENS ----

 [(',', ','), ('first', 'JJ'), ('introduce', 'NN'), ('NLP', 'NNP'), ('briefly', 'NN'), ('describe', 'NN'), ('NLP', 'NNP'), ('used', 'VBD'), ('Content', 'NNP'), ('Services', 'NNPS'), ('capturing', 'NN'), (',', ','), ('ingesting', 'VBG'), ('managing', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 [', first', 'first introduce', 'introduce NLP', 'NLP briefly', 'briefly describe', 'describe NLP', 'NLP used', 'used Content', 'Content Services', 'Services capturing', 'capturing ,', ', ingesting', 'ingesting managing', 'managing .'] 

 TOTAL BIGRAMS --> 14 



 ---- TRI-GRAMS ---- 

 [', first introduce', 'first introduce NLP', 'introduce NLP briefly', 'NLP briefly describe', 'briefly describe NLP', 'describe NLP used', 'NLP used Content', 'used Content Services', 'Content Services capturing', 'Services capturing ,', 'capturing , ingesting', ', ingesting managing', 'ingesting managing .'] 

 TOTAL TRIGRAMS --> 13 



 ---- NOUN PHRASES ---- 

 ['first introduce', 'briefly', 'describe', 'capturing', 'managing'] 

 TOTAL NOUN PHRASES --> 5 



 ---- NER ----

 
 ORGANIZATION ---> ['NLP', 'NLP', 'Content Services']
 TOTAL ORGANIZATION ENTITY --> 3 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

[',', 'first', 'introduc', 'nlp', 'briefli', 'describ', 'nlp', 'use', 'content', 'servic', 'captur', ',', 'ingest', 'manag', '.']

 TOTAL PORTER STEM WORDS ==> 15



 ---- SNOWBALL STEMMING ----

[',', 'first', 'introduc', 'nlp', 'briefli', 'describ', 'nlp', 'use', 'content', 'servic', 'captur', ',', 'ingest', 'manag', '.']

 TOTAL SNOWBALL STEM WORDS ==> 15



 ---- LEMMATIZATION ----

[',', 'first', 'introduce', 'NLP', 'briefly', 'describe', 'NLP', 'used', 'Content', 'Services', 'capturing', ',', 'ingesting', 'managing', '.']

 TOTAL LEMMATIZE WORDS ==> 15

************************************************************************************************************************

7 --> 1.1 What is NLP Natural language processing is a subfield  of artificial intelligence, dealing with the  interactions between computers and  human languages, in particular on how to  program computers to process and analyze  large amounts of natural language data. 


 ---- TOKENS ----

 ['1.1', 'What', 'is', 'NLP', 'Natural', 'language', 'processing', 'is', 'a', 'subfield', 'of', 'artificial', 'intelligence', ',', 'dealing', 'with', 'the', 'interactions', 'between', 'computers', 'and', 'human', 'languages', ',', 'in', 'particular', 'on', 'how', 'to', 'program', 'computers', 'to', 'process', 'and', 'analyze', 'large', 'amounts', 'of', 'natural', 'language', 'data', '.'] 

 TOTAL TOKENS ==> 42

 ---- POST ----

 [('1.1', 'CD'), ('What', 'WP'), ('is', 'VBZ'), ('NLP', 'NNP'), ('Natural', 'NNP'), ('language', 'NN'), ('processing', 'NN'), ('is', 'VBZ'), ('a', 'DT'), ('subfield', 'NN'), ('of', 'IN'), ('artificial', 'JJ'), ('intelligence', 'NN'), (',', ','), ('dealing', 'VBG'), ('with', 'IN'), ('the', 'DT'), ('interactions', 'NNS'), ('between', 'IN'), ('computers', 'NNS'), ('and', 'CC'), ('human', 'JJ'), ('languages', 'NNS'), (',', ','), ('in', 'IN'), ('particular', 'JJ'), ('on', 'IN'), ('how', 'WRB'), ('to', 'TO'), ('program', 'NN'), ('computers', 'NNS'), ('to', 'TO'), ('process', 'VB'), ('and', 'CC'), ('analyze', 'VB'), ('large', 'JJ'), ('amounts', 'NNS'), ('of', 'IN'), ('natural', 'JJ'), ('language', 'NN'), ('data', 'NNS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['1.1', 'NLP', 'Natural', 'language', 'processing', 'subfield', 'artificial', 'intelligence', ',', 'dealing', 'interactions', 'computers', 'human', 'languages', ',', 'particular', 'program', 'computers', 'process', 'analyze', 'large', 'amounts', 'natural', 'language', 'data', '.']

 TOTAL FILTERED TOKENS ==>  26

 ---- POST FOR FILTERED TOKENS ----

 [('1.1', 'CD'), ('NLP', 'NNP'), ('Natural', 'NNP'), ('language', 'NN'), ('processing', 'NN'), ('subfield', 'VBD'), ('artificial', 'JJ'), ('intelligence', 'NN'), (',', ','), ('dealing', 'VBG'), ('interactions', 'NNS'), ('computers', 'NNS'), ('human', 'JJ'), ('languages', 'NNS'), (',', ','), ('particular', 'JJ'), ('program', 'NN'), ('computers', 'NNS'), ('process', 'VBP'), ('analyze', 'JJ'), ('large', 'JJ'), ('amounts', 'NNS'), ('natural', 'JJ'), ('language', 'NN'), ('data', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['1.1 NLP', 'NLP Natural', 'Natural language', 'language processing', 'processing subfield', 'subfield artificial', 'artificial intelligence', 'intelligence ,', ', dealing', 'dealing interactions', 'interactions computers', 'computers human', 'human languages', 'languages ,', ', particular', 'particular program', 'program computers', 'computers process', 'process analyze', 'analyze large', 'large amounts', 'amounts natural', 'natural language', 'language data', 'data .'] 

 TOTAL BIGRAMS --> 25 



 ---- TRI-GRAMS ---- 

 ['1.1 NLP Natural', 'NLP Natural language', 'Natural language processing', 'language processing subfield', 'processing subfield artificial', 'subfield artificial intelligence', 'artificial intelligence ,', 'intelligence , dealing', ', dealing interactions', 'dealing interactions computers', 'interactions computers human', 'computers human languages', 'human languages ,', 'languages , particular', ', particular program', 'particular program computers', 'program computers process', 'computers process analyze', 'process analyze large', 'analyze large amounts', 'large amounts natural', 'amounts natural language', 'natural language data', 'language data .'] 

 TOTAL TRIGRAMS --> 24 



 ---- NOUN PHRASES ---- 

 ['language', 'processing', 'artificial intelligence', 'particular program', 'natural language'] 

 TOTAL NOUN PHRASES --> 5 



 ---- NER ----

 
 ORGANIZATION ---> ['NLP Natural']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['1.1', 'nlp', 'natur', 'languag', 'process', 'subfield', 'artifici', 'intellig', ',', 'deal', 'interact', 'comput', 'human', 'languag', ',', 'particular', 'program', 'comput', 'process', 'analyz', 'larg', 'amount', 'natur', 'languag', 'data', '.']

 TOTAL PORTER STEM WORDS ==> 26



 ---- SNOWBALL STEMMING ----

['1.1', 'nlp', 'natur', 'languag', 'process', 'subfield', 'artifici', 'intellig', ',', 'deal', 'interact', 'comput', 'human', 'languag', ',', 'particular', 'program', 'comput', 'process', 'analyz', 'larg', 'amount', 'natur', 'languag', 'data', '.']

 TOTAL SNOWBALL STEM WORDS ==> 26



 ---- LEMMATIZATION ----

['1.1', 'NLP', 'Natural', 'language', 'processing', 'subfield', 'artificial', 'intelligence', ',', 'dealing', 'interaction', 'computer', 'human', 'language', ',', 'particular', 'program', 'computer', 'process', 'analyze', 'large', 'amount', 'natural', 'language', 'data', '.']

 TOTAL LEMMATIZE WORDS ==> 26

************************************************************************************************************************

8 --> NLP includes text preprocessing, sentence  segmentation, tokenization, POS tagging,  Named Entity Recognition (NER), chunking,  parsing, co-reference resolution and text  categorizer. 


 ---- TOKENS ----

 ['NLP', 'includes', 'text', 'preprocessing', ',', 'sentence', 'segmentation', ',', 'tokenization', ',', 'POS', 'tagging', ',', 'Named', 'Entity', 'Recognition', '(', 'NER', ')', ',', 'chunking', ',', 'parsing', ',', 'co-reference', 'resolution', 'and', 'text', 'categorizer', '.'] 

 TOTAL TOKENS ==> 30

 ---- POST ----

 [('NLP', 'NNP'), ('includes', 'VBZ'), ('text', 'JJ'), ('preprocessing', 'NN'), (',', ','), ('sentence', 'NN'), ('segmentation', 'NN'), (',', ','), ('tokenization', 'NN'), (',', ','), ('POS', 'NNP'), ('tagging', 'NN'), (',', ','), ('Named', 'NNP'), ('Entity', 'NNP'), ('Recognition', 'NNP'), ('(', '('), ('NER', 'NNP'), (')', ')'), (',', ','), ('chunking', 'VBG'), (',', ','), ('parsing', 'VBG'), (',', ','), ('co-reference', 'JJ'), ('resolution', 'NN'), ('and', 'CC'), ('text', 'JJ'), ('categorizer', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['NLP', 'includes', 'text', 'preprocessing', ',', 'sentence', 'segmentation', ',', 'tokenization', ',', 'POS', 'tagging', ',', 'Named', 'Entity', 'Recognition', '(', 'NER', ')', ',', 'chunking', ',', 'parsing', ',', 'co-reference', 'resolution', 'text', 'categorizer', '.']

 TOTAL FILTERED TOKENS ==>  29

 ---- POST FOR FILTERED TOKENS ----

 [('NLP', 'NNP'), ('includes', 'VBZ'), ('text', 'JJ'), ('preprocessing', 'NN'), (',', ','), ('sentence', 'NN'), ('segmentation', 'NN'), (',', ','), ('tokenization', 'NN'), (',', ','), ('POS', 'NNP'), ('tagging', 'NN'), (',', ','), ('Named', 'NNP'), ('Entity', 'NNP'), ('Recognition', 'NNP'), ('(', '('), ('NER', 'NNP'), (')', ')'), (',', ','), ('chunking', 'VBG'), (',', ','), ('parsing', 'VBG'), (',', ','), ('co-reference', 'JJ'), ('resolution', 'NN'), ('text', 'NN'), ('categorizer', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['NLP includes', 'includes text', 'text preprocessing', 'preprocessing ,', ', sentence', 'sentence segmentation', 'segmentation ,', ', tokenization', 'tokenization ,', ', POS', 'POS tagging', 'tagging ,', ', Named', 'Named Entity', 'Entity Recognition', 'Recognition (', '( NER', 'NER )', ') ,', ', chunking', 'chunking ,', ', parsing', 'parsing ,', ', co-reference', 'co-reference resolution', 'resolution text', 'text categorizer', 'categorizer .'] 

 TOTAL BIGRAMS --> 28 



 ---- TRI-GRAMS ---- 

 ['NLP includes text', 'includes text preprocessing', 'text preprocessing ,', 'preprocessing , sentence', ', sentence segmentation', 'sentence segmentation ,', 'segmentation , tokenization', ', tokenization ,', 'tokenization , POS', ', POS tagging', 'POS tagging ,', 'tagging , Named', ', Named Entity', 'Named Entity Recognition', 'Entity Recognition (', 'Recognition ( NER', '( NER )', 'NER ) ,', ') , chunking', ', chunking ,', 'chunking , parsing', ', parsing ,', 'parsing , co-reference', ', co-reference resolution', 'co-reference resolution text', 'resolution text categorizer', 'text categorizer .'] 

 TOTAL TRIGRAMS --> 27 



 ---- NOUN PHRASES ---- 

 ['text preprocessing', 'sentence', 'segmentation', 'tokenization', 'tagging', 'co-reference resolution', 'text', 'categorizer'] 

 TOTAL NOUN PHRASES --> 8 



 ---- NER ----

 
 ORGANIZATION ---> ['NLP', 'POS']
 TOTAL ORGANIZATION ENTITY --> 2 


 PERSON ---> ['Named Entity']
 TOTAL PERSON ENTITY --> 1 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['nlp', 'includ', 'text', 'preprocess', ',', 'sentenc', 'segment', ',', 'token', ',', 'po', 'tag', ',', 'name', 'entiti', 'recognit', '(', 'ner', ')', ',', 'chunk', ',', 'pars', ',', 'co-refer', 'resolut', 'text', 'categor', '.']

 TOTAL PORTER STEM WORDS ==> 29



 ---- SNOWBALL STEMMING ----

['nlp', 'includ', 'text', 'preprocess', ',', 'sentenc', 'segment', ',', 'token', ',', 'pos', 'tag', ',', 'name', 'entiti', 'recognit', '(', 'ner', ')', ',', 'chunk', ',', 'pars', ',', 'co-refer', 'resolut', 'text', 'categor', '.']

 TOTAL SNOWBALL STEM WORDS ==> 29



 ---- LEMMATIZATION ----

['NLP', 'includes', 'text', 'preprocessing', ',', 'sentence', 'segmentation', ',', 'tokenization', ',', 'POS', 'tagging', ',', 'Named', 'Entity', 'Recognition', '(', 'NER', ')', ',', 'chunking', ',', 'parsing', ',', 'co-reference', 'resolution', 'text', 'categorizer', '.']

 TOTAL LEMMATIZE WORDS ==> 29

************************************************************************************************************************

9 --> In this white paper we will  focus on NER and categorizer. 


 ---- TOKENS ----

 ['In', 'this', 'white', 'paper', 'we', 'will', 'focus', 'on', 'NER', 'and', 'categorizer', '.'] 

 TOTAL TOKENS ==> 12

 ---- POST ----

 [('In', 'IN'), ('this', 'DT'), ('white', 'JJ'), ('paper', 'NN'), ('we', 'PRP'), ('will', 'MD'), ('focus', 'VB'), ('on', 'IN'), ('NER', 'NNP'), ('and', 'CC'), ('categorizer', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['white', 'paper', 'focus', 'NER', 'categorizer', '.']

 TOTAL FILTERED TOKENS ==>  6

 ---- POST FOR FILTERED TOKENS ----

 [('white', 'JJ'), ('paper', 'NN'), ('focus', 'NN'), ('NER', 'NNP'), ('categorizer', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['white paper', 'paper focus', 'focus NER', 'NER categorizer', 'categorizer .'] 

 TOTAL BIGRAMS --> 5 



 ---- TRI-GRAMS ---- 

 ['white paper focus', 'paper focus NER', 'focus NER categorizer', 'NER categorizer .'] 

 TOTAL TRIGRAMS --> 4 



 ---- NOUN PHRASES ---- 

 ['white paper', 'focus', 'categorizer'] 

 TOTAL NOUN PHRASES --> 3 



 ---- NER ----

 
 ORGANIZATION ---> ['NER']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['white', 'paper', 'focu', 'ner', 'categor', '.']

 TOTAL PORTER STEM WORDS ==> 6



 ---- SNOWBALL STEMMING ----

['white', 'paper', 'focus', 'ner', 'categor', '.']

 TOTAL SNOWBALL STEM WORDS ==> 6



 ---- LEMMATIZATION ----

['white', 'paper', 'focus', 'NER', 'categorizer', '.']

 TOTAL LEMMATIZE WORDS ==> 6

************************************************************************************************************************

10 --> 1.1.1 Named Entity Recognition (NER) Named Entity Recognition, is for  identifying the entities in unstructured  content. 


 ---- TOKENS ----

 ['1.1.1', 'Named', 'Entity', 'Recognition', '(', 'NER', ')', 'Named', 'Entity', 'Recognition', ',', 'is', 'for', 'identifying', 'the', 'entities', 'in', 'unstructured', 'content', '.'] 

 TOTAL TOKENS ==> 20

 ---- POST ----

 [('1.1.1', 'CD'), ('Named', 'NNP'), ('Entity', 'NNP'), ('Recognition', 'NNP'), ('(', '('), ('NER', 'NNP'), (')', ')'), ('Named', 'NNP'), ('Entity', 'NNP'), ('Recognition', 'NNP'), (',', ','), ('is', 'VBZ'), ('for', 'IN'), ('identifying', 'VBG'), ('the', 'DT'), ('entities', 'NNS'), ('in', 'IN'), ('unstructured', 'JJ'), ('content', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['1.1.1', 'Named', 'Entity', 'Recognition', '(', 'NER', ')', 'Named', 'Entity', 'Recognition', ',', 'identifying', 'entities', 'unstructured', 'content', '.']

 TOTAL FILTERED TOKENS ==>  16

 ---- POST FOR FILTERED TOKENS ----

 [('1.1.1', 'CD'), ('Named', 'NNP'), ('Entity', 'NNP'), ('Recognition', 'NNP'), ('(', '('), ('NER', 'NNP'), (')', ')'), ('Named', 'NNP'), ('Entity', 'NNP'), ('Recognition', 'NNP'), (',', ','), ('identifying', 'VBG'), ('entities', 'NNS'), ('unstructured', 'JJ'), ('content', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['1.1.1 Named', 'Named Entity', 'Entity Recognition', 'Recognition (', '( NER', 'NER )', ') Named', 'Named Entity', 'Entity Recognition', 'Recognition ,', ', identifying', 'identifying entities', 'entities unstructured', 'unstructured content', 'content .'] 

 TOTAL BIGRAMS --> 15 



 ---- TRI-GRAMS ---- 

 ['1.1.1 Named Entity', 'Named Entity Recognition', 'Entity Recognition (', 'Recognition ( NER', '( NER )', 'NER ) Named', ') Named Entity', 'Named Entity Recognition', 'Entity Recognition ,', 'Recognition , identifying', ', identifying entities', 'identifying entities unstructured', 'entities unstructured content', 'unstructured content .'] 

 TOTAL TRIGRAMS --> 14 



 ---- NOUN PHRASES ---- 

 ['unstructured content'] 

 TOTAL NOUN PHRASES --> 1 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> ['Named Entity Recognition']
 TOTAL PERSON ENTITY --> 1 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['1.1.1', 'name', 'entiti', 'recognit', '(', 'ner', ')', 'name', 'entiti', 'recognit', ',', 'identifi', 'entiti', 'unstructur', 'content', '.']

 TOTAL PORTER STEM WORDS ==> 16



 ---- SNOWBALL STEMMING ----

['1.1.1', 'name', 'entiti', 'recognit', '(', 'ner', ')', 'name', 'entiti', 'recognit', ',', 'identifi', 'entiti', 'unstructur', 'content', '.']

 TOTAL SNOWBALL STEM WORDS ==> 16



 ---- LEMMATIZATION ----

['1.1.1', 'Named', 'Entity', 'Recognition', '(', 'NER', ')', 'Named', 'Entity', 'Recognition', ',', 'identifying', 'entity', 'unstructured', 'content', '.']

 TOTAL LEMMATIZE WORDS ==> 16

************************************************************************************************************************

11 --> Entities such as name, person,  location, time, money are few of the  common entities which can be extracted. 


 ---- TOKENS ----

 ['Entities', 'such', 'as', 'name', ',', 'person', ',', 'location', ',', 'time', ',', 'money', 'are', 'few', 'of', 'the', 'common', 'entities', 'which', 'can', 'be', 'extracted', '.'] 

 TOTAL TOKENS ==> 23

 ---- POST ----

 [('Entities', 'NNS'), ('such', 'JJ'), ('as', 'IN'), ('name', 'NN'), (',', ','), ('person', 'NN'), (',', ','), ('location', 'NN'), (',', ','), ('time', 'NN'), (',', ','), ('money', 'NN'), ('are', 'VBP'), ('few', 'JJ'), ('of', 'IN'), ('the', 'DT'), ('common', 'JJ'), ('entities', 'NNS'), ('which', 'WDT'), ('can', 'MD'), ('be', 'VB'), ('extracted', 'VBN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Entities', 'name', ',', 'person', ',', 'location', ',', 'time', ',', 'money', 'common', 'entities', 'extracted', '.']

 TOTAL FILTERED TOKENS ==>  14

 ---- POST FOR FILTERED TOKENS ----

 [('Entities', 'NNS'), ('name', 'NN'), (',', ','), ('person', 'NN'), (',', ','), ('location', 'NN'), (',', ','), ('time', 'NN'), (',', ','), ('money', 'NN'), ('common', 'JJ'), ('entities', 'NNS'), ('extracted', 'VBN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Entities name', 'name ,', ', person', 'person ,', ', location', 'location ,', ', time', 'time ,', ', money', 'money common', 'common entities', 'entities extracted', 'extracted .'] 

 TOTAL BIGRAMS --> 13 



 ---- TRI-GRAMS ---- 

 ['Entities name ,', 'name , person', ', person ,', 'person , location', ', location ,', 'location , time', ', time ,', 'time , money', ', money common', 'money common entities', 'common entities extracted', 'entities extracted .'] 

 TOTAL TRIGRAMS --> 12 



 ---- NOUN PHRASES ---- 

 ['name', 'person', 'location', 'time', 'money'] 

 TOTAL NOUN PHRASES --> 5 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['entiti', 'name', ',', 'person', ',', 'locat', ',', 'time', ',', 'money', 'common', 'entiti', 'extract', '.']

 TOTAL PORTER STEM WORDS ==> 14



 ---- SNOWBALL STEMMING ----

['entiti', 'name', ',', 'person', ',', 'locat', ',', 'time', ',', 'money', 'common', 'entiti', 'extract', '.']

 TOTAL SNOWBALL STEM WORDS ==> 14



 ---- LEMMATIZATION ----

['Entities', 'name', ',', 'person', ',', 'location', ',', 'time', ',', 'money', 'common', 'entity', 'extracted', '.']

 TOTAL LEMMATIZE WORDS ==> 14

************************************************************************************************************************

12 --> Apart from these, models can be trained  to extract business specific entities such as  account number, tax file number specific  to the business. 


 ---- TOKENS ----

 ['Apart', 'from', 'these', ',', 'models', 'can', 'be', 'trained', 'to', 'extract', 'business', 'specific', 'entities', 'such', 'as', 'account', 'number', ',', 'tax', 'file', 'number', 'specific', 'to', 'the', 'business', '.'] 

 TOTAL TOKENS ==> 26

 ---- POST ----

 [('Apart', 'RB'), ('from', 'IN'), ('these', 'DT'), (',', ','), ('models', 'NNS'), ('can', 'MD'), ('be', 'VB'), ('trained', 'VBN'), ('to', 'TO'), ('extract', 'VB'), ('business', 'NN'), ('specific', 'JJ'), ('entities', 'NNS'), ('such', 'JJ'), ('as', 'IN'), ('account', 'NN'), ('number', 'NN'), (',', ','), ('tax', 'NN'), ('file', 'NN'), ('number', 'NN'), ('specific', 'JJ'), ('to', 'TO'), ('the', 'DT'), ('business', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Apart', ',', 'models', 'trained', 'extract', 'business', 'specific', 'entities', 'account', 'number', ',', 'tax', 'file', 'number', 'specific', 'business', '.']

 TOTAL FILTERED TOKENS ==>  17

 ---- POST FOR FILTERED TOKENS ----

 [('Apart', 'RB'), (',', ','), ('models', 'NNS'), ('trained', 'VBN'), ('extract', 'JJ'), ('business', 'NN'), ('specific', 'JJ'), ('entities', 'NNS'), ('account', 'VBP'), ('number', 'NN'), (',', ','), ('tax', 'NN'), ('file', 'NN'), ('number', 'NN'), ('specific', 'JJ'), ('business', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Apart ,', ', models', 'models trained', 'trained extract', 'extract business', 'business specific', 'specific entities', 'entities account', 'account number', 'number ,', ', tax', 'tax file', 'file number', 'number specific', 'specific business', 'business .'] 

 TOTAL BIGRAMS --> 16 



 ---- TRI-GRAMS ---- 

 ['Apart , models', ', models trained', 'models trained extract', 'trained extract business', 'extract business specific', 'business specific entities', 'specific entities account', 'entities account number', 'account number ,', 'number , tax', ', tax file', 'tax file number', 'file number specific', 'number specific business', 'specific business .'] 

 TOTAL TRIGRAMS --> 15 



 ---- NOUN PHRASES ---- 

 ['extract business', 'number', 'tax', 'file', 'number', 'specific business'] 

 TOTAL NOUN PHRASES --> 6 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['apart', ',', 'model', 'train', 'extract', 'busi', 'specif', 'entiti', 'account', 'number', ',', 'tax', 'file', 'number', 'specif', 'busi', '.']

 TOTAL PORTER STEM WORDS ==> 17



 ---- SNOWBALL STEMMING ----

['apart', ',', 'model', 'train', 'extract', 'busi', 'specif', 'entiti', 'account', 'number', ',', 'tax', 'file', 'number', 'specif', 'busi', '.']

 TOTAL SNOWBALL STEM WORDS ==> 17



 ---- LEMMATIZATION ----

['Apart', ',', 'model', 'trained', 'extract', 'business', 'specific', 'entity', 'account', 'number', ',', 'tax', 'file', 'number', 'specific', 'business', '.']

 TOTAL LEMMATIZE WORDS ==> 17

************************************************************************************************************************

13 --> Core capability of NER is,  classification and extraction of the named  entities. 


 ---- TOKENS ----

 ['Core', 'capability', 'of', 'NER', 'is', ',', 'classification', 'and', 'extraction', 'of', 'the', 'named', 'entities', '.'] 

 TOTAL TOKENS ==> 14

 ---- POST ----

 [('Core', 'NNP'), ('capability', 'NN'), ('of', 'IN'), ('NER', 'NNP'), ('is', 'VBZ'), (',', ','), ('classification', 'NN'), ('and', 'CC'), ('extraction', 'NN'), ('of', 'IN'), ('the', 'DT'), ('named', 'JJ'), ('entities', 'NNS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Core', 'capability', 'NER', ',', 'classification', 'extraction', 'named', 'entities', '.']

 TOTAL FILTERED TOKENS ==>  9

 ---- POST FOR FILTERED TOKENS ----

 [('Core', 'NNP'), ('capability', 'NN'), ('NER', 'NNP'), (',', ','), ('classification', 'NN'), ('extraction', 'NN'), ('named', 'VBN'), ('entities', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Core capability', 'capability NER', 'NER ,', ', classification', 'classification extraction', 'extraction named', 'named entities', 'entities .'] 

 TOTAL BIGRAMS --> 8 



 ---- TRI-GRAMS ---- 

 ['Core capability NER', 'capability NER ,', 'NER , classification', ', classification extraction', 'classification extraction named', 'extraction named entities', 'named entities .'] 

 TOTAL TRIGRAMS --> 7 



 ---- NOUN PHRASES ---- 

 ['capability', 'classification', 'extraction'] 

 TOTAL NOUN PHRASES --> 3 



 ---- NER ----

 
 ORGANIZATION ---> ['NER']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> ['Core']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['core', 'capabl', 'ner', ',', 'classif', 'extract', 'name', 'entiti', '.']

 TOTAL PORTER STEM WORDS ==> 9



 ---- SNOWBALL STEMMING ----

['core', 'capabl', 'ner', ',', 'classif', 'extract', 'name', 'entiti', '.']

 TOTAL SNOWBALL STEM WORDS ==> 9



 ---- LEMMATIZATION ----

['Core', 'capability', 'NER', ',', 'classification', 'extraction', 'named', 'entity', '.']

 TOTAL LEMMATIZE WORDS ==> 9

************************************************************************************************************************

14 --> NER can extract static as well as  dynamic entities. 


 ---- TOKENS ----

 ['NER', 'can', 'extract', 'static', 'as', 'well', 'as', 'dynamic', 'entities', '.'] 

 TOTAL TOKENS ==> 10

 ---- POST ----

 [('NER', 'NNP'), ('can', 'MD'), ('extract', 'VB'), ('static', 'JJ'), ('as', 'RB'), ('well', 'RB'), ('as', 'IN'), ('dynamic', 'JJ'), ('entities', 'NNS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['NER', 'extract', 'static', 'well', 'dynamic', 'entities', '.']

 TOTAL FILTERED TOKENS ==>  7

 ---- POST FOR FILTERED TOKENS ----

 [('NER', 'NNP'), ('extract', 'JJ'), ('static', 'JJ'), ('well', 'RB'), ('dynamic', 'JJ'), ('entities', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['NER extract', 'extract static', 'static well', 'well dynamic', 'dynamic entities', 'entities .'] 

 TOTAL BIGRAMS --> 6 



 ---- TRI-GRAMS ---- 

 ['NER extract static', 'extract static well', 'static well dynamic', 'well dynamic entities', 'dynamic entities .'] 

 TOTAL TRIGRAMS --> 5 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> ['NER']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['ner', 'extract', 'static', 'well', 'dynam', 'entiti', '.']

 TOTAL PORTER STEM WORDS ==> 7



 ---- SNOWBALL STEMMING ----

['ner', 'extract', 'static', 'well', 'dynam', 'entiti', '.']

 TOTAL SNOWBALL STEM WORDS ==> 7



 ---- LEMMATIZATION ----

['NER', 'extract', 'static', 'well', 'dynamic', 'entity', '.']

 TOTAL LEMMATIZE WORDS ==> 7

************************************************************************************************************************

15 --> 1.1.2 Categorizer Core capability of categorizer is  classification. 


 ---- TOKENS ----

 ['1.1.2', 'Categorizer', 'Core', 'capability', 'of', 'categorizer', 'is', 'classification', '.'] 

 TOTAL TOKENS ==> 9

 ---- POST ----

 [('1.1.2', 'CD'), ('Categorizer', 'NNP'), ('Core', 'NNP'), ('capability', 'NN'), ('of', 'IN'), ('categorizer', 'NN'), ('is', 'VBZ'), ('classification', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['1.1.2', 'Categorizer', 'Core', 'capability', 'categorizer', 'classification', '.']

 TOTAL FILTERED TOKENS ==>  7

 ---- POST FOR FILTERED TOKENS ----

 [('1.1.2', 'CD'), ('Categorizer', 'NNP'), ('Core', 'NNP'), ('capability', 'NN'), ('categorizer', 'NN'), ('classification', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['1.1.2 Categorizer', 'Categorizer Core', 'Core capability', 'capability categorizer', 'categorizer classification', 'classification .'] 

 TOTAL BIGRAMS --> 6 



 ---- TRI-GRAMS ---- 

 ['1.1.2 Categorizer Core', 'Categorizer Core capability', 'Core capability categorizer', 'capability categorizer classification', 'categorizer classification .'] 

 TOTAL TRIGRAMS --> 5 



 ---- NOUN PHRASES ---- 

 ['capability', 'categorizer', 'classification'] 

 TOTAL NOUN PHRASES --> 3 



 ---- NER ----

 
 ORGANIZATION ---> ['Categorizer Core']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['1.1.2', 'categor', 'core', 'capabl', 'categor', 'classif', '.']

 TOTAL PORTER STEM WORDS ==> 7



 ---- SNOWBALL STEMMING ----

['1.1.2', 'categor', 'core', 'capabl', 'categor', 'classif', '.']

 TOTAL SNOWBALL STEM WORDS ==> 7



 ---- LEMMATIZATION ----

['1.1.2', 'Categorizer', 'Core', 'capability', 'categorizer', 'classification', '.']

 TOTAL LEMMATIZE WORDS ==> 7

************************************************************************************************************************

16 --> It helps in  categorizing a  chunk of unstructured text into predefined  categories. 


 ---- TOKENS ----

 ['It', 'helps', 'in', 'categorizing', 'a', 'chunk', 'of', 'unstructured', 'text', 'into', 'predefined', 'categories', '.'] 

 TOTAL TOKENS ==> 13

 ---- POST ----

 [('It', 'PRP'), ('helps', 'VBZ'), ('in', 'IN'), ('categorizing', 'VBG'), ('a', 'DT'), ('chunk', 'NN'), ('of', 'IN'), ('unstructured', 'JJ'), ('text', 'NN'), ('into', 'IN'), ('predefined', 'JJ'), ('categories', 'NNS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['helps', 'categorizing', 'chunk', 'unstructured', 'text', 'predefined', 'categories', '.']

 TOTAL FILTERED TOKENS ==>  8

 ---- POST FOR FILTERED TOKENS ----

 [('helps', 'VBZ'), ('categorizing', 'VBG'), ('chunk', 'NN'), ('unstructured', 'VBD'), ('text', 'NN'), ('predefined', 'JJ'), ('categories', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['helps categorizing', 'categorizing chunk', 'chunk unstructured', 'unstructured text', 'text predefined', 'predefined categories', 'categories .'] 

 TOTAL BIGRAMS --> 7 



 ---- TRI-GRAMS ---- 

 ['helps categorizing chunk', 'categorizing chunk unstructured', 'chunk unstructured text', 'unstructured text predefined', 'text predefined categories', 'predefined categories .'] 

 TOTAL TRIGRAMS --> 6 



 ---- NOUN PHRASES ---- 

 ['chunk', 'text'] 

 TOTAL NOUN PHRASES --> 2 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['help', 'categor', 'chunk', 'unstructur', 'text', 'predefin', 'categori', '.']

 TOTAL PORTER STEM WORDS ==> 8



 ---- SNOWBALL STEMMING ----

['help', 'categor', 'chunk', 'unstructur', 'text', 'predefin', 'categori', '.']

 TOTAL SNOWBALL STEM WORDS ==> 8



 ---- LEMMATIZATION ----

['help', 'categorizing', 'chunk', 'unstructured', 'text', 'predefined', 'category', '.']

 TOTAL LEMMATIZE WORDS ==> 8

************************************************************************************************************************

17 --> For example, categorizer  models can be used to categorize news  articles into different genre, sentiment  analysis of messages, identifying  the custom document types for an  organization, etc. 


 ---- TOKENS ----

 ['For', 'example', ',', 'categorizer', 'models', 'can', 'be', 'used', 'to', 'categorize', 'news', 'articles', 'into', 'different', 'genre', ',', 'sentiment', 'analysis', 'of', 'messages', ',', 'identifying', 'the', 'custom', 'document', 'types', 'for', 'an', 'organization', ',', 'etc', '.'] 

 TOTAL TOKENS ==> 32

 ---- POST ----

 [('For', 'IN'), ('example', 'NN'), (',', ','), ('categorizer', 'NN'), ('models', 'NNS'), ('can', 'MD'), ('be', 'VB'), ('used', 'VBN'), ('to', 'TO'), ('categorize', 'VB'), ('news', 'NN'), ('articles', 'NNS'), ('into', 'IN'), ('different', 'JJ'), ('genre', 'NN'), (',', ','), ('sentiment', 'JJ'), ('analysis', 'NN'), ('of', 'IN'), ('messages', 'NNS'), (',', ','), ('identifying', 'VBG'), ('the', 'DT'), ('custom', 'NN'), ('document', 'NN'), ('types', 'NNS'), ('for', 'IN'), ('an', 'DT'), ('organization', 'NN'), (',', ','), ('etc', 'FW'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['example', ',', 'categorizer', 'models', 'used', 'categorize', 'news', 'articles', 'different', 'genre', ',', 'sentiment', 'analysis', 'messages', ',', 'identifying', 'custom', 'document', 'types', 'organization', ',', 'etc', '.']

 TOTAL FILTERED TOKENS ==>  23

 ---- POST FOR FILTERED TOKENS ----

 [('example', 'NN'), (',', ','), ('categorizer', 'NN'), ('models', 'NNS'), ('used', 'VBN'), ('categorize', 'JJ'), ('news', 'NN'), ('articles', 'NNS'), ('different', 'JJ'), ('genre', 'NN'), (',', ','), ('sentiment', 'NN'), ('analysis', 'NN'), ('messages', 'NNS'), (',', ','), ('identifying', 'VBG'), ('custom', 'NN'), ('document', 'NN'), ('types', 'NNS'), ('organization', 'NN'), (',', ','), ('etc', 'FW'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['example ,', ', categorizer', 'categorizer models', 'models used', 'used categorize', 'categorize news', 'news articles', 'articles different', 'different genre', 'genre ,', ', sentiment', 'sentiment analysis', 'analysis messages', 'messages ,', ', identifying', 'identifying custom', 'custom document', 'document types', 'types organization', 'organization ,', ', etc', 'etc .'] 

 TOTAL BIGRAMS --> 22 



 ---- TRI-GRAMS ---- 

 ['example , categorizer', ', categorizer models', 'categorizer models used', 'models used categorize', 'used categorize news', 'categorize news articles', 'news articles different', 'articles different genre', 'different genre ,', 'genre , sentiment', ', sentiment analysis', 'sentiment analysis messages', 'analysis messages ,', 'messages , identifying', ', identifying custom', 'identifying custom document', 'custom document types', 'document types organization', 'types organization ,', 'organization , etc', ', etc .'] 

 TOTAL TRIGRAMS --> 21 



 ---- NOUN PHRASES ---- 

 ['example', 'categorizer', 'categorize news', 'different genre', 'sentiment', 'analysis', 'custom', 'document', 'organization'] 

 TOTAL NOUN PHRASES --> 9 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['exampl', ',', 'categor', 'model', 'use', 'categor', 'news', 'articl', 'differ', 'genr', ',', 'sentiment', 'analysi', 'messag', ',', 'identifi', 'custom', 'document', 'type', 'organ', ',', 'etc', '.']

 TOTAL PORTER STEM WORDS ==> 23



 ---- SNOWBALL STEMMING ----

['exampl', ',', 'categor', 'model', 'use', 'categor', 'news', 'articl', 'differ', 'genr', ',', 'sentiment', 'analysi', 'messag', ',', 'identifi', 'custom', 'document', 'type', 'organ', ',', 'etc', '.']

 TOTAL SNOWBALL STEM WORDS ==> 23



 ---- LEMMATIZATION ----

['example', ',', 'categorizer', 'model', 'used', 'categorize', 'news', 'article', 'different', 'genre', ',', 'sentiment', 'analysis', 'message', ',', 'identifying', 'custom', 'document', 'type', 'organization', ',', 'etc', '.']

 TOTAL LEMMATIZE WORDS ==> 23

************************************************************************************************************************

18 --> using the predefined  category phrases, categorizer can best  classify the content. 


 ---- TOKENS ----

 ['using', 'the', 'predefined', 'category', 'phrases', ',', 'categorizer', 'can', 'best', 'classify', 'the', 'content', '.'] 

 TOTAL TOKENS ==> 13

 ---- POST ----

 [('using', 'VBG'), ('the', 'DT'), ('predefined', 'JJ'), ('category', 'NN'), ('phrases', 'NNS'), (',', ','), ('categorizer', 'NN'), ('can', 'MD'), ('best', 'VB'), ('classify', 'VB'), ('the', 'DT'), ('content', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['using', 'predefined', 'category', 'phrases', ',', 'categorizer', 'best', 'classify', 'content', '.']

 TOTAL FILTERED TOKENS ==>  10

 ---- POST FOR FILTERED TOKENS ----

 [('using', 'VBG'), ('predefined', 'JJ'), ('category', 'NN'), ('phrases', 'NNS'), (',', ','), ('categorizer', 'NN'), ('best', 'JJS'), ('classify', 'NN'), ('content', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['using predefined', 'predefined category', 'category phrases', 'phrases ,', ', categorizer', 'categorizer best', 'best classify', 'classify content', 'content .'] 

 TOTAL BIGRAMS --> 9 



 ---- TRI-GRAMS ---- 

 ['using predefined category', 'predefined category phrases', 'category phrases ,', 'phrases , categorizer', ', categorizer best', 'categorizer best classify', 'best classify content', 'classify content .'] 

 TOTAL TRIGRAMS --> 8 



 ---- NOUN PHRASES ---- 

 ['predefined category', 'categorizer', 'classify', 'content'] 

 TOTAL NOUN PHRASES --> 4 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['use', 'predefin', 'categori', 'phrase', ',', 'categor', 'best', 'classifi', 'content', '.']

 TOTAL PORTER STEM WORDS ==> 10



 ---- SNOWBALL STEMMING ----

['use', 'predefin', 'categori', 'phrase', ',', 'categor', 'best', 'classifi', 'content', '.']

 TOTAL SNOWBALL STEM WORDS ==> 10



 ---- LEMMATIZATION ----

['using', 'predefined', 'category', 'phrase', ',', 'categorizer', 'best', 'classify', 'content', '.']

 TOTAL LEMMATIZE WORDS ==> 10

************************************************************************************************************************

19 --> 1.2 Terminology  1.2.1 Entity Entity is a singular, identifiable and  separate object. 


 ---- TOKENS ----

 ['1.2', 'Terminology', '1.2.1', 'Entity', 'Entity', 'is', 'a', 'singular', ',', 'identifiable', 'and', 'separate', 'object', '.'] 

 TOTAL TOKENS ==> 14

 ---- POST ----

 [('1.2', 'CD'), ('Terminology', 'NNP'), ('1.2.1', 'CD'), ('Entity', 'NNP'), ('Entity', 'NNP'), ('is', 'VBZ'), ('a', 'DT'), ('singular', 'JJ'), (',', ','), ('identifiable', 'JJ'), ('and', 'CC'), ('separate', 'JJ'), ('object', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['1.2', 'Terminology', '1.2.1', 'Entity', 'Entity', 'singular', ',', 'identifiable', 'separate', 'object', '.']

 TOTAL FILTERED TOKENS ==>  11

 ---- POST FOR FILTERED TOKENS ----

 [('1.2', 'CD'), ('Terminology', 'NNP'), ('1.2.1', 'CD'), ('Entity', 'NNP'), ('Entity', 'NNP'), ('singular', 'JJ'), (',', ','), ('identifiable', 'JJ'), ('separate', 'JJ'), ('object', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['1.2 Terminology', 'Terminology 1.2.1', '1.2.1 Entity', 'Entity Entity', 'Entity singular', 'singular ,', ', identifiable', 'identifiable separate', 'separate object', 'object .'] 

 TOTAL BIGRAMS --> 10 



 ---- TRI-GRAMS ---- 

 ['1.2 Terminology 1.2.1', 'Terminology 1.2.1 Entity', '1.2.1 Entity Entity', 'Entity Entity singular', 'Entity singular ,', 'singular , identifiable', ', identifiable separate', 'identifiable separate object', 'separate object .'] 

 TOTAL TRIGRAMS --> 9 



 ---- NOUN PHRASES ---- 

 ['identifiable separate object'] 

 TOTAL NOUN PHRASES --> 1 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['1.2', 'terminolog', '1.2.1', 'entiti', 'entiti', 'singular', ',', 'identifi', 'separ', 'object', '.']

 TOTAL PORTER STEM WORDS ==> 11



 ---- SNOWBALL STEMMING ----

['1.2', 'terminolog', '1.2.1', 'entiti', 'entiti', 'singular', ',', 'identifi', 'separ', 'object', '.']

 TOTAL SNOWBALL STEM WORDS ==> 11



 ---- LEMMATIZATION ----

['1.2', 'Terminology', '1.2.1', 'Entity', 'Entity', 'singular', ',', 'identifiable', 'separate', 'object', '.']

 TOTAL LEMMATIZE WORDS ==> 11

************************************************************************************************************************

20 --> For example; name of a  person, organization, city, account number,  TFN number can be considered as an  entity. 


 ---- TOKENS ----

 ['For', 'example', ';', 'name', 'of', 'a', 'person', ',', 'organization', ',', 'city', ',', 'account', 'number', ',', 'TFN', 'number', 'can', 'be', 'considered', 'as', 'an', 'entity', '.'] 

 TOTAL TOKENS ==> 24

 ---- POST ----

 [('For', 'IN'), ('example', 'NN'), (';', ':'), ('name', 'NN'), ('of', 'IN'), ('a', 'DT'), ('person', 'NN'), (',', ','), ('organization', 'NN'), (',', ','), ('city', 'NN'), (',', ','), ('account', 'VBP'), ('number', 'NN'), (',', ','), ('TFN', 'NNP'), ('number', 'NN'), ('can', 'MD'), ('be', 'VB'), ('considered', 'VBN'), ('as', 'IN'), ('an', 'DT'), ('entity', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['example', ';', 'name', 'person', ',', 'organization', ',', 'city', ',', 'account', 'number', ',', 'TFN', 'number', 'considered', 'entity', '.']

 TOTAL FILTERED TOKENS ==>  17

 ---- POST FOR FILTERED TOKENS ----

 [('example', 'NN'), (';', ':'), ('name', 'CC'), ('person', 'NN'), (',', ','), ('organization', 'NN'), (',', ','), ('city', 'NN'), (',', ','), ('account', 'VBP'), ('number', 'NN'), (',', ','), ('TFN', 'NNP'), ('number', 'NN'), ('considered', 'VBN'), ('entity', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['example ;', '; name', 'name person', 'person ,', ', organization', 'organization ,', ', city', 'city ,', ', account', 'account number', 'number ,', ', TFN', 'TFN number', 'number considered', 'considered entity', 'entity .'] 

 TOTAL BIGRAMS --> 16 



 ---- TRI-GRAMS ---- 

 ['example ; name', '; name person', 'name person ,', 'person , organization', ', organization ,', 'organization , city', ', city ,', 'city , account', ', account number', 'account number ,', 'number , TFN', ', TFN number', 'TFN number considered', 'number considered entity', 'considered entity .'] 

 TOTAL TRIGRAMS --> 15 



 ---- NOUN PHRASES ---- 

 ['example', 'person', 'organization', 'city', 'number', 'number', 'entity'] 

 TOTAL NOUN PHRASES --> 7 



 ---- NER ----

 
 ORGANIZATION ---> ['TFN']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['exampl', ';', 'name', 'person', ',', 'organ', ',', 'citi', ',', 'account', 'number', ',', 'tfn', 'number', 'consid', 'entiti', '.']

 TOTAL PORTER STEM WORDS ==> 17



 ---- SNOWBALL STEMMING ----

['exampl', ';', 'name', 'person', ',', 'organ', ',', 'citi', ',', 'account', 'number', ',', 'tfn', 'number', 'consid', 'entiti', '.']

 TOTAL SNOWBALL STEM WORDS ==> 17



 ---- LEMMATIZATION ----

['example', ';', 'name', 'person', ',', 'organization', ',', 'city', ',', 'account', 'number', ',', 'TFN', 'number', 'considered', 'entity', '.']

 TOTAL LEMMATIZE WORDS ==> 17

************************************************************************************************************************

21 --> 1.2.2 Entity Relationship  The semantic relationship between two  different entities are called as entity  relationship. 


 ---- TOKENS ----

 ['1.2.2', 'Entity', 'Relationship', 'The', 'semantic', 'relationship', 'between', 'two', 'different', 'entities', 'are', 'called', 'as', 'entity', 'relationship', '.'] 

 TOTAL TOKENS ==> 16

 ---- POST ----

 [('1.2.2', 'CD'), ('Entity', 'NNP'), ('Relationship', 'NNP'), ('The', 'DT'), ('semantic', 'JJ'), ('relationship', 'NN'), ('between', 'IN'), ('two', 'CD'), ('different', 'JJ'), ('entities', 'NNS'), ('are', 'VBP'), ('called', 'VBN'), ('as', 'IN'), ('entity', 'NN'), ('relationship', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['1.2.2', 'Entity', 'Relationship', 'semantic', 'relationship', 'two', 'different', 'entities', 'called', 'entity', 'relationship', '.']

 TOTAL FILTERED TOKENS ==>  12

 ---- POST FOR FILTERED TOKENS ----

 [('1.2.2', 'CD'), ('Entity', 'NNP'), ('Relationship', 'NNP'), ('semantic', 'JJ'), ('relationship', 'NN'), ('two', 'CD'), ('different', 'JJ'), ('entities', 'NNS'), ('called', 'VBN'), ('entity', 'NN'), ('relationship', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['1.2.2 Entity', 'Entity Relationship', 'Relationship semantic', 'semantic relationship', 'relationship two', 'two different', 'different entities', 'entities called', 'called entity', 'entity relationship', 'relationship .'] 

 TOTAL BIGRAMS --> 11 



 ---- TRI-GRAMS ---- 

 ['1.2.2 Entity Relationship', 'Entity Relationship semantic', 'Relationship semantic relationship', 'semantic relationship two', 'relationship two different', 'two different entities', 'different entities called', 'entities called entity', 'called entity relationship', 'entity relationship .'] 

 TOTAL TRIGRAMS --> 10 



 ---- NOUN PHRASES ---- 

 ['semantic relationship', 'entity', 'relationship'] 

 TOTAL NOUN PHRASES --> 3 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['1.2.2', 'entiti', 'relationship', 'semant', 'relationship', 'two', 'differ', 'entiti', 'call', 'entiti', 'relationship', '.']

 TOTAL PORTER STEM WORDS ==> 12



 ---- SNOWBALL STEMMING ----

['1.2.2', 'entiti', 'relationship', 'semant', 'relationship', 'two', 'differ', 'entiti', 'call', 'entiti', 'relationship', '.']

 TOTAL SNOWBALL STEM WORDS ==> 12



 ---- LEMMATIZATION ----

['1.2.2', 'Entity', 'Relationship', 'semantic', 'relationship', 'two', 'different', 'entity', 'called', 'entity', 'relationship', '.']

 TOTAL LEMMATIZE WORDS ==> 12

************************************************************************************************************************

22 --> For example; relationship  between a person name and an  organization is “employed by”. 


 ---- TOKENS ----

 ['For', 'example', ';', 'relationship', 'between', 'a', 'person', 'name', 'and', 'an', 'organization', 'is', '“', 'employed', 'by', '”', '.'] 

 TOTAL TOKENS ==> 17

 ---- POST ----

 [('For', 'IN'), ('example', 'NN'), (';', ':'), ('relationship', 'NN'), ('between', 'IN'), ('a', 'DT'), ('person', 'NN'), ('name', 'NN'), ('and', 'CC'), ('an', 'DT'), ('organization', 'NN'), ('is', 'VBZ'), ('“', 'JJ'), ('employed', 'VBN'), ('by', 'IN'), ('”', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['example', ';', 'relationship', 'person', 'name', 'organization', '“', 'employed', '”', '.']

 TOTAL FILTERED TOKENS ==>  10

 ---- POST FOR FILTERED TOKENS ----

 [('example', 'NN'), (';', ':'), ('relationship', 'NN'), ('person', 'NN'), ('name', 'NN'), ('organization', 'NN'), ('“', 'NNP'), ('employed', 'VBD'), ('”', 'NNP'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['example ;', '; relationship', 'relationship person', 'person name', 'name organization', 'organization “', '“ employed', 'employed ”', '” .'] 

 TOTAL BIGRAMS --> 9 



 ---- TRI-GRAMS ---- 

 ['example ; relationship', '; relationship person', 'relationship person name', 'person name organization', 'name organization “', 'organization “ employed', '“ employed ”', 'employed ” .'] 

 TOTAL TRIGRAMS --> 8 



 ---- NOUN PHRASES ---- 

 ['example', 'relationship', 'person', 'name', 'organization'] 

 TOTAL NOUN PHRASES --> 5 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['exampl', ';', 'relationship', 'person', 'name', 'organ', '“', 'employ', '”', '.']

 TOTAL PORTER STEM WORDS ==> 10



 ---- SNOWBALL STEMMING ----

['exampl', ';', 'relationship', 'person', 'name', 'organ', '“', 'employ', '”', '.']

 TOTAL SNOWBALL STEM WORDS ==> 10



 ---- LEMMATIZATION ----

['example', ';', 'relationship', 'person', 'name', 'organization', '“', 'employed', '”', '.']

 TOTAL LEMMATIZE WORDS ==> 10

************************************************************************************************************************

23 --> 1.2.3 Annotation Annotation is a process, in which, the  information of interest is tagged with an  entity name. 


 ---- TOKENS ----

 ['1.2.3', 'Annotation', 'Annotation', 'is', 'a', 'process', ',', 'in', 'which', ',', 'the', 'information', 'of', 'interest', 'is', 'tagged', 'with', 'an', 'entity', 'name', '.'] 

 TOTAL TOKENS ==> 21

 ---- POST ----

 [('1.2.3', 'CD'), ('Annotation', 'NNP'), ('Annotation', 'NNP'), ('is', 'VBZ'), ('a', 'DT'), ('process', 'NN'), (',', ','), ('in', 'IN'), ('which', 'WDT'), (',', ','), ('the', 'DT'), ('information', 'NN'), ('of', 'IN'), ('interest', 'NN'), ('is', 'VBZ'), ('tagged', 'VBN'), ('with', 'IN'), ('an', 'DT'), ('entity', 'NN'), ('name', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['1.2.3', 'Annotation', 'Annotation', 'process', ',', ',', 'information', 'interest', 'tagged', 'entity', 'name', '.']

 TOTAL FILTERED TOKENS ==>  12

 ---- POST FOR FILTERED TOKENS ----

 [('1.2.3', 'CD'), ('Annotation', 'NNP'), ('Annotation', 'NNP'), ('process', 'NN'), (',', ','), (',', ','), ('information', 'NN'), ('interest', 'NN'), ('tagged', 'VBN'), ('entity', 'NN'), ('name', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['1.2.3 Annotation', 'Annotation Annotation', 'Annotation process', 'process ,', ', ,', ', information', 'information interest', 'interest tagged', 'tagged entity', 'entity name', 'name .'] 

 TOTAL BIGRAMS --> 11 



 ---- TRI-GRAMS ---- 

 ['1.2.3 Annotation Annotation', 'Annotation Annotation process', 'Annotation process ,', 'process , ,', ', , information', ', information interest', 'information interest tagged', 'interest tagged entity', 'tagged entity name', 'entity name .'] 

 TOTAL TRIGRAMS --> 10 



 ---- NOUN PHRASES ---- 

 ['process', 'information', 'interest', 'entity', 'name'] 

 TOTAL NOUN PHRASES --> 5 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['1.2.3', 'annot', 'annot', 'process', ',', ',', 'inform', 'interest', 'tag', 'entiti', 'name', '.']

 TOTAL PORTER STEM WORDS ==> 12



 ---- SNOWBALL STEMMING ----

['1.2.3', 'annot', 'annot', 'process', ',', ',', 'inform', 'interest', 'tag', 'entiti', 'name', '.']

 TOTAL SNOWBALL STEM WORDS ==> 12



 ---- LEMMATIZATION ----

['1.2.3', 'Annotation', 'Annotation', 'process', ',', ',', 'information', 'interest', 'tagged', 'entity', 'name', '.']

 TOTAL LEMMATIZE WORDS ==> 12

************************************************************************************************************************

24 --> For example, John is tagged  with the entity “person_name”. 


 ---- TOKENS ----

 ['For', 'example', ',', 'John', 'is', 'tagged', 'with', 'the', 'entity', '“', 'person_name', '”', '.'] 

 TOTAL TOKENS ==> 13

 ---- POST ----

 [('For', 'IN'), ('example', 'NN'), (',', ','), ('John', 'NNP'), ('is', 'VBZ'), ('tagged', 'VBN'), ('with', 'IN'), ('the', 'DT'), ('entity', 'NN'), ('“', 'NNP'), ('person_name', 'NN'), ('”', 'NNP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['example', ',', 'John', 'tagged', 'entity', '“', 'person_name', '”', '.']

 TOTAL FILTERED TOKENS ==>  9

 ---- POST FOR FILTERED TOKENS ----

 [('example', 'NN'), (',', ','), ('John', 'NNP'), ('tagged', 'VBD'), ('entity', 'NN'), ('“', 'NNP'), ('person_name', 'NN'), ('”', 'NNP'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['example ,', ', John', 'John tagged', 'tagged entity', 'entity “', '“ person_name', 'person_name ”', '” .'] 

 TOTAL BIGRAMS --> 8 



 ---- TRI-GRAMS ---- 

 ['example , John', ', John tagged', 'John tagged entity', 'tagged entity “', 'entity “ person_name', '“ person_name ”', 'person_name ” .'] 

 TOTAL TRIGRAMS --> 7 



 ---- NOUN PHRASES ---- 

 ['example', 'entity', 'person_name'] 

 TOTAL NOUN PHRASES --> 3 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> ['John']
 TOTAL PERSON ENTITY --> 1 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['exampl', ',', 'john', 'tag', 'entiti', '“', 'person_nam', '”', '.']

 TOTAL PORTER STEM WORDS ==> 9



 ---- SNOWBALL STEMMING ----

['exampl', ',', 'john', 'tag', 'entiti', '“', 'person_nam', '”', '.']

 TOTAL SNOWBALL STEM WORDS ==> 9



 ---- LEMMATIZATION ----

['example', ',', 'John', 'tagged', 'entity', '“', 'person_name', '”', '.']

 TOTAL LEMMATIZE WORDS ==> 9

************************************************************************************************************************

25 --> 1.2.4 Labeled Content Labeled content is the content, whose  classification category and/or entities  are already known and which can serve  as “training content” for training the AI  models. 


 ---- TOKENS ----

 ['1.2.4', 'Labeled', 'Content', 'Labeled', 'content', 'is', 'the', 'content', ',', 'whose', 'classification', 'category', 'and/or', 'entities', 'are', 'already', 'known', 'and', 'which', 'can', 'serve', 'as', '“', 'training', 'content', '”', 'for', 'training', 'the', 'AI', 'models', '.'] 

 TOTAL TOKENS ==> 32

 ---- POST ----

 [('1.2.4', 'CD'), ('Labeled', 'VBN'), ('Content', 'NNP'), ('Labeled', 'NNP'), ('content', 'NN'), ('is', 'VBZ'), ('the', 'DT'), ('content', 'NN'), (',', ','), ('whose', 'WP$'), ('classification', 'NN'), ('category', 'NN'), ('and/or', 'NN'), ('entities', 'NNS'), ('are', 'VBP'), ('already', 'RB'), ('known', 'VBN'), ('and', 'CC'), ('which', 'WDT'), ('can', 'MD'), ('serve', 'VB'), ('as', 'IN'), ('“', 'JJ'), ('training', 'NN'), ('content', 'NN'), ('”', 'NN'), ('for', 'IN'), ('training', 'VBG'), ('the', 'DT'), ('AI', 'NNP'), ('models', 'NNS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['1.2.4', 'Labeled', 'Content', 'Labeled', 'content', 'content', ',', 'whose', 'classification', 'category', 'and/or', 'entities', 'already', 'known', 'serve', '“', 'training', 'content', '”', 'training', 'AI', 'models', '.']

 TOTAL FILTERED TOKENS ==>  23

 ---- POST FOR FILTERED TOKENS ----

 [('1.2.4', 'CD'), ('Labeled', 'VBN'), ('Content', 'NNP'), ('Labeled', 'NNP'), ('content', 'NN'), ('content', 'NN'), (',', ','), ('whose', 'WP$'), ('classification', 'NN'), ('category', 'NN'), ('and/or', 'JJ'), ('entities', 'NNS'), ('already', 'RB'), ('known', 'VBN'), ('serve', 'VBP'), ('“', 'JJ'), ('training', 'NN'), ('content', 'NN'), ('”', 'NNP'), ('training', 'NN'), ('AI', 'NNP'), ('models', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['1.2.4 Labeled', 'Labeled Content', 'Content Labeled', 'Labeled content', 'content content', 'content ,', ', whose', 'whose classification', 'classification category', 'category and/or', 'and/or entities', 'entities already', 'already known', 'known serve', 'serve “', '“ training', 'training content', 'content ”', '” training', 'training AI', 'AI models', 'models .'] 

 TOTAL BIGRAMS --> 22 



 ---- TRI-GRAMS ---- 

 ['1.2.4 Labeled Content', 'Labeled Content Labeled', 'Content Labeled content', 'Labeled content content', 'content content ,', 'content , whose', ', whose classification', 'whose classification category', 'classification category and/or', 'category and/or entities', 'and/or entities already', 'entities already known', 'already known serve', 'known serve “', 'serve “ training', '“ training content', 'training content ”', 'content ” training', '” training AI', 'training AI models', 'AI models .'] 

 TOTAL TRIGRAMS --> 21 



 ---- NOUN PHRASES ---- 

 ['content', 'content', 'classification', 'category', '“ training', 'content', 'training'] 

 TOTAL NOUN PHRASES --> 7 



 ---- NER ----

 
 ORGANIZATION ---> ['Content Labeled']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['1.2.4', 'label', 'content', 'label', 'content', 'content', ',', 'whose', 'classif', 'categori', 'and/or', 'entiti', 'alreadi', 'known', 'serv', '“', 'train', 'content', '”', 'train', 'ai', 'model', '.']

 TOTAL PORTER STEM WORDS ==> 23



 ---- SNOWBALL STEMMING ----

['1.2.4', 'label', 'content', 'label', 'content', 'content', ',', 'whose', 'classif', 'categori', 'and/or', 'entiti', 'alreadi', 'known', 'serv', '“', 'train', 'content', '”', 'train', 'ai', 'model', '.']

 TOTAL SNOWBALL STEM WORDS ==> 23



 ---- LEMMATIZATION ----

['1.2.4', 'Labeled', 'Content', 'Labeled', 'content', 'content', ',', 'whose', 'classification', 'category', 'and/or', 'entity', 'already', 'known', 'serve', '“', 'training', 'content', '”', 'training', 'AI', 'model', '.']

 TOTAL LEMMATIZE WORDS ==> 23

************************************************************************************************************************

26 --> 1 Natural Language Processing (NLP) External Document © 2019 Infosys Limited External Document © 2019 Infosys Limited  2.1 Content Capture Capturing content in digital format is  one of the first steps in Content Services. 


 ---- TOKENS ----

 ['1', 'Natural', 'Language', 'Processing', '(', 'NLP', ')', 'External', 'Document', '©', '2019', 'Infosys', 'Limited', 'External', 'Document', '©', '2019', 'Infosys', 'Limited', '2.1', 'Content', 'Capture', 'Capturing', 'content', 'in', 'digital', 'format', 'is', 'one', 'of', 'the', 'first', 'steps', 'in', 'Content', 'Services', '.'] 

 TOTAL TOKENS ==> 37

 ---- POST ----

 [('1', 'CD'), ('Natural', 'JJ'), ('Language', 'NNP'), ('Processing', 'NNP'), ('(', '('), ('NLP', 'NNP'), (')', ')'), ('External', 'NNP'), ('Document', 'NNP'), ('©', 'NN'), ('2019', 'CD'), ('Infosys', 'NNP'), ('Limited', 'NNP'), ('External', 'NNP'), ('Document', 'NNP'), ('©', 'NN'), ('2019', 'CD'), ('Infosys', 'NNP'), ('Limited', 'VBD'), ('2.1', 'CD'), ('Content', 'NNP'), ('Capture', 'NNP'), ('Capturing', 'NNP'), ('content', 'NN'), ('in', 'IN'), ('digital', 'JJ'), ('format', 'NN'), ('is', 'VBZ'), ('one', 'CD'), ('of', 'IN'), ('the', 'DT'), ('first', 'JJ'), ('steps', 'NNS'), ('in', 'IN'), ('Content', 'NNP'), ('Services', 'NNPS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['1', 'Natural', 'Language', 'Processing', '(', 'NLP', ')', 'External', 'Document', '©', '2019', 'Infosys', 'Limited', 'External', 'Document', '©', '2019', 'Infosys', 'Limited', '2.1', 'Content', 'Capture', 'Capturing', 'content', 'digital', 'format', 'one', 'first', 'steps', 'Content', 'Services', '.']

 TOTAL FILTERED TOKENS ==>  32

 ---- POST FOR FILTERED TOKENS ----

 [('1', 'CD'), ('Natural', 'JJ'), ('Language', 'NNP'), ('Processing', 'NNP'), ('(', '('), ('NLP', 'NNP'), (')', ')'), ('External', 'NNP'), ('Document', 'NNP'), ('©', 'NN'), ('2019', 'CD'), ('Infosys', 'NNP'), ('Limited', 'NNP'), ('External', 'NNP'), ('Document', 'NNP'), ('©', 'NN'), ('2019', 'CD'), ('Infosys', 'NNP'), ('Limited', 'VBD'), ('2.1', 'CD'), ('Content', 'NNP'), ('Capture', 'NNP'), ('Capturing', 'NNP'), ('content', 'JJ'), ('digital', 'JJ'), ('format', 'NN'), ('one', 'CD'), ('first', 'JJ'), ('steps', 'VBZ'), ('Content', 'NNP'), ('Services', 'NNPS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['1 Natural', 'Natural Language', 'Language Processing', 'Processing (', '( NLP', 'NLP )', ') External', 'External Document', 'Document ©', '© 2019', '2019 Infosys', 'Infosys Limited', 'Limited External', 'External Document', 'Document ©', '© 2019', '2019 Infosys', 'Infosys Limited', 'Limited 2.1', '2.1 Content', 'Content Capture', 'Capture Capturing', 'Capturing content', 'content digital', 'digital format', 'format one', 'one first', 'first steps', 'steps Content', 'Content Services', 'Services .'] 

 TOTAL BIGRAMS --> 31 



 ---- TRI-GRAMS ---- 

 ['1 Natural Language', 'Natural Language Processing', 'Language Processing (', 'Processing ( NLP', '( NLP )', 'NLP ) External', ') External Document', 'External Document ©', 'Document © 2019', '© 2019 Infosys', '2019 Infosys Limited', 'Infosys Limited External', 'Limited External Document', 'External Document ©', 'Document © 2019', '© 2019 Infosys', '2019 Infosys Limited', 'Infosys Limited 2.1', 'Limited 2.1 Content', '2.1 Content Capture', 'Content Capture Capturing', 'Capture Capturing content', 'Capturing content digital', 'content digital format', 'digital format one', 'format one first', 'one first steps', 'first steps Content', 'steps Content Services', 'Content Services .'] 

 TOTAL TRIGRAMS --> 30 



 ---- NOUN PHRASES ---- 

 ['©', '©', 'content digital format'] 

 TOTAL NOUN PHRASES --> 3 



 ---- NER ----

 
 ORGANIZATION ---> ['Content Services']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> ['Infosys']
 TOTAL PERSON ENTITY --> 1 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['1', 'natur', 'languag', 'process', '(', 'nlp', ')', 'extern', 'document', '©', '2019', 'infosi', 'limit', 'extern', 'document', '©', '2019', 'infosi', 'limit', '2.1', 'content', 'captur', 'captur', 'content', 'digit', 'format', 'one', 'first', 'step', 'content', 'servic', '.']

 TOTAL PORTER STEM WORDS ==> 32



 ---- SNOWBALL STEMMING ----

['1', 'natur', 'languag', 'process', '(', 'nlp', ')', 'extern', 'document', '©', '2019', 'infosi', 'limit', 'extern', 'document', '©', '2019', 'infosi', 'limit', '2.1', 'content', 'captur', 'captur', 'content', 'digit', 'format', 'one', 'first', 'step', 'content', 'servic', '.']

 TOTAL SNOWBALL STEM WORDS ==> 32



 ---- LEMMATIZATION ----

['1', 'Natural', 'Language', 'Processing', '(', 'NLP', ')', 'External', 'Document', '©', '2019', 'Infosys', 'Limited', 'External', 'Document', '©', '2019', 'Infosys', 'Limited', '2.1', 'Content', 'Capture', 'Capturing', 'content', 'digital', 'format', 'one', 'first', 'step', 'Content', 'Services', '.']

 TOTAL LEMMATIZE WORDS ==> 32

************************************************************************************************************************

27 --> Along with the content, information such  as customer name, zip code, type of the  document is also captured. 


 ---- TOKENS ----

 ['Along', 'with', 'the', 'content', ',', 'information', 'such', 'as', 'customer', 'name', ',', 'zip', 'code', ',', 'type', 'of', 'the', 'document', 'is', 'also', 'captured', '.'] 

 TOTAL TOKENS ==> 22

 ---- POST ----

 [('Along', 'IN'), ('with', 'IN'), ('the', 'DT'), ('content', 'NN'), (',', ','), ('information', 'NN'), ('such', 'JJ'), ('as', 'IN'), ('customer', 'NN'), ('name', 'NN'), (',', ','), ('zip', 'NN'), ('code', 'NN'), (',', ','), ('type', 'NN'), ('of', 'IN'), ('the', 'DT'), ('document', 'NN'), ('is', 'VBZ'), ('also', 'RB'), ('captured', 'VBN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Along', 'content', ',', 'information', 'customer', 'name', ',', 'zip', 'code', ',', 'type', 'document', 'also', 'captured', '.']

 TOTAL FILTERED TOKENS ==>  15

 ---- POST FOR FILTERED TOKENS ----

 [('Along', 'IN'), ('content', 'NN'), (',', ','), ('information', 'NN'), ('customer', 'NN'), ('name', 'NN'), (',', ','), ('zip', 'NN'), ('code', 'NN'), (',', ','), ('type', 'NN'), ('document', 'NN'), ('also', 'RB'), ('captured', 'VBD'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Along content', 'content ,', ', information', 'information customer', 'customer name', 'name ,', ', zip', 'zip code', 'code ,', ', type', 'type document', 'document also', 'also captured', 'captured .'] 

 TOTAL BIGRAMS --> 14 



 ---- TRI-GRAMS ---- 

 ['Along content ,', 'content , information', ', information customer', 'information customer name', 'customer name ,', 'name , zip', ', zip code', 'zip code ,', 'code , type', ', type document', 'type document also', 'document also captured', 'also captured .'] 

 TOTAL TRIGRAMS --> 13 



 ---- NOUN PHRASES ---- 

 ['content', 'information', 'customer', 'name', 'zip', 'code', 'type', 'document'] 

 TOTAL NOUN PHRASES --> 8 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['along', 'content', ',', 'inform', 'custom', 'name', ',', 'zip', 'code', ',', 'type', 'document', 'also', 'captur', '.']

 TOTAL PORTER STEM WORDS ==> 15



 ---- SNOWBALL STEMMING ----

['along', 'content', ',', 'inform', 'custom', 'name', ',', 'zip', 'code', ',', 'type', 'document', 'also', 'captur', '.']

 TOTAL SNOWBALL STEM WORDS ==> 15



 ---- LEMMATIZATION ----

['Along', 'content', ',', 'information', 'customer', 'name', ',', 'zip', 'code', ',', 'type', 'document', 'also', 'captured', '.']

 TOTAL LEMMATIZE WORDS ==> 15

************************************************************************************************************************

28 --> Current capture  process is based on the location of the  information. 


 ---- TOKENS ----

 ['Current', 'capture', 'process', 'is', 'based', 'on', 'the', 'location', 'of', 'the', 'information', '.'] 

 TOTAL TOKENS ==> 12

 ---- POST ----

 [('Current', 'JJ'), ('capture', 'NN'), ('process', 'NN'), ('is', 'VBZ'), ('based', 'VBN'), ('on', 'IN'), ('the', 'DT'), ('location', 'NN'), ('of', 'IN'), ('the', 'DT'), ('information', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Current', 'capture', 'process', 'based', 'location', 'information', '.']

 TOTAL FILTERED TOKENS ==>  7

 ---- POST FOR FILTERED TOKENS ----

 [('Current', 'JJ'), ('capture', 'NN'), ('process', 'NN'), ('based', 'VBN'), ('location', 'NN'), ('information', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Current capture', 'capture process', 'process based', 'based location', 'location information', 'information .'] 

 TOTAL BIGRAMS --> 6 



 ---- TRI-GRAMS ---- 

 ['Current capture process', 'capture process based', 'process based location', 'based location information', 'location information .'] 

 TOTAL TRIGRAMS --> 5 



 ---- NOUN PHRASES ---- 

 ['Current capture', 'process', 'location', 'information'] 

 TOTAL NOUN PHRASES --> 4 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['current', 'captur', 'process', 'base', 'locat', 'inform', '.']

 TOTAL PORTER STEM WORDS ==> 7



 ---- SNOWBALL STEMMING ----

['current', 'captur', 'process', 'base', 'locat', 'inform', '.']

 TOTAL SNOWBALL STEM WORDS ==> 7



 ---- LEMMATIZATION ----

['Current', 'capture', 'process', 'based', 'location', 'information', '.']

 TOTAL LEMMATIZE WORDS ==> 7

************************************************************************************************************************

29 --> In this process, templates are  To implement the information extraction,  as a first step, AI model needs to be trained. 


 ---- TOKENS ----

 ['In', 'this', 'process', ',', 'templates', 'are', 'To', 'implement', 'the', 'information', 'extraction', ',', 'as', 'a', 'first', 'step', ',', 'AI', 'model', 'needs', 'to', 'be', 'trained', '.'] 

 TOTAL TOKENS ==> 24

 ---- POST ----

 [('In', 'IN'), ('this', 'DT'), ('process', 'NN'), (',', ','), ('templates', 'NNS'), ('are', 'VBP'), ('To', 'TO'), ('implement', 'VB'), ('the', 'DT'), ('information', 'NN'), ('extraction', 'NN'), (',', ','), ('as', 'IN'), ('a', 'DT'), ('first', 'JJ'), ('step', 'NN'), (',', ','), ('AI', 'NNP'), ('model', 'NN'), ('needs', 'NNS'), ('to', 'TO'), ('be', 'VB'), ('trained', 'VBN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['process', ',', 'templates', 'implement', 'information', 'extraction', ',', 'first', 'step', ',', 'AI', 'model', 'needs', 'trained', '.']

 TOTAL FILTERED TOKENS ==>  15

 ---- POST FOR FILTERED TOKENS ----

 [('process', 'NN'), (',', ','), ('templates', 'VBZ'), ('implement', 'JJ'), ('information', 'NN'), ('extraction', 'NN'), (',', ','), ('first', 'JJ'), ('step', 'NN'), (',', ','), ('AI', 'NNP'), ('model', 'NN'), ('needs', 'NNS'), ('trained', 'VBN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['process ,', ', templates', 'templates implement', 'implement information', 'information extraction', 'extraction ,', ', first', 'first step', 'step ,', ', AI', 'AI model', 'model needs', 'needs trained', 'trained .'] 

 TOTAL BIGRAMS --> 14 



 ---- TRI-GRAMS ---- 

 ['process , templates', ', templates implement', 'templates implement information', 'implement information extraction', 'information extraction ,', 'extraction , first', ', first step', 'first step ,', 'step , AI', ', AI model', 'AI model needs', 'model needs trained', 'needs trained .'] 

 TOTAL TRIGRAMS --> 13 



 ---- NOUN PHRASES ---- 

 ['process', 'implement information', 'extraction', 'first step', 'model'] 

 TOTAL NOUN PHRASES --> 5 



 ---- NER ----

 
 ORGANIZATION ---> ['AI']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['process', ',', 'templat', 'implement', 'inform', 'extract', ',', 'first', 'step', ',', 'ai', 'model', 'need', 'train', '.']

 TOTAL PORTER STEM WORDS ==> 15



 ---- SNOWBALL STEMMING ----

['process', ',', 'templat', 'implement', 'inform', 'extract', ',', 'first', 'step', ',', 'ai', 'model', 'need', 'train', '.']

 TOTAL SNOWBALL STEM WORDS ==> 15



 ---- LEMMATIZATION ----

['process', ',', 'template', 'implement', 'information', 'extraction', ',', 'first', 'step', ',', 'AI', 'model', 'need', 'trained', '.']

 TOTAL LEMMATIZE WORDS ==> 15

************************************************************************************************************************

30 --> NLP engines accepts annotated text files  as training data set. 


 ---- TOKENS ----

 ['NLP', 'engines', 'accepts', 'annotated', 'text', 'files', 'as', 'training', 'data', 'set', '.'] 

 TOTAL TOKENS ==> 11

 ---- POST ----

 [('NLP', 'NNP'), ('engines', 'NNS'), ('accepts', 'NNS'), ('annotated', 'VBD'), ('text', 'JJ'), ('files', 'NNS'), ('as', 'IN'), ('training', 'NN'), ('data', 'NNS'), ('set', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['NLP', 'engines', 'accepts', 'annotated', 'text', 'files', 'training', 'data', 'set', '.']

 TOTAL FILTERED TOKENS ==>  10

 ---- POST FOR FILTERED TOKENS ----

 [('NLP', 'NNP'), ('engines', 'NNS'), ('accepts', 'NNS'), ('annotated', 'VBD'), ('text', 'JJ'), ('files', 'NNS'), ('training', 'VBG'), ('data', 'NNS'), ('set', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['NLP engines', 'engines accepts', 'accepts annotated', 'annotated text', 'text files', 'files training', 'training data', 'data set', 'set .'] 

 TOTAL BIGRAMS --> 9 



 ---- TRI-GRAMS ---- 

 ['NLP engines accepts', 'engines accepts annotated', 'accepts annotated text', 'annotated text files', 'text files training', 'files training data', 'training data set', 'data set .'] 

 TOTAL TRIGRAMS --> 8 



 ---- NOUN PHRASES ---- 

 ['set'] 

 TOTAL NOUN PHRASES --> 1 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['nlp', 'engin', 'accept', 'annot', 'text', 'file', 'train', 'data', 'set', '.']

 TOTAL PORTER STEM WORDS ==> 10



 ---- SNOWBALL STEMMING ----

['nlp', 'engin', 'accept', 'annot', 'text', 'file', 'train', 'data', 'set', '.']

 TOTAL SNOWBALL STEM WORDS ==> 10



 ---- LEMMATIZATION ----

['NLP', 'engine', 'accepts', 'annotated', 'text', 'file', 'training', 'data', 'set', '.']

 TOTAL LEMMATIZE WORDS ==> 10

************************************************************************************************************************

31 --> Once the information  in text document (or labeled content) is  annotated, it can be used to train NLP  engine. 


 ---- TOKENS ----

 ['Once', 'the', 'information', 'in', 'text', 'document', '(', 'or', 'labeled', 'content', ')', 'is', 'annotated', ',', 'it', 'can', 'be', 'used', 'to', 'train', 'NLP', 'engine', '.'] 

 TOTAL TOKENS ==> 23

 ---- POST ----

 [('Once', 'RB'), ('the', 'DT'), ('information', 'NN'), ('in', 'IN'), ('text', 'NN'), ('document', 'NN'), ('(', '('), ('or', 'CC'), ('labeled', 'VBN'), ('content', 'NN'), (')', ')'), ('is', 'VBZ'), ('annotated', 'VBN'), (',', ','), ('it', 'PRP'), ('can', 'MD'), ('be', 'VB'), ('used', 'VBN'), ('to', 'TO'), ('train', 'VB'), ('NLP', 'NNP'), ('engine', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['information', 'text', 'document', '(', 'labeled', 'content', ')', 'annotated', ',', 'used', 'train', 'NLP', 'engine', '.']

 TOTAL FILTERED TOKENS ==>  14

 ---- POST FOR FILTERED TOKENS ----

 [('information', 'NN'), ('text', 'NN'), ('document', 'NN'), ('(', '('), ('labeled', 'VBN'), ('content', 'NN'), (')', ')'), ('annotated', 'VBD'), (',', ','), ('used', 'VBD'), ('train', 'NN'), ('NLP', 'NNP'), ('engine', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['information text', 'text document', 'document (', '( labeled', 'labeled content', 'content )', ') annotated', 'annotated ,', ', used', 'used train', 'train NLP', 'NLP engine', 'engine .'] 

 TOTAL BIGRAMS --> 13 



 ---- TRI-GRAMS ---- 

 ['information text document', 'text document (', 'document ( labeled', '( labeled content', 'labeled content )', 'content ) annotated', ') annotated ,', 'annotated , used', ', used train', 'used train NLP', 'train NLP engine', 'NLP engine .'] 

 TOTAL TRIGRAMS --> 12 



 ---- NOUN PHRASES ---- 

 ['information', 'text', 'document', 'train', 'engine'] 

 TOTAL NOUN PHRASES --> 5 



 ---- NER ----

 
 ORGANIZATION ---> ['NLP']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['inform', 'text', 'document', '(', 'label', 'content', ')', 'annot', ',', 'use', 'train', 'nlp', 'engin', '.']

 TOTAL PORTER STEM WORDS ==> 14



 ---- SNOWBALL STEMMING ----

['inform', 'text', 'document', '(', 'label', 'content', ')', 'annot', ',', 'use', 'train', 'nlp', 'engin', '.']

 TOTAL SNOWBALL STEM WORDS ==> 14



 ---- LEMMATIZATION ----

['information', 'text', 'document', '(', 'labeled', 'content', ')', 'annotated', ',', 'used', 'train', 'NLP', 'engine', '.']

 TOTAL LEMMATIZE WORDS ==> 14

************************************************************************************************************************

32 --> More the number of annotated  fed to capture tool and the tool is trained  to extract information from a particular  location, for that template. 


 ---- TOKENS ----

 ['More', 'the', 'number', 'of', 'annotated', 'fed', 'to', 'capture', 'tool', 'and', 'the', 'tool', 'is', 'trained', 'to', 'extract', 'information', 'from', 'a', 'particular', 'location', ',', 'for', 'that', 'template', '.'] 

 TOTAL TOKENS ==> 26

 ---- POST ----

 [('More', 'RBR'), ('the', 'DT'), ('number', 'NN'), ('of', 'IN'), ('annotated', 'JJ'), ('fed', 'NN'), ('to', 'TO'), ('capture', 'VB'), ('tool', 'NN'), ('and', 'CC'), ('the', 'DT'), ('tool', 'NN'), ('is', 'VBZ'), ('trained', 'VBN'), ('to', 'TO'), ('extract', 'VB'), ('information', 'NN'), ('from', 'IN'), ('a', 'DT'), ('particular', 'JJ'), ('location', 'NN'), (',', ','), ('for', 'IN'), ('that', 'DT'), ('template', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['number', 'annotated', 'fed', 'capture', 'tool', 'tool', 'trained', 'extract', 'information', 'particular', 'location', ',', 'template', '.']

 TOTAL FILTERED TOKENS ==>  14

 ---- POST FOR FILTERED TOKENS ----

 [('number', 'NN'), ('annotated', 'VBD'), ('fed', 'JJ'), ('capture', 'NN'), ('tool', 'NN'), ('tool', 'NN'), ('trained', 'VBD'), ('extract', 'JJ'), ('information', 'NN'), ('particular', 'JJ'), ('location', 'NN'), (',', ','), ('template', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['number annotated', 'annotated fed', 'fed capture', 'capture tool', 'tool tool', 'tool trained', 'trained extract', 'extract information', 'information particular', 'particular location', 'location ,', ', template', 'template .'] 

 TOTAL BIGRAMS --> 13 



 ---- TRI-GRAMS ---- 

 ['number annotated fed', 'annotated fed capture', 'fed capture tool', 'capture tool tool', 'tool tool trained', 'tool trained extract', 'trained extract information', 'extract information particular', 'information particular location', 'particular location ,', 'location , template', ', template .'] 

 TOTAL TRIGRAMS --> 12 



 ---- NOUN PHRASES ---- 

 ['number', 'fed capture', 'tool', 'tool', 'extract information', 'particular location', 'template'] 

 TOTAL NOUN PHRASES --> 7 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['number', 'annot', 'fed', 'captur', 'tool', 'tool', 'train', 'extract', 'inform', 'particular', 'locat', ',', 'templat', '.']

 TOTAL PORTER STEM WORDS ==> 14



 ---- SNOWBALL STEMMING ----

['number', 'annot', 'fed', 'captur', 'tool', 'tool', 'train', 'extract', 'inform', 'particular', 'locat', ',', 'templat', '.']

 TOTAL SNOWBALL STEM WORDS ==> 14



 ---- LEMMATIZATION ----

['number', 'annotated', 'fed', 'capture', 'tool', 'tool', 'trained', 'extract', 'information', 'particular', 'location', ',', 'template', '.']

 TOTAL LEMMATIZE WORDS ==> 14

************************************************************************************************************************

33 --> This location  based information extraction is no longer  productive due to huge variation in  content layouts. 


 ---- TOKENS ----

 ['This', 'location', 'based', 'information', 'extraction', 'is', 'no', 'longer', 'productive', 'due', 'to', 'huge', 'variation', 'in', 'content', 'layouts', '.'] 

 TOTAL TOKENS ==> 17

 ---- POST ----

 [('This', 'DT'), ('location', 'NN'), ('based', 'VBN'), ('information', 'NN'), ('extraction', 'NN'), ('is', 'VBZ'), ('no', 'RB'), ('longer', 'RBR'), ('productive', 'JJ'), ('due', 'JJ'), ('to', 'TO'), ('huge', 'JJ'), ('variation', 'NN'), ('in', 'IN'), ('content', 'NN'), ('layouts', 'NNS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['location', 'based', 'information', 'extraction', 'longer', 'productive', 'due', 'huge', 'variation', 'content', 'layouts', '.']

 TOTAL FILTERED TOKENS ==>  12

 ---- POST FOR FILTERED TOKENS ----

 [('location', 'NN'), ('based', 'VBN'), ('information', 'NN'), ('extraction', 'NN'), ('longer', 'RBR'), ('productive', 'JJ'), ('due', 'JJ'), ('huge', 'JJ'), ('variation', 'NN'), ('content', 'NN'), ('layouts', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['location based', 'based information', 'information extraction', 'extraction longer', 'longer productive', 'productive due', 'due huge', 'huge variation', 'variation content', 'content layouts', 'layouts .'] 

 TOTAL BIGRAMS --> 11 



 ---- TRI-GRAMS ---- 

 ['location based information', 'based information extraction', 'information extraction longer', 'extraction longer productive', 'longer productive due', 'productive due huge', 'due huge variation', 'huge variation content', 'variation content layouts', 'content layouts .'] 

 TOTAL TRIGRAMS --> 10 



 ---- NOUN PHRASES ---- 

 ['location', 'information', 'extraction', 'productive due huge variation', 'content'] 

 TOTAL NOUN PHRASES --> 5 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['locat', 'base', 'inform', 'extract', 'longer', 'product', 'due', 'huge', 'variat', 'content', 'layout', '.']

 TOTAL PORTER STEM WORDS ==> 12



 ---- SNOWBALL STEMMING ----

['locat', 'base', 'inform', 'extract', 'longer', 'product', 'due', 'huge', 'variat', 'content', 'layout', '.']

 TOTAL SNOWBALL STEM WORDS ==> 12



 ---- LEMMATIZATION ----

['location', 'based', 'information', 'extraction', 'longer', 'productive', 'due', 'huge', 'variation', 'content', 'layout', '.']

 TOTAL LEMMATIZE WORDS ==> 12

************************************************************************************************************************

34 --> This issue can be solved by context based  extraction. 


 ---- TOKENS ----

 ['This', 'issue', 'can', 'be', 'solved', 'by', 'context', 'based', 'extraction', '.'] 

 TOTAL TOKENS ==> 10

 ---- POST ----

 [('This', 'DT'), ('issue', 'NN'), ('can', 'MD'), ('be', 'VB'), ('solved', 'VBN'), ('by', 'IN'), ('context', 'NN'), ('based', 'VBN'), ('extraction', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['issue', 'solved', 'context', 'based', 'extraction', '.']

 TOTAL FILTERED TOKENS ==>  6

 ---- POST FOR FILTERED TOKENS ----

 [('issue', 'NN'), ('solved', 'VBD'), ('context', 'NNS'), ('based', 'VBN'), ('extraction', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['issue solved', 'solved context', 'context based', 'based extraction', 'extraction .'] 

 TOTAL BIGRAMS --> 5 



 ---- TRI-GRAMS ---- 

 ['issue solved context', 'solved context based', 'context based extraction', 'based extraction .'] 

 TOTAL TRIGRAMS --> 4 



 ---- NOUN PHRASES ---- 

 ['issue', 'extraction'] 

 TOTAL NOUN PHRASES --> 2 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['issu', 'solv', 'context', 'base', 'extract', '.']

 TOTAL PORTER STEM WORDS ==> 6



 ---- SNOWBALL STEMMING ----

['issu', 'solv', 'context', 'base', 'extract', '.']

 TOTAL SNOWBALL STEM WORDS ==> 6



 ---- LEMMATIZATION ----

['issue', 'solved', 'context', 'based', 'extraction', '.']

 TOTAL LEMMATIZE WORDS ==> 6

************************************************************************************************************************

35 --> Using NLP, an AI model can  files, accommodating variations in the  interested information, more will be the  accuracy of the AI model. 


 ---- TOKENS ----

 ['Using', 'NLP', ',', 'an', 'AI', 'model', 'can', 'files', ',', 'accommodating', 'variations', 'in', 'the', 'interested', 'information', ',', 'more', 'will', 'be', 'the', 'accuracy', 'of', 'the', 'AI', 'model', '.'] 

 TOTAL TOKENS ==> 26

 ---- POST ----

 [('Using', 'VBG'), ('NLP', 'NNP'), (',', ','), ('an', 'DT'), ('AI', 'NNP'), ('model', 'NN'), ('can', 'MD'), ('files', 'VB'), (',', ','), ('accommodating', 'VBG'), ('variations', 'NNS'), ('in', 'IN'), ('the', 'DT'), ('interested', 'JJ'), ('information', 'NN'), (',', ','), ('more', 'JJR'), ('will', 'MD'), ('be', 'VB'), ('the', 'DT'), ('accuracy', 'NN'), ('of', 'IN'), ('the', 'DT'), ('AI', 'NNP'), ('model', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Using', 'NLP', ',', 'AI', 'model', 'files', ',', 'accommodating', 'variations', 'interested', 'information', ',', 'accuracy', 'AI', 'model', '.']

 TOTAL FILTERED TOKENS ==>  16

 ---- POST FOR FILTERED TOKENS ----

 [('Using', 'VBG'), ('NLP', 'NNP'), (',', ','), ('AI', 'NNP'), ('model', 'NN'), ('files', 'NNS'), (',', ','), ('accommodating', 'VBG'), ('variations', 'NNS'), ('interested', 'JJ'), ('information', 'NN'), (',', ','), ('accuracy', 'NN'), ('AI', 'NNP'), ('model', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Using NLP', 'NLP ,', ', AI', 'AI model', 'model files', 'files ,', ', accommodating', 'accommodating variations', 'variations interested', 'interested information', 'information ,', ', accuracy', 'accuracy AI', 'AI model', 'model .'] 

 TOTAL BIGRAMS --> 15 



 ---- TRI-GRAMS ---- 

 ['Using NLP ,', 'NLP , AI', ', AI model', 'AI model files', 'model files ,', 'files , accommodating', ', accommodating variations', 'accommodating variations interested', 'variations interested information', 'interested information ,', 'information , accuracy', ', accuracy AI', 'accuracy AI model', 'AI model .'] 

 TOTAL TRIGRAMS --> 14 



 ---- NOUN PHRASES ---- 

 ['model', 'interested information', 'accuracy', 'model'] 

 TOTAL NOUN PHRASES --> 4 



 ---- NER ----

 
 ORGANIZATION ---> ['NLP', 'AI']
 TOTAL ORGANIZATION ENTITY --> 2 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['use', 'nlp', ',', 'ai', 'model', 'file', ',', 'accommod', 'variat', 'interest', 'inform', ',', 'accuraci', 'ai', 'model', '.']

 TOTAL PORTER STEM WORDS ==> 16



 ---- SNOWBALL STEMMING ----

['use', 'nlp', ',', 'ai', 'model', 'file', ',', 'accommod', 'variat', 'interest', 'inform', ',', 'accuraci', 'ai', 'model', '.']

 TOTAL SNOWBALL STEM WORDS ==> 16



 ---- LEMMATIZATION ----

['Using', 'NLP', ',', 'AI', 'model', 'file', ',', 'accommodating', 'variation', 'interested', 'information', ',', 'accuracy', 'AI', 'model', '.']

 TOTAL LEMMATIZE WORDS ==> 16

************************************************************************************************************************

36 --> Once the AI model is trained, it can be used  to capture the information from new or  unseen documents. 


 ---- TOKENS ----

 ['Once', 'the', 'AI', 'model', 'is', 'trained', ',', 'it', 'can', 'be', 'used', 'to', 'capture', 'the', 'information', 'from', 'new', 'or', 'unseen', 'documents', '.'] 

 TOTAL TOKENS ==> 21

 ---- POST ----

 [('Once', 'RB'), ('the', 'DT'), ('AI', 'NNP'), ('model', 'NN'), ('is', 'VBZ'), ('trained', 'VBN'), (',', ','), ('it', 'PRP'), ('can', 'MD'), ('be', 'VB'), ('used', 'VBN'), ('to', 'TO'), ('capture', 'VB'), ('the', 'DT'), ('information', 'NN'), ('from', 'IN'), ('new', 'JJ'), ('or', 'CC'), ('unseen', 'JJ'), ('documents', 'NNS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['AI', 'model', 'trained', ',', 'used', 'capture', 'information', 'new', 'unseen', 'documents', '.']

 TOTAL FILTERED TOKENS ==>  11

 ---- POST FOR FILTERED TOKENS ----

 [('AI', 'NNP'), ('model', 'NN'), ('trained', 'VBD'), (',', ','), ('used', 'VBD'), ('capture', 'NN'), ('information', 'NN'), ('new', 'JJ'), ('unseen', 'JJ'), ('documents', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['AI model', 'model trained', 'trained ,', ', used', 'used capture', 'capture information', 'information new', 'new unseen', 'unseen documents', 'documents .'] 

 TOTAL BIGRAMS --> 10 



 ---- TRI-GRAMS ---- 

 ['AI model trained', 'model trained ,', 'trained , used', ', used capture', 'used capture information', 'capture information new', 'information new unseen', 'new unseen documents', 'unseen documents .'] 

 TOTAL TRIGRAMS --> 9 



 ---- NOUN PHRASES ---- 

 ['model', 'capture', 'information'] 

 TOTAL NOUN PHRASES --> 3 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['ai', 'model', 'train', ',', 'use', 'captur', 'inform', 'new', 'unseen', 'document', '.']

 TOTAL PORTER STEM WORDS ==> 11



 ---- SNOWBALL STEMMING ----

['ai', 'model', 'train', ',', 'use', 'captur', 'inform', 'new', 'unseen', 'document', '.']

 TOTAL SNOWBALL STEM WORDS ==> 11



 ---- LEMMATIZATION ----

['AI', 'model', 'trained', ',', 'used', 'capture', 'information', 'new', 'unseen', 'document', '.']

 TOTAL LEMMATIZE WORDS ==> 11

************************************************************************************************************************

37 --> The new or unseen  be trained which can then be used for  extraction. 


 ---- TOKENS ----

 ['The', 'new', 'or', 'unseen', 'be', 'trained', 'which', 'can', 'then', 'be', 'used', 'for', 'extraction', '.'] 

 TOTAL TOKENS ==> 14

 ---- POST ----

 [('The', 'DT'), ('new', 'JJ'), ('or', 'CC'), ('unseen', 'JJ'), ('be', 'VB'), ('trained', 'VBN'), ('which', 'WDT'), ('can', 'MD'), ('then', 'RB'), ('be', 'VB'), ('used', 'VBN'), ('for', 'IN'), ('extraction', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['new', 'unseen', 'trained', 'used', 'extraction', '.']

 TOTAL FILTERED TOKENS ==>  6

 ---- POST FOR FILTERED TOKENS ----

 [('new', 'JJ'), ('unseen', 'JJ'), ('trained', 'VBN'), ('used', 'JJ'), ('extraction', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['new unseen', 'unseen trained', 'trained used', 'used extraction', 'extraction .'] 

 TOTAL BIGRAMS --> 5 



 ---- TRI-GRAMS ---- 

 ['new unseen trained', 'unseen trained used', 'trained used extraction', 'used extraction .'] 

 TOTAL TRIGRAMS --> 4 



 ---- NOUN PHRASES ---- 

 ['used extraction'] 

 TOTAL NOUN PHRASES --> 1 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['new', 'unseen', 'train', 'use', 'extract', '.']

 TOTAL PORTER STEM WORDS ==> 6



 ---- SNOWBALL STEMMING ----

['new', 'unseen', 'train', 'use', 'extract', '.']

 TOTAL SNOWBALL STEM WORDS ==> 6



 ---- LEMMATIZATION ----

['new', 'unseen', 'trained', 'used', 'extraction', '.']

 TOTAL LEMMATIZE WORDS ==> 6

************************************************************************************************************************

38 --> This process is also called as  Cognitive Capture. 


 ---- TOKENS ----

 ['This', 'process', 'is', 'also', 'called', 'as', 'Cognitive', 'Capture', '.'] 

 TOTAL TOKENS ==> 9

 ---- POST ----

 [('This', 'DT'), ('process', 'NN'), ('is', 'VBZ'), ('also', 'RB'), ('called', 'VBN'), ('as', 'IN'), ('Cognitive', 'JJ'), ('Capture', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['process', 'also', 'called', 'Cognitive', 'Capture', '.']

 TOTAL FILTERED TOKENS ==>  6

 ---- POST FOR FILTERED TOKENS ----

 [('process', 'NN'), ('also', 'RB'), ('called', 'VBD'), ('Cognitive', 'JJ'), ('Capture', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['process also', 'also called', 'called Cognitive', 'Cognitive Capture', 'Capture .'] 

 TOTAL BIGRAMS --> 5 



 ---- TRI-GRAMS ---- 

 ['process also called', 'also called Cognitive', 'called Cognitive Capture', 'Cognitive Capture .'] 

 TOTAL TRIGRAMS --> 4 



 ---- NOUN PHRASES ---- 

 ['process', 'Cognitive Capture'] 

 TOTAL NOUN PHRASES --> 2 



 ---- NER ----

 
 ORGANIZATION ---> ['Cognitive Capture']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['process', 'also', 'call', 'cognit', 'captur', '.']

 TOTAL PORTER STEM WORDS ==> 6



 ---- SNOWBALL STEMMING ----

['process', 'also', 'call', 'cognit', 'captur', '.']

 TOTAL SNOWBALL STEM WORDS ==> 6



 ---- LEMMATIZATION ----

['process', 'also', 'called', 'Cognitive', 'Capture', '.']

 TOTAL LEMMATIZE WORDS ==> 6

************************************************************************************************************************

39 --> Extracted information  then can be directly tagged to the content  while saving into the Content Services or it  can be used for verification during straight  through processing, etc. 


 ---- TOKENS ----

 ['Extracted', 'information', 'then', 'can', 'be', 'directly', 'tagged', 'to', 'the', 'content', 'while', 'saving', 'into', 'the', 'Content', 'Services', 'or', 'it', 'can', 'be', 'used', 'for', 'verification', 'during', 'straight', 'through', 'processing', ',', 'etc', '.'] 

 TOTAL TOKENS ==> 30

 ---- POST ----

 [('Extracted', 'VBN'), ('information', 'NN'), ('then', 'RB'), ('can', 'MD'), ('be', 'VB'), ('directly', 'RB'), ('tagged', 'VBN'), ('to', 'TO'), ('the', 'DT'), ('content', 'NN'), ('while', 'IN'), ('saving', 'VBG'), ('into', 'IN'), ('the', 'DT'), ('Content', 'NNP'), ('Services', 'NNPS'), ('or', 'CC'), ('it', 'PRP'), ('can', 'MD'), ('be', 'VB'), ('used', 'VBN'), ('for', 'IN'), ('verification', 'NN'), ('during', 'IN'), ('straight', 'JJ'), ('through', 'IN'), ('processing', 'NN'), (',', ','), ('etc', 'FW'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Extracted', 'information', 'directly', 'tagged', 'content', 'saving', 'Content', 'Services', 'used', 'verification', 'straight', 'processing', ',', 'etc', '.']

 TOTAL FILTERED TOKENS ==>  15

 ---- POST FOR FILTERED TOKENS ----

 [('Extracted', 'VBN'), ('information', 'NN'), ('directly', 'RB'), ('tagged', 'VBD'), ('content', 'JJ'), ('saving', 'VBG'), ('Content', 'NNP'), ('Services', 'NNPS'), ('used', 'VBD'), ('verification', 'NN'), ('straight', 'RB'), ('processing', 'NN'), (',', ','), ('etc', 'FW'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Extracted information', 'information directly', 'directly tagged', 'tagged content', 'content saving', 'saving Content', 'Content Services', 'Services used', 'used verification', 'verification straight', 'straight processing', 'processing ,', ', etc', 'etc .'] 

 TOTAL BIGRAMS --> 14 



 ---- TRI-GRAMS ---- 

 ['Extracted information directly', 'information directly tagged', 'directly tagged content', 'tagged content saving', 'content saving Content', 'saving Content Services', 'Content Services used', 'Services used verification', 'used verification straight', 'verification straight processing', 'straight processing ,', 'processing , etc', ', etc .'] 

 TOTAL TRIGRAMS --> 13 



 ---- NOUN PHRASES ---- 

 ['information', 'verification', 'processing'] 

 TOTAL NOUN PHRASES --> 3 



 ---- NER ----

 
 ORGANIZATION ---> ['Content Services']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['extract', 'inform', 'directli', 'tag', 'content', 'save', 'content', 'servic', 'use', 'verif', 'straight', 'process', ',', 'etc', '.']

 TOTAL PORTER STEM WORDS ==> 15



 ---- SNOWBALL STEMMING ----

['extract', 'inform', 'direct', 'tag', 'content', 'save', 'content', 'servic', 'use', 'verif', 'straight', 'process', ',', 'etc', '.']

 TOTAL SNOWBALL STEM WORDS ==> 15



 ---- LEMMATIZATION ----

['Extracted', 'information', 'directly', 'tagged', 'content', 'saving', 'Content', 'Services', 'used', 'verification', 'straight', 'processing', ',', 'etc', '.']

 TOTAL LEMMATIZE WORDS ==> 15

************************************************************************************************************************

40 --> documents may need to be OCRed if  required. 


 ---- TOKENS ----

 ['documents', 'may', 'need', 'to', 'be', 'OCRed', 'if', 'required', '.'] 

 TOTAL TOKENS ==> 9

 ---- POST ----

 [('documents', 'NNS'), ('may', 'MD'), ('need', 'VB'), ('to', 'TO'), ('be', 'VB'), ('OCRed', 'NNP'), ('if', 'IN'), ('required', 'VBN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['documents', 'may', 'need', 'OCRed', 'required', '.']

 TOTAL FILTERED TOKENS ==>  6

 ---- POST FOR FILTERED TOKENS ----

 [('documents', 'NNS'), ('may', 'MD'), ('need', 'VB'), ('OCRed', 'NNP'), ('required', 'VBN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['documents may', 'may need', 'need OCRed', 'OCRed required', 'required .'] 

 TOTAL BIGRAMS --> 5 



 ---- TRI-GRAMS ---- 

 ['documents may need', 'may need OCRed', 'need OCRed required', 'OCRed required .'] 

 TOTAL TRIGRAMS --> 4 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> ['OCRed']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['document', 'may', 'need', 'ocr', 'requir', '.']

 TOTAL PORTER STEM WORDS ==> 6



 ---- SNOWBALL STEMMING ----

['document', 'may', 'need', 'ocr', 'requir', '.']

 TOTAL SNOWBALL STEM WORDS ==> 6



 ---- LEMMATIZATION ----

['document', 'may', 'need', 'OCRed', 'required', '.']

 TOTAL LEMMATIZE WORDS ==> 6

************************************************************************************************************************

41 --> Capture process implemented  using NLP will not depend on layout of  the content and still be able to capture  appropriate information even if there  is change in the layout, as long as the  information is in the same context. 


 ---- TOKENS ----

 ['Capture', 'process', 'implemented', 'using', 'NLP', 'will', 'not', 'depend', 'on', 'layout', 'of', 'the', 'content', 'and', 'still', 'be', 'able', 'to', 'capture', 'appropriate', 'information', 'even', 'if', 'there', 'is', 'change', 'in', 'the', 'layout', ',', 'as', 'long', 'as', 'the', 'information', 'is', 'in', 'the', 'same', 'context', '.'] 

 TOTAL TOKENS ==> 41

 ---- POST ----

 [('Capture', 'NN'), ('process', 'NN'), ('implemented', 'VBD'), ('using', 'VBG'), ('NLP', 'NNP'), ('will', 'MD'), ('not', 'RB'), ('depend', 'VB'), ('on', 'IN'), ('layout', 'NN'), ('of', 'IN'), ('the', 'DT'), ('content', 'NN'), ('and', 'CC'), ('still', 'RB'), ('be', 'VB'), ('able', 'JJ'), ('to', 'TO'), ('capture', 'VB'), ('appropriate', 'JJ'), ('information', 'NN'), ('even', 'RB'), ('if', 'IN'), ('there', 'EX'), ('is', 'VBZ'), ('change', 'NN'), ('in', 'IN'), ('the', 'DT'), ('layout', 'NN'), (',', ','), ('as', 'RB'), ('long', 'RB'), ('as', 'IN'), ('the', 'DT'), ('information', 'NN'), ('is', 'VBZ'), ('in', 'IN'), ('the', 'DT'), ('same', 'JJ'), ('context', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Capture', 'process', 'implemented', 'using', 'NLP', 'depend', 'layout', 'content', 'still', 'able', 'capture', 'appropriate', 'information', 'even', 'change', 'layout', ',', 'long', 'information', 'context', '.']

 TOTAL FILTERED TOKENS ==>  21

 ---- POST FOR FILTERED TOKENS ----

 [('Capture', 'NN'), ('process', 'NN'), ('implemented', 'VBD'), ('using', 'VBG'), ('NLP', 'NNP'), ('depend', 'NN'), ('layout', 'NN'), ('content', 'NN'), ('still', 'RB'), ('able', 'JJ'), ('capture', 'NN'), ('appropriate', 'JJ'), ('information', 'NN'), ('even', 'RB'), ('change', 'NN'), ('layout', 'NN'), (',', ','), ('long', 'JJ'), ('information', 'NN'), ('context', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Capture process', 'process implemented', 'implemented using', 'using NLP', 'NLP depend', 'depend layout', 'layout content', 'content still', 'still able', 'able capture', 'capture appropriate', 'appropriate information', 'information even', 'even change', 'change layout', 'layout ,', ', long', 'long information', 'information context', 'context .'] 

 TOTAL BIGRAMS --> 20 



 ---- TRI-GRAMS ---- 

 ['Capture process implemented', 'process implemented using', 'implemented using NLP', 'using NLP depend', 'NLP depend layout', 'depend layout content', 'layout content still', 'content still able', 'still able capture', 'able capture appropriate', 'capture appropriate information', 'appropriate information even', 'information even change', 'even change layout', 'change layout ,', 'layout , long', ', long information', 'long information context', 'information context .'] 

 TOTAL TRIGRAMS --> 19 



 ---- NOUN PHRASES ---- 

 ['Capture', 'process', 'depend', 'layout', 'content', 'able capture', 'appropriate information', 'change', 'layout', 'long information', 'context'] 

 TOTAL NOUN PHRASES --> 11 



 ---- NER ----

 
 ORGANIZATION ---> ['NLP']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> ['Capture']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['captur', 'process', 'implement', 'use', 'nlp', 'depend', 'layout', 'content', 'still', 'abl', 'captur', 'appropri', 'inform', 'even', 'chang', 'layout', ',', 'long', 'inform', 'context', '.']

 TOTAL PORTER STEM WORDS ==> 21



 ---- SNOWBALL STEMMING ----

['captur', 'process', 'implement', 'use', 'nlp', 'depend', 'layout', 'content', 'still', 'abl', 'captur', 'appropri', 'inform', 'even', 'chang', 'layout', ',', 'long', 'inform', 'context', '.']

 TOTAL SNOWBALL STEM WORDS ==> 21



 ---- LEMMATIZATION ----

['Capture', 'process', 'implemented', 'using', 'NLP', 'depend', 'layout', 'content', 'still', 'able', 'capture', 'appropriate', 'information', 'even', 'change', 'layout', ',', 'long', 'information', 'context', '.']

 TOTAL LEMMATIZE WORDS ==> 21

************************************************************************************************************************

42 --> 2 How NLP Can Make Content Services Smarter External Document © 2019 Infosys Limited External Document © 2019 Infosys Limited  2.1.1  Implementation Scenarios Below described are the some of the  different implementation scenarios in the  context of cognitive capture 2.1.1.1  Extracting metadata while  Capture Captured information can be tagged to the  content while saving into Content Services. 


 ---- TOKENS ----

 ['2', 'How', 'NLP', 'Can', 'Make', 'Content', 'Services', 'Smarter', 'External', 'Document', '©', '2019', 'Infosys', 'Limited', 'External', 'Document', '©', '2019', 'Infosys', 'Limited', '2.1.1', 'Implementation', 'Scenarios', 'Below', 'described', 'are', 'the', 'some', 'of', 'the', 'different', 'implementation', 'scenarios', 'in', 'the', 'context', 'of', 'cognitive', 'capture', '2.1.1.1', 'Extracting', 'metadata', 'while', 'Capture', 'Captured', 'information', 'can', 'be', 'tagged', 'to', 'the', 'content', 'while', 'saving', 'into', 'Content', 'Services', '.'] 

 TOTAL TOKENS ==> 58

 ---- POST ----

 [('2', 'CD'), ('How', 'WRB'), ('NLP', 'NNP'), ('Can', 'NNP'), ('Make', 'NNP'), ('Content', 'NNP'), ('Services', 'NNPS'), ('Smarter', 'NNP'), ('External', 'NNP'), ('Document', 'NNP'), ('©', 'NN'), ('2019', 'CD'), ('Infosys', 'NNP'), ('Limited', 'NNP'), ('External', 'NNP'), ('Document', 'NNP'), ('©', 'NN'), ('2019', 'CD'), ('Infosys', 'NNP'), ('Limited', 'VBD'), ('2.1.1', 'CD'), ('Implementation', 'NNP'), ('Scenarios', 'NNP'), ('Below', 'NNP'), ('described', 'VBD'), ('are', 'VBP'), ('the', 'DT'), ('some', 'DT'), ('of', 'IN'), ('the', 'DT'), ('different', 'JJ'), ('implementation', 'NN'), ('scenarios', 'NNS'), ('in', 'IN'), ('the', 'DT'), ('context', 'NN'), ('of', 'IN'), ('cognitive', 'JJ'), ('capture', 'NN'), ('2.1.1.1', 'CD'), ('Extracting', 'NNP'), ('metadata', 'NNS'), ('while', 'IN'), ('Capture', 'NNP'), ('Captured', 'NNP'), ('information', 'NN'), ('can', 'MD'), ('be', 'VB'), ('tagged', 'VBN'), ('to', 'TO'), ('the', 'DT'), ('content', 'NN'), ('while', 'IN'), ('saving', 'VBG'), ('into', 'IN'), ('Content', 'NNP'), ('Services', 'NNPS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['2', 'NLP', 'Make', 'Content', 'Services', 'Smarter', 'External', 'Document', '©', '2019', 'Infosys', 'Limited', 'External', 'Document', '©', '2019', 'Infosys', 'Limited', '2.1.1', 'Implementation', 'Scenarios', 'described', 'different', 'implementation', 'scenarios', 'context', 'cognitive', 'capture', '2.1.1.1', 'Extracting', 'metadata', 'Capture', 'Captured', 'information', 'tagged', 'content', 'saving', 'Content', 'Services', '.']

 TOTAL FILTERED TOKENS ==>  40

 ---- POST FOR FILTERED TOKENS ----

 [('2', 'CD'), ('NLP', 'NNP'), ('Make', 'NNP'), ('Content', 'NNP'), ('Services', 'NNPS'), ('Smarter', 'NNP'), ('External', 'NNP'), ('Document', 'NNP'), ('©', 'NN'), ('2019', 'CD'), ('Infosys', 'NNP'), ('Limited', 'NNP'), ('External', 'NNP'), ('Document', 'NNP'), ('©', 'NN'), ('2019', 'CD'), ('Infosys', 'NNP'), ('Limited', 'VBD'), ('2.1.1', 'CD'), ('Implementation', 'NNP'), ('Scenarios', 'NNP'), ('described', 'VBD'), ('different', 'JJ'), ('implementation', 'NN'), ('scenarios', 'NNS'), ('context', 'VBP'), ('cognitive', 'JJ'), ('capture', 'NN'), ('2.1.1.1', 'CD'), ('Extracting', 'NNP'), ('metadata', 'NN'), ('Capture', 'NNP'), ('Captured', 'NNP'), ('information', 'NN'), ('tagged', 'VBD'), ('content', 'NN'), ('saving', 'VBG'), ('Content', 'NNP'), ('Services', 'NNPS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['2 NLP', 'NLP Make', 'Make Content', 'Content Services', 'Services Smarter', 'Smarter External', 'External Document', 'Document ©', '© 2019', '2019 Infosys', 'Infosys Limited', 'Limited External', 'External Document', 'Document ©', '© 2019', '2019 Infosys', 'Infosys Limited', 'Limited 2.1.1', '2.1.1 Implementation', 'Implementation Scenarios', 'Scenarios described', 'described different', 'different implementation', 'implementation scenarios', 'scenarios context', 'context cognitive', 'cognitive capture', 'capture 2.1.1.1', '2.1.1.1 Extracting', 'Extracting metadata', 'metadata Capture', 'Capture Captured', 'Captured information', 'information tagged', 'tagged content', 'content saving', 'saving Content', 'Content Services', 'Services .'] 

 TOTAL BIGRAMS --> 39 



 ---- TRI-GRAMS ---- 

 ['2 NLP Make', 'NLP Make Content', 'Make Content Services', 'Content Services Smarter', 'Services Smarter External', 'Smarter External Document', 'External Document ©', 'Document © 2019', '© 2019 Infosys', '2019 Infosys Limited', 'Infosys Limited External', 'Limited External Document', 'External Document ©', 'Document © 2019', '© 2019 Infosys', '2019 Infosys Limited', 'Infosys Limited 2.1.1', 'Limited 2.1.1 Implementation', '2.1.1 Implementation Scenarios', 'Implementation Scenarios described', 'Scenarios described different', 'described different implementation', 'different implementation scenarios', 'implementation scenarios context', 'scenarios context cognitive', 'context cognitive capture', 'cognitive capture 2.1.1.1', 'capture 2.1.1.1 Extracting', '2.1.1.1 Extracting metadata', 'Extracting metadata Capture', 'metadata Capture Captured', 'Capture Captured information', 'Captured information tagged', 'information tagged content', 'tagged content saving', 'content saving Content', 'saving Content Services', 'Content Services .'] 

 TOTAL TRIGRAMS --> 38 



 ---- NOUN PHRASES ---- 

 ['©', '©', 'different implementation', 'cognitive capture', 'metadata', 'information', 'content'] 

 TOTAL NOUN PHRASES --> 7 



 ---- NER ----

 
 ORGANIZATION ---> ['NLP Make Content Services Smarter External', 'Content Services']
 TOTAL ORGANIZATION ENTITY --> 2 


 PERSON ---> ['Infosys', 'Scenarios', 'Capture Captured']
 TOTAL PERSON ENTITY --> 3 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['2', 'nlp', 'make', 'content', 'servic', 'smarter', 'extern', 'document', '©', '2019', 'infosi', 'limit', 'extern', 'document', '©', '2019', 'infosi', 'limit', '2.1.1', 'implement', 'scenario', 'describ', 'differ', 'implement', 'scenario', 'context', 'cognit', 'captur', '2.1.1.1', 'extract', 'metadata', 'captur', 'captur', 'inform', 'tag', 'content', 'save', 'content', 'servic', '.']

 TOTAL PORTER STEM WORDS ==> 40



 ---- SNOWBALL STEMMING ----

['2', 'nlp', 'make', 'content', 'servic', 'smarter', 'extern', 'document', '©', '2019', 'infosi', 'limit', 'extern', 'document', '©', '2019', 'infosi', 'limit', '2.1.1', 'implement', 'scenario', 'describ', 'differ', 'implement', 'scenario', 'context', 'cognit', 'captur', '2.1.1.1', 'extract', 'metadata', 'captur', 'captur', 'inform', 'tag', 'content', 'save', 'content', 'servic', '.']

 TOTAL SNOWBALL STEM WORDS ==> 40



 ---- LEMMATIZATION ----

['2', 'NLP', 'Make', 'Content', 'Services', 'Smarter', 'External', 'Document', '©', '2019', 'Infosys', 'Limited', 'External', 'Document', '©', '2019', 'Infosys', 'Limited', '2.1.1', 'Implementation', 'Scenarios', 'described', 'different', 'implementation', 'scenario', 'context', 'cognitive', 'capture', '2.1.1.1', 'Extracting', 'metadata', 'Capture', 'Captured', 'information', 'tagged', 'content', 'saving', 'Content', 'Services', '.']

 TOTAL LEMMATIZE WORDS ==> 40

************************************************************************************************************************

43 --> The tagged information will give identity  to the content, which helps in locating and  reusing the content. 


 ---- TOKENS ----

 ['The', 'tagged', 'information', 'will', 'give', 'identity', 'to', 'the', 'content', ',', 'which', 'helps', 'in', 'locating', 'and', 'reusing', 'the', 'content', '.'] 

 TOTAL TOKENS ==> 19

 ---- POST ----

 [('The', 'DT'), ('tagged', 'VBN'), ('information', 'NN'), ('will', 'MD'), ('give', 'VB'), ('identity', 'NN'), ('to', 'TO'), ('the', 'DT'), ('content', 'NN'), (',', ','), ('which', 'WDT'), ('helps', 'VBZ'), ('in', 'IN'), ('locating', 'VBG'), ('and', 'CC'), ('reusing', 'VBG'), ('the', 'DT'), ('content', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['tagged', 'information', 'give', 'identity', 'content', ',', 'helps', 'locating', 'reusing', 'content', '.']

 TOTAL FILTERED TOKENS ==>  11

 ---- POST FOR FILTERED TOKENS ----

 [('tagged', 'VBN'), ('information', 'NN'), ('give', 'NN'), ('identity', 'NN'), ('content', 'NN'), (',', ','), ('helps', 'VBZ'), ('locating', 'VBG'), ('reusing', 'VBG'), ('content', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['tagged information', 'information give', 'give identity', 'identity content', 'content ,', ', helps', 'helps locating', 'locating reusing', 'reusing content', 'content .'] 

 TOTAL BIGRAMS --> 10 



 ---- TRI-GRAMS ---- 

 ['tagged information give', 'information give identity', 'give identity content', 'identity content ,', 'content , helps', ', helps locating', 'helps locating reusing', 'locating reusing content', 'reusing content .'] 

 TOTAL TRIGRAMS --> 9 



 ---- NOUN PHRASES ---- 

 ['information', 'give', 'identity', 'content', 'content'] 

 TOTAL NOUN PHRASES --> 5 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['tag', 'inform', 'give', 'ident', 'content', ',', 'help', 'locat', 'reus', 'content', '.']

 TOTAL PORTER STEM WORDS ==> 11



 ---- SNOWBALL STEMMING ----

['tag', 'inform', 'give', 'ident', 'content', ',', 'help', 'locat', 'reus', 'content', '.']

 TOTAL SNOWBALL STEM WORDS ==> 11



 ---- LEMMATIZATION ----

['tagged', 'information', 'give', 'identity', 'content', ',', 'help', 'locating', 'reusing', 'content', '.']

 TOTAL LEMMATIZE WORDS ==> 11

************************************************************************************************************************

44 --> 2.1.1.2  Data Verification for Straight  through Processing Data captured can be verified with master  As a first step, the AI model is to be trained  with labeled documents to retrieve  metadata information from the document. 


 ---- TOKENS ----

 ['2.1.1.2', 'Data', 'Verification', 'for', 'Straight', 'through', 'Processing', 'Data', 'captured', 'can', 'be', 'verified', 'with', 'master', 'As', 'a', 'first', 'step', ',', 'the', 'AI', 'model', 'is', 'to', 'be', 'trained', 'with', 'labeled', 'documents', 'to', 'retrieve', 'metadata', 'information', 'from', 'the', 'document', '.'] 

 TOTAL TOKENS ==> 37

 ---- POST ----

 [('2.1.1.2', 'CD'), ('Data', 'NNP'), ('Verification', 'NNP'), ('for', 'IN'), ('Straight', 'NNP'), ('through', 'IN'), ('Processing', 'NNP'), ('Data', 'NNP'), ('captured', 'VBD'), ('can', 'MD'), ('be', 'VB'), ('verified', 'VBN'), ('with', 'IN'), ('master', 'NN'), ('As', 'IN'), ('a', 'DT'), ('first', 'JJ'), ('step', 'NN'), (',', ','), ('the', 'DT'), ('AI', 'NNP'), ('model', 'NN'), ('is', 'VBZ'), ('to', 'TO'), ('be', 'VB'), ('trained', 'VBN'), ('with', 'IN'), ('labeled', 'JJ'), ('documents', 'NNS'), ('to', 'TO'), ('retrieve', 'VB'), ('metadata', 'NN'), ('information', 'NN'), ('from', 'IN'), ('the', 'DT'), ('document', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['2.1.1.2', 'Data', 'Verification', 'Straight', 'Processing', 'Data', 'captured', 'verified', 'master', 'first', 'step', ',', 'AI', 'model', 'trained', 'labeled', 'documents', 'retrieve', 'metadata', 'information', 'document', '.']

 TOTAL FILTERED TOKENS ==>  22

 ---- POST FOR FILTERED TOKENS ----

 [('2.1.1.2', 'CD'), ('Data', 'NNP'), ('Verification', 'NNP'), ('Straight', 'NNP'), ('Processing', 'NNP'), ('Data', 'NNP'), ('captured', 'VBD'), ('verified', 'JJ'), ('master', 'NN'), ('first', 'JJ'), ('step', 'NN'), (',', ','), ('AI', 'NNP'), ('model', 'NN'), ('trained', 'VBD'), ('labeled', 'JJ'), ('documents', 'NNS'), ('retrieve', 'VBP'), ('metadata', 'NN'), ('information', 'NN'), ('document', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['2.1.1.2 Data', 'Data Verification', 'Verification Straight', 'Straight Processing', 'Processing Data', 'Data captured', 'captured verified', 'verified master', 'master first', 'first step', 'step ,', ', AI', 'AI model', 'model trained', 'trained labeled', 'labeled documents', 'documents retrieve', 'retrieve metadata', 'metadata information', 'information document', 'document .'] 

 TOTAL BIGRAMS --> 21 



 ---- TRI-GRAMS ---- 

 ['2.1.1.2 Data Verification', 'Data Verification Straight', 'Verification Straight Processing', 'Straight Processing Data', 'Processing Data captured', 'Data captured verified', 'captured verified master', 'verified master first', 'master first step', 'first step ,', 'step , AI', ', AI model', 'AI model trained', 'model trained labeled', 'trained labeled documents', 'labeled documents retrieve', 'documents retrieve metadata', 'retrieve metadata information', 'metadata information document', 'information document .'] 

 TOTAL TRIGRAMS --> 20 



 ---- NOUN PHRASES ---- 

 ['verified master', 'first step', 'model', 'metadata', 'information', 'document'] 

 TOTAL NOUN PHRASES --> 6 



 ---- NER ----

 
 ORGANIZATION ---> ['AI']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> ['Data']
 TOTAL PERSON ENTITY --> 1 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['2.1.1.2', 'data', 'verif', 'straight', 'process', 'data', 'captur', 'verifi', 'master', 'first', 'step', ',', 'ai', 'model', 'train', 'label', 'document', 'retriev', 'metadata', 'inform', 'document', '.']

 TOTAL PORTER STEM WORDS ==> 22



 ---- SNOWBALL STEMMING ----

['2.1.1.2', 'data', 'verif', 'straight', 'process', 'data', 'captur', 'verifi', 'master', 'first', 'step', ',', 'ai', 'model', 'train', 'label', 'document', 'retriev', 'metadata', 'inform', 'document', '.']

 TOTAL SNOWBALL STEM WORDS ==> 22



 ---- LEMMATIZATION ----

['2.1.1.2', 'Data', 'Verification', 'Straight', 'Processing', 'Data', 'captured', 'verified', 'master', 'first', 'step', ',', 'AI', 'model', 'trained', 'labeled', 'document', 'retrieve', 'metadata', 'information', 'document', '.']

 TOTAL LEMMATIZE WORDS ==> 22

************************************************************************************************************************

45 --> Since it is context based metadata  identification, both models, NER as well as  Categorizer, can be used. 


 ---- TOKENS ----

 ['Since', 'it', 'is', 'context', 'based', 'metadata', 'identification', ',', 'both', 'models', ',', 'NER', 'as', 'well', 'as', 'Categorizer', ',', 'can', 'be', 'used', '.'] 

 TOTAL TOKENS ==> 21

 ---- POST ----

 [('Since', 'IN'), ('it', 'PRP'), ('is', 'VBZ'), ('context', 'JJ'), ('based', 'VBN'), ('metadata', 'NN'), ('identification', 'NN'), (',', ','), ('both', 'DT'), ('models', 'NNS'), (',', ','), ('NER', 'NNP'), ('as', 'RB'), ('well', 'RB'), ('as', 'IN'), ('Categorizer', 'NNP'), (',', ','), ('can', 'MD'), ('be', 'VB'), ('used', 'VBN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Since', 'context', 'based', 'metadata', 'identification', ',', 'models', ',', 'NER', 'well', 'Categorizer', ',', 'used', '.']

 TOTAL FILTERED TOKENS ==>  14

 ---- POST FOR FILTERED TOKENS ----

 [('Since', 'IN'), ('context', 'NN'), ('based', 'VBN'), ('metadata', 'JJ'), ('identification', 'NN'), (',', ','), ('models', 'NNS'), (',', ','), ('NER', 'NNP'), ('well', 'RB'), ('Categorizer', 'NNP'), (',', ','), ('used', 'VBD'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Since context', 'context based', 'based metadata', 'metadata identification', 'identification ,', ', models', 'models ,', ', NER', 'NER well', 'well Categorizer', 'Categorizer ,', ', used', 'used .'] 

 TOTAL BIGRAMS --> 13 



 ---- TRI-GRAMS ---- 

 ['Since context based', 'context based metadata', 'based metadata identification', 'metadata identification ,', 'identification , models', ', models ,', 'models , NER', ', NER well', 'NER well Categorizer', 'well Categorizer ,', 'Categorizer , used', ', used .'] 

 TOTAL TRIGRAMS --> 12 



 ---- NOUN PHRASES ---- 

 ['context', 'metadata identification'] 

 TOTAL NOUN PHRASES --> 2 



 ---- NER ----

 
 ORGANIZATION ---> ['NER']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> ['Categorizer']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['sinc', 'context', 'base', 'metadata', 'identif', ',', 'model', ',', 'ner', 'well', 'categor', ',', 'use', '.']

 TOTAL PORTER STEM WORDS ==> 14



 ---- SNOWBALL STEMMING ----

['sinc', 'context', 'base', 'metadata', 'identif', ',', 'model', ',', 'ner', 'well', 'categor', ',', 'use', '.']

 TOTAL SNOWBALL STEM WORDS ==> 14



 ---- LEMMATIZATION ----

['Since', 'context', 'based', 'metadata', 'identification', ',', 'model', ',', 'NER', 'well', 'Categorizer', ',', 'used', '.']

 TOTAL LEMMATIZE WORDS ==> 14

************************************************************************************************************************

46 --> For training NER models, the process is  same, that is to annotate the documents  and then use the annotated documents for  training. 


 ---- TOKENS ----

 ['For', 'training', 'NER', 'models', ',', 'the', 'process', 'is', 'same', ',', 'that', 'is', 'to', 'annotate', 'the', 'documents', 'and', 'then', 'use', 'the', 'annotated', 'documents', 'for', 'training', '.'] 

 TOTAL TOKENS ==> 25

 ---- POST ----

 [('For', 'IN'), ('training', 'VBG'), ('NER', 'NNP'), ('models', 'NNS'), (',', ','), ('the', 'DT'), ('process', 'NN'), ('is', 'VBZ'), ('same', 'JJ'), (',', ','), ('that', 'WDT'), ('is', 'VBZ'), ('to', 'TO'), ('annotate', 'VB'), ('the', 'DT'), ('documents', 'NNS'), ('and', 'CC'), ('then', 'RB'), ('use', 'VB'), ('the', 'DT'), ('annotated', 'JJ'), ('documents', 'NNS'), ('for', 'IN'), ('training', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['training', 'NER', 'models', ',', 'process', ',', 'annotate', 'documents', 'use', 'annotated', 'documents', 'training', '.']

 TOTAL FILTERED TOKENS ==>  13

 ---- POST FOR FILTERED TOKENS ----

 [('training', 'NN'), ('NER', 'NNP'), ('models', 'NNS'), (',', ','), ('process', 'NN'), (',', ','), ('annotate', 'NN'), ('documents', 'NNS'), ('use', 'VBP'), ('annotated', 'JJ'), ('documents', 'NNS'), ('training', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['training NER', 'NER models', 'models ,', ', process', 'process ,', ', annotate', 'annotate documents', 'documents use', 'use annotated', 'annotated documents', 'documents training', 'training .'] 

 TOTAL BIGRAMS --> 12 



 ---- TRI-GRAMS ---- 

 ['training NER models', 'NER models ,', 'models , process', ', process ,', 'process , annotate', ', annotate documents', 'annotate documents use', 'documents use annotated', 'use annotated documents', 'annotated documents training', 'documents training .'] 

 TOTAL TRIGRAMS --> 11 



 ---- NOUN PHRASES ---- 

 ['training', 'process', 'annotate', 'training'] 

 TOTAL NOUN PHRASES --> 4 



 ---- NER ----

 
 ORGANIZATION ---> ['NER']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['train', 'ner', 'model', ',', 'process', ',', 'annot', 'document', 'use', 'annot', 'document', 'train', '.']

 TOTAL PORTER STEM WORDS ==> 13



 ---- SNOWBALL STEMMING ----

['train', 'ner', 'model', ',', 'process', ',', 'annot', 'document', 'use', 'annot', 'document', 'train', '.']

 TOTAL SNOWBALL STEM WORDS ==> 13



 ---- LEMMATIZATION ----

['training', 'NER', 'model', ',', 'process', ',', 'annotate', 'document', 'use', 'annotated', 'document', 'training', '.']

 TOTAL LEMMATIZE WORDS ==> 13

************************************************************************************************************************

47 --> Categorizer model is supported by  some of the NLP tools such as Apache  OpenNLP. 


 ---- TOKENS ----

 ['Categorizer', 'model', 'is', 'supported', 'by', 'some', 'of', 'the', 'NLP', 'tools', 'such', 'as', 'Apache', 'OpenNLP', '.'] 

 TOTAL TOKENS ==> 15

 ---- POST ----

 [('Categorizer', 'NNP'), ('model', 'NN'), ('is', 'VBZ'), ('supported', 'VBN'), ('by', 'IN'), ('some', 'DT'), ('of', 'IN'), ('the', 'DT'), ('NLP', 'NNP'), ('tools', 'NNS'), ('such', 'JJ'), ('as', 'IN'), ('Apache', 'NNP'), ('OpenNLP', 'NNP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Categorizer', 'model', 'supported', 'NLP', 'tools', 'Apache', 'OpenNLP', '.']

 TOTAL FILTERED TOKENS ==>  8

 ---- POST FOR FILTERED TOKENS ----

 [('Categorizer', 'NNP'), ('model', 'NN'), ('supported', 'VBD'), ('NLP', 'NNP'), ('tools', 'NNS'), ('Apache', 'NNP'), ('OpenNLP', 'NNP'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Categorizer model', 'model supported', 'supported NLP', 'NLP tools', 'tools Apache', 'Apache OpenNLP', 'OpenNLP .'] 

 TOTAL BIGRAMS --> 7 



 ---- TRI-GRAMS ---- 

 ['Categorizer model supported', 'model supported NLP', 'supported NLP tools', 'NLP tools Apache', 'tools Apache OpenNLP', 'Apache OpenNLP .'] 

 TOTAL TRIGRAMS --> 6 



 ---- NOUN PHRASES ---- 

 ['model'] 

 TOTAL NOUN PHRASES --> 1 



 ---- NER ----

 
 ORGANIZATION ---> ['NLP']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> ['Apache OpenNLP']
 TOTAL PERSON ENTITY --> 1 


 GPE ---> ['Categorizer']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['categor', 'model', 'support', 'nlp', 'tool', 'apach', 'opennlp', '.']

 TOTAL PORTER STEM WORDS ==> 8



 ---- SNOWBALL STEMMING ----

['categor', 'model', 'support', 'nlp', 'tool', 'apach', 'opennlp', '.']

 TOTAL SNOWBALL STEM WORDS ==> 8



 ---- LEMMATIZATION ----

['Categorizer', 'model', 'supported', 'NLP', 'tool', 'Apache', 'OpenNLP', '.']

 TOTAL LEMMATIZE WORDS ==> 8

************************************************************************************************************************

48 --> To train a categorizer model,  first the categories need to be defined. 


 ---- TOKENS ----

 ['To', 'train', 'a', 'categorizer', 'model', ',', 'first', 'the', 'categories', 'need', 'to', 'be', 'defined', '.'] 

 TOTAL TOKENS ==> 14

 ---- POST ----

 [('To', 'TO'), ('train', 'VB'), ('a', 'DT'), ('categorizer', 'NN'), ('model', 'NN'), (',', ','), ('first', 'RB'), ('the', 'DT'), ('categories', 'NNS'), ('need', 'VBP'), ('to', 'TO'), ('be', 'VB'), ('defined', 'VBN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['train', 'categorizer', 'model', ',', 'first', 'categories', 'need', 'defined', '.']

 TOTAL FILTERED TOKENS ==>  9

 ---- POST FOR FILTERED TOKENS ----

 [('train', 'NN'), ('categorizer', 'NN'), ('model', 'NN'), (',', ','), ('first', 'JJ'), ('categories', 'NNS'), ('need', 'VBP'), ('defined', 'VBN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['train categorizer', 'categorizer model', 'model ,', ', first', 'first categories', 'categories need', 'need defined', 'defined .'] 

 TOTAL BIGRAMS --> 8 



 ---- TRI-GRAMS ---- 

 ['train categorizer model', 'categorizer model ,', 'model , first', ', first categories', 'first categories need', 'categories need defined', 'need defined .'] 

 TOTAL TRIGRAMS --> 7 



 ---- NOUN PHRASES ---- 

 ['train', 'categorizer', 'model'] 

 TOTAL NOUN PHRASES --> 3 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['train', 'categor', 'model', ',', 'first', 'categori', 'need', 'defin', '.']

 TOTAL PORTER STEM WORDS ==> 9



 ---- SNOWBALL STEMMING ----

['train', 'categor', 'model', ',', 'first', 'categori', 'need', 'defin', '.']

 TOTAL SNOWBALL STEM WORDS ==> 9



 ---- LEMMATIZATION ----

['train', 'categorizer', 'model', ',', 'first', 'category', 'need', 'defined', '.']

 TOTAL LEMMATIZE WORDS ==> 9

************************************************************************************************************************

49 --> For example, to implement metadata  identification through classification,  a category needs to be mapped to a  metadata value. 


 ---- TOKENS ----

 ['For', 'example', ',', 'to', 'implement', 'metadata', 'identification', 'through', 'classification', ',', 'a', 'category', 'needs', 'to', 'be', 'mapped', 'to', 'a', 'metadata', 'value', '.'] 

 TOTAL TOKENS ==> 21

 ---- POST ----

 [('For', 'IN'), ('example', 'NN'), (',', ','), ('to', 'TO'), ('implement', 'VB'), ('metadata', 'NN'), ('identification', 'NN'), ('through', 'IN'), ('classification', 'NN'), (',', ','), ('a', 'DT'), ('category', 'NN'), ('needs', 'VBZ'), ('to', 'TO'), ('be', 'VB'), ('mapped', 'VBN'), ('to', 'TO'), ('a', 'DT'), ('metadata', 'NN'), ('value', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['example', ',', 'implement', 'metadata', 'identification', 'classification', ',', 'category', 'needs', 'mapped', 'metadata', 'value', '.']

 TOTAL FILTERED TOKENS ==>  13

 ---- POST FOR FILTERED TOKENS ----

 [('example', 'NN'), (',', ','), ('implement', 'NN'), ('metadata', 'NNS'), ('identification', 'NN'), ('classification', 'NN'), (',', ','), ('category', 'NN'), ('needs', 'NNS'), ('mapped', 'VBD'), ('metadata', 'NN'), ('value', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['example ,', ', implement', 'implement metadata', 'metadata identification', 'identification classification', 'classification ,', ', category', 'category needs', 'needs mapped', 'mapped metadata', 'metadata value', 'value .'] 

 TOTAL BIGRAMS --> 12 



 ---- TRI-GRAMS ---- 

 ['example , implement', ', implement metadata', 'implement metadata identification', 'metadata identification classification', 'identification classification ,', 'classification , category', ', category needs', 'category needs mapped', 'needs mapped metadata', 'mapped metadata value', 'metadata value .'] 

 TOTAL TRIGRAMS --> 11 



 ---- NOUN PHRASES ---- 

 ['example', 'implement', 'identification', 'classification', 'category', 'metadata', 'value'] 

 TOTAL NOUN PHRASES --> 7 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['exampl', ',', 'implement', 'metadata', 'identif', 'classif', ',', 'categori', 'need', 'map', 'metadata', 'valu', '.']

 TOTAL PORTER STEM WORDS ==> 13



 ---- SNOWBALL STEMMING ----

['exampl', ',', 'implement', 'metadata', 'identif', 'classif', ',', 'categori', 'need', 'map', 'metadata', 'valu', '.']

 TOTAL SNOWBALL STEM WORDS ==> 13



 ---- LEMMATIZATION ----

['example', ',', 'implement', 'metadata', 'identification', 'classification', ',', 'category', 'need', 'mapped', 'metadata', 'value', '.']

 TOTAL LEMMATIZE WORDS ==> 13

************************************************************************************************************************

50 --> As next step, from labelled  data management system for automatic  processing, such as claim processing, etc. 


 ---- TOKENS ----

 ['As', 'next', 'step', ',', 'from', 'labelled', 'data', 'management', 'system', 'for', 'automatic', 'processing', ',', 'such', 'as', 'claim', 'processing', ',', 'etc', '.'] 

 TOTAL TOKENS ==> 20

 ---- POST ----

 [('As', 'IN'), ('next', 'JJ'), ('step', 'NN'), (',', ','), ('from', 'IN'), ('labelled', 'VBN'), ('data', 'NNS'), ('management', 'NN'), ('system', 'NN'), ('for', 'IN'), ('automatic', 'JJ'), ('processing', 'NN'), (',', ','), ('such', 'JJ'), ('as', 'IN'), ('claim', 'NN'), ('processing', 'NN'), (',', ','), ('etc', 'FW'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['next', 'step', ',', 'labelled', 'data', 'management', 'system', 'automatic', 'processing', ',', 'claim', 'processing', ',', 'etc', '.']

 TOTAL FILTERED TOKENS ==>  15

 ---- POST FOR FILTERED TOKENS ----

 [('next', 'JJ'), ('step', 'NN'), (',', ','), ('labelled', 'VBD'), ('data', 'NNS'), ('management', 'NN'), ('system', 'NN'), ('automatic', 'JJ'), ('processing', 'NN'), (',', ','), ('claim', 'NN'), ('processing', 'NN'), (',', ','), ('etc', 'FW'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['next step', 'step ,', ', labelled', 'labelled data', 'data management', 'management system', 'system automatic', 'automatic processing', 'processing ,', ', claim', 'claim processing', 'processing ,', ', etc', 'etc .'] 

 TOTAL BIGRAMS --> 14 



 ---- TRI-GRAMS ---- 

 ['next step ,', 'step , labelled', ', labelled data', 'labelled data management', 'data management system', 'management system automatic', 'system automatic processing', 'automatic processing ,', 'processing , claim', ', claim processing', 'claim processing ,', 'processing , etc', ', etc .'] 

 TOTAL TRIGRAMS --> 13 



 ---- NOUN PHRASES ---- 

 ['next step', 'management', 'system', 'automatic processing', 'claim', 'processing'] 

 TOTAL NOUN PHRASES --> 6 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['next', 'step', ',', 'label', 'data', 'manag', 'system', 'automat', 'process', ',', 'claim', 'process', ',', 'etc', '.']

 TOTAL PORTER STEM WORDS ==> 15



 ---- SNOWBALL STEMMING ----

['next', 'step', ',', 'label', 'data', 'manag', 'system', 'automat', 'process', ',', 'claim', 'process', ',', 'etc', '.']

 TOTAL SNOWBALL STEM WORDS ==> 15



 ---- LEMMATIZATION ----

['next', 'step', ',', 'labelled', 'data', 'management', 'system', 'automatic', 'processing', ',', 'claim', 'processing', ',', 'etc', '.']

 TOTAL LEMMATIZE WORDS ==> 15

************************************************************************************************************************

51 --> 2.2 Content Ingestion Content Ingestion is a process where  content is loaded into Content Services  platform, either through a batch process  or manually through a user interface. 


 ---- TOKENS ----

 ['2.2', 'Content', 'Ingestion', 'Content', 'Ingestion', 'is', 'a', 'process', 'where', 'content', 'is', 'loaded', 'into', 'Content', 'Services', 'platform', ',', 'either', 'through', 'a', 'batch', 'process', 'or', 'manually', 'through', 'a', 'user', 'interface', '.'] 

 TOTAL TOKENS ==> 29

 ---- POST ----

 [('2.2', 'CD'), ('Content', 'JJ'), ('Ingestion', 'NNP'), ('Content', 'NNP'), ('Ingestion', 'NNP'), ('is', 'VBZ'), ('a', 'DT'), ('process', 'NN'), ('where', 'WRB'), ('content', 'NN'), ('is', 'VBZ'), ('loaded', 'VBN'), ('into', 'IN'), ('Content', 'NNP'), ('Services', 'NNPS'), ('platform', 'NN'), (',', ','), ('either', 'CC'), ('through', 'IN'), ('a', 'DT'), ('batch', 'NN'), ('process', 'NN'), ('or', 'CC'), ('manually', 'RB'), ('through', 'IN'), ('a', 'DT'), ('user', 'JJ'), ('interface', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['2.2', 'Content', 'Ingestion', 'Content', 'Ingestion', 'process', 'content', 'loaded', 'Content', 'Services', 'platform', ',', 'either', 'batch', 'process', 'manually', 'user', 'interface', '.']

 TOTAL FILTERED TOKENS ==>  19

 ---- POST FOR FILTERED TOKENS ----

 [('2.2', 'CD'), ('Content', 'JJ'), ('Ingestion', 'NNP'), ('Content', 'NNP'), ('Ingestion', 'NNP'), ('process', 'NN'), ('content', 'NN'), ('loaded', 'VBD'), ('Content', 'NNP'), ('Services', 'NNPS'), ('platform', 'NN'), (',', ','), ('either', 'CC'), ('batch', 'NN'), ('process', 'NN'), ('manually', 'RB'), ('user', 'JJ'), ('interface', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['2.2 Content', 'Content Ingestion', 'Ingestion Content', 'Content Ingestion', 'Ingestion process', 'process content', 'content loaded', 'loaded Content', 'Content Services', 'Services platform', 'platform ,', ', either', 'either batch', 'batch process', 'process manually', 'manually user', 'user interface', 'interface .'] 

 TOTAL BIGRAMS --> 18 



 ---- TRI-GRAMS ---- 

 ['2.2 Content Ingestion', 'Content Ingestion Content', 'Ingestion Content Ingestion', 'Content Ingestion process', 'Ingestion process content', 'process content loaded', 'content loaded Content', 'loaded Content Services', 'Content Services platform', 'Services platform ,', 'platform , either', ', either batch', 'either batch process', 'batch process manually', 'process manually user', 'manually user interface', 'user interface .'] 

 TOTAL TRIGRAMS --> 17 



 ---- NOUN PHRASES ---- 

 ['process', 'content', 'platform', 'batch', 'process', 'user interface'] 

 TOTAL NOUN PHRASES --> 6 



 ---- NER ----

 
 ORGANIZATION ---> ['Content Services']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['2.2', 'content', 'ingest', 'content', 'ingest', 'process', 'content', 'load', 'content', 'servic', 'platform', ',', 'either', 'batch', 'process', 'manual', 'user', 'interfac', '.']

 TOTAL PORTER STEM WORDS ==> 19



 ---- SNOWBALL STEMMING ----

['2.2', 'content', 'ingest', 'content', 'ingest', 'process', 'content', 'load', 'content', 'servic', 'platform', ',', 'either', 'batch', 'process', 'manual', 'user', 'interfac', '.']

 TOTAL SNOWBALL STEM WORDS ==> 19



 ---- LEMMATIZATION ----

['2.2', 'Content', 'Ingestion', 'Content', 'Ingestion', 'process', 'content', 'loaded', 'Content', 'Services', 'platform', ',', 'either', 'batch', 'process', 'manually', 'user', 'interface', '.']

 TOTAL LEMMATIZE WORDS ==> 19

************************************************************************************************************************

52 --> Biggest challenge here is identifying the  metadata to tag with. 


 ---- TOKENS ----

 ['Biggest', 'challenge', 'here', 'is', 'identifying', 'the', 'metadata', 'to', 'tag', 'with', '.'] 

 TOTAL TOKENS ==> 11

 ---- POST ----

 [('Biggest', 'NNP'), ('challenge', 'NN'), ('here', 'RB'), ('is', 'VBZ'), ('identifying', 'VBG'), ('the', 'DT'), ('metadata', 'NN'), ('to', 'TO'), ('tag', 'VB'), ('with', 'IN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Biggest', 'challenge', 'identifying', 'metadata', 'tag', '.']

 TOTAL FILTERED TOKENS ==>  6

 ---- POST FOR FILTERED TOKENS ----

 [('Biggest', 'NNP'), ('challenge', 'NN'), ('identifying', 'VBG'), ('metadata', 'NNS'), ('tag', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Biggest challenge', 'challenge identifying', 'identifying metadata', 'metadata tag', 'tag .'] 

 TOTAL BIGRAMS --> 5 



 ---- TRI-GRAMS ---- 

 ['Biggest challenge identifying', 'challenge identifying metadata', 'identifying metadata tag', 'metadata tag .'] 

 TOTAL TRIGRAMS --> 4 



 ---- NOUN PHRASES ---- 

 ['challenge', 'tag'] 

 TOTAL NOUN PHRASES --> 2 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> ['Biggest']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['biggest', 'challeng', 'identifi', 'metadata', 'tag', '.']

 TOTAL PORTER STEM WORDS ==> 6



 ---- SNOWBALL STEMMING ----

['biggest', 'challeng', 'identifi', 'metadata', 'tag', '.']

 TOTAL SNOWBALL STEM WORDS ==> 6



 ---- LEMMATIZATION ----

['Biggest', 'challenge', 'identifying', 'metadata', 'tag', '.']

 TOTAL LEMMATIZE WORDS ==> 6

************************************************************************************************************************

53 --> For batch process  metadata needs to be made available to  the ingestion process, through an XML,  CVS, database, etc. 


 ---- TOKENS ----

 ['For', 'batch', 'process', 'metadata', 'needs', 'to', 'be', 'made', 'available', 'to', 'the', 'ingestion', 'process', ',', 'through', 'an', 'XML', ',', 'CVS', ',', 'database', ',', 'etc', '.'] 

 TOTAL TOKENS ==> 24

 ---- POST ----

 [('For', 'IN'), ('batch', 'NN'), ('process', 'NN'), ('metadata', 'NN'), ('needs', 'NNS'), ('to', 'TO'), ('be', 'VB'), ('made', 'VBN'), ('available', 'JJ'), ('to', 'TO'), ('the', 'DT'), ('ingestion', 'NN'), ('process', 'NN'), (',', ','), ('through', 'IN'), ('an', 'DT'), ('XML', 'NNP'), (',', ','), ('CVS', 'NNP'), (',', ','), ('database', 'NN'), (',', ','), ('etc', 'FW'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['batch', 'process', 'metadata', 'needs', 'made', 'available', 'ingestion', 'process', ',', 'XML', ',', 'CVS', ',', 'database', ',', 'etc', '.']

 TOTAL FILTERED TOKENS ==>  17

 ---- POST FOR FILTERED TOKENS ----

 [('batch', 'NN'), ('process', 'NN'), ('metadata', 'NN'), ('needs', 'NNS'), ('made', 'VBN'), ('available', 'JJ'), ('ingestion', 'NN'), ('process', 'NN'), (',', ','), ('XML', 'NNP'), (',', ','), ('CVS', 'NNP'), (',', ','), ('database', 'NN'), (',', ','), ('etc', 'FW'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['batch process', 'process metadata', 'metadata needs', 'needs made', 'made available', 'available ingestion', 'ingestion process', 'process ,', ', XML', 'XML ,', ', CVS', 'CVS ,', ', database', 'database ,', ', etc', 'etc .'] 

 TOTAL BIGRAMS --> 16 



 ---- TRI-GRAMS ---- 

 ['batch process metadata', 'process metadata needs', 'metadata needs made', 'needs made available', 'made available ingestion', 'available ingestion process', 'ingestion process ,', 'process , XML', ', XML ,', 'XML , CVS', ', CVS ,', 'CVS , database', ', database ,', 'database , etc', ', etc .'] 

 TOTAL TRIGRAMS --> 15 



 ---- NOUN PHRASES ---- 

 ['batch', 'process', 'metadata', 'available ingestion', 'process', 'database'] 

 TOTAL NOUN PHRASES --> 6 



 ---- NER ----

 
 ORGANIZATION ---> ['XML', 'CVS']
 TOTAL ORGANIZATION ENTITY --> 2 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['batch', 'process', 'metadata', 'need', 'made', 'avail', 'ingest', 'process', ',', 'xml', ',', 'cv', ',', 'databas', ',', 'etc', '.']

 TOTAL PORTER STEM WORDS ==> 17



 ---- SNOWBALL STEMMING ----

['batch', 'process', 'metadata', 'need', 'made', 'avail', 'ingest', 'process', ',', 'xml', ',', 'cvs', ',', 'databas', ',', 'etc', '.']

 TOTAL SNOWBALL STEM WORDS ==> 17



 ---- LEMMATIZATION ----

['batch', 'process', 'metadata', 'need', 'made', 'available', 'ingestion', 'process', ',', 'XML', ',', 'CVS', ',', 'database', ',', 'etc', '.']

 TOTAL LEMMATIZE WORDS ==> 17

************************************************************************************************************************

54 --> For manual ingestion or  upload, user needs to think what metadata  the file should have, and then accordingly  enter the metadata on user interface. 


 ---- TOKENS ----

 ['For', 'manual', 'ingestion', 'or', 'upload', ',', 'user', 'needs', 'to', 'think', 'what', 'metadata', 'the', 'file', 'should', 'have', ',', 'and', 'then', 'accordingly', 'enter', 'the', 'metadata', 'on', 'user', 'interface', '.'] 

 TOTAL TOKENS ==> 27

 ---- POST ----

 [('For', 'IN'), ('manual', 'JJ'), ('ingestion', 'NN'), ('or', 'CC'), ('upload', 'NN'), (',', ','), ('user', 'RB'), ('needs', 'VBZ'), ('to', 'TO'), ('think', 'VB'), ('what', 'WP'), ('metadata', 'VBZ'), ('the', 'DT'), ('file', 'NN'), ('should', 'MD'), ('have', 'VB'), (',', ','), ('and', 'CC'), ('then', 'RB'), ('accordingly', 'RB'), ('enter', 'VBP'), ('the', 'DT'), ('metadata', 'NN'), ('on', 'IN'), ('user', 'JJ'), ('interface', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['manual', 'ingestion', 'upload', ',', 'user', 'needs', 'think', 'metadata', 'file', ',', 'accordingly', 'enter', 'metadata', 'user', 'interface', '.']

 TOTAL FILTERED TOKENS ==>  16

 ---- POST FOR FILTERED TOKENS ----

 [('manual', 'JJ'), ('ingestion', 'NN'), ('upload', 'NN'), (',', ','), ('user', 'JJ'), ('needs', 'NNS'), ('think', 'VBP'), ('metadata', 'NNS'), ('file', 'NN'), (',', ','), ('accordingly', 'RB'), ('enter', 'JJ'), ('metadata', 'NNS'), ('user', 'RB'), ('interface', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['manual ingestion', 'ingestion upload', 'upload ,', ', user', 'user needs', 'needs think', 'think metadata', 'metadata file', 'file ,', ', accordingly', 'accordingly enter', 'enter metadata', 'metadata user', 'user interface', 'interface .'] 

 TOTAL BIGRAMS --> 15 



 ---- TRI-GRAMS ---- 

 ['manual ingestion upload', 'ingestion upload ,', 'upload , user', ', user needs', 'user needs think', 'needs think metadata', 'think metadata file', 'metadata file ,', 'file , accordingly', ', accordingly enter', 'accordingly enter metadata', 'enter metadata user', 'metadata user interface', 'user interface .'] 

 TOTAL TRIGRAMS --> 14 



 ---- NOUN PHRASES ---- 

 ['manual ingestion', 'upload', 'file', 'interface'] 

 TOTAL NOUN PHRASES --> 4 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['manual', 'ingest', 'upload', ',', 'user', 'need', 'think', 'metadata', 'file', ',', 'accordingli', 'enter', 'metadata', 'user', 'interfac', '.']

 TOTAL PORTER STEM WORDS ==> 16



 ---- SNOWBALL STEMMING ----

['manual', 'ingest', 'upload', ',', 'user', 'need', 'think', 'metadata', 'file', ',', 'accord', 'enter', 'metadata', 'user', 'interfac', '.']

 TOTAL SNOWBALL STEM WORDS ==> 16



 ---- LEMMATIZATION ----

['manual', 'ingestion', 'upload', ',', 'user', 'need', 'think', 'metadata', 'file', ',', 'accordingly', 'enter', 'metadata', 'user', 'interface', '.']

 TOTAL LEMMATIZE WORDS ==> 16

************************************************************************************************************************

55 --> It  content, important business terms/phrases,  which helps in classifying the content to  the category, need to be listed against that  category. 


 ---- TOKENS ----

 ['It', 'content', ',', 'important', 'business', 'terms/phrases', ',', 'which', 'helps', 'in', 'classifying', 'the', 'content', 'to', 'the', 'category', ',', 'need', 'to', 'be', 'listed', 'against', 'that', 'category', '.'] 

 TOTAL TOKENS ==> 25

 ---- POST ----

 [('It', 'PRP'), ('content', 'NN'), (',', ','), ('important', 'JJ'), ('business', 'NN'), ('terms/phrases', 'NNS'), (',', ','), ('which', 'WDT'), ('helps', 'VBZ'), ('in', 'IN'), ('classifying', 'VBG'), ('the', 'DT'), ('content', 'NN'), ('to', 'TO'), ('the', 'DT'), ('category', 'NN'), (',', ','), ('need', 'VBP'), ('to', 'TO'), ('be', 'VB'), ('listed', 'VBN'), ('against', 'IN'), ('that', 'DT'), ('category', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['content', ',', 'important', 'business', 'terms/phrases', ',', 'helps', 'classifying', 'content', 'category', ',', 'need', 'listed', 'category', '.']

 TOTAL FILTERED TOKENS ==>  15

 ---- POST FOR FILTERED TOKENS ----

 [('content', 'NN'), (',', ','), ('important', 'JJ'), ('business', 'NN'), ('terms/phrases', 'NNS'), (',', ','), ('helps', 'VBZ'), ('classifying', 'VBG'), ('content', 'JJ'), ('category', 'NN'), (',', ','), ('need', 'VBP'), ('listed', 'VBN'), ('category', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['content ,', ', important', 'important business', 'business terms/phrases', 'terms/phrases ,', ', helps', 'helps classifying', 'classifying content', 'content category', 'category ,', ', need', 'need listed', 'listed category', 'category .'] 

 TOTAL BIGRAMS --> 14 



 ---- TRI-GRAMS ---- 

 ['content , important', ', important business', 'important business terms/phrases', 'business terms/phrases ,', 'terms/phrases , helps', ', helps classifying', 'helps classifying content', 'classifying content category', 'content category ,', 'category , need', ', need listed', 'need listed category', 'listed category .'] 

 TOTAL TRIGRAMS --> 13 



 ---- NOUN PHRASES ---- 

 ['content', 'important business', 'content category', 'category'] 

 TOTAL NOUN PHRASES --> 4 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['content', ',', 'import', 'busi', 'terms/phras', ',', 'help', 'classifi', 'content', 'categori', ',', 'need', 'list', 'categori', '.']

 TOTAL PORTER STEM WORDS ==> 15



 ---- SNOWBALL STEMMING ----

['content', ',', 'import', 'busi', 'terms/phras', ',', 'help', 'classifi', 'content', 'categori', ',', 'need', 'list', 'categori', '.']

 TOTAL SNOWBALL STEM WORDS ==> 15



 ---- LEMMATIZATION ----

['content', ',', 'important', 'business', 'terms/phrases', ',', 'help', 'classifying', 'content', 'category', ',', 'need', 'listed', 'category', '.']

 TOTAL LEMMATIZE WORDS ==> 15

************************************************************************************************************************

56 --> This list of category mapped to  business terms can be fed to NLP engine,  which then build a categorizer model. 


 ---- TOKENS ----

 ['This', 'list', 'of', 'category', 'mapped', 'to', 'business', 'terms', 'can', 'be', 'fed', 'to', 'NLP', 'engine', ',', 'which', 'then', 'build', 'a', 'categorizer', 'model', '.'] 

 TOTAL TOKENS ==> 22

 ---- POST ----

 [('This', 'DT'), ('list', 'NN'), ('of', 'IN'), ('category', 'NN'), ('mapped', 'VBN'), ('to', 'TO'), ('business', 'NN'), ('terms', 'NNS'), ('can', 'MD'), ('be', 'VB'), ('fed', 'VBN'), ('to', 'TO'), ('NLP', 'NNP'), ('engine', 'NN'), (',', ','), ('which', 'WDT'), ('then', 'RB'), ('build', 'VBP'), ('a', 'DT'), ('categorizer', 'NN'), ('model', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['list', 'category', 'mapped', 'business', 'terms', 'fed', 'NLP', 'engine', ',', 'build', 'categorizer', 'model', '.']

 TOTAL FILTERED TOKENS ==>  13

 ---- POST FOR FILTERED TOKENS ----

 [('list', 'NN'), ('category', 'NN'), ('mapped', 'VBD'), ('business', 'NN'), ('terms', 'NNS'), ('fed', 'VBP'), ('NLP', 'NNP'), ('engine', 'NN'), (',', ','), ('build', 'VB'), ('categorizer', 'NN'), ('model', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['list category', 'category mapped', 'mapped business', 'business terms', 'terms fed', 'fed NLP', 'NLP engine', 'engine ,', ', build', 'build categorizer', 'categorizer model', 'model .'] 

 TOTAL BIGRAMS --> 12 



 ---- TRI-GRAMS ---- 

 ['list category mapped', 'category mapped business', 'mapped business terms', 'business terms fed', 'terms fed NLP', 'fed NLP engine', 'NLP engine ,', 'engine , build', ', build categorizer', 'build categorizer model', 'categorizer model .'] 

 TOTAL TRIGRAMS --> 11 



 ---- NOUN PHRASES ---- 

 ['list', 'category', 'business', 'engine', 'categorizer', 'model'] 

 TOTAL NOUN PHRASES --> 6 



 ---- NER ----

 
 ORGANIZATION ---> ['NLP']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['list', 'categori', 'map', 'busi', 'term', 'fed', 'nlp', 'engin', ',', 'build', 'categor', 'model', '.']

 TOTAL PORTER STEM WORDS ==> 13



 ---- SNOWBALL STEMMING ----

['list', 'categori', 'map', 'busi', 'term', 'fed', 'nlp', 'engin', ',', 'build', 'categor', 'model', '.']

 TOTAL SNOWBALL STEM WORDS ==> 13



 ---- LEMMATIZATION ----

['list', 'category', 'mapped', 'business', 'term', 'fed', 'NLP', 'engine', ',', 'build', 'categorizer', 'model', '.']

 TOTAL LEMMATIZE WORDS ==> 13

************************************************************************************************************************

57 --> Based on the occurrences of the business  terms/phrases in the new or unseen  content, it can be classified into a category  (which is being mapped to a metadata  value). 


 ---- TOKENS ----

 ['Based', 'on', 'the', 'occurrences', 'of', 'the', 'business', 'terms/phrases', 'in', 'the', 'new', 'or', 'unseen', 'content', ',', 'it', 'can', 'be', 'classified', 'into', 'a', 'category', '(', 'which', 'is', 'being', 'mapped', 'to', 'a', 'metadata', 'value', ')', '.'] 

 TOTAL TOKENS ==> 33

 ---- POST ----

 [('Based', 'VBN'), ('on', 'IN'), ('the', 'DT'), ('occurrences', 'NNS'), ('of', 'IN'), ('the', 'DT'), ('business', 'NN'), ('terms/phrases', 'NNS'), ('in', 'IN'), ('the', 'DT'), ('new', 'JJ'), ('or', 'CC'), ('unseen', 'JJ'), ('content', 'NN'), (',', ','), ('it', 'PRP'), ('can', 'MD'), ('be', 'VB'), ('classified', 'VBN'), ('into', 'IN'), ('a', 'DT'), ('category', 'NN'), ('(', '('), ('which', 'WDT'), ('is', 'VBZ'), ('being', 'VBG'), ('mapped', 'VBN'), ('to', 'TO'), ('a', 'DT'), ('metadata', 'NN'), ('value', 'NN'), (')', ')'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Based', 'occurrences', 'business', 'terms/phrases', 'new', 'unseen', 'content', ',', 'classified', 'category', '(', 'mapped', 'metadata', 'value', ')', '.']

 TOTAL FILTERED TOKENS ==>  16

 ---- POST FOR FILTERED TOKENS ----

 [('Based', 'VBN'), ('occurrences', 'NNS'), ('business', 'NN'), ('terms/phrases', 'VBZ'), ('new', 'JJ'), ('unseen', 'JJ'), ('content', 'NN'), (',', ','), ('classified', 'JJ'), ('category', 'NN'), ('(', '('), ('mapped', 'VBN'), ('metadata', 'RB'), ('value', 'NN'), (')', ')'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Based occurrences', 'occurrences business', 'business terms/phrases', 'terms/phrases new', 'new unseen', 'unseen content', 'content ,', ', classified', 'classified category', 'category (', '( mapped', 'mapped metadata', 'metadata value', 'value )', ') .'] 

 TOTAL BIGRAMS --> 15 



 ---- TRI-GRAMS ---- 

 ['Based occurrences business', 'occurrences business terms/phrases', 'business terms/phrases new', 'terms/phrases new unseen', 'new unseen content', 'unseen content ,', 'content , classified', ', classified category', 'classified category (', 'category ( mapped', '( mapped metadata', 'mapped metadata value', 'metadata value )', 'value ) .'] 

 TOTAL TRIGRAMS --> 14 



 ---- NOUN PHRASES ---- 

 ['business', 'new unseen content', 'classified category'] 

 TOTAL NOUN PHRASES --> 3 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['base', 'occurr', 'busi', 'terms/phras', 'new', 'unseen', 'content', ',', 'classifi', 'categori', '(', 'map', 'metadata', 'valu', ')', '.']

 TOTAL PORTER STEM WORDS ==> 16



 ---- SNOWBALL STEMMING ----

['base', 'occurr', 'busi', 'terms/phras', 'new', 'unseen', 'content', ',', 'classifi', 'categori', '(', 'map', 'metadata', 'valu', ')', '.']

 TOTAL SNOWBALL STEM WORDS ==> 16



 ---- LEMMATIZATION ----

['Based', 'occurrence', 'business', 'terms/phrases', 'new', 'unseen', 'content', ',', 'classified', 'category', '(', 'mapped', 'metadata', 'value', ')', '.']

 TOTAL LEMMATIZE WORDS ==> 16

************************************************************************************************************************

58 --> New or unseen content is subjected to  either NER or Categorizer or both the  models. 


 ---- TOKENS ----

 ['New', 'or', 'unseen', 'content', 'is', 'subjected', 'to', 'either', 'NER', 'or', 'Categorizer', 'or', 'both', 'the', 'models', '.'] 

 TOTAL TOKENS ==> 16

 ---- POST ----

 [('New', 'NNP'), ('or', 'CC'), ('unseen', 'JJ'), ('content', 'NN'), ('is', 'VBZ'), ('subjected', 'VBN'), ('to', 'TO'), ('either', 'DT'), ('NER', 'NNP'), ('or', 'CC'), ('Categorizer', 'NNP'), ('or', 'CC'), ('both', 'CC'), ('the', 'DT'), ('models', 'NNS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['New', 'unseen', 'content', 'subjected', 'either', 'NER', 'Categorizer', 'models', '.']

 TOTAL FILTERED TOKENS ==>  9

 ---- POST FOR FILTERED TOKENS ----

 [('New', 'NNP'), ('unseen', 'JJ'), ('content', 'NN'), ('subjected', 'VBD'), ('either', 'CC'), ('NER', 'NNP'), ('Categorizer', 'NNP'), ('models', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['New unseen', 'unseen content', 'content subjected', 'subjected either', 'either NER', 'NER Categorizer', 'Categorizer models', 'models .'] 

 TOTAL BIGRAMS --> 8 



 ---- TRI-GRAMS ---- 

 ['New unseen content', 'unseen content subjected', 'content subjected either', 'subjected either NER', 'either NER Categorizer', 'NER Categorizer models', 'Categorizer models .'] 

 TOTAL TRIGRAMS --> 7 



 ---- NOUN PHRASES ---- 

 ['unseen content'] 

 TOTAL NOUN PHRASES --> 1 



 ---- NER ----

 
 ORGANIZATION ---> ['NER Categorizer']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> ['New']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['new', 'unseen', 'content', 'subject', 'either', 'ner', 'categor', 'model', '.']

 TOTAL PORTER STEM WORDS ==> 9



 ---- SNOWBALL STEMMING ----

['new', 'unseen', 'content', 'subject', 'either', 'ner', 'categor', 'model', '.']

 TOTAL SNOWBALL STEM WORDS ==> 9



 ---- LEMMATIZATION ----

['New', 'unseen', 'content', 'subjected', 'either', 'NER', 'Categorizer', 'model', '.']

 TOTAL LEMMATIZE WORDS ==> 9

************************************************************************************************************************

59 --> Use of both models will need more  time to train, will give higher accuracy and  of course, less throughput. 


 ---- TOKENS ----

 ['Use', 'of', 'both', 'models', 'will', 'need', 'more', 'time', 'to', 'train', ',', 'will', 'give', 'higher', 'accuracy', 'and', 'of', 'course', ',', 'less', 'throughput', '.'] 

 TOTAL TOKENS ==> 22

 ---- POST ----

 [('Use', 'NN'), ('of', 'IN'), ('both', 'DT'), ('models', 'NNS'), ('will', 'MD'), ('need', 'VB'), ('more', 'JJR'), ('time', 'NN'), ('to', 'TO'), ('train', 'VB'), (',', ','), ('will', 'MD'), ('give', 'VB'), ('higher', 'JJR'), ('accuracy', 'NN'), ('and', 'CC'), ('of', 'IN'), ('course', 'NN'), (',', ','), ('less', 'JJR'), ('throughput', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Use', 'models', 'need', 'time', 'train', ',', 'give', 'higher', 'accuracy', 'course', ',', 'less', 'throughput', '.']

 TOTAL FILTERED TOKENS ==>  14

 ---- POST FOR FILTERED TOKENS ----

 [('Use', 'NNP'), ('models', 'NNS'), ('need', 'VBP'), ('time', 'NN'), ('train', 'NN'), (',', ','), ('give', 'VB'), ('higher', 'JJR'), ('accuracy', 'NN'), ('course', 'NN'), (',', ','), ('less', 'JJR'), ('throughput', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Use models', 'models need', 'need time', 'time train', 'train ,', ', give', 'give higher', 'higher accuracy', 'accuracy course', 'course ,', ', less', 'less throughput', 'throughput .'] 

 TOTAL BIGRAMS --> 13 



 ---- TRI-GRAMS ---- 

 ['Use models need', 'models need time', 'need time train', 'time train ,', 'train , give', ', give higher', 'give higher accuracy', 'higher accuracy course', 'accuracy course ,', 'course , less', ', less throughput', 'less throughput .'] 

 TOTAL TRIGRAMS --> 12 



 ---- NOUN PHRASES ---- 

 ['time', 'train', 'accuracy', 'course', 'throughput'] 

 TOTAL NOUN PHRASES --> 5 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['use', 'model', 'need', 'time', 'train', ',', 'give', 'higher', 'accuraci', 'cours', ',', 'less', 'throughput', '.']

 TOTAL PORTER STEM WORDS ==> 14



 ---- SNOWBALL STEMMING ----

['use', 'model', 'need', 'time', 'train', ',', 'give', 'higher', 'accuraci', 'cours', ',', 'less', 'throughput', '.']

 TOTAL SNOWBALL STEM WORDS ==> 14



 ---- LEMMATIZATION ----

['Use', 'model', 'need', 'time', 'train', ',', 'give', 'higher', 'accuracy', 'course', ',', 'le', 'throughput', '.']

 TOTAL LEMMATIZE WORDS ==> 14

************************************************************************************************************************

60 --> With appropriate plug-ins, text from audio  and video files can be generated and  takes significant amount of time to finalize  what metadata the content should have. 


 ---- TOKENS ----

 ['With', 'appropriate', 'plug-ins', ',', 'text', 'from', 'audio', 'and', 'video', 'files', 'can', 'be', 'generated', 'and', 'takes', 'significant', 'amount', 'of', 'time', 'to', 'finalize', 'what', 'metadata', 'the', 'content', 'should', 'have', '.'] 

 TOTAL TOKENS ==> 28

 ---- POST ----

 [('With', 'IN'), ('appropriate', 'JJ'), ('plug-ins', 'NNS'), (',', ','), ('text', 'NN'), ('from', 'IN'), ('audio', 'JJ'), ('and', 'CC'), ('video', 'JJ'), ('files', 'NNS'), ('can', 'MD'), ('be', 'VB'), ('generated', 'VBN'), ('and', 'CC'), ('takes', 'VBZ'), ('significant', 'JJ'), ('amount', 'NN'), ('of', 'IN'), ('time', 'NN'), ('to', 'TO'), ('finalize', 'VB'), ('what', 'WP'), ('metadata', 'VBZ'), ('the', 'DT'), ('content', 'NN'), ('should', 'MD'), ('have', 'VB'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['appropriate', 'plug-ins', ',', 'text', 'audio', 'video', 'files', 'generated', 'takes', 'significant', 'amount', 'time', 'finalize', 'metadata', 'content', '.']

 TOTAL FILTERED TOKENS ==>  16

 ---- POST FOR FILTERED TOKENS ----

 [('appropriate', 'JJ'), ('plug-ins', 'NNS'), (',', ','), ('text', 'NN'), ('audio', 'NN'), ('video', 'NN'), ('files', 'NNS'), ('generated', 'VBD'), ('takes', 'VBZ'), ('significant', 'JJ'), ('amount', 'NN'), ('time', 'NN'), ('finalize', 'VB'), ('metadata', 'NN'), ('content', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['appropriate plug-ins', 'plug-ins ,', ', text', 'text audio', 'audio video', 'video files', 'files generated', 'generated takes', 'takes significant', 'significant amount', 'amount time', 'time finalize', 'finalize metadata', 'metadata content', 'content .'] 

 TOTAL BIGRAMS --> 15 



 ---- TRI-GRAMS ---- 

 ['appropriate plug-ins ,', 'plug-ins , text', ', text audio', 'text audio video', 'audio video files', 'video files generated', 'files generated takes', 'generated takes significant', 'takes significant amount', 'significant amount time', 'amount time finalize', 'time finalize metadata', 'finalize metadata content', 'metadata content .'] 

 TOTAL TRIGRAMS --> 14 



 ---- NOUN PHRASES ---- 

 ['text', 'audio', 'video', 'significant amount', 'time', 'metadata', 'content'] 

 TOTAL NOUN PHRASES --> 7 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['appropri', 'plug-in', ',', 'text', 'audio', 'video', 'file', 'gener', 'take', 'signific', 'amount', 'time', 'final', 'metadata', 'content', '.']

 TOTAL PORTER STEM WORDS ==> 16



 ---- SNOWBALL STEMMING ----

['appropri', 'plug-in', ',', 'text', 'audio', 'video', 'file', 'generat', 'take', 'signific', 'amount', 'time', 'final', 'metadata', 'content', '.']

 TOTAL SNOWBALL STEM WORDS ==> 16



 ---- LEMMATIZATION ----

['appropriate', 'plug-in', ',', 'text', 'audio', 'video', 'file', 'generated', 'take', 'significant', 'amount', 'time', 'finalize', 'metadata', 'content', '.']

 TOTAL LEMMATIZE WORDS ==> 16

************************************************************************************************************************

61 --> This manual process is also prone to  mistakes. 


 ---- TOKENS ----

 ['This', 'manual', 'process', 'is', 'also', 'prone', 'to', 'mistakes', '.'] 

 TOTAL TOKENS ==> 9

 ---- POST ----

 [('This', 'DT'), ('manual', 'JJ'), ('process', 'NN'), ('is', 'VBZ'), ('also', 'RB'), ('prone', 'JJ'), ('to', 'TO'), ('mistakes', 'NNS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['manual', 'process', 'also', 'prone', 'mistakes', '.']

 TOTAL FILTERED TOKENS ==>  6

 ---- POST FOR FILTERED TOKENS ----

 [('manual', 'JJ'), ('process', 'NN'), ('also', 'RB'), ('prone', 'JJ'), ('mistakes', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['manual process', 'process also', 'also prone', 'prone mistakes', 'mistakes .'] 

 TOTAL BIGRAMS --> 5 



 ---- TRI-GRAMS ---- 

 ['manual process also', 'process also prone', 'also prone mistakes', 'prone mistakes .'] 

 TOTAL TRIGRAMS --> 4 



 ---- NOUN PHRASES ---- 

 ['manual process'] 

 TOTAL NOUN PHRASES --> 1 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['manual', 'process', 'also', 'prone', 'mistak', '.']

 TOTAL PORTER STEM WORDS ==> 6



 ---- SNOWBALL STEMMING ----

['manual', 'process', 'also', 'prone', 'mistak', '.']

 TOTAL SNOWBALL STEM WORDS ==> 6



 ---- LEMMATIZATION ----

['manual', 'process', 'also', 'prone', 'mistake', '.']

 TOTAL LEMMATIZE WORDS ==> 6

************************************************************************************************************************

62 --> NLP can help here by cognitively tagging  the metadata to the content during  ingestion. 


 ---- TOKENS ----

 ['NLP', 'can', 'help', 'here', 'by', 'cognitively', 'tagging', 'the', 'metadata', 'to', 'the', 'content', 'during', 'ingestion', '.'] 

 TOTAL TOKENS ==> 15

 ---- POST ----

 [('NLP', 'NNP'), ('can', 'MD'), ('help', 'VB'), ('here', 'RB'), ('by', 'IN'), ('cognitively', 'RB'), ('tagging', 'VBG'), ('the', 'DT'), ('metadata', 'NN'), ('to', 'TO'), ('the', 'DT'), ('content', 'NN'), ('during', 'IN'), ('ingestion', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['NLP', 'help', 'cognitively', 'tagging', 'metadata', 'content', 'ingestion', '.']

 TOTAL FILTERED TOKENS ==>  8

 ---- POST FOR FILTERED TOKENS ----

 [('NLP', 'NNP'), ('help', 'NN'), ('cognitively', 'RB'), ('tagging', 'VBG'), ('metadata', 'NN'), ('content', 'NN'), ('ingestion', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['NLP help', 'help cognitively', 'cognitively tagging', 'tagging metadata', 'metadata content', 'content ingestion', 'ingestion .'] 

 TOTAL BIGRAMS --> 7 



 ---- TRI-GRAMS ---- 

 ['NLP help cognitively', 'help cognitively tagging', 'cognitively tagging metadata', 'tagging metadata content', 'metadata content ingestion', 'content ingestion .'] 

 TOTAL TRIGRAMS --> 6 



 ---- NOUN PHRASES ---- 

 ['help', 'metadata', 'content', 'ingestion'] 

 TOTAL NOUN PHRASES --> 4 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['nlp', 'help', 'cognit', 'tag', 'metadata', 'content', 'ingest', '.']

 TOTAL PORTER STEM WORDS ==> 8



 ---- SNOWBALL STEMMING ----

['nlp', 'help', 'cognit', 'tag', 'metadata', 'content', 'ingest', '.']

 TOTAL SNOWBALL STEM WORDS ==> 8



 ---- LEMMATIZATION ----

['NLP', 'help', 'cognitively', 'tagging', 'metadata', 'content', 'ingestion', '.']

 TOTAL LEMMATIZE WORDS ==> 8

************************************************************************************************************************

63 --> NLP engine can be trained  to identify the metadata to be tagged,  based on the context of the content. 


 ---- TOKENS ----

 ['NLP', 'engine', 'can', 'be', 'trained', 'to', 'identify', 'the', 'metadata', 'to', 'be', 'tagged', ',', 'based', 'on', 'the', 'context', 'of', 'the', 'content', '.'] 

 TOTAL TOKENS ==> 21

 ---- POST ----

 [('NLP', 'NNP'), ('engine', 'NN'), ('can', 'MD'), ('be', 'VB'), ('trained', 'VBN'), ('to', 'TO'), ('identify', 'VB'), ('the', 'DT'), ('metadata', 'NN'), ('to', 'TO'), ('be', 'VB'), ('tagged', 'VBN'), (',', ','), ('based', 'VBN'), ('on', 'IN'), ('the', 'DT'), ('context', 'NN'), ('of', 'IN'), ('the', 'DT'), ('content', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['NLP', 'engine', 'trained', 'identify', 'metadata', 'tagged', ',', 'based', 'context', 'content', '.']

 TOTAL FILTERED TOKENS ==>  11

 ---- POST FOR FILTERED TOKENS ----

 [('NLP', 'NNP'), ('engine', 'NN'), ('trained', 'VBD'), ('identify', 'JJ'), ('metadata', 'NNS'), ('tagged', 'VBD'), (',', ','), ('based', 'VBN'), ('context', 'NN'), ('content', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['NLP engine', 'engine trained', 'trained identify', 'identify metadata', 'metadata tagged', 'tagged ,', ', based', 'based context', 'context content', 'content .'] 

 TOTAL BIGRAMS --> 10 



 ---- TRI-GRAMS ---- 

 ['NLP engine trained', 'engine trained identify', 'trained identify metadata', 'identify metadata tagged', 'metadata tagged ,', 'tagged , based', ', based context', 'based context content', 'context content .'] 

 TOTAL TRIGRAMS --> 9 



 ---- NOUN PHRASES ---- 

 ['engine', 'context', 'content'] 

 TOTAL NOUN PHRASES --> 3 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['nlp', 'engin', 'train', 'identifi', 'metadata', 'tag', ',', 'base', 'context', 'content', '.']

 TOTAL PORTER STEM WORDS ==> 11



 ---- SNOWBALL STEMMING ----

['nlp', 'engin', 'train', 'identifi', 'metadata', 'tag', ',', 'base', 'context', 'content', '.']

 TOTAL SNOWBALL STEM WORDS ==> 11



 ---- LEMMATIZATION ----

['NLP', 'engine', 'trained', 'identify', 'metadata', 'tagged', ',', 'based', 'context', 'content', '.']

 TOTAL LEMMATIZE WORDS ==> 11

************************************************************************************************************************

64 --> The  identified metadata can be tagged with  the content during ingestion. 


 ---- TOKENS ----

 ['The', 'identified', 'metadata', 'can', 'be', 'tagged', 'with', 'the', 'content', 'during', 'ingestion', '.'] 

 TOTAL TOKENS ==> 12

 ---- POST ----

 [('The', 'DT'), ('identified', 'VBN'), ('metadata', 'NN'), ('can', 'MD'), ('be', 'VB'), ('tagged', 'VBN'), ('with', 'IN'), ('the', 'DT'), ('content', 'NN'), ('during', 'IN'), ('ingestion', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['identified', 'metadata', 'tagged', 'content', 'ingestion', '.']

 TOTAL FILTERED TOKENS ==>  6

 ---- POST FOR FILTERED TOKENS ----

 [('identified', 'VBN'), ('metadata', 'NNS'), ('tagged', 'VBD'), ('content', 'JJ'), ('ingestion', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['identified metadata', 'metadata tagged', 'tagged content', 'content ingestion', 'ingestion .'] 

 TOTAL BIGRAMS --> 5 



 ---- TRI-GRAMS ---- 

 ['identified metadata tagged', 'metadata tagged content', 'tagged content ingestion', 'content ingestion .'] 

 TOTAL TRIGRAMS --> 4 



 ---- NOUN PHRASES ---- 

 ['content ingestion'] 

 TOTAL NOUN PHRASES --> 1 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['identifi', 'metadata', 'tag', 'content', 'ingest', '.']

 TOTAL PORTER STEM WORDS ==> 6



 ---- SNOWBALL STEMMING ----

['identifi', 'metadata', 'tag', 'content', 'ingest', '.']

 TOTAL SNOWBALL STEM WORDS ==> 6



 ---- LEMMATIZATION ----

['identified', 'metadata', 'tagged', 'content', 'ingestion', '.']

 TOTAL LEMMATIZE WORDS ==> 6

************************************************************************************************************************

65 --> This can be  illustrated by following diagram: subjected to the AI models for metadata  identification and then ingested into the  Content Services Platform. 


 ---- TOKENS ----

 ['This', 'can', 'be', 'illustrated', 'by', 'following', 'diagram', ':', 'subjected', 'to', 'the', 'AI', 'models', 'for', 'metadata', 'identification', 'and', 'then', 'ingested', 'into', 'the', 'Content', 'Services', 'Platform', '.'] 

 TOTAL TOKENS ==> 25

 ---- POST ----

 [('This', 'DT'), ('can', 'MD'), ('be', 'VB'), ('illustrated', 'VBN'), ('by', 'IN'), ('following', 'VBG'), ('diagram', 'NN'), (':', ':'), ('subjected', 'VBN'), ('to', 'TO'), ('the', 'DT'), ('AI', 'NNP'), ('models', 'NNS'), ('for', 'IN'), ('metadata', 'JJ'), ('identification', 'NN'), ('and', 'CC'), ('then', 'RB'), ('ingested', 'VBD'), ('into', 'IN'), ('the', 'DT'), ('Content', 'NNP'), ('Services', 'NNPS'), ('Platform', 'NNP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['illustrated', 'following', 'diagram', ':', 'subjected', 'AI', 'models', 'metadata', 'identification', 'ingested', 'Content', 'Services', 'Platform', '.']

 TOTAL FILTERED TOKENS ==>  14

 ---- POST FOR FILTERED TOKENS ----

 [('illustrated', 'VBN'), ('following', 'VBG'), ('diagram', 'NN'), (':', ':'), ('subjected', 'VBN'), ('AI', 'NNP'), ('models', 'NNS'), ('metadata', 'VBP'), ('identification', 'NN'), ('ingested', 'VBN'), ('Content', 'NNP'), ('Services', 'NNPS'), ('Platform', 'NNP'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['illustrated following', 'following diagram', 'diagram :', ': subjected', 'subjected AI', 'AI models', 'models metadata', 'metadata identification', 'identification ingested', 'ingested Content', 'Content Services', 'Services Platform', 'Platform .'] 

 TOTAL BIGRAMS --> 13 



 ---- TRI-GRAMS ---- 

 ['illustrated following diagram', 'following diagram :', 'diagram : subjected', ': subjected AI', 'subjected AI models', 'AI models metadata', 'models metadata identification', 'metadata identification ingested', 'identification ingested Content', 'ingested Content Services', 'Content Services Platform', 'Services Platform .'] 

 TOTAL TRIGRAMS --> 12 



 ---- NOUN PHRASES ---- 

 ['diagram', 'identification'] 

 TOTAL NOUN PHRASES --> 2 



 ---- NER ----

 
 ORGANIZATION ---> ['Content Services Platform']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['illustr', 'follow', 'diagram', ':', 'subject', 'ai', 'model', 'metadata', 'identif', 'ingest', 'content', 'servic', 'platform', '.']

 TOTAL PORTER STEM WORDS ==> 14



 ---- SNOWBALL STEMMING ----

['illustr', 'follow', 'diagram', ':', 'subject', 'ai', 'model', 'metadata', 'identif', 'ingest', 'content', 'servic', 'platform', '.']

 TOTAL SNOWBALL STEM WORDS ==> 14



 ---- LEMMATIZATION ----

['illustrated', 'following', 'diagram', ':', 'subjected', 'AI', 'model', 'metadata', 'identification', 'ingested', 'Content', 'Services', 'Platform', '.']

 TOTAL LEMMATIZE WORDS ==> 14

************************************************************************************************************************

66 --> 2.2.1 Implementations scenarios Described below are the different  implementation scenarios in the context of  cognitive content ingestion. 


 ---- TOKENS ----

 ['2.2.1', 'Implementations', 'scenarios', 'Described', 'below', 'are', 'the', 'different', 'implementation', 'scenarios', 'in', 'the', 'context', 'of', 'cognitive', 'content', 'ingestion', '.'] 

 TOTAL TOKENS ==> 18

 ---- POST ----

 [('2.2.1', 'CD'), ('Implementations', 'NNS'), ('scenarios', 'NNS'), ('Described', 'NNP'), ('below', 'IN'), ('are', 'VBP'), ('the', 'DT'), ('different', 'JJ'), ('implementation', 'NN'), ('scenarios', 'NNS'), ('in', 'IN'), ('the', 'DT'), ('context', 'NN'), ('of', 'IN'), ('cognitive', 'JJ'), ('content', 'NN'), ('ingestion', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['2.2.1', 'Implementations', 'scenarios', 'Described', 'different', 'implementation', 'scenarios', 'context', 'cognitive', 'content', 'ingestion', '.']

 TOTAL FILTERED TOKENS ==>  12

 ---- POST FOR FILTERED TOKENS ----

 [('2.2.1', 'CD'), ('Implementations', 'NNS'), ('scenarios', 'NNS'), ('Described', 'NNP'), ('different', 'JJ'), ('implementation', 'NN'), ('scenarios', 'NNS'), ('context', 'VBP'), ('cognitive', 'JJ'), ('content', 'NN'), ('ingestion', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['2.2.1 Implementations', 'Implementations scenarios', 'scenarios Described', 'Described different', 'different implementation', 'implementation scenarios', 'scenarios context', 'context cognitive', 'cognitive content', 'content ingestion', 'ingestion .'] 

 TOTAL BIGRAMS --> 11 



 ---- TRI-GRAMS ---- 

 ['2.2.1 Implementations scenarios', 'Implementations scenarios Described', 'scenarios Described different', 'Described different implementation', 'different implementation scenarios', 'implementation scenarios context', 'scenarios context cognitive', 'context cognitive content', 'cognitive content ingestion', 'content ingestion .'] 

 TOTAL TRIGRAMS --> 10 



 ---- NOUN PHRASES ---- 

 ['different implementation', 'cognitive content', 'ingestion'] 

 TOTAL NOUN PHRASES --> 3 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['2.2.1', 'implement', 'scenario', 'describ', 'differ', 'implement', 'scenario', 'context', 'cognit', 'content', 'ingest', '.']

 TOTAL PORTER STEM WORDS ==> 12



 ---- SNOWBALL STEMMING ----

['2.2.1', 'implement', 'scenario', 'describ', 'differ', 'implement', 'scenario', 'context', 'cognit', 'content', 'ingest', '.']

 TOTAL SNOWBALL STEM WORDS ==> 12



 ---- LEMMATIZATION ----

['2.2.1', 'Implementations', 'scenario', 'Described', 'different', 'implementation', 'scenario', 'context', 'cognitive', 'content', 'ingestion', '.']

 TOTAL LEMMATIZE WORDS ==> 12

************************************************************************************************************************

67 --> 2.2.1.1 During Batch Ingestion In any batch ingestion process, appropriate  metadata values are needed to complete  the process. 


 ---- TOKENS ----

 ['2.2.1.1', 'During', 'Batch', 'Ingestion', 'In', 'any', 'batch', 'ingestion', 'process', ',', 'appropriate', 'metadata', 'values', 'are', 'needed', 'to', 'complete', 'the', 'process', '.'] 

 TOTAL TOKENS ==> 20

 ---- POST ----

 [('2.2.1.1', 'CD'), ('During', 'IN'), ('Batch', 'NNP'), ('Ingestion', 'NNP'), ('In', 'IN'), ('any', 'DT'), ('batch', 'NN'), ('ingestion', 'NN'), ('process', 'NN'), (',', ','), ('appropriate', 'JJ'), ('metadata', 'NN'), ('values', 'NNS'), ('are', 'VBP'), ('needed', 'VBN'), ('to', 'TO'), ('complete', 'VB'), ('the', 'DT'), ('process', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['2.2.1.1', 'Batch', 'Ingestion', 'batch', 'ingestion', 'process', ',', 'appropriate', 'metadata', 'values', 'needed', 'complete', 'process', '.']

 TOTAL FILTERED TOKENS ==>  14

 ---- POST FOR FILTERED TOKENS ----

 [('2.2.1.1', 'CD'), ('Batch', 'NNP'), ('Ingestion', 'NNP'), ('batch', 'NN'), ('ingestion', 'NN'), ('process', 'NN'), (',', ','), ('appropriate', 'JJ'), ('metadata', 'NNS'), ('values', 'NNS'), ('needed', 'VBD'), ('complete', 'JJ'), ('process', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['2.2.1.1 Batch', 'Batch Ingestion', 'Ingestion batch', 'batch ingestion', 'ingestion process', 'process ,', ', appropriate', 'appropriate metadata', 'metadata values', 'values needed', 'needed complete', 'complete process', 'process .'] 

 TOTAL BIGRAMS --> 13 



 ---- TRI-GRAMS ---- 

 ['2.2.1.1 Batch Ingestion', 'Batch Ingestion batch', 'Ingestion batch ingestion', 'batch ingestion process', 'ingestion process ,', 'process , appropriate', ', appropriate metadata', 'appropriate metadata values', 'metadata values needed', 'values needed complete', 'needed complete process', 'complete process .'] 

 TOTAL TRIGRAMS --> 12 



 ---- NOUN PHRASES ---- 

 ['batch', 'ingestion', 'process', 'complete process'] 

 TOTAL NOUN PHRASES --> 4 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> ['Batch Ingestion']
 TOTAL PERSON ENTITY --> 1 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['2.2.1.1', 'batch', 'ingest', 'batch', 'ingest', 'process', ',', 'appropri', 'metadata', 'valu', 'need', 'complet', 'process', '.']

 TOTAL PORTER STEM WORDS ==> 14



 ---- SNOWBALL STEMMING ----

['2.2.1.1', 'batch', 'ingest', 'batch', 'ingest', 'process', ',', 'appropri', 'metadata', 'valu', 'need', 'complet', 'process', '.']

 TOTAL SNOWBALL STEM WORDS ==> 14



 ---- LEMMATIZATION ----

['2.2.1.1', 'Batch', 'Ingestion', 'batch', 'ingestion', 'process', ',', 'appropriate', 'metadata', 'value', 'needed', 'complete', 'process', '.']

 TOTAL LEMMATIZE WORDS ==> 14

************************************************************************************************************************

68 --> One of the major challenges is  the non-availability of the metadata values,  which are being expected by Content  Service Platform. 


 ---- TOKENS ----

 ['One', 'of', 'the', 'major', 'challenges', 'is', 'the', 'non-availability', 'of', 'the', 'metadata', 'values', ',', 'which', 'are', 'being', 'expected', 'by', 'Content', 'Service', 'Platform', '.'] 

 TOTAL TOKENS ==> 22

 ---- POST ----

 [('One', 'CD'), ('of', 'IN'), ('the', 'DT'), ('major', 'JJ'), ('challenges', 'NNS'), ('is', 'VBZ'), ('the', 'DT'), ('non-availability', 'NN'), ('of', 'IN'), ('the', 'DT'), ('metadata', 'NN'), ('values', 'NNS'), (',', ','), ('which', 'WDT'), ('are', 'VBP'), ('being', 'VBG'), ('expected', 'VBN'), ('by', 'IN'), ('Content', 'NNP'), ('Service', 'NNP'), ('Platform', 'NNP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['One', 'major', 'challenges', 'non-availability', 'metadata', 'values', ',', 'expected', 'Content', 'Service', 'Platform', '.']

 TOTAL FILTERED TOKENS ==>  12

 ---- POST FOR FILTERED TOKENS ----

 [('One', 'CD'), ('major', 'JJ'), ('challenges', 'NNS'), ('non-availability', 'JJ'), ('metadata', 'NN'), ('values', 'NNS'), (',', ','), ('expected', 'VBN'), ('Content', 'NNP'), ('Service', 'NNP'), ('Platform', 'NNP'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['One major', 'major challenges', 'challenges non-availability', 'non-availability metadata', 'metadata values', 'values ,', ', expected', 'expected Content', 'Content Service', 'Service Platform', 'Platform .'] 

 TOTAL BIGRAMS --> 11 



 ---- TRI-GRAMS ---- 

 ['One major challenges', 'major challenges non-availability', 'challenges non-availability metadata', 'non-availability metadata values', 'metadata values ,', 'values , expected', ', expected Content', 'expected Content Service', 'Content Service Platform', 'Service Platform .'] 

 TOTAL TRIGRAMS --> 10 



 ---- NOUN PHRASES ---- 

 ['non-availability metadata'] 

 TOTAL NOUN PHRASES --> 1 



 ---- NER ----

 
 ORGANIZATION ---> ['Content Service Platform']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['one', 'major', 'challeng', 'non-avail', 'metadata', 'valu', ',', 'expect', 'content', 'servic', 'platform', '.']

 TOTAL PORTER STEM WORDS ==> 12



 ---- SNOWBALL STEMMING ----

['one', 'major', 'challeng', 'non-avail', 'metadata', 'valu', ',', 'expect', 'content', 'servic', 'platform', '.']

 TOTAL SNOWBALL STEM WORDS ==> 12



 ---- LEMMATIZATION ----

['One', 'major', 'challenge', 'non-availability', 'metadata', 'value', ',', 'expected', 'Content', 'Service', 'Platform', '.']

 TOTAL LEMMATIZE WORDS ==> 12

************************************************************************************************************************

69 --> NLP can help in bridging  the expectation, by extracting context  based values. 


 ---- TOKENS ----

 ['NLP', 'can', 'help', 'in', 'bridging', 'the', 'expectation', ',', 'by', 'extracting', 'context', 'based', 'values', '.'] 

 TOTAL TOKENS ==> 14

 ---- POST ----

 [('NLP', 'NNP'), ('can', 'MD'), ('help', 'VB'), ('in', 'IN'), ('bridging', 'VBG'), ('the', 'DT'), ('expectation', 'NN'), (',', ','), ('by', 'IN'), ('extracting', 'VBG'), ('context', 'NN'), ('based', 'VBN'), ('values', 'NNS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['NLP', 'help', 'bridging', 'expectation', ',', 'extracting', 'context', 'based', 'values', '.']

 TOTAL FILTERED TOKENS ==>  10

 ---- POST FOR FILTERED TOKENS ----

 [('NLP', 'NNP'), ('help', 'NN'), ('bridging', 'VBG'), ('expectation', 'NN'), (',', ','), ('extracting', 'VBG'), ('context', 'NN'), ('based', 'VBN'), ('values', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['NLP help', 'help bridging', 'bridging expectation', 'expectation ,', ', extracting', 'extracting context', 'context based', 'based values', 'values .'] 

 TOTAL BIGRAMS --> 9 



 ---- TRI-GRAMS ---- 

 ['NLP help bridging', 'help bridging expectation', 'bridging expectation ,', 'expectation , extracting', ', extracting context', 'extracting context based', 'context based values', 'based values .'] 

 TOTAL TRIGRAMS --> 8 



 ---- NOUN PHRASES ---- 

 ['help', 'expectation', 'context'] 

 TOTAL NOUN PHRASES --> 3 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['nlp', 'help', 'bridg', 'expect', ',', 'extract', 'context', 'base', 'valu', '.']

 TOTAL PORTER STEM WORDS ==> 10



 ---- SNOWBALL STEMMING ----

['nlp', 'help', 'bridg', 'expect', ',', 'extract', 'context', 'base', 'valu', '.']

 TOTAL SNOWBALL STEM WORDS ==> 10



 ---- LEMMATIZATION ----

['NLP', 'help', 'bridging', 'expectation', ',', 'extracting', 'context', 'based', 'value', '.']

 TOTAL LEMMATIZE WORDS ==> 10

************************************************************************************************************************

70 --> AI models can be trained  which can identify the metadata values as  External Document © 2019 Infosys Limited External Document © 2019 Infosys Limited  a part of ingestion process and then load  the content with appropriate metadata  values. 


 ---- TOKENS ----

 ['AI', 'models', 'can', 'be', 'trained', 'which', 'can', 'identify', 'the', 'metadata', 'values', 'as', 'External', 'Document', '©', '2019', 'Infosys', 'Limited', 'External', 'Document', '©', '2019', 'Infosys', 'Limited', 'a', 'part', 'of', 'ingestion', 'process', 'and', 'then', 'load', 'the', 'content', 'with', 'appropriate', 'metadata', 'values', '.'] 

 TOTAL TOKENS ==> 39

 ---- POST ----

 [('AI', 'NNP'), ('models', 'NNS'), ('can', 'MD'), ('be', 'VB'), ('trained', 'VBN'), ('which', 'WDT'), ('can', 'MD'), ('identify', 'VB'), ('the', 'DT'), ('metadata', 'NN'), ('values', 'NNS'), ('as', 'IN'), ('External', 'NNP'), ('Document', 'NNP'), ('©', 'NN'), ('2019', 'CD'), ('Infosys', 'NNP'), ('Limited', 'NNP'), ('External', 'NNP'), ('Document', 'NNP'), ('©', 'NN'), ('2019', 'CD'), ('Infosys', 'NNP'), ('Limited', 'VBD'), ('a', 'DT'), ('part', 'NN'), ('of', 'IN'), ('ingestion', 'NN'), ('process', 'NN'), ('and', 'CC'), ('then', 'RB'), ('load', 'VB'), ('the', 'DT'), ('content', 'NN'), ('with', 'IN'), ('appropriate', 'JJ'), ('metadata', 'NNS'), ('values', 'NNS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['AI', 'models', 'trained', 'identify', 'metadata', 'values', 'External', 'Document', '©', '2019', 'Infosys', 'Limited', 'External', 'Document', '©', '2019', 'Infosys', 'Limited', 'part', 'ingestion', 'process', 'load', 'content', 'appropriate', 'metadata', 'values', '.']

 TOTAL FILTERED TOKENS ==>  27

 ---- POST FOR FILTERED TOKENS ----

 [('AI', 'NNP'), ('models', 'NNS'), ('trained', 'VBD'), ('identify', 'JJ'), ('metadata', 'NN'), ('values', 'NNS'), ('External', 'NNP'), ('Document', 'NNP'), ('©', 'NN'), ('2019', 'CD'), ('Infosys', 'NNP'), ('Limited', 'NNP'), ('External', 'NNP'), ('Document', 'NNP'), ('©', 'NN'), ('2019', 'CD'), ('Infosys', 'NNP'), ('Limited', 'NNP'), ('part', 'NN'), ('ingestion', 'NN'), ('process', 'NN'), ('load', 'NN'), ('content', 'NN'), ('appropriate', 'JJ'), ('metadata', 'NN'), ('values', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['AI models', 'models trained', 'trained identify', 'identify metadata', 'metadata values', 'values External', 'External Document', 'Document ©', '© 2019', '2019 Infosys', 'Infosys Limited', 'Limited External', 'External Document', 'Document ©', '© 2019', '2019 Infosys', 'Infosys Limited', 'Limited part', 'part ingestion', 'ingestion process', 'process load', 'load content', 'content appropriate', 'appropriate metadata', 'metadata values', 'values .'] 

 TOTAL BIGRAMS --> 26 



 ---- TRI-GRAMS ---- 

 ['AI models trained', 'models trained identify', 'trained identify metadata', 'identify metadata values', 'metadata values External', 'values External Document', 'External Document ©', 'Document © 2019', '© 2019 Infosys', '2019 Infosys Limited', 'Infosys Limited External', 'Limited External Document', 'External Document ©', 'Document © 2019', '© 2019 Infosys', '2019 Infosys Limited', 'Infosys Limited part', 'Limited part ingestion', 'part ingestion process', 'ingestion process load', 'process load content', 'load content appropriate', 'content appropriate metadata', 'appropriate metadata values', 'metadata values .'] 

 TOTAL TRIGRAMS --> 25 



 ---- NOUN PHRASES ---- 

 ['identify metadata', '©', '©', 'part', 'ingestion', 'process', 'load', 'content', 'appropriate metadata'] 

 TOTAL NOUN PHRASES --> 9 



 ---- NER ----

 
 ORGANIZATION ---> ['External Document']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['ai', 'model', 'train', 'identifi', 'metadata', 'valu', 'extern', 'document', '©', '2019', 'infosi', 'limit', 'extern', 'document', '©', '2019', 'infosi', 'limit', 'part', 'ingest', 'process', 'load', 'content', 'appropri', 'metadata', 'valu', '.']

 TOTAL PORTER STEM WORDS ==> 27



 ---- SNOWBALL STEMMING ----

['ai', 'model', 'train', 'identifi', 'metadata', 'valu', 'extern', 'document', '©', '2019', 'infosi', 'limit', 'extern', 'document', '©', '2019', 'infosi', 'limit', 'part', 'ingest', 'process', 'load', 'content', 'appropri', 'metadata', 'valu', '.']

 TOTAL SNOWBALL STEM WORDS ==> 27



 ---- LEMMATIZATION ----

['AI', 'model', 'trained', 'identify', 'metadata', 'value', 'External', 'Document', '©', '2019', 'Infosys', 'Limited', 'External', 'Document', '©', '2019', 'Infosys', 'Limited', 'part', 'ingestion', 'process', 'load', 'content', 'appropriate', 'metadata', 'value', '.']

 TOTAL LEMMATIZE WORDS ==> 27

************************************************************************************************************************

71 --> With NLP tool, as a part of ingestion  pipeline, content from sources where the  metadata values are not available, can be  ingested into Content Services Platform. 


 ---- TOKENS ----

 ['With', 'NLP', 'tool', ',', 'as', 'a', 'part', 'of', 'ingestion', 'pipeline', ',', 'content', 'from', 'sources', 'where', 'the', 'metadata', 'values', 'are', 'not', 'available', ',', 'can', 'be', 'ingested', 'into', 'Content', 'Services', 'Platform', '.'] 

 TOTAL TOKENS ==> 30

 ---- POST ----

 [('With', 'IN'), ('NLP', 'NNP'), ('tool', 'NN'), (',', ','), ('as', 'IN'), ('a', 'DT'), ('part', 'NN'), ('of', 'IN'), ('ingestion', 'NN'), ('pipeline', 'NN'), (',', ','), ('content', 'NN'), ('from', 'IN'), ('sources', 'NNS'), ('where', 'WRB'), ('the', 'DT'), ('metadata', 'NN'), ('values', 'NNS'), ('are', 'VBP'), ('not', 'RB'), ('available', 'JJ'), (',', ','), ('can', 'MD'), ('be', 'VB'), ('ingested', 'VBN'), ('into', 'IN'), ('Content', 'NNP'), ('Services', 'NNPS'), ('Platform', 'NNP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['NLP', 'tool', ',', 'part', 'ingestion', 'pipeline', ',', 'content', 'sources', 'metadata', 'values', 'available', ',', 'ingested', 'Content', 'Services', 'Platform', '.']

 TOTAL FILTERED TOKENS ==>  18

 ---- POST FOR FILTERED TOKENS ----

 [('NLP', 'NNP'), ('tool', 'NN'), (',', ','), ('part', 'NN'), ('ingestion', 'NN'), ('pipeline', 'NN'), (',', ','), ('content', 'JJ'), ('sources', 'NNS'), ('metadata', 'VBP'), ('values', 'NNS'), ('available', 'JJ'), (',', ','), ('ingested', 'JJ'), ('Content', 'NNP'), ('Services', 'NNPS'), ('Platform', 'NNP'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['NLP tool', 'tool ,', ', part', 'part ingestion', 'ingestion pipeline', 'pipeline ,', ', content', 'content sources', 'sources metadata', 'metadata values', 'values available', 'available ,', ', ingested', 'ingested Content', 'Content Services', 'Services Platform', 'Platform .'] 

 TOTAL BIGRAMS --> 17 



 ---- TRI-GRAMS ---- 

 ['NLP tool ,', 'tool , part', ', part ingestion', 'part ingestion pipeline', 'ingestion pipeline ,', 'pipeline , content', ', content sources', 'content sources metadata', 'sources metadata values', 'metadata values available', 'values available ,', 'available , ingested', ', ingested Content', 'ingested Content Services', 'Content Services Platform', 'Services Platform .'] 

 TOTAL TRIGRAMS --> 16 



 ---- NOUN PHRASES ---- 

 ['tool', 'part', 'ingestion', 'pipeline'] 

 TOTAL NOUN PHRASES --> 4 



 ---- NER ----

 
 ORGANIZATION ---> ['Content Services Platform']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['nlp', 'tool', ',', 'part', 'ingest', 'pipelin', ',', 'content', 'sourc', 'metadata', 'valu', 'avail', ',', 'ingest', 'content', 'servic', 'platform', '.']

 TOTAL PORTER STEM WORDS ==> 18



 ---- SNOWBALL STEMMING ----

['nlp', 'tool', ',', 'part', 'ingest', 'pipelin', ',', 'content', 'sourc', 'metadata', 'valu', 'avail', ',', 'ingest', 'content', 'servic', 'platform', '.']

 TOTAL SNOWBALL STEM WORDS ==> 18



 ---- LEMMATIZATION ----

['NLP', 'tool', ',', 'part', 'ingestion', 'pipeline', ',', 'content', 'source', 'metadata', 'value', 'available', ',', 'ingested', 'Content', 'Services', 'Platform', '.']

 TOTAL LEMMATIZE WORDS ==> 18

************************************************************************************************************************

72 --> 2.2.1.2 Manual content upload When users upload content manually into  Content Services Platform, using a user  interface, they have to enter metadata  values. 


 ---- TOKENS ----

 ['2.2.1.2', 'Manual', 'content', 'upload', 'When', 'users', 'upload', 'content', 'manually', 'into', 'Content', 'Services', 'Platform', ',', 'using', 'a', 'user', 'interface', ',', 'they', 'have', 'to', 'enter', 'metadata', 'values', '.'] 

 TOTAL TOKENS ==> 26

 ---- POST ----

 [('2.2.1.2', 'CD'), ('Manual', 'JJ'), ('content', 'NN'), ('upload', 'NN'), ('When', 'WRB'), ('users', 'NNS'), ('upload', 'VBP'), ('content', 'JJ'), ('manually', 'RB'), ('into', 'IN'), ('Content', 'NNP'), ('Services', 'NNPS'), ('Platform', 'NNP'), (',', ','), ('using', 'VBG'), ('a', 'DT'), ('user', 'JJ'), ('interface', 'NN'), (',', ','), ('they', 'PRP'), ('have', 'VBP'), ('to', 'TO'), ('enter', 'VB'), ('metadata', 'NN'), ('values', 'NNS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['2.2.1.2', 'Manual', 'content', 'upload', 'users', 'upload', 'content', 'manually', 'Content', 'Services', 'Platform', ',', 'using', 'user', 'interface', ',', 'enter', 'metadata', 'values', '.']

 TOTAL FILTERED TOKENS ==>  20

 ---- POST FOR FILTERED TOKENS ----

 [('2.2.1.2', 'CD'), ('Manual', 'JJ'), ('content', 'NN'), ('upload', 'NN'), ('users', 'NNS'), ('upload', 'JJ'), ('content', 'JJ'), ('manually', 'RB'), ('Content', 'NNP'), ('Services', 'NNPS'), ('Platform', 'NNP'), (',', ','), ('using', 'VBG'), ('user', 'JJ'), ('interface', 'NN'), (',', ','), ('enter', 'NN'), ('metadata', 'NN'), ('values', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['2.2.1.2 Manual', 'Manual content', 'content upload', 'upload users', 'users upload', 'upload content', 'content manually', 'manually Content', 'Content Services', 'Services Platform', 'Platform ,', ', using', 'using user', 'user interface', 'interface ,', ', enter', 'enter metadata', 'metadata values', 'values .'] 

 TOTAL BIGRAMS --> 19 



 ---- TRI-GRAMS ---- 

 ['2.2.1.2 Manual content', 'Manual content upload', 'content upload users', 'upload users upload', 'users upload content', 'upload content manually', 'content manually Content', 'manually Content Services', 'Content Services Platform', 'Services Platform ,', 'Platform , using', ', using user', 'using user interface', 'user interface ,', 'interface , enter', ', enter metadata', 'enter metadata values', 'metadata values .'] 

 TOTAL TRIGRAMS --> 18 



 ---- NOUN PHRASES ---- 

 ['Manual content', 'upload', 'user interface', 'enter', 'metadata'] 

 TOTAL NOUN PHRASES --> 5 



 ---- NER ----

 
 ORGANIZATION ---> ['Content Services Platform']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['2.2.1.2', 'manual', 'content', 'upload', 'user', 'upload', 'content', 'manual', 'content', 'servic', 'platform', ',', 'use', 'user', 'interfac', ',', 'enter', 'metadata', 'valu', '.']

 TOTAL PORTER STEM WORDS ==> 20



 ---- SNOWBALL STEMMING ----

['2.2.1.2', 'manual', 'content', 'upload', 'user', 'upload', 'content', 'manual', 'content', 'servic', 'platform', ',', 'use', 'user', 'interfac', ',', 'enter', 'metadata', 'valu', '.']

 TOTAL SNOWBALL STEM WORDS ==> 20



 ---- LEMMATIZATION ----

['2.2.1.2', 'Manual', 'content', 'upload', 'user', 'upload', 'content', 'manually', 'Content', 'Services', 'Platform', ',', 'using', 'user', 'interface', ',', 'enter', 'metadata', 'value', '.']

 TOTAL LEMMATIZE WORDS ==> 20

************************************************************************************************************************

73 --> In order to do this, users have to  read through the document, listen or  view the content and then decide what  metadata values this content can have. 


 ---- TOKENS ----

 ['In', 'order', 'to', 'do', 'this', ',', 'users', 'have', 'to', 'read', 'through', 'the', 'document', ',', 'listen', 'or', 'view', 'the', 'content', 'and', 'then', 'decide', 'what', 'metadata', 'values', 'this', 'content', 'can', 'have', '.'] 

 TOTAL TOKENS ==> 30

 ---- POST ----

 [('In', 'IN'), ('order', 'NN'), ('to', 'TO'), ('do', 'VB'), ('this', 'DT'), (',', ','), ('users', 'NNS'), ('have', 'VBP'), ('to', 'TO'), ('read', 'VB'), ('through', 'IN'), ('the', 'DT'), ('document', 'NN'), (',', ','), ('listen', 'VBP'), ('or', 'CC'), ('view', 'VBP'), ('the', 'DT'), ('content', 'NN'), ('and', 'CC'), ('then', 'RB'), ('decide', 'VB'), ('what', 'WP'), ('metadata', 'NN'), ('values', 'VBZ'), ('this', 'DT'), ('content', 'NN'), ('can', 'MD'), ('have', 'VB'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['order', ',', 'users', 'read', 'document', ',', 'listen', 'view', 'content', 'decide', 'metadata', 'values', 'content', '.']

 TOTAL FILTERED TOKENS ==>  14

 ---- POST FOR FILTERED TOKENS ----

 [('order', 'NN'), (',', ','), ('users', 'NNS'), ('read', 'VBP'), ('document', 'NN'), (',', ','), ('listen', 'JJ'), ('view', 'NN'), ('content', 'NN'), ('decide', 'NN'), ('metadata', 'NN'), ('values', 'NNS'), ('content', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['order ,', ', users', 'users read', 'read document', 'document ,', ', listen', 'listen view', 'view content', 'content decide', 'decide metadata', 'metadata values', 'values content', 'content .'] 

 TOTAL BIGRAMS --> 13 



 ---- TRI-GRAMS ---- 

 ['order , users', ', users read', 'users read document', 'read document ,', 'document , listen', ', listen view', 'listen view content', 'view content decide', 'content decide metadata', 'decide metadata values', 'metadata values content', 'values content .'] 

 TOTAL TRIGRAMS --> 12 



 ---- NOUN PHRASES ---- 

 ['order', 'document', 'listen view', 'content', 'decide', 'metadata', 'content'] 

 TOTAL NOUN PHRASES --> 7 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['order', ',', 'user', 'read', 'document', ',', 'listen', 'view', 'content', 'decid', 'metadata', 'valu', 'content', '.']

 TOTAL PORTER STEM WORDS ==> 14



 ---- SNOWBALL STEMMING ----

['order', ',', 'user', 'read', 'document', ',', 'listen', 'view', 'content', 'decid', 'metadata', 'valu', 'content', '.']

 TOTAL SNOWBALL STEM WORDS ==> 14



 ---- LEMMATIZATION ----

['order', ',', 'user', 'read', 'document', ',', 'listen', 'view', 'content', 'decide', 'metadata', 'value', 'content', '.']

 TOTAL LEMMATIZE WORDS ==> 14

************************************************************************************************************************

74 --> From users’ perspective, this process is very  time consuming and prone to errors. 


 ---- TOKENS ----

 ['From', 'users', '’', 'perspective', ',', 'this', 'process', 'is', 'very', 'time', 'consuming', 'and', 'prone', 'to', 'errors', '.'] 

 TOTAL TOKENS ==> 16

 ---- POST ----

 [('From', 'IN'), ('users', 'NNS'), ('’', 'NNP'), ('perspective', 'NN'), (',', ','), ('this', 'DT'), ('process', 'NN'), ('is', 'VBZ'), ('very', 'RB'), ('time', 'NN'), ('consuming', 'NN'), ('and', 'CC'), ('prone', 'NN'), ('to', 'TO'), ('errors', 'NNS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['users', '’', 'perspective', ',', 'process', 'time', 'consuming', 'prone', 'errors', '.']

 TOTAL FILTERED TOKENS ==>  10

 ---- POST FOR FILTERED TOKENS ----

 [('users', 'NNS'), ('’', 'VBP'), ('perspective', 'JJ'), (',', ','), ('process', 'JJ'), ('time', 'NN'), ('consuming', 'VBG'), ('prone', 'JJ'), ('errors', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['users ’', '’ perspective', 'perspective ,', ', process', 'process time', 'time consuming', 'consuming prone', 'prone errors', 'errors .'] 

 TOTAL BIGRAMS --> 9 



 ---- TRI-GRAMS ---- 

 ['users ’ perspective', '’ perspective ,', 'perspective , process', ', process time', 'process time consuming', 'time consuming prone', 'consuming prone errors', 'prone errors .'] 

 TOTAL TRIGRAMS --> 8 



 ---- NOUN PHRASES ---- 

 ['process time'] 

 TOTAL NOUN PHRASES --> 1 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['user', '’', 'perspect', ',', 'process', 'time', 'consum', 'prone', 'error', '.']

 TOTAL PORTER STEM WORDS ==> 10



 ---- SNOWBALL STEMMING ----

['user', '’', 'perspect', ',', 'process', 'time', 'consum', 'prone', 'error', '.']

 TOTAL SNOWBALL STEM WORDS ==> 10



 ---- LEMMATIZATION ----

['user', '’', 'perspective', ',', 'process', 'time', 'consuming', 'prone', 'error', '.']

 TOTAL LEMMATIZE WORDS ==> 10

************************************************************************************************************************

75 --> NLP can solve this challenge. 


 ---- TOKENS ----

 ['NLP', 'can', 'solve', 'this', 'challenge', '.'] 

 TOTAL TOKENS ==> 6

 ---- POST ----

 [('NLP', 'NNP'), ('can', 'MD'), ('solve', 'VB'), ('this', 'DT'), ('challenge', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['NLP', 'solve', 'challenge', '.']

 TOTAL FILTERED TOKENS ==>  4

 ---- POST FOR FILTERED TOKENS ----

 [('NLP', 'NNP'), ('solve', 'NN'), ('challenge', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['NLP solve', 'solve challenge', 'challenge .'] 

 TOTAL BIGRAMS --> 3 



 ---- TRI-GRAMS ---- 

 ['NLP solve challenge', 'solve challenge .'] 

 TOTAL TRIGRAMS --> 2 



 ---- NOUN PHRASES ---- 

 ['solve', 'challenge'] 

 TOTAL NOUN PHRASES --> 2 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['nlp', 'solv', 'challeng', '.']

 TOTAL PORTER STEM WORDS ==> 4



 ---- SNOWBALL STEMMING ----

['nlp', 'solv', 'challeng', '.']

 TOTAL SNOWBALL STEM WORDS ==> 4



 ---- LEMMATIZATION ----

['NLP', 'solve', 'challenge', '.']

 TOTAL LEMMATIZE WORDS ==> 4

************************************************************************************************************************

76 --> A trained NER  or Categorizer model can identify what  metadata values the content can have. 


 ---- TOKENS ----

 ['A', 'trained', 'NER', 'or', 'Categorizer', 'model', 'can', 'identify', 'what', 'metadata', 'values', 'the', 'content', 'can', 'have', '.'] 

 TOTAL TOKENS ==> 16

 ---- POST ----

 [('A', 'DT'), ('trained', 'JJ'), ('NER', 'NNP'), ('or', 'CC'), ('Categorizer', 'NNP'), ('model', 'NN'), ('can', 'MD'), ('identify', 'VB'), ('what', 'WP'), ('metadata', 'NN'), ('values', 'VBZ'), ('the', 'DT'), ('content', 'NN'), ('can', 'MD'), ('have', 'VB'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['trained', 'NER', 'Categorizer', 'model', 'identify', 'metadata', 'values', 'content', '.']

 TOTAL FILTERED TOKENS ==>  9

 ---- POST FOR FILTERED TOKENS ----

 [('trained', 'JJ'), ('NER', 'NNP'), ('Categorizer', 'NNP'), ('model', 'NN'), ('identify', 'NN'), ('metadata', 'NN'), ('values', 'NNS'), ('content', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['trained NER', 'NER Categorizer', 'Categorizer model', 'model identify', 'identify metadata', 'metadata values', 'values content', 'content .'] 

 TOTAL BIGRAMS --> 8 



 ---- TRI-GRAMS ---- 

 ['trained NER Categorizer', 'NER Categorizer model', 'Categorizer model identify', 'model identify metadata', 'identify metadata values', 'metadata values content', 'values content .'] 

 TOTAL TRIGRAMS --> 7 



 ---- NOUN PHRASES ---- 

 ['model', 'identify', 'metadata', 'content'] 

 TOTAL NOUN PHRASES --> 4 



 ---- NER ----

 
 ORGANIZATION ---> ['NER Categorizer']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['train', 'ner', 'categor', 'model', 'identifi', 'metadata', 'valu', 'content', '.']

 TOTAL PORTER STEM WORDS ==> 9



 ---- SNOWBALL STEMMING ----

['train', 'ner', 'categor', 'model', 'identifi', 'metadata', 'valu', 'content', '.']

 TOTAL SNOWBALL STEM WORDS ==> 9



 ---- LEMMATIZATION ----

['trained', 'NER', 'Categorizer', 'model', 'identify', 'metadata', 'value', 'content', '.']

 TOTAL LEMMATIZE WORDS ==> 9

************************************************************************************************************************

77 --> Users are first asked to upload content  which can be analyzed by the AI model  and the suggested metadata that can be  prefilled on user interface. 


 ---- TOKENS ----

 ['Users', 'are', 'first', 'asked', 'to', 'upload', 'content', 'which', 'can', 'be', 'analyzed', 'by', 'the', 'AI', 'model', 'and', 'the', 'suggested', 'metadata', 'that', 'can', 'be', 'prefilled', 'on', 'user', 'interface', '.'] 

 TOTAL TOKENS ==> 27

 ---- POST ----

 [('Users', 'NNS'), ('are', 'VBP'), ('first', 'JJ'), ('asked', 'VBN'), ('to', 'TO'), ('upload', 'VB'), ('content', 'NN'), ('which', 'WDT'), ('can', 'MD'), ('be', 'VB'), ('analyzed', 'VBN'), ('by', 'IN'), ('the', 'DT'), ('AI', 'NNP'), ('model', 'NN'), ('and', 'CC'), ('the', 'DT'), ('suggested', 'VBN'), ('metadata', 'NN'), ('that', 'WDT'), ('can', 'MD'), ('be', 'VB'), ('prefilled', 'VBN'), ('on', 'IN'), ('user', 'JJ'), ('interface', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Users', 'first', 'asked', 'upload', 'content', 'analyzed', 'AI', 'model', 'suggested', 'metadata', 'prefilled', 'user', 'interface', '.']

 TOTAL FILTERED TOKENS ==>  14

 ---- POST FOR FILTERED TOKENS ----

 [('Users', 'NNS'), ('first', 'RB'), ('asked', 'VBD'), ('upload', 'JJ'), ('content', 'NN'), ('analyzed', 'VBD'), ('AI', 'NNP'), ('model', 'NN'), ('suggested', 'VBD'), ('metadata', 'NNS'), ('prefilled', 'VBD'), ('user', 'JJ'), ('interface', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Users first', 'first asked', 'asked upload', 'upload content', 'content analyzed', 'analyzed AI', 'AI model', 'model suggested', 'suggested metadata', 'metadata prefilled', 'prefilled user', 'user interface', 'interface .'] 

 TOTAL BIGRAMS --> 13 



 ---- TRI-GRAMS ---- 

 ['Users first asked', 'first asked upload', 'asked upload content', 'upload content analyzed', 'content analyzed AI', 'analyzed AI model', 'AI model suggested', 'model suggested metadata', 'suggested metadata prefilled', 'metadata prefilled user', 'prefilled user interface', 'user interface .'] 

 TOTAL TRIGRAMS --> 12 



 ---- NOUN PHRASES ---- 

 ['upload content', 'model', 'user interface'] 

 TOTAL NOUN PHRASES --> 3 



 ---- NER ----

 
 ORGANIZATION ---> ['AI']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['user', 'first', 'ask', 'upload', 'content', 'analyz', 'ai', 'model', 'suggest', 'metadata', 'prefil', 'user', 'interfac', '.']

 TOTAL PORTER STEM WORDS ==> 14



 ---- SNOWBALL STEMMING ----

['user', 'first', 'ask', 'upload', 'content', 'analyz', 'ai', 'model', 'suggest', 'metadata', 'prefil', 'user', 'interfac', '.']

 TOTAL SNOWBALL STEM WORDS ==> 14



 ---- LEMMATIZATION ----

['Users', 'first', 'asked', 'upload', 'content', 'analyzed', 'AI', 'model', 'suggested', 'metadata', 'prefilled', 'user', 'interface', '.']

 TOTAL LEMMATIZE WORDS ==> 14

************************************************************************************************************************

78 --> Users need to  just verify it and submit it. 


 ---- TOKENS ----

 ['Users', 'need', 'to', 'just', 'verify', 'it', 'and', 'submit', 'it', '.'] 

 TOTAL TOKENS ==> 10

 ---- POST ----

 [('Users', 'NNS'), ('need', 'VBP'), ('to', 'TO'), ('just', 'RB'), ('verify', 'VB'), ('it', 'PRP'), ('and', 'CC'), ('submit', 'VB'), ('it', 'PRP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Users', 'need', 'verify', 'submit', '.']

 TOTAL FILTERED TOKENS ==>  5

 ---- POST FOR FILTERED TOKENS ----

 [('Users', 'NNS'), ('need', 'VBP'), ('verify', 'JJ'), ('submit', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Users need', 'need verify', 'verify submit', 'submit .'] 

 TOTAL BIGRAMS --> 4 



 ---- TRI-GRAMS ---- 

 ['Users need verify', 'need verify submit', 'verify submit .'] 

 TOTAL TRIGRAMS --> 3 



 ---- NOUN PHRASES ---- 

 ['verify submit'] 

 TOTAL NOUN PHRASES --> 1 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['user', 'need', 'verifi', 'submit', '.']

 TOTAL PORTER STEM WORDS ==> 5



 ---- SNOWBALL STEMMING ----

['user', 'need', 'verifi', 'submit', '.']

 TOTAL SNOWBALL STEM WORDS ==> 5



 ---- LEMMATIZATION ----

['Users', 'need', 'verify', 'submit', '.']

 TOTAL LEMMATIZE WORDS ==> 5

************************************************************************************************************************

79 --> This saves time  for the user. 


 ---- TOKENS ----

 ['This', 'saves', 'time', 'for', 'the', 'user', '.'] 

 TOTAL TOKENS ==> 7

 ---- POST ----

 [('This', 'DT'), ('saves', 'VBZ'), ('time', 'NN'), ('for', 'IN'), ('the', 'DT'), ('user', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['saves', 'time', 'user', '.']

 TOTAL FILTERED TOKENS ==>  4

 ---- POST FOR FILTERED TOKENS ----

 [('saves', 'NNS'), ('time', 'NN'), ('user', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['saves time', 'time user', 'user .'] 

 TOTAL BIGRAMS --> 3 



 ---- TRI-GRAMS ---- 

 ['saves time user', 'time user .'] 

 TOTAL TRIGRAMS --> 2 



 ---- NOUN PHRASES ---- 

 ['time', 'user'] 

 TOTAL NOUN PHRASES --> 2 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['save', 'time', 'user', '.']

 TOTAL PORTER STEM WORDS ==> 4



 ---- SNOWBALL STEMMING ----

['save', 'time', 'user', '.']

 TOTAL SNOWBALL STEM WORDS ==> 4



 ---- LEMMATIZATION ----

['save', 'time', 'user', '.']

 TOTAL LEMMATIZE WORDS ==> 4

************************************************************************************************************************

80 --> 2.2.1.3 Shared Drive Migration Most of the organization store content  on shared network drives. 


 ---- TOKENS ----

 ['2.2.1.3', 'Shared', 'Drive', 'Migration', 'Most', 'of', 'the', 'organization', 'store', 'content', 'on', 'shared', 'network', 'drives', '.'] 

 TOTAL TOKENS ==> 15

 ---- POST ----

 [('2.2.1.3', 'CD'), ('Shared', 'NNP'), ('Drive', 'NNP'), ('Migration', 'NNP'), ('Most', 'NNP'), ('of', 'IN'), ('the', 'DT'), ('organization', 'NN'), ('store', 'NN'), ('content', 'NN'), ('on', 'IN'), ('shared', 'VBN'), ('network', 'NN'), ('drives', 'NNS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['2.2.1.3', 'Shared', 'Drive', 'Migration', 'organization', 'store', 'content', 'shared', 'network', 'drives', '.']

 TOTAL FILTERED TOKENS ==>  11

 ---- POST FOR FILTERED TOKENS ----

 [('2.2.1.3', 'CD'), ('Shared', 'NNP'), ('Drive', 'NNP'), ('Migration', 'NNP'), ('organization', 'NN'), ('store', 'NN'), ('content', 'NN'), ('shared', 'VBN'), ('network', 'NN'), ('drives', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['2.2.1.3 Shared', 'Shared Drive', 'Drive Migration', 'Migration organization', 'organization store', 'store content', 'content shared', 'shared network', 'network drives', 'drives .'] 

 TOTAL BIGRAMS --> 10 



 ---- TRI-GRAMS ---- 

 ['2.2.1.3 Shared Drive', 'Shared Drive Migration', 'Drive Migration organization', 'Migration organization store', 'organization store content', 'store content shared', 'content shared network', 'shared network drives', 'network drives .'] 

 TOTAL TRIGRAMS --> 9 



 ---- NOUN PHRASES ---- 

 ['organization', 'store', 'content', 'network'] 

 TOTAL NOUN PHRASES --> 4 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> ['Shared Drive']
 TOTAL PERSON ENTITY --> 1 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['2.2.1.3', 'share', 'drive', 'migrat', 'organ', 'store', 'content', 'share', 'network', 'drive', '.']

 TOTAL PORTER STEM WORDS ==> 11



 ---- SNOWBALL STEMMING ----

['2.2.1.3', 'share', 'drive', 'migrat', 'organ', 'store', 'content', 'share', 'network', 'drive', '.']

 TOTAL SNOWBALL STEM WORDS ==> 11



 ---- LEMMATIZATION ----

['2.2.1.3', 'Shared', 'Drive', 'Migration', 'organization', 'store', 'content', 'shared', 'network', 'drive', '.']

 TOTAL LEMMATIZE WORDS ==> 11

************************************************************************************************************************

81 --> Content, which  is stored on shared drive are not tagged  with any metadata or organized in any  directory structure. 


 ---- TOKENS ----

 ['Content', ',', 'which', 'is', 'stored', 'on', 'shared', 'drive', 'are', 'not', 'tagged', 'with', 'any', 'metadata', 'or', 'organized', 'in', 'any', 'directory', 'structure', '.'] 

 TOTAL TOKENS ==> 21

 ---- POST ----

 [('Content', 'NNP'), (',', ','), ('which', 'WDT'), ('is', 'VBZ'), ('stored', 'VBN'), ('on', 'IN'), ('shared', 'VBN'), ('drive', 'NN'), ('are', 'VBP'), ('not', 'RB'), ('tagged', 'VBN'), ('with', 'IN'), ('any', 'DT'), ('metadata', 'NNS'), ('or', 'CC'), ('organized', 'VBN'), ('in', 'IN'), ('any', 'DT'), ('directory', 'JJ'), ('structure', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Content', ',', 'stored', 'shared', 'drive', 'tagged', 'metadata', 'organized', 'directory', 'structure', '.']

 TOTAL FILTERED TOKENS ==>  11

 ---- POST FOR FILTERED TOKENS ----

 [('Content', 'NNP'), (',', ','), ('stored', 'VBD'), ('shared', 'VBN'), ('drive', 'NN'), ('tagged', 'VBN'), ('metadata', 'NNS'), ('organized', 'VBN'), ('directory', 'NN'), ('structure', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Content ,', ', stored', 'stored shared', 'shared drive', 'drive tagged', 'tagged metadata', 'metadata organized', 'organized directory', 'directory structure', 'structure .'] 

 TOTAL BIGRAMS --> 10 



 ---- TRI-GRAMS ---- 

 ['Content , stored', ', stored shared', 'stored shared drive', 'shared drive tagged', 'drive tagged metadata', 'tagged metadata organized', 'metadata organized directory', 'organized directory structure', 'directory structure .'] 

 TOTAL TRIGRAMS --> 9 



 ---- NOUN PHRASES ---- 

 ['drive', 'directory', 'structure'] 

 TOTAL NOUN PHRASES --> 3 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['content', ',', 'store', 'share', 'drive', 'tag', 'metadata', 'organ', 'directori', 'structur', '.']

 TOTAL PORTER STEM WORDS ==> 11



 ---- SNOWBALL STEMMING ----

['content', ',', 'store', 'share', 'drive', 'tag', 'metadata', 'organ', 'directori', 'structur', '.']

 TOTAL SNOWBALL STEM WORDS ==> 11



 ---- LEMMATIZATION ----

['Content', ',', 'stored', 'shared', 'drive', 'tagged', 'metadata', 'organized', 'directory', 'structure', '.']

 TOTAL LEMMATIZE WORDS ==> 11

************************************************************************************************************************

82 --> Due to this, the content  cannot be located and reused. 


 ---- TOKENS ----

 ['Due', 'to', 'this', ',', 'the', 'content', 'can', 'not', 'be', 'located', 'and', 'reused', '.'] 

 TOTAL TOKENS ==> 13

 ---- POST ----

 [('Due', 'JJ'), ('to', 'TO'), ('this', 'DT'), (',', ','), ('the', 'DT'), ('content', 'NN'), ('can', 'MD'), ('not', 'RB'), ('be', 'VB'), ('located', 'VBN'), ('and', 'CC'), ('reused', 'VBN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Due', ',', 'content', 'located', 'reused', '.']

 TOTAL FILTERED TOKENS ==>  6

 ---- POST FOR FILTERED TOKENS ----

 [('Due', 'NNP'), (',', ','), ('content', 'NN'), ('located', 'VBN'), ('reused', 'VBD'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Due ,', ', content', 'content located', 'located reused', 'reused .'] 

 TOTAL BIGRAMS --> 5 



 ---- TRI-GRAMS ---- 

 ['Due , content', ', content located', 'content located reused', 'located reused .'] 

 TOTAL TRIGRAMS --> 4 



 ---- NOUN PHRASES ---- 

 ['content'] 

 TOTAL NOUN PHRASES --> 1 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> ['Due']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['due', ',', 'content', 'locat', 'reus', '.']

 TOTAL PORTER STEM WORDS ==> 6



 ---- SNOWBALL STEMMING ----

['due', ',', 'content', 'locat', 'reus', '.']

 TOTAL SNOWBALL STEM WORDS ==> 6



 ---- LEMMATIZATION ----

['Due', ',', 'content', 'located', 'reused', '.']

 TOTAL LEMMATIZE WORDS ==> 6

************************************************************************************************************************

83 --> More than  that, content laying on shared drive can  be a risk on adherence to compliance and  regulatory standards. 


 ---- TOKENS ----

 ['More', 'than', 'that', ',', 'content', 'laying', 'on', 'shared', 'drive', 'can', 'be', 'a', 'risk', 'on', 'adherence', 'to', 'compliance', 'and', 'regulatory', 'standards', '.'] 

 TOTAL TOKENS ==> 21

 ---- POST ----

 [('More', 'JJR'), ('than', 'IN'), ('that', 'DT'), (',', ','), ('content', 'JJ'), ('laying', 'VBG'), ('on', 'IN'), ('shared', 'VBN'), ('drive', 'NN'), ('can', 'MD'), ('be', 'VB'), ('a', 'DT'), ('risk', 'NN'), ('on', 'IN'), ('adherence', 'NN'), ('to', 'TO'), ('compliance', 'NN'), ('and', 'CC'), ('regulatory', 'JJ'), ('standards', 'NNS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 [',', 'content', 'laying', 'shared', 'drive', 'risk', 'adherence', 'compliance', 'regulatory', 'standards', '.']

 TOTAL FILTERED TOKENS ==>  11

 ---- POST FOR FILTERED TOKENS ----

 [(',', ','), ('content', 'JJ'), ('laying', 'NN'), ('shared', 'VBD'), ('drive', 'NN'), ('risk', 'NN'), ('adherence', 'NN'), ('compliance', 'NN'), ('regulatory', 'JJ'), ('standards', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 [', content', 'content laying', 'laying shared', 'shared drive', 'drive risk', 'risk adherence', 'adherence compliance', 'compliance regulatory', 'regulatory standards', 'standards .'] 

 TOTAL BIGRAMS --> 10 



 ---- TRI-GRAMS ---- 

 [', content laying', 'content laying shared', 'laying shared drive', 'shared drive risk', 'drive risk adherence', 'risk adherence compliance', 'adherence compliance regulatory', 'compliance regulatory standards', 'regulatory standards .'] 

 TOTAL TRIGRAMS --> 9 



 ---- NOUN PHRASES ---- 

 ['content laying', 'drive', 'risk', 'adherence', 'compliance'] 

 TOTAL NOUN PHRASES --> 5 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

[',', 'content', 'lay', 'share', 'drive', 'risk', 'adher', 'complianc', 'regulatori', 'standard', '.']

 TOTAL PORTER STEM WORDS ==> 11



 ---- SNOWBALL STEMMING ----

[',', 'content', 'lay', 'share', 'drive', 'risk', 'adher', 'complianc', 'regulatori', 'standard', '.']

 TOTAL SNOWBALL STEM WORDS ==> 11



 ---- LEMMATIZATION ----

[',', 'content', 'laying', 'shared', 'drive', 'risk', 'adherence', 'compliance', 'regulatory', 'standard', '.']

 TOTAL LEMMATIZE WORDS ==> 11

************************************************************************************************************************

84 --> To overcome these  challenges and to manage the content  efficiently, it needs to be migrated to a  Content Service platform. 


 ---- TOKENS ----

 ['To', 'overcome', 'these', 'challenges', 'and', 'to', 'manage', 'the', 'content', 'efficiently', ',', 'it', 'needs', 'to', 'be', 'migrated', 'to', 'a', 'Content', 'Service', 'platform', '.'] 

 TOTAL TOKENS ==> 22

 ---- POST ----

 [('To', 'TO'), ('overcome', 'VB'), ('these', 'DT'), ('challenges', 'NNS'), ('and', 'CC'), ('to', 'TO'), ('manage', 'VB'), ('the', 'DT'), ('content', 'NN'), ('efficiently', 'RB'), (',', ','), ('it', 'PRP'), ('needs', 'VBZ'), ('to', 'TO'), ('be', 'VB'), ('migrated', 'VBN'), ('to', 'TO'), ('a', 'DT'), ('Content', 'NNP'), ('Service', 'NNP'), ('platform', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['overcome', 'challenges', 'manage', 'content', 'efficiently', ',', 'needs', 'migrated', 'Content', 'Service', 'platform', '.']

 TOTAL FILTERED TOKENS ==>  12

 ---- POST FOR FILTERED TOKENS ----

 [('overcome', 'JJ'), ('challenges', 'NNS'), ('manage', 'VBP'), ('content', 'NN'), ('efficiently', 'RB'), (',', ','), ('needs', 'NNS'), ('migrated', 'VBD'), ('Content', 'NNP'), ('Service', 'NNP'), ('platform', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['overcome challenges', 'challenges manage', 'manage content', 'content efficiently', 'efficiently ,', ', needs', 'needs migrated', 'migrated Content', 'Content Service', 'Service platform', 'platform .'] 

 TOTAL BIGRAMS --> 11 



 ---- TRI-GRAMS ---- 

 ['overcome challenges manage', 'challenges manage content', 'manage content efficiently', 'content efficiently ,', 'efficiently , needs', ', needs migrated', 'needs migrated Content', 'migrated Content Service', 'Content Service platform', 'Service platform .'] 

 TOTAL TRIGRAMS --> 10 



 ---- NOUN PHRASES ---- 

 ['content', 'platform'] 

 TOTAL NOUN PHRASES --> 2 



 ---- NER ----

 
 ORGANIZATION ---> ['Content Service']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['overcom', 'challeng', 'manag', 'content', 'effici', ',', 'need', 'migrat', 'content', 'servic', 'platform', '.']

 TOTAL PORTER STEM WORDS ==> 12



 ---- SNOWBALL STEMMING ----

['overcom', 'challeng', 'manag', 'content', 'effici', ',', 'need', 'migrat', 'content', 'servic', 'platform', '.']

 TOTAL SNOWBALL STEM WORDS ==> 12



 ---- LEMMATIZATION ----

['overcome', 'challenge', 'manage', 'content', 'efficiently', ',', 'need', 'migrated', 'Content', 'Service', 'platform', '.']

 TOTAL LEMMATIZE WORDS ==> 12

************************************************************************************************************************

85 --> To migrate the content of shared drives, a  trained NER or Categorizer model can be  plugged-in into the migration framework. 


 ---- TOKENS ----

 ['To', 'migrate', 'the', 'content', 'of', 'shared', 'drives', ',', 'a', 'trained', 'NER', 'or', 'Categorizer', 'model', 'can', 'be', 'plugged-in', 'into', 'the', 'migration', 'framework', '.'] 

 TOTAL TOKENS ==> 22

 ---- POST ----

 [('To', 'TO'), ('migrate', 'VB'), ('the', 'DT'), ('content', 'NN'), ('of', 'IN'), ('shared', 'VBN'), ('drives', 'NNS'), (',', ','), ('a', 'DT'), ('trained', 'JJ'), ('NER', 'NNP'), ('or', 'CC'), ('Categorizer', 'NNP'), ('model', 'NN'), ('can', 'MD'), ('be', 'VB'), ('plugged-in', 'JJ'), ('into', 'IN'), ('the', 'DT'), ('migration', 'NN'), ('framework', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['migrate', 'content', 'shared', 'drives', ',', 'trained', 'NER', 'Categorizer', 'model', 'plugged-in', 'migration', 'framework', '.']

 TOTAL FILTERED TOKENS ==>  13

 ---- POST FOR FILTERED TOKENS ----

 [('migrate', 'NN'), ('content', 'NN'), ('shared', 'VBN'), ('drives', 'NNS'), (',', ','), ('trained', 'VBD'), ('NER', 'NNP'), ('Categorizer', 'NNP'), ('model', 'POS'), ('plugged-in', 'JJ'), ('migration', 'NN'), ('framework', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['migrate content', 'content shared', 'shared drives', 'drives ,', ', trained', 'trained NER', 'NER Categorizer', 'Categorizer model', 'model plugged-in', 'plugged-in migration', 'migration framework', 'framework .'] 

 TOTAL BIGRAMS --> 12 



 ---- TRI-GRAMS ---- 

 ['migrate content shared', 'content shared drives', 'shared drives ,', 'drives , trained', ', trained NER', 'trained NER Categorizer', 'NER Categorizer model', 'Categorizer model plugged-in', 'model plugged-in migration', 'plugged-in migration framework', 'migration framework .'] 

 TOTAL TRIGRAMS --> 11 



 ---- NOUN PHRASES ---- 

 ['migrate', 'content', 'plugged-in migration', 'framework'] 

 TOTAL NOUN PHRASES --> 4 



 ---- NER ----

 
 ORGANIZATION ---> ['NER Categorizer']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['migrat', 'content', 'share', 'drive', ',', 'train', 'ner', 'categor', 'model', 'plugged-in', 'migrat', 'framework', '.']

 TOTAL PORTER STEM WORDS ==> 13



 ---- SNOWBALL STEMMING ----

['migrat', 'content', 'share', 'drive', ',', 'train', 'ner', 'categor', 'model', 'plugged-in', 'migrat', 'framework', '.']

 TOTAL SNOWBALL STEM WORDS ==> 13



 ---- LEMMATIZATION ----

['migrate', 'content', 'shared', 'drive', ',', 'trained', 'NER', 'Categorizer', 'model', 'plugged-in', 'migration', 'framework', '.']

 TOTAL LEMMATIZE WORDS ==> 13

************************************************************************************************************************

86 --> NLP model can identify the metadata  of content and it can be tagged to the  content while loading the content into the  content service platform. 


 ---- TOKENS ----

 ['NLP', 'model', 'can', 'identify', 'the', 'metadata', 'of', 'content', 'and', 'it', 'can', 'be', 'tagged', 'to', 'the', 'content', 'while', 'loading', 'the', 'content', 'into', 'the', 'content', 'service', 'platform', '.'] 

 TOTAL TOKENS ==> 26

 ---- POST ----

 [('NLP', 'NNP'), ('model', 'NN'), ('can', 'MD'), ('identify', 'VB'), ('the', 'DT'), ('metadata', 'NN'), ('of', 'IN'), ('content', 'NN'), ('and', 'CC'), ('it', 'PRP'), ('can', 'MD'), ('be', 'VB'), ('tagged', 'VBN'), ('to', 'TO'), ('the', 'DT'), ('content', 'NN'), ('while', 'IN'), ('loading', 'VBG'), ('the', 'DT'), ('content', 'NN'), ('into', 'IN'), ('the', 'DT'), ('content', 'JJ'), ('service', 'NN'), ('platform', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['NLP', 'model', 'identify', 'metadata', 'content', 'tagged', 'content', 'loading', 'content', 'content', 'service', 'platform', '.']

 TOTAL FILTERED TOKENS ==>  13

 ---- POST FOR FILTERED TOKENS ----

 [('NLP', 'NNP'), ('model', 'NN'), ('identify', 'NN'), ('metadata', 'NN'), ('content', 'NN'), ('tagged', 'VBD'), ('content', 'JJ'), ('loading', 'NN'), ('content', 'NN'), ('content', 'NN'), ('service', 'NN'), ('platform', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['NLP model', 'model identify', 'identify metadata', 'metadata content', 'content tagged', 'tagged content', 'content loading', 'loading content', 'content content', 'content service', 'service platform', 'platform .'] 

 TOTAL BIGRAMS --> 12 



 ---- TRI-GRAMS ---- 

 ['NLP model identify', 'model identify metadata', 'identify metadata content', 'metadata content tagged', 'content tagged content', 'tagged content loading', 'content loading content', 'loading content content', 'content content service', 'content service platform', 'service platform .'] 

 TOTAL TRIGRAMS --> 11 



 ---- NOUN PHRASES ---- 

 ['model', 'identify', 'metadata', 'content', 'content loading', 'content', 'content', 'service', 'platform'] 

 TOTAL NOUN PHRASES --> 9 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['nlp', 'model', 'identifi', 'metadata', 'content', 'tag', 'content', 'load', 'content', 'content', 'servic', 'platform', '.']

 TOTAL PORTER STEM WORDS ==> 13



 ---- SNOWBALL STEMMING ----

['nlp', 'model', 'identifi', 'metadata', 'content', 'tag', 'content', 'load', 'content', 'content', 'servic', 'platform', '.']

 TOTAL SNOWBALL STEM WORDS ==> 13



 ---- LEMMATIZATION ----

['NLP', 'model', 'identify', 'metadata', 'content', 'tagged', 'content', 'loading', 'content', 'content', 'service', 'platform', '.']

 TOTAL LEMMATIZE WORDS ==> 13

************************************************************************************************************************

87 --> 2.3 Content Management Unstructured content stored on Content  Services Platform needs to be managed  effectively and efficiently, in order to locate  and reuse the content, to enable the use  of the content in business processes, etc. 


 ---- TOKENS ----

 ['2.3', 'Content', 'Management', 'Unstructured', 'content', 'stored', 'on', 'Content', 'Services', 'Platform', 'needs', 'to', 'be', 'managed', 'effectively', 'and', 'efficiently', ',', 'in', 'order', 'to', 'locate', 'and', 'reuse', 'the', 'content', ',', 'to', 'enable', 'the', 'use', 'of', 'the', 'content', 'in', 'business', 'processes', ',', 'etc', '.'] 

 TOTAL TOKENS ==> 40

 ---- POST ----

 [('2.3', 'CD'), ('Content', 'JJ'), ('Management', 'NNP'), ('Unstructured', 'NNP'), ('content', 'NN'), ('stored', 'VBD'), ('on', 'IN'), ('Content', 'NNP'), ('Services', 'NNPS'), ('Platform', 'NNP'), ('needs', 'VBZ'), ('to', 'TO'), ('be', 'VB'), ('managed', 'VBN'), ('effectively', 'RB'), ('and', 'CC'), ('efficiently', 'RB'), (',', ','), ('in', 'IN'), ('order', 'NN'), ('to', 'TO'), ('locate', 'VB'), ('and', 'CC'), ('reuse', 'VB'), ('the', 'DT'), ('content', 'NN'), (',', ','), ('to', 'TO'), ('enable', 'VB'), ('the', 'DT'), ('use', 'NN'), ('of', 'IN'), ('the', 'DT'), ('content', 'NN'), ('in', 'IN'), ('business', 'NN'), ('processes', 'NNS'), (',', ','), ('etc', 'FW'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['2.3', 'Content', 'Management', 'Unstructured', 'content', 'stored', 'Content', 'Services', 'Platform', 'needs', 'managed', 'effectively', 'efficiently', ',', 'order', 'locate', 'reuse', 'content', ',', 'enable', 'use', 'content', 'business', 'processes', ',', 'etc', '.']

 TOTAL FILTERED TOKENS ==>  27

 ---- POST FOR FILTERED TOKENS ----

 [('2.3', 'CD'), ('Content', 'JJ'), ('Management', 'NNP'), ('Unstructured', 'NNP'), ('content', 'NN'), ('stored', 'VBD'), ('Content', 'NNP'), ('Services', 'NNPS'), ('Platform', 'NNP'), ('needs', 'VBZ'), ('managed', 'VBN'), ('effectively', 'RB'), ('efficiently', 'RB'), (',', ','), ('order', 'NN'), ('locate', 'NN'), ('reuse', 'NN'), ('content', 'NN'), (',', ','), ('enable', 'JJ'), ('use', 'NN'), ('content', 'NN'), ('business', 'NN'), ('processes', 'NNS'), (',', ','), ('etc', 'FW'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['2.3 Content', 'Content Management', 'Management Unstructured', 'Unstructured content', 'content stored', 'stored Content', 'Content Services', 'Services Platform', 'Platform needs', 'needs managed', 'managed effectively', 'effectively efficiently', 'efficiently ,', ', order', 'order locate', 'locate reuse', 'reuse content', 'content ,', ', enable', 'enable use', 'use content', 'content business', 'business processes', 'processes ,', ', etc', 'etc .'] 

 TOTAL BIGRAMS --> 26 



 ---- TRI-GRAMS ---- 

 ['2.3 Content Management', 'Content Management Unstructured', 'Management Unstructured content', 'Unstructured content stored', 'content stored Content', 'stored Content Services', 'Content Services Platform', 'Services Platform needs', 'Platform needs managed', 'needs managed effectively', 'managed effectively efficiently', 'effectively efficiently ,', 'efficiently , order', ', order locate', 'order locate reuse', 'locate reuse content', 'reuse content ,', 'content , enable', ', enable use', 'enable use content', 'use content business', 'content business processes', 'business processes ,', 'processes , etc', ', etc .'] 

 TOTAL TRIGRAMS --> 25 



 ---- NOUN PHRASES ---- 

 ['content', 'order', 'locate', 'reuse', 'content', 'enable use', 'content', 'business'] 

 TOTAL NOUN PHRASES --> 8 



 ---- NER ----

 
 ORGANIZATION ---> ['Content Services Platform']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['2.3', 'content', 'manag', 'unstructur', 'content', 'store', 'content', 'servic', 'platform', 'need', 'manag', 'effect', 'effici', ',', 'order', 'locat', 'reus', 'content', ',', 'enabl', 'use', 'content', 'busi', 'process', ',', 'etc', '.']

 TOTAL PORTER STEM WORDS ==> 27



 ---- SNOWBALL STEMMING ----

['2.3', 'content', 'manag', 'unstructur', 'content', 'store', 'content', 'servic', 'platform', 'need', 'manag', 'effect', 'effici', ',', 'order', 'locat', 'reus', 'content', ',', 'enabl', 'use', 'content', 'busi', 'process', ',', 'etc', '.']

 TOTAL SNOWBALL STEM WORDS ==> 27



 ---- LEMMATIZATION ----

['2.3', 'Content', 'Management', 'Unstructured', 'content', 'stored', 'Content', 'Services', 'Platform', 'need', 'managed', 'effectively', 'efficiently', ',', 'order', 'locate', 'reuse', 'content', ',', 'enable', 'use', 'content', 'business', 'process', ',', 'etc', '.']

 TOTAL LEMMATIZE WORDS ==> 27

************************************************************************************************************************

88 --> This can be achieved by maintaining the  proper identity of content. 


 ---- TOKENS ----

 ['This', 'can', 'be', 'achieved', 'by', 'maintaining', 'the', 'proper', 'identity', 'of', 'content', '.'] 

 TOTAL TOKENS ==> 12

 ---- POST ----

 [('This', 'DT'), ('can', 'MD'), ('be', 'VB'), ('achieved', 'VBN'), ('by', 'IN'), ('maintaining', 'VBG'), ('the', 'DT'), ('proper', 'JJ'), ('identity', 'NN'), ('of', 'IN'), ('content', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['achieved', 'maintaining', 'proper', 'identity', 'content', '.']

 TOTAL FILTERED TOKENS ==>  6

 ---- POST FOR FILTERED TOKENS ----

 [('achieved', 'VBN'), ('maintaining', 'VBG'), ('proper', 'JJ'), ('identity', 'NN'), ('content', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['achieved maintaining', 'maintaining proper', 'proper identity', 'identity content', 'content .'] 

 TOTAL BIGRAMS --> 5 



 ---- TRI-GRAMS ---- 

 ['achieved maintaining proper', 'maintaining proper identity', 'proper identity content', 'identity content .'] 

 TOTAL TRIGRAMS --> 4 



 ---- NOUN PHRASES ---- 

 ['proper identity', 'content'] 

 TOTAL NOUN PHRASES --> 2 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['achiev', 'maintain', 'proper', 'ident', 'content', '.']

 TOTAL PORTER STEM WORDS ==> 6



 ---- SNOWBALL STEMMING ----

['achiev', 'maintain', 'proper', 'ident', 'content', '.']

 TOTAL SNOWBALL STEM WORDS ==> 6



 ---- LEMMATIZATION ----

['achieved', 'maintaining', 'proper', 'identity', 'content', '.']

 TOTAL LEMMATIZE WORDS ==> 6

************************************************************************************************************************

89 --> In Content  Services Platform, identity of a document  is determined by the metadata values the  content is tagged with. 


 ---- TOKENS ----

 ['In', 'Content', 'Services', 'Platform', ',', 'identity', 'of', 'a', 'document', 'is', 'determined', 'by', 'the', 'metadata', 'values', 'the', 'content', 'is', 'tagged', 'with', '.'] 

 TOTAL TOKENS ==> 21

 ---- POST ----

 [('In', 'IN'), ('Content', 'NNP'), ('Services', 'NNPS'), ('Platform', 'NNP'), (',', ','), ('identity', 'NN'), ('of', 'IN'), ('a', 'DT'), ('document', 'NN'), ('is', 'VBZ'), ('determined', 'VBN'), ('by', 'IN'), ('the', 'DT'), ('metadata', 'NN'), ('values', 'VBZ'), ('the', 'DT'), ('content', 'NN'), ('is', 'VBZ'), ('tagged', 'VBN'), ('with', 'IN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Content', 'Services', 'Platform', ',', 'identity', 'document', 'determined', 'metadata', 'values', 'content', 'tagged', '.']

 TOTAL FILTERED TOKENS ==>  12

 ---- POST FOR FILTERED TOKENS ----

 [('Content', 'NNP'), ('Services', 'NNPS'), ('Platform', 'NNP'), (',', ','), ('identity', 'NN'), ('document', 'NN'), ('determined', 'VBD'), ('metadata', 'NNS'), ('values', 'NNS'), ('content', 'VBP'), ('tagged', 'VBN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Content Services', 'Services Platform', 'Platform ,', ', identity', 'identity document', 'document determined', 'determined metadata', 'metadata values', 'values content', 'content tagged', 'tagged .'] 

 TOTAL BIGRAMS --> 11 



 ---- TRI-GRAMS ---- 

 ['Content Services Platform', 'Services Platform ,', 'Platform , identity', ', identity document', 'identity document determined', 'document determined metadata', 'determined metadata values', 'metadata values content', 'values content tagged', 'content tagged .'] 

 TOTAL TRIGRAMS --> 10 



 ---- NOUN PHRASES ---- 

 ['identity', 'document'] 

 TOTAL NOUN PHRASES --> 2 



 ---- NER ----

 
 ORGANIZATION ---> ['Content Services Platform']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['content', 'servic', 'platform', ',', 'ident', 'document', 'determin', 'metadata', 'valu', 'content', 'tag', '.']

 TOTAL PORTER STEM WORDS ==> 12



 ---- SNOWBALL STEMMING ----

['content', 'servic', 'platform', ',', 'ident', 'document', 'determin', 'metadata', 'valu', 'content', 'tag', '.']

 TOTAL SNOWBALL STEM WORDS ==> 12



 ---- LEMMATIZATION ----

['Content', 'Services', 'Platform', ',', 'identity', 'document', 'determined', 'metadata', 'value', 'content', 'tagged', '.']

 TOTAL LEMMATIZE WORDS ==> 12

************************************************************************************************************************

90 --> Due to various compliance regulation, such  as GDPR, identifying Personally Identifiable  (PI) Information from the content is  becoming a mandatory business function. 


 ---- TOKENS ----

 ['Due', 'to', 'various', 'compliance', 'regulation', ',', 'such', 'as', 'GDPR', ',', 'identifying', 'Personally', 'Identifiable', '(', 'PI', ')', 'Information', 'from', 'the', 'content', 'is', 'becoming', 'a', 'mandatory', 'business', 'function', '.'] 

 TOTAL TOKENS ==> 27

 ---- POST ----

 [('Due', 'JJ'), ('to', 'TO'), ('various', 'JJ'), ('compliance', 'NN'), ('regulation', 'NN'), (',', ','), ('such', 'JJ'), ('as', 'IN'), ('GDPR', 'NNP'), (',', ','), ('identifying', 'VBG'), ('Personally', 'RB'), ('Identifiable', 'NNP'), ('(', '('), ('PI', 'NNP'), (')', ')'), ('Information', 'NN'), ('from', 'IN'), ('the', 'DT'), ('content', 'NN'), ('is', 'VBZ'), ('becoming', 'VBG'), ('a', 'DT'), ('mandatory', 'JJ'), ('business', 'NN'), ('function', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Due', 'various', 'compliance', 'regulation', ',', 'GDPR', ',', 'identifying', 'Personally', 'Identifiable', '(', 'PI', ')', 'Information', 'content', 'becoming', 'mandatory', 'business', 'function', '.']

 TOTAL FILTERED TOKENS ==>  20

 ---- POST FOR FILTERED TOKENS ----

 [('Due', 'NNP'), ('various', 'JJ'), ('compliance', 'NN'), ('regulation', 'NN'), (',', ','), ('GDPR', 'NNP'), (',', ','), ('identifying', 'VBG'), ('Personally', 'RB'), ('Identifiable', 'NNP'), ('(', '('), ('PI', 'NNP'), (')', ')'), ('Information', 'NNP'), ('content', 'NN'), ('becoming', 'VBG'), ('mandatory', 'JJ'), ('business', 'NN'), ('function', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Due various', 'various compliance', 'compliance regulation', 'regulation ,', ', GDPR', 'GDPR ,', ', identifying', 'identifying Personally', 'Personally Identifiable', 'Identifiable (', '( PI', 'PI )', ') Information', 'Information content', 'content becoming', 'becoming mandatory', 'mandatory business', 'business function', 'function .'] 

 TOTAL BIGRAMS --> 19 



 ---- TRI-GRAMS ---- 

 ['Due various compliance', 'various compliance regulation', 'compliance regulation ,', 'regulation , GDPR', ', GDPR ,', 'GDPR , identifying', ', identifying Personally', 'identifying Personally Identifiable', 'Personally Identifiable (', 'Identifiable ( PI', '( PI )', 'PI ) Information', ') Information content', 'Information content becoming', 'content becoming mandatory', 'becoming mandatory business', 'mandatory business function', 'business function .'] 

 TOTAL TRIGRAMS --> 18 



 ---- NOUN PHRASES ---- 

 ['various compliance', 'regulation', 'content', 'mandatory business', 'function'] 

 TOTAL NOUN PHRASES --> 5 



 ---- NER ----

 
 ORGANIZATION ---> ['GDPR']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> ['Due', 'Identifiable']
 TOTAL GPE ENTITY --> 2 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['due', 'variou', 'complianc', 'regul', ',', 'gdpr', ',', 'identifi', 'person', 'identifi', '(', 'pi', ')', 'inform', 'content', 'becom', 'mandatori', 'busi', 'function', '.']

 TOTAL PORTER STEM WORDS ==> 20



 ---- SNOWBALL STEMMING ----

['due', 'various', 'complianc', 'regul', ',', 'gdpr', ',', 'identifi', 'person', 'identifi', '(', 'pi', ')', 'inform', 'content', 'becom', 'mandatori', 'busi', 'function', '.']

 TOTAL SNOWBALL STEM WORDS ==> 20



 ---- LEMMATIZATION ----

['Due', 'various', 'compliance', 'regulation', ',', 'GDPR', ',', 'identifying', 'Personally', 'Identifiable', '(', 'PI', ')', 'Information', 'content', 'becoming', 'mandatory', 'business', 'function', '.']

 TOTAL LEMMATIZE WORDS ==> 20

************************************************************************************************************************

91 --> NLP based trained AI models can, identify  PI information from the content. 


 ---- TOKENS ----

 ['NLP', 'based', 'trained', 'AI', 'models', 'can', ',', 'identify', 'PI', 'information', 'from', 'the', 'content', '.'] 

 TOTAL TOKENS ==> 14

 ---- POST ----

 [('NLP', 'NNP'), ('based', 'VBN'), ('trained', 'VBD'), ('AI', 'NNP'), ('models', 'NNS'), ('can', 'MD'), (',', ','), ('identify', 'VB'), ('PI', 'NNP'), ('information', 'NN'), ('from', 'IN'), ('the', 'DT'), ('content', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['NLP', 'based', 'trained', 'AI', 'models', ',', 'identify', 'PI', 'information', 'content', '.']

 TOTAL FILTERED TOKENS ==>  11

 ---- POST FOR FILTERED TOKENS ----

 [('NLP', 'NNP'), ('based', 'VBN'), ('trained', 'VBD'), ('AI', 'NNP'), ('models', 'NNS'), (',', ','), ('identify', 'VB'), ('PI', 'NNP'), ('information', 'NN'), ('content', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['NLP based', 'based trained', 'trained AI', 'AI models', 'models ,', ', identify', 'identify PI', 'PI information', 'information content', 'content .'] 

 TOTAL BIGRAMS --> 10 



 ---- TRI-GRAMS ---- 

 ['NLP based trained', 'based trained AI', 'trained AI models', 'AI models ,', 'models , identify', ', identify PI', 'identify PI information', 'PI information content', 'information content .'] 

 TOTAL TRIGRAMS --> 9 



 ---- NOUN PHRASES ---- 

 ['information', 'content'] 

 TOTAL NOUN PHRASES --> 2 



 ---- NER ----

 
 ORGANIZATION ---> ['PI']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['nlp', 'base', 'train', 'ai', 'model', ',', 'identifi', 'pi', 'inform', 'content', '.']

 TOTAL PORTER STEM WORDS ==> 11



 ---- SNOWBALL STEMMING ----

['nlp', 'base', 'train', 'ai', 'model', ',', 'identifi', 'pi', 'inform', 'content', '.']

 TOTAL SNOWBALL STEM WORDS ==> 11



 ---- LEMMATIZATION ----

['NLP', 'based', 'trained', 'AI', 'model', ',', 'identify', 'PI', 'information', 'content', '.']

 TOTAL LEMMATIZE WORDS ==> 11

************************************************************************************************************************

92 --> Going through huge content to  understand the context the content is a  daunting task. 


 ---- TOKENS ----

 ['Going', 'through', 'huge', 'content', 'to', 'understand', 'the', 'context', 'the', 'content', 'is', 'a', 'daunting', 'task', '.'] 

 TOTAL TOKENS ==> 15

 ---- POST ----

 [('Going', 'VBG'), ('through', 'IN'), ('huge', 'JJ'), ('content', 'NN'), ('to', 'TO'), ('understand', 'VB'), ('the', 'DT'), ('context', 'NN'), ('the', 'DT'), ('content', 'NN'), ('is', 'VBZ'), ('a', 'DT'), ('daunting', 'VBG'), ('task', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Going', 'huge', 'content', 'understand', 'context', 'content', 'daunting', 'task', '.']

 TOTAL FILTERED TOKENS ==>  9

 ---- POST FOR FILTERED TOKENS ----

 [('Going', 'VBG'), ('huge', 'JJ'), ('content', 'NN'), ('understand', 'NN'), ('context', 'NN'), ('content', 'NN'), ('daunting', 'VBG'), ('task', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Going huge', 'huge content', 'content understand', 'understand context', 'context content', 'content daunting', 'daunting task', 'task .'] 

 TOTAL BIGRAMS --> 8 



 ---- TRI-GRAMS ---- 

 ['Going huge content', 'huge content understand', 'content understand context', 'understand context content', 'context content daunting', 'content daunting task', 'daunting task .'] 

 TOTAL TRIGRAMS --> 7 



 ---- NOUN PHRASES ---- 

 ['huge content', 'understand', 'context', 'content', 'task'] 

 TOTAL NOUN PHRASES --> 5 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['go', 'huge', 'content', 'understand', 'context', 'content', 'daunt', 'task', '.']

 TOTAL PORTER STEM WORDS ==> 9



 ---- SNOWBALL STEMMING ----

['go', 'huge', 'content', 'understand', 'context', 'content', 'daunt', 'task', '.']

 TOTAL SNOWBALL STEM WORDS ==> 9



 ---- LEMMATIZATION ----

['Going', 'huge', 'content', 'understand', 'context', 'content', 'daunting', 'task', '.']

 TOTAL LEMMATIZE WORDS ==> 9

************************************************************************************************************************

93 --> It would be great for users, if  a summary of the content is available. 


 ---- TOKENS ----

 ['It', 'would', 'be', 'great', 'for', 'users', ',', 'if', 'a', 'summary', 'of', 'the', 'content', 'is', 'available', '.'] 

 TOTAL TOKENS ==> 16

 ---- POST ----

 [('It', 'PRP'), ('would', 'MD'), ('be', 'VB'), ('great', 'JJ'), ('for', 'IN'), ('users', 'NNS'), (',', ','), ('if', 'IN'), ('a', 'DT'), ('summary', 'NN'), ('of', 'IN'), ('the', 'DT'), ('content', 'NN'), ('is', 'VBZ'), ('available', 'JJ'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['would', 'great', 'users', ',', 'summary', 'content', 'available', '.']

 TOTAL FILTERED TOKENS ==>  8

 ---- POST FOR FILTERED TOKENS ----

 [('would', 'MD'), ('great', 'JJ'), ('users', 'NNS'), (',', ','), ('summary', 'JJ'), ('content', 'NN'), ('available', 'JJ'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['would great', 'great users', 'users ,', ', summary', 'summary content', 'content available', 'available .'] 

 TOTAL BIGRAMS --> 7 



 ---- TRI-GRAMS ---- 

 ['would great users', 'great users ,', 'users , summary', ', summary content', 'summary content available', 'content available .'] 

 TOTAL TRIGRAMS --> 6 



 ---- NOUN PHRASES ---- 

 ['summary content'] 

 TOTAL NOUN PHRASES --> 1 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['would', 'great', 'user', ',', 'summari', 'content', 'avail', '.']

 TOTAL PORTER STEM WORDS ==> 8



 ---- SNOWBALL STEMMING ----

['would', 'great', 'user', ',', 'summari', 'content', 'avail', '.']

 TOTAL SNOWBALL STEM WORDS ==> 8



 ---- LEMMATIZATION ----

['would', 'great', 'user', ',', 'summary', 'content', 'available', '.']

 TOTAL LEMMATIZE WORDS ==> 8

************************************************************************************************************************

94 --> Following diagram illustrate three  implementations which shows how NLP  can add value while managing content. 


 ---- TOKENS ----

 ['Following', 'diagram', 'illustrate', 'three', 'implementations', 'which', 'shows', 'how', 'NLP', 'can', 'add', 'value', 'while', 'managing', 'content', '.'] 

 TOTAL TOKENS ==> 16

 ---- POST ----

 [('Following', 'VBG'), ('diagram', 'NN'), ('illustrate', 'VBP'), ('three', 'CD'), ('implementations', 'NNS'), ('which', 'WDT'), ('shows', 'VBZ'), ('how', 'WRB'), ('NLP', 'NNP'), ('can', 'MD'), ('add', 'VB'), ('value', 'NN'), ('while', 'IN'), ('managing', 'VBG'), ('content', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Following', 'diagram', 'illustrate', 'three', 'implementations', 'shows', 'NLP', 'add', 'value', 'managing', 'content', '.']

 TOTAL FILTERED TOKENS ==>  12

 ---- POST FOR FILTERED TOKENS ----

 [('Following', 'VBG'), ('diagram', 'NN'), ('illustrate', 'VBP'), ('three', 'CD'), ('implementations', 'NNS'), ('shows', 'VBZ'), ('NLP', 'NNP'), ('add', 'IN'), ('value', 'NN'), ('managing', 'VBG'), ('content', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Following diagram', 'diagram illustrate', 'illustrate three', 'three implementations', 'implementations shows', 'shows NLP', 'NLP add', 'add value', 'value managing', 'managing content', 'content .'] 

 TOTAL BIGRAMS --> 11 



 ---- TRI-GRAMS ---- 

 ['Following diagram illustrate', 'diagram illustrate three', 'illustrate three implementations', 'three implementations shows', 'implementations shows NLP', 'shows NLP add', 'NLP add value', 'add value managing', 'value managing content', 'managing content .'] 

 TOTAL TRIGRAMS --> 10 



 ---- NOUN PHRASES ---- 

 ['diagram', 'value', 'content'] 

 TOTAL NOUN PHRASES --> 3 



 ---- NER ----

 
 ORGANIZATION ---> ['NLP']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['follow', 'diagram', 'illustr', 'three', 'implement', 'show', 'nlp', 'add', 'valu', 'manag', 'content', '.']

 TOTAL PORTER STEM WORDS ==> 12



 ---- SNOWBALL STEMMING ----

['follow', 'diagram', 'illustr', 'three', 'implement', 'show', 'nlp', 'add', 'valu', 'manag', 'content', '.']

 TOTAL SNOWBALL STEM WORDS ==> 12



 ---- LEMMATIZATION ----

['Following', 'diagram', 'illustrate', 'three', 'implementation', 'show', 'NLP', 'add', 'value', 'managing', 'content', '.']

 TOTAL LEMMATIZE WORDS ==> 12

************************************************************************************************************************

95 --> External Document © 2019 Infosys Limited External Document © 2019 Infosys Limited  2.3.1 Implementation scenarios Described below are implementation  scenarios in the context of cognitive  content management. 


 ---- TOKENS ----

 ['External', 'Document', '©', '2019', 'Infosys', 'Limited', 'External', 'Document', '©', '2019', 'Infosys', 'Limited', '2.3.1', 'Implementation', 'scenarios', 'Described', 'below', 'are', 'implementation', 'scenarios', 'in', 'the', 'context', 'of', 'cognitive', 'content', 'management', '.'] 

 TOTAL TOKENS ==> 28

 ---- POST ----

 [('External', 'JJ'), ('Document', 'NNP'), ('©', 'NN'), ('2019', 'CD'), ('Infosys', 'NNP'), ('Limited', 'NNP'), ('External', 'NNP'), ('Document', 'NNP'), ('©', 'NN'), ('2019', 'CD'), ('Infosys', 'NNP'), ('Limited', 'VBD'), ('2.3.1', 'CD'), ('Implementation', 'NNP'), ('scenarios', 'NNS'), ('Described', 'NNP'), ('below', 'IN'), ('are', 'VBP'), ('implementation', 'JJ'), ('scenarios', 'NNS'), ('in', 'IN'), ('the', 'DT'), ('context', 'NN'), ('of', 'IN'), ('cognitive', 'JJ'), ('content', 'JJ'), ('management', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['External', 'Document', '©', '2019', 'Infosys', 'Limited', 'External', 'Document', '©', '2019', 'Infosys', 'Limited', '2.3.1', 'Implementation', 'scenarios', 'Described', 'implementation', 'scenarios', 'context', 'cognitive', 'content', 'management', '.']

 TOTAL FILTERED TOKENS ==>  23

 ---- POST FOR FILTERED TOKENS ----

 [('External', 'JJ'), ('Document', 'NNP'), ('©', 'NN'), ('2019', 'CD'), ('Infosys', 'NNP'), ('Limited', 'NNP'), ('External', 'NNP'), ('Document', 'NNP'), ('©', 'NN'), ('2019', 'CD'), ('Infosys', 'NNP'), ('Limited', 'VBD'), ('2.3.1', 'CD'), ('Implementation', 'NNP'), ('scenarios', 'NNS'), ('Described', 'NNP'), ('implementation', 'NN'), ('scenarios', 'NNS'), ('context', 'VBP'), ('cognitive', 'JJ'), ('content', 'JJ'), ('management', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['External Document', 'Document ©', '© 2019', '2019 Infosys', 'Infosys Limited', 'Limited External', 'External Document', 'Document ©', '© 2019', '2019 Infosys', 'Infosys Limited', 'Limited 2.3.1', '2.3.1 Implementation', 'Implementation scenarios', 'scenarios Described', 'Described implementation', 'implementation scenarios', 'scenarios context', 'context cognitive', 'cognitive content', 'content management', 'management .'] 

 TOTAL BIGRAMS --> 22 



 ---- TRI-GRAMS ---- 

 ['External Document ©', 'Document © 2019', '© 2019 Infosys', '2019 Infosys Limited', 'Infosys Limited External', 'Limited External Document', 'External Document ©', 'Document © 2019', '© 2019 Infosys', '2019 Infosys Limited', 'Infosys Limited 2.3.1', 'Limited 2.3.1 Implementation', '2.3.1 Implementation scenarios', 'Implementation scenarios Described', 'scenarios Described implementation', 'Described implementation scenarios', 'implementation scenarios context', 'scenarios context cognitive', 'context cognitive content', 'cognitive content management', 'content management .'] 

 TOTAL TRIGRAMS --> 21 



 ---- NOUN PHRASES ---- 

 ['©', '©', 'implementation', 'cognitive content management'] 

 TOTAL NOUN PHRASES --> 4 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> ['Infosys']
 TOTAL PERSON ENTITY --> 1 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['extern', 'document', '©', '2019', 'infosi', 'limit', 'extern', 'document', '©', '2019', 'infosi', 'limit', '2.3.1', 'implement', 'scenario', 'describ', 'implement', 'scenario', 'context', 'cognit', 'content', 'manag', '.']

 TOTAL PORTER STEM WORDS ==> 23



 ---- SNOWBALL STEMMING ----

['extern', 'document', '©', '2019', 'infosi', 'limit', 'extern', 'document', '©', '2019', 'infosi', 'limit', '2.3.1', 'implement', 'scenario', 'describ', 'implement', 'scenario', 'context', 'cognit', 'content', 'manag', '.']

 TOTAL SNOWBALL STEM WORDS ==> 23



 ---- LEMMATIZATION ----

['External', 'Document', '©', '2019', 'Infosys', 'Limited', 'External', 'Document', '©', '2019', 'Infosys', 'Limited', '2.3.1', 'Implementation', 'scenario', 'Described', 'implementation', 'scenario', 'context', 'cognitive', 'content', 'management', '.']

 TOTAL LEMMATIZE WORDS ==> 23

************************************************************************************************************************

96 --> 2.3.1.1 Maintaining Metadata Quality  In Content Service Platform, metadata  values represent the identity of the content  and using the metadata values, the content  will be located, accessed and reused. 


 ---- TOKENS ----

 ['2.3.1.1', 'Maintaining', 'Metadata', 'Quality', 'In', 'Content', 'Service', 'Platform', ',', 'metadata', 'values', 'represent', 'the', 'identity', 'of', 'the', 'content', 'and', 'using', 'the', 'metadata', 'values', ',', 'the', 'content', 'will', 'be', 'located', ',', 'accessed', 'and', 'reused', '.'] 

 TOTAL TOKENS ==> 33

 ---- POST ----

 [('2.3.1.1', 'CD'), ('Maintaining', 'NNP'), ('Metadata', 'NNP'), ('Quality', 'NNP'), ('In', 'IN'), ('Content', 'NNP'), ('Service', 'NNP'), ('Platform', 'NNP'), (',', ','), ('metadata', 'NN'), ('values', 'NNS'), ('represent', 'VBP'), ('the', 'DT'), ('identity', 'NN'), ('of', 'IN'), ('the', 'DT'), ('content', 'NN'), ('and', 'CC'), ('using', 'VBG'), ('the', 'DT'), ('metadata', 'NN'), ('values', 'NNS'), (',', ','), ('the', 'DT'), ('content', 'NN'), ('will', 'MD'), ('be', 'VB'), ('located', 'VBN'), (',', ','), ('accessed', 'VBN'), ('and', 'CC'), ('reused', 'VBN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['2.3.1.1', 'Maintaining', 'Metadata', 'Quality', 'Content', 'Service', 'Platform', ',', 'metadata', 'values', 'represent', 'identity', 'content', 'using', 'metadata', 'values', ',', 'content', 'located', ',', 'accessed', 'reused', '.']

 TOTAL FILTERED TOKENS ==>  23

 ---- POST FOR FILTERED TOKENS ----

 [('2.3.1.1', 'CD'), ('Maintaining', 'NNP'), ('Metadata', 'NNP'), ('Quality', 'NNP'), ('Content', 'NNP'), ('Service', 'NNP'), ('Platform', 'NNP'), (',', ','), ('metadata', 'NN'), ('values', 'NNS'), ('represent', 'VBP'), ('identity', 'NN'), ('content', 'NN'), ('using', 'VBG'), ('metadata', 'NNS'), ('values', 'NNS'), (',', ','), ('content', 'NN'), ('located', 'VBN'), (',', ','), ('accessed', 'VBD'), ('reused', 'VBN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['2.3.1.1 Maintaining', 'Maintaining Metadata', 'Metadata Quality', 'Quality Content', 'Content Service', 'Service Platform', 'Platform ,', ', metadata', 'metadata values', 'values represent', 'represent identity', 'identity content', 'content using', 'using metadata', 'metadata values', 'values ,', ', content', 'content located', 'located ,', ', accessed', 'accessed reused', 'reused .'] 

 TOTAL BIGRAMS --> 22 



 ---- TRI-GRAMS ---- 

 ['2.3.1.1 Maintaining Metadata', 'Maintaining Metadata Quality', 'Metadata Quality Content', 'Quality Content Service', 'Content Service Platform', 'Service Platform ,', 'Platform , metadata', ', metadata values', 'metadata values represent', 'values represent identity', 'represent identity content', 'identity content using', 'content using metadata', 'using metadata values', 'metadata values ,', 'values , content', ', content located', 'content located ,', 'located , accessed', ', accessed reused', 'accessed reused .'] 

 TOTAL TRIGRAMS --> 21 



 ---- NOUN PHRASES ---- 

 ['metadata', 'identity', 'content', 'content'] 

 TOTAL NOUN PHRASES --> 4 



 ---- NER ----

 
 ORGANIZATION ---> ['Metadata Quality Content Service Platform']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['2.3.1.1', 'maintain', 'metadata', 'qualiti', 'content', 'servic', 'platform', ',', 'metadata', 'valu', 'repres', 'ident', 'content', 'use', 'metadata', 'valu', ',', 'content', 'locat', ',', 'access', 'reus', '.']

 TOTAL PORTER STEM WORDS ==> 23



 ---- SNOWBALL STEMMING ----

['2.3.1.1', 'maintain', 'metadata', 'qualiti', 'content', 'servic', 'platform', ',', 'metadata', 'valu', 'repres', 'ident', 'content', 'use', 'metadata', 'valu', ',', 'content', 'locat', ',', 'access', 'reus', '.']

 TOTAL SNOWBALL STEM WORDS ==> 23



 ---- LEMMATIZATION ----

['2.3.1.1', 'Maintaining', 'Metadata', 'Quality', 'Content', 'Service', 'Platform', ',', 'metadata', 'value', 'represent', 'identity', 'content', 'using', 'metadata', 'value', ',', 'content', 'located', ',', 'accessed', 'reused', '.']

 TOTAL LEMMATIZE WORDS ==> 23

************************************************************************************************************************

97 --> If a  content is tagged with inaccurate values  or some values are missing, then either  inaccurate content will be referred and  reused, or correct content will not be  located. 


 ---- TOKENS ----

 ['If', 'a', 'content', 'is', 'tagged', 'with', 'inaccurate', 'values', 'or', 'some', 'values', 'are', 'missing', ',', 'then', 'either', 'inaccurate', 'content', 'will', 'be', 'referred', 'and', 'reused', ',', 'or', 'correct', 'content', 'will', 'not', 'be', 'located', '.'] 

 TOTAL TOKENS ==> 32

 ---- POST ----

 [('If', 'IN'), ('a', 'DT'), ('content', 'NN'), ('is', 'VBZ'), ('tagged', 'VBN'), ('with', 'IN'), ('inaccurate', 'JJ'), ('values', 'NNS'), ('or', 'CC'), ('some', 'DT'), ('values', 'NNS'), ('are', 'VBP'), ('missing', 'VBG'), (',', ','), ('then', 'RB'), ('either', 'DT'), ('inaccurate', 'JJ'), ('content', 'NN'), ('will', 'MD'), ('be', 'VB'), ('referred', 'VBN'), ('and', 'CC'), ('reused', 'VBN'), (',', ','), ('or', 'CC'), ('correct', 'JJ'), ('content', 'NN'), ('will', 'MD'), ('not', 'RB'), ('be', 'VB'), ('located', 'VBN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['content', 'tagged', 'inaccurate', 'values', 'values', 'missing', ',', 'either', 'inaccurate', 'content', 'referred', 'reused', ',', 'correct', 'content', 'located', '.']

 TOTAL FILTERED TOKENS ==>  17

 ---- POST FOR FILTERED TOKENS ----

 [('content', 'NN'), ('tagged', 'VBD'), ('inaccurate', 'JJ'), ('values', 'NNS'), ('values', 'NNS'), ('missing', 'VBG'), (',', ','), ('either', 'RB'), ('inaccurate', 'JJ'), ('content', 'NN'), ('referred', 'VBD'), ('reused', 'JJ'), (',', ','), ('correct', 'JJ'), ('content', 'NN'), ('located', 'VBN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['content tagged', 'tagged inaccurate', 'inaccurate values', 'values values', 'values missing', 'missing ,', ', either', 'either inaccurate', 'inaccurate content', 'content referred', 'referred reused', 'reused ,', ', correct', 'correct content', 'content located', 'located .'] 

 TOTAL BIGRAMS --> 16 



 ---- TRI-GRAMS ---- 

 ['content tagged inaccurate', 'tagged inaccurate values', 'inaccurate values values', 'values values missing', 'values missing ,', 'missing , either', ', either inaccurate', 'either inaccurate content', 'inaccurate content referred', 'content referred reused', 'referred reused ,', 'reused , correct', ', correct content', 'correct content located', 'content located .'] 

 TOTAL TRIGRAMS --> 15 



 ---- NOUN PHRASES ---- 

 ['content', 'inaccurate content', 'correct content'] 

 TOTAL NOUN PHRASES --> 3 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['content', 'tag', 'inaccur', 'valu', 'valu', 'miss', ',', 'either', 'inaccur', 'content', 'refer', 'reus', ',', 'correct', 'content', 'locat', '.']

 TOTAL PORTER STEM WORDS ==> 17



 ---- SNOWBALL STEMMING ----

['content', 'tag', 'inaccur', 'valu', 'valu', 'miss', ',', 'either', 'inaccur', 'content', 'refer', 'reus', ',', 'correct', 'content', 'locat', '.']

 TOTAL SNOWBALL STEM WORDS ==> 17



 ---- LEMMATIZATION ----

['content', 'tagged', 'inaccurate', 'value', 'value', 'missing', ',', 'either', 'inaccurate', 'content', 'referred', 'reused', ',', 'correct', 'content', 'located', '.']

 TOTAL LEMMATIZE WORDS ==> 17

************************************************************************************************************************

98 --> Hence maintaining the quality  of metadata (or identity) of content is  extremely important. 


 ---- TOKENS ----

 ['Hence', 'maintaining', 'the', 'quality', 'of', 'metadata', '(', 'or', 'identity', ')', 'of', 'content', 'is', 'extremely', 'important', '.'] 

 TOTAL TOKENS ==> 16

 ---- POST ----

 [('Hence', 'RB'), ('maintaining', 'VBG'), ('the', 'DT'), ('quality', 'NN'), ('of', 'IN'), ('metadata', 'NN'), ('(', '('), ('or', 'CC'), ('identity', 'NN'), (')', ')'), ('of', 'IN'), ('content', 'NN'), ('is', 'VBZ'), ('extremely', 'RB'), ('important', 'JJ'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Hence', 'maintaining', 'quality', 'metadata', '(', 'identity', ')', 'content', 'extremely', 'important', '.']

 TOTAL FILTERED TOKENS ==>  11

 ---- POST FOR FILTERED TOKENS ----

 [('Hence', 'NNP'), ('maintaining', 'VBG'), ('quality', 'NN'), ('metadata', 'NNS'), ('(', '('), ('identity', 'NN'), (')', ')'), ('content', 'NN'), ('extremely', 'RB'), ('important', 'JJ'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Hence maintaining', 'maintaining quality', 'quality metadata', 'metadata (', '( identity', 'identity )', ') content', 'content extremely', 'extremely important', 'important .'] 

 TOTAL BIGRAMS --> 10 



 ---- TRI-GRAMS ---- 

 ['Hence maintaining quality', 'maintaining quality metadata', 'quality metadata (', 'metadata ( identity', '( identity )', 'identity ) content', ') content extremely', 'content extremely important', 'extremely important .'] 

 TOTAL TRIGRAMS --> 9 



 ---- NOUN PHRASES ---- 

 ['quality', 'content'] 

 TOTAL NOUN PHRASES --> 2 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> ['Hence']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['henc', 'maintain', 'qualiti', 'metadata', '(', 'ident', ')', 'content', 'extrem', 'import', '.']

 TOTAL PORTER STEM WORDS ==> 11



 ---- SNOWBALL STEMMING ----

['henc', 'maintain', 'qualiti', 'metadata', '(', 'ident', ')', 'content', 'extrem', 'import', '.']

 TOTAL SNOWBALL STEM WORDS ==> 11



 ---- LEMMATIZATION ----

['Hence', 'maintaining', 'quality', 'metadata', '(', 'identity', ')', 'content', 'extremely', 'important', '.']

 TOTAL LEMMATIZE WORDS ==> 11

************************************************************************************************************************

99 --> NLP based trained AI models can help  us in this. 


 ---- TOKENS ----

 ['NLP', 'based', 'trained', 'AI', 'models', 'can', 'help', 'us', 'in', 'this', '.'] 

 TOTAL TOKENS ==> 11

 ---- POST ----

 [('NLP', 'NNP'), ('based', 'VBN'), ('trained', 'VBD'), ('AI', 'NNP'), ('models', 'NNS'), ('can', 'MD'), ('help', 'VB'), ('us', 'PRP'), ('in', 'IN'), ('this', 'DT'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['NLP', 'based', 'trained', 'AI', 'models', 'help', 'us', '.']

 TOTAL FILTERED TOKENS ==>  8

 ---- POST FOR FILTERED TOKENS ----

 [('NLP', 'NNP'), ('based', 'VBN'), ('trained', 'VBD'), ('AI', 'NNP'), ('models', 'NNS'), ('help', 'VBP'), ('us', 'PRP'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['NLP based', 'based trained', 'trained AI', 'AI models', 'models help', 'help us', 'us .'] 

 TOTAL BIGRAMS --> 7 



 ---- TRI-GRAMS ---- 

 ['NLP based trained', 'based trained AI', 'trained AI models', 'AI models help', 'models help us', 'help us .'] 

 TOTAL TRIGRAMS --> 6 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['nlp', 'base', 'train', 'ai', 'model', 'help', 'us', '.']

 TOTAL PORTER STEM WORDS ==> 8



 ---- SNOWBALL STEMMING ----

['nlp', 'base', 'train', 'ai', 'model', 'help', 'us', '.']

 TOTAL SNOWBALL STEM WORDS ==> 8



 ---- LEMMATIZATION ----

['NLP', 'based', 'trained', 'AI', 'model', 'help', 'u', '.']

 TOTAL LEMMATIZE WORDS ==> 8

************************************************************************************************************************

100 --> It cannot only make sure that  the content is tagged with corrected  metadata, but also tag the content with  new metadata values or missing values. 


 ---- TOKENS ----

 ['It', 'can', 'not', 'only', 'make', 'sure', 'that', 'the', 'content', 'is', 'tagged', 'with', 'corrected', 'metadata', ',', 'but', 'also', 'tag', 'the', 'content', 'with', 'new', 'metadata', 'values', 'or', 'missing', 'values', '.'] 

 TOTAL TOKENS ==> 28

 ---- POST ----

 [('It', 'PRP'), ('can', 'MD'), ('not', 'RB'), ('only', 'RB'), ('make', 'VB'), ('sure', 'JJ'), ('that', 'IN'), ('the', 'DT'), ('content', 'NN'), ('is', 'VBZ'), ('tagged', 'VBN'), ('with', 'IN'), ('corrected', 'VBN'), ('metadata', 'NN'), (',', ','), ('but', 'CC'), ('also', 'RB'), ('tag', 'VBD'), ('the', 'DT'), ('content', 'NN'), ('with', 'IN'), ('new', 'JJ'), ('metadata', 'NNS'), ('values', 'NNS'), ('or', 'CC'), ('missing', 'VBG'), ('values', 'NNS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['make', 'sure', 'content', 'tagged', 'corrected', 'metadata', ',', 'also', 'tag', 'content', 'new', 'metadata', 'values', 'missing', 'values', '.']

 TOTAL FILTERED TOKENS ==>  16

 ---- POST FOR FILTERED TOKENS ----

 [('make', 'VB'), ('sure', 'JJ'), ('content', 'NN'), ('tagged', 'VBD'), ('corrected', 'VBN'), ('metadata', 'NN'), (',', ','), ('also', 'RB'), ('tag', 'VBP'), ('content', 'JJ'), ('new', 'JJ'), ('metadata', 'NN'), ('values', 'NNS'), ('missing', 'VBG'), ('values', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['make sure', 'sure content', 'content tagged', 'tagged corrected', 'corrected metadata', 'metadata ,', ', also', 'also tag', 'tag content', 'content new', 'new metadata', 'metadata values', 'values missing', 'missing values', 'values .'] 

 TOTAL BIGRAMS --> 15 



 ---- TRI-GRAMS ---- 

 ['make sure content', 'sure content tagged', 'content tagged corrected', 'tagged corrected metadata', 'corrected metadata ,', 'metadata , also', ', also tag', 'also tag content', 'tag content new', 'content new metadata', 'new metadata values', 'metadata values missing', 'values missing values', 'missing values .'] 

 TOTAL TRIGRAMS --> 14 



 ---- NOUN PHRASES ---- 

 ['sure content', 'metadata', 'content new metadata'] 

 TOTAL NOUN PHRASES --> 3 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['make', 'sure', 'content', 'tag', 'correct', 'metadata', ',', 'also', 'tag', 'content', 'new', 'metadata', 'valu', 'miss', 'valu', '.']

 TOTAL PORTER STEM WORDS ==> 16



 ---- SNOWBALL STEMMING ----

['make', 'sure', 'content', 'tag', 'correct', 'metadata', ',', 'also', 'tag', 'content', 'new', 'metadata', 'valu', 'miss', 'valu', '.']

 TOTAL SNOWBALL STEM WORDS ==> 16



 ---- LEMMATIZATION ----

['make', 'sure', 'content', 'tagged', 'corrected', 'metadata', ',', 'also', 'tag', 'content', 'new', 'metadata', 'value', 'missing', 'value', '.']

 TOTAL LEMMATIZE WORDS ==> 16

************************************************************************************************************************

101 --> Implementation steps, for maintaining  the metadata quality, is similar to that  of metadata identification but the  post processing is different. 


 ---- TOKENS ----

 ['Implementation', 'steps', ',', 'for', 'maintaining', 'the', 'metadata', 'quality', ',', 'is', 'similar', 'to', 'that', 'of', 'metadata', 'identification', 'but', 'the', 'post', 'processing', 'is', 'different', '.'] 

 TOTAL TOKENS ==> 23

 ---- POST ----

 [('Implementation', 'NNP'), ('steps', 'NNS'), (',', ','), ('for', 'IN'), ('maintaining', 'VBG'), ('the', 'DT'), ('metadata', 'NN'), ('quality', 'NN'), (',', ','), ('is', 'VBZ'), ('similar', 'JJ'), ('to', 'TO'), ('that', 'DT'), ('of', 'IN'), ('metadata', 'JJ'), ('identification', 'NN'), ('but', 'CC'), ('the', 'DT'), ('post', 'NN'), ('processing', 'NN'), ('is', 'VBZ'), ('different', 'JJ'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Implementation', 'steps', ',', 'maintaining', 'metadata', 'quality', ',', 'similar', 'metadata', 'identification', 'post', 'processing', 'different', '.']

 TOTAL FILTERED TOKENS ==>  14

 ---- POST FOR FILTERED TOKENS ----

 [('Implementation', 'NNP'), ('steps', 'NNS'), (',', ','), ('maintaining', 'VBG'), ('metadata', 'NNS'), ('quality', 'NN'), (',', ','), ('similar', 'JJ'), ('metadata', 'NN'), ('identification', 'NN'), ('post', 'NN'), ('processing', 'NN'), ('different', 'JJ'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Implementation steps', 'steps ,', ', maintaining', 'maintaining metadata', 'metadata quality', 'quality ,', ', similar', 'similar metadata', 'metadata identification', 'identification post', 'post processing', 'processing different', 'different .'] 

 TOTAL BIGRAMS --> 13 



 ---- TRI-GRAMS ---- 

 ['Implementation steps ,', 'steps , maintaining', ', maintaining metadata', 'maintaining metadata quality', 'metadata quality ,', 'quality , similar', ', similar metadata', 'similar metadata identification', 'metadata identification post', 'identification post processing', 'post processing different', 'processing different .'] 

 TOTAL TRIGRAMS --> 12 



 ---- NOUN PHRASES ---- 

 ['quality', 'similar metadata', 'identification', 'post', 'processing'] 

 TOTAL NOUN PHRASES --> 5 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['implement', 'step', ',', 'maintain', 'metadata', 'qualiti', ',', 'similar', 'metadata', 'identif', 'post', 'process', 'differ', '.']

 TOTAL PORTER STEM WORDS ==> 14



 ---- SNOWBALL STEMMING ----

['implement', 'step', ',', 'maintain', 'metadata', 'qualiti', ',', 'similar', 'metadata', 'identif', 'post', 'process', 'differ', '.']

 TOTAL SNOWBALL STEM WORDS ==> 14



 ---- LEMMATIZATION ----

['Implementation', 'step', ',', 'maintaining', 'metadata', 'quality', ',', 'similar', 'metadata', 'identification', 'post', 'processing', 'different', '.']

 TOTAL LEMMATIZE WORDS ==> 14

************************************************************************************************************************

102 --> Process of  maintaining metadata quality can run in  background, as batch process and make  sure the metadata values are accurate. 


 ---- TOKENS ----

 ['Process', 'of', 'maintaining', 'metadata', 'quality', 'can', 'run', 'in', 'background', ',', 'as', 'batch', 'process', 'and', 'make', 'sure', 'the', 'metadata', 'values', 'are', 'accurate', '.'] 

 TOTAL TOKENS ==> 22

 ---- POST ----

 [('Process', 'NN'), ('of', 'IN'), ('maintaining', 'VBG'), ('metadata', 'NNS'), ('quality', 'NN'), ('can', 'MD'), ('run', 'VB'), ('in', 'IN'), ('background', 'NN'), (',', ','), ('as', 'IN'), ('batch', 'NN'), ('process', 'NN'), ('and', 'CC'), ('make', 'VB'), ('sure', 'JJ'), ('the', 'DT'), ('metadata', 'NN'), ('values', 'NNS'), ('are', 'VBP'), ('accurate', 'JJ'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Process', 'maintaining', 'metadata', 'quality', 'run', 'background', ',', 'batch', 'process', 'make', 'sure', 'metadata', 'values', 'accurate', '.']

 TOTAL FILTERED TOKENS ==>  15

 ---- POST FOR FILTERED TOKENS ----

 [('Process', 'NN'), ('maintaining', 'VBG'), ('metadata', 'NNS'), ('quality', 'NN'), ('run', 'VBP'), ('background', 'NN'), (',', ','), ('batch', 'NN'), ('process', 'NN'), ('make', 'VBP'), ('sure', 'JJ'), ('metadata', 'NN'), ('values', 'NNS'), ('accurate', 'VBP'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Process maintaining', 'maintaining metadata', 'metadata quality', 'quality run', 'run background', 'background ,', ', batch', 'batch process', 'process make', 'make sure', 'sure metadata', 'metadata values', 'values accurate', 'accurate .'] 

 TOTAL BIGRAMS --> 14 



 ---- TRI-GRAMS ---- 

 ['Process maintaining metadata', 'maintaining metadata quality', 'metadata quality run', 'quality run background', 'run background ,', 'background , batch', ', batch process', 'batch process make', 'process make sure', 'make sure metadata', 'sure metadata values', 'metadata values accurate', 'values accurate .'] 

 TOTAL TRIGRAMS --> 13 



 ---- NOUN PHRASES ---- 

 ['Process', 'quality', 'background', 'batch', 'process', 'sure metadata'] 

 TOTAL NOUN PHRASES --> 6 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['process', 'maintain', 'metadata', 'qualiti', 'run', 'background', ',', 'batch', 'process', 'make', 'sure', 'metadata', 'valu', 'accur', '.']

 TOTAL PORTER STEM WORDS ==> 15



 ---- SNOWBALL STEMMING ----

['process', 'maintain', 'metadata', 'qualiti', 'run', 'background', ',', 'batch', 'process', 'make', 'sure', 'metadata', 'valu', 'accur', '.']

 TOTAL SNOWBALL STEM WORDS ==> 15



 ---- LEMMATIZATION ----

['Process', 'maintaining', 'metadata', 'quality', 'run', 'background', ',', 'batch', 'process', 'make', 'sure', 'metadata', 'value', 'accurate', '.']

 TOTAL LEMMATIZE WORDS ==> 15

************************************************************************************************************************

103 --> 2.3.1.2 Intelligent Summarization Content stored in Content Services  Platform can be large. 


 ---- TOKENS ----

 ['2.3.1.2', 'Intelligent', 'Summarization', 'Content', 'stored', 'in', 'Content', 'Services', 'Platform', 'can', 'be', 'large', '.'] 

 TOTAL TOKENS ==> 13

 ---- POST ----

 [('2.3.1.2', 'CD'), ('Intelligent', 'JJ'), ('Summarization', 'NNP'), ('Content', 'NNP'), ('stored', 'VBD'), ('in', 'IN'), ('Content', 'NNP'), ('Services', 'NNPS'), ('Platform', 'NNP'), ('can', 'MD'), ('be', 'VB'), ('large', 'JJ'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['2.3.1.2', 'Intelligent', 'Summarization', 'Content', 'stored', 'Content', 'Services', 'Platform', 'large', '.']

 TOTAL FILTERED TOKENS ==>  10

 ---- POST FOR FILTERED TOKENS ----

 [('2.3.1.2', 'CD'), ('Intelligent', 'JJ'), ('Summarization', 'NNP'), ('Content', 'NNP'), ('stored', 'VBD'), ('Content', 'NNP'), ('Services', 'NNPS'), ('Platform', 'NNP'), ('large', 'JJ'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['2.3.1.2 Intelligent', 'Intelligent Summarization', 'Summarization Content', 'Content stored', 'stored Content', 'Content Services', 'Services Platform', 'Platform large', 'large .'] 

 TOTAL BIGRAMS --> 9 



 ---- TRI-GRAMS ---- 

 ['2.3.1.2 Intelligent Summarization', 'Intelligent Summarization Content', 'Summarization Content stored', 'Content stored Content', 'stored Content Services', 'Content Services Platform', 'Services Platform large', 'Platform large .'] 

 TOTAL TRIGRAMS --> 8 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> ['Content Services']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['2.3.1.2', 'intellig', 'summar', 'content', 'store', 'content', 'servic', 'platform', 'larg', '.']

 TOTAL PORTER STEM WORDS ==> 10



 ---- SNOWBALL STEMMING ----

['2.3.1.2', 'intellig', 'summar', 'content', 'store', 'content', 'servic', 'platform', 'larg', '.']

 TOTAL SNOWBALL STEM WORDS ==> 10



 ---- LEMMATIZATION ----

['2.3.1.2', 'Intelligent', 'Summarization', 'Content', 'stored', 'Content', 'Services', 'Platform', 'large', '.']

 TOTAL LEMMATIZE WORDS ==> 10

************************************************************************************************************************

104 --> It will be time  consuming for users to go through the  entire content and understand the content  completely. 


 ---- TOKENS ----

 ['It', 'will', 'be', 'time', 'consuming', 'for', 'users', 'to', 'go', 'through', 'the', 'entire', 'content', 'and', 'understand', 'the', 'content', 'completely', '.'] 

 TOTAL TOKENS ==> 19

 ---- POST ----

 [('It', 'PRP'), ('will', 'MD'), ('be', 'VB'), ('time', 'NN'), ('consuming', 'VBG'), ('for', 'IN'), ('users', 'NNS'), ('to', 'TO'), ('go', 'VB'), ('through', 'IN'), ('the', 'DT'), ('entire', 'JJ'), ('content', 'NN'), ('and', 'CC'), ('understand', 'VB'), ('the', 'DT'), ('content', 'NN'), ('completely', 'RB'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['time', 'consuming', 'users', 'go', 'entire', 'content', 'understand', 'content', 'completely', '.']

 TOTAL FILTERED TOKENS ==>  10

 ---- POST FOR FILTERED TOKENS ----

 [('time', 'NN'), ('consuming', 'VBG'), ('users', 'NNS'), ('go', 'VBP'), ('entire', 'JJ'), ('content', 'NN'), ('understand', 'NN'), ('content', 'NN'), ('completely', 'RB'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['time consuming', 'consuming users', 'users go', 'go entire', 'entire content', 'content understand', 'understand content', 'content completely', 'completely .'] 

 TOTAL BIGRAMS --> 9 



 ---- TRI-GRAMS ---- 

 ['time consuming users', 'consuming users go', 'users go entire', 'go entire content', 'entire content understand', 'content understand content', 'understand content completely', 'content completely .'] 

 TOTAL TRIGRAMS --> 8 



 ---- NOUN PHRASES ---- 

 ['time', 'entire content', 'understand', 'content'] 

 TOTAL NOUN PHRASES --> 4 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['time', 'consum', 'user', 'go', 'entir', 'content', 'understand', 'content', 'complet', '.']

 TOTAL PORTER STEM WORDS ==> 10



 ---- SNOWBALL STEMMING ----

['time', 'consum', 'user', 'go', 'entir', 'content', 'understand', 'content', 'complet', '.']

 TOTAL SNOWBALL STEM WORDS ==> 10



 ---- LEMMATIZATION ----

['time', 'consuming', 'user', 'go', 'entire', 'content', 'understand', 'content', 'completely', '.']

 TOTAL LEMMATIZE WORDS ==> 10

************************************************************************************************************************

105 --> Legal contracts, agreements  etc. 


 ---- TOKENS ----

 ['Legal', 'contracts', ',', 'agreements', 'etc', '.'] 

 TOTAL TOKENS ==> 6

 ---- POST ----

 [('Legal', 'NNP'), ('contracts', 'NNS'), (',', ','), ('agreements', 'NNS'), ('etc', 'VBP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Legal', 'contracts', ',', 'agreements', 'etc', '.']

 TOTAL FILTERED TOKENS ==>  6

 ---- POST FOR FILTERED TOKENS ----

 [('Legal', 'NNP'), ('contracts', 'NNS'), (',', ','), ('agreements', 'NNS'), ('etc', 'VBP'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Legal contracts', 'contracts ,', ', agreements', 'agreements etc', 'etc .'] 

 TOTAL BIGRAMS --> 5 



 ---- TRI-GRAMS ---- 

 ['Legal contracts ,', 'contracts , agreements', ', agreements etc', 'agreements etc .'] 

 TOTAL TRIGRAMS --> 4 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> ['Legal']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['legal', 'contract', ',', 'agreement', 'etc', '.']

 TOTAL PORTER STEM WORDS ==> 6



 ---- SNOWBALL STEMMING ----

['legal', 'contract', ',', 'agreement', 'etc', '.']

 TOTAL SNOWBALL STEM WORDS ==> 6



 ---- LEMMATIZATION ----

['Legal', 'contract', ',', 'agreement', 'etc', '.']

 TOTAL LEMMATIZE WORDS ==> 6

************************************************************************************************************************

106 --> falls in these category of content. 


 ---- TOKENS ----

 ['falls', 'in', 'these', 'category', 'of', 'content', '.'] 

 TOTAL TOKENS ==> 7

 ---- POST ----

 [('falls', 'NNS'), ('in', 'IN'), ('these', 'DT'), ('category', 'NN'), ('of', 'IN'), ('content', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['falls', 'category', 'content', '.']

 TOTAL FILTERED TOKENS ==>  4

 ---- POST FOR FILTERED TOKENS ----

 [('falls', 'NNS'), ('category', 'JJ'), ('content', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['falls category', 'category content', 'content .'] 

 TOTAL BIGRAMS --> 3 



 ---- TRI-GRAMS ---- 

 ['falls category content', 'category content .'] 

 TOTAL TRIGRAMS --> 2 



 ---- NOUN PHRASES ---- 

 ['category content'] 

 TOTAL NOUN PHRASES --> 1 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['fall', 'categori', 'content', '.']

 TOTAL PORTER STEM WORDS ==> 4



 ---- SNOWBALL STEMMING ----

['fall', 'categori', 'content', '.']

 TOTAL SNOWBALL STEM WORDS ==> 4



 ---- LEMMATIZATION ----

['fall', 'category', 'content', '.']

 TOTAL LEMMATIZE WORDS ==> 4

************************************************************************************************************************

107 --> The challenge of large content can be  addressed by NLP. 


 ---- TOKENS ----

 ['The', 'challenge', 'of', 'large', 'content', 'can', 'be', 'addressed', 'by', 'NLP', '.'] 

 TOTAL TOKENS ==> 11

 ---- POST ----

 [('The', 'DT'), ('challenge', 'NN'), ('of', 'IN'), ('large', 'JJ'), ('content', 'NN'), ('can', 'MD'), ('be', 'VB'), ('addressed', 'VBN'), ('by', 'IN'), ('NLP', 'NNP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['challenge', 'large', 'content', 'addressed', 'NLP', '.']

 TOTAL FILTERED TOKENS ==>  6

 ---- POST FOR FILTERED TOKENS ----

 [('challenge', 'NN'), ('large', 'JJ'), ('content', 'NN'), ('addressed', 'VBD'), ('NLP', 'NNP'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['challenge large', 'large content', 'content addressed', 'addressed NLP', 'NLP .'] 

 TOTAL BIGRAMS --> 5 



 ---- TRI-GRAMS ---- 

 ['challenge large content', 'large content addressed', 'content addressed NLP', 'addressed NLP .'] 

 TOTAL TRIGRAMS --> 4 



 ---- NOUN PHRASES ---- 

 ['challenge', 'large content'] 

 TOTAL NOUN PHRASES --> 2 



 ---- NER ----

 
 ORGANIZATION ---> ['NLP']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['challeng', 'larg', 'content', 'address', 'nlp', '.']

 TOTAL PORTER STEM WORDS ==> 6



 ---- SNOWBALL STEMMING ----

['challeng', 'larg', 'content', 'address', 'nlp', '.']

 TOTAL SNOWBALL STEM WORDS ==> 6



 ---- LEMMATIZATION ----

['challenge', 'large', 'content', 'addressed', 'NLP', '.']

 TOTAL LEMMATIZE WORDS ==> 6

************************************************************************************************************************

108 --> Using NLP, an AI model  can be trained which identifies important  business terms in the content and then  create the summary of the content using  the sentences or paragraph around these  important business terms. 


 ---- TOKENS ----

 ['Using', 'NLP', ',', 'an', 'AI', 'model', 'can', 'be', 'trained', 'which', 'identifies', 'important', 'business', 'terms', 'in', 'the', 'content', 'and', 'then', 'create', 'the', 'summary', 'of', 'the', 'content', 'using', 'the', 'sentences', 'or', 'paragraph', 'around', 'these', 'important', 'business', 'terms', '.'] 

 TOTAL TOKENS ==> 36

 ---- POST ----

 [('Using', 'VBG'), ('NLP', 'NNP'), (',', ','), ('an', 'DT'), ('AI', 'NNP'), ('model', 'NN'), ('can', 'MD'), ('be', 'VB'), ('trained', 'VBN'), ('which', 'WDT'), ('identifies', 'VBZ'), ('important', 'JJ'), ('business', 'NN'), ('terms', 'NNS'), ('in', 'IN'), ('the', 'DT'), ('content', 'NN'), ('and', 'CC'), ('then', 'RB'), ('create', 'VB'), ('the', 'DT'), ('summary', 'NN'), ('of', 'IN'), ('the', 'DT'), ('content', 'NN'), ('using', 'VBG'), ('the', 'DT'), ('sentences', 'NNS'), ('or', 'CC'), ('paragraph', 'NN'), ('around', 'IN'), ('these', 'DT'), ('important', 'JJ'), ('business', 'NN'), ('terms', 'NNS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Using', 'NLP', ',', 'AI', 'model', 'trained', 'identifies', 'important', 'business', 'terms', 'content', 'create', 'summary', 'content', 'using', 'sentences', 'paragraph', 'around', 'important', 'business', 'terms', '.']

 TOTAL FILTERED TOKENS ==>  22

 ---- POST FOR FILTERED TOKENS ----

 [('Using', 'VBG'), ('NLP', 'NNP'), (',', ','), ('AI', 'NNP'), ('model', 'NN'), ('trained', 'VBD'), ('identifies', 'NNS'), ('important', 'JJ'), ('business', 'NN'), ('terms', 'NNS'), ('content', 'JJ'), ('create', 'NN'), ('summary', 'JJ'), ('content', 'NN'), ('using', 'VBG'), ('sentences', 'NNS'), ('paragraph', 'VBP'), ('around', 'RB'), ('important', 'JJ'), ('business', 'NN'), ('terms', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Using NLP', 'NLP ,', ', AI', 'AI model', 'model trained', 'trained identifies', 'identifies important', 'important business', 'business terms', 'terms content', 'content create', 'create summary', 'summary content', 'content using', 'using sentences', 'sentences paragraph', 'paragraph around', 'around important', 'important business', 'business terms', 'terms .'] 

 TOTAL BIGRAMS --> 21 



 ---- TRI-GRAMS ---- 

 ['Using NLP ,', 'NLP , AI', ', AI model', 'AI model trained', 'model trained identifies', 'trained identifies important', 'identifies important business', 'important business terms', 'business terms content', 'terms content create', 'content create summary', 'create summary content', 'summary content using', 'content using sentences', 'using sentences paragraph', 'sentences paragraph around', 'paragraph around important', 'around important business', 'important business terms', 'business terms .'] 

 TOTAL TRIGRAMS --> 20 



 ---- NOUN PHRASES ---- 

 ['model', 'important business', 'content create', 'summary content', 'important business'] 

 TOTAL NOUN PHRASES --> 5 



 ---- NER ----

 
 ORGANIZATION ---> ['NLP', 'AI']
 TOTAL ORGANIZATION ENTITY --> 2 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['use', 'nlp', ',', 'ai', 'model', 'train', 'identifi', 'import', 'busi', 'term', 'content', 'creat', 'summari', 'content', 'use', 'sentenc', 'paragraph', 'around', 'import', 'busi', 'term', '.']

 TOTAL PORTER STEM WORDS ==> 22



 ---- SNOWBALL STEMMING ----

['use', 'nlp', ',', 'ai', 'model', 'train', 'identifi', 'import', 'busi', 'term', 'content', 'creat', 'summari', 'content', 'use', 'sentenc', 'paragraph', 'around', 'import', 'busi', 'term', '.']

 TOTAL SNOWBALL STEM WORDS ==> 22



 ---- LEMMATIZATION ----

['Using', 'NLP', ',', 'AI', 'model', 'trained', 'identifies', 'important', 'business', 'term', 'content', 'create', 'summary', 'content', 'using', 'sentence', 'paragraph', 'around', 'important', 'business', 'term', '.']

 TOTAL LEMMATIZE WORDS ==> 22

************************************************************************************************************************

109 --> The summarized  document will be the new document  and can be linked to the original content. 


 ---- TOKENS ----

 ['The', 'summarized', 'document', 'will', 'be', 'the', 'new', 'document', 'and', 'can', 'be', 'linked', 'to', 'the', 'original', 'content', '.'] 

 TOTAL TOKENS ==> 17

 ---- POST ----

 [('The', 'DT'), ('summarized', 'JJ'), ('document', 'NN'), ('will', 'MD'), ('be', 'VB'), ('the', 'DT'), ('new', 'JJ'), ('document', 'NN'), ('and', 'CC'), ('can', 'MD'), ('be', 'VB'), ('linked', 'VBN'), ('to', 'TO'), ('the', 'DT'), ('original', 'JJ'), ('content', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['summarized', 'document', 'new', 'document', 'linked', 'original', 'content', '.']

 TOTAL FILTERED TOKENS ==>  8

 ---- POST FOR FILTERED TOKENS ----

 [('summarized', 'VBN'), ('document', 'JJ'), ('new', 'JJ'), ('document', 'NN'), ('linked', 'VBN'), ('original', 'JJ'), ('content', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['summarized document', 'document new', 'new document', 'document linked', 'linked original', 'original content', 'content .'] 

 TOTAL BIGRAMS --> 7 



 ---- TRI-GRAMS ---- 

 ['summarized document new', 'document new document', 'new document linked', 'document linked original', 'linked original content', 'original content .'] 

 TOTAL TRIGRAMS --> 6 



 ---- NOUN PHRASES ---- 

 ['document new document', 'original content'] 

 TOTAL NOUN PHRASES --> 2 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['summar', 'document', 'new', 'document', 'link', 'origin', 'content', '.']

 TOTAL PORTER STEM WORDS ==> 8



 ---- SNOWBALL STEMMING ----

['summar', 'document', 'new', 'document', 'link', 'origin', 'content', '.']

 TOTAL SNOWBALL STEM WORDS ==> 8



 ---- LEMMATIZATION ----

['summarized', 'document', 'new', 'document', 'linked', 'original', 'content', '.']

 TOTAL LEMMATIZE WORDS ==> 8

************************************************************************************************************************

110 --> Once summary is created, users can just  go through the summary document  to understand the complete content,  thereby saving lot of time. 


 ---- TOKENS ----

 ['Once', 'summary', 'is', 'created', ',', 'users', 'can', 'just', 'go', 'through', 'the', 'summary', 'document', 'to', 'understand', 'the', 'complete', 'content', ',', 'thereby', 'saving', 'lot', 'of', 'time', '.'] 

 TOTAL TOKENS ==> 25

 ---- POST ----

 [('Once', 'RB'), ('summary', 'JJ'), ('is', 'VBZ'), ('created', 'VBN'), (',', ','), ('users', 'NNS'), ('can', 'MD'), ('just', 'RB'), ('go', 'VB'), ('through', 'IN'), ('the', 'DT'), ('summary', 'JJ'), ('document', 'NN'), ('to', 'TO'), ('understand', 'VB'), ('the', 'DT'), ('complete', 'JJ'), ('content', 'NN'), (',', ','), ('thereby', 'RB'), ('saving', 'VBG'), ('lot', 'NN'), ('of', 'IN'), ('time', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['summary', 'created', ',', 'users', 'go', 'summary', 'document', 'understand', 'complete', 'content', ',', 'thereby', 'saving', 'lot', 'time', '.']

 TOTAL FILTERED TOKENS ==>  16

 ---- POST FOR FILTERED TOKENS ----

 [('summary', 'JJ'), ('created', 'VBD'), (',', ','), ('users', 'NNS'), ('go', 'VBP'), ('summary', 'JJ'), ('document', 'NN'), ('understand', 'NN'), ('complete', 'JJ'), ('content', 'NN'), (',', ','), ('thereby', 'RB'), ('saving', 'VBG'), ('lot', 'NN'), ('time', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['summary created', 'created ,', ', users', 'users go', 'go summary', 'summary document', 'document understand', 'understand complete', 'complete content', 'content ,', ', thereby', 'thereby saving', 'saving lot', 'lot time', 'time .'] 

 TOTAL BIGRAMS --> 15 



 ---- TRI-GRAMS ---- 

 ['summary created ,', 'created , users', ', users go', 'users go summary', 'go summary document', 'summary document understand', 'document understand complete', 'understand complete content', 'complete content ,', 'content , thereby', ', thereby saving', 'thereby saving lot', 'saving lot time', 'lot time .'] 

 TOTAL TRIGRAMS --> 14 



 ---- NOUN PHRASES ---- 

 ['summary document', 'understand', 'complete content', 'lot', 'time'] 

 TOTAL NOUN PHRASES --> 5 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['summari', 'creat', ',', 'user', 'go', 'summari', 'document', 'understand', 'complet', 'content', ',', 'therebi', 'save', 'lot', 'time', '.']

 TOTAL PORTER STEM WORDS ==> 16



 ---- SNOWBALL STEMMING ----

['summari', 'creat', ',', 'user', 'go', 'summari', 'document', 'understand', 'complet', 'content', ',', 'therebi', 'save', 'lot', 'time', '.']

 TOTAL SNOWBALL STEM WORDS ==> 16



 ---- LEMMATIZATION ----

['summary', 'created', ',', 'user', 'go', 'summary', 'document', 'understand', 'complete', 'content', ',', 'thereby', 'saving', 'lot', 'time', '.']

 TOTAL LEMMATIZE WORDS ==> 16

************************************************************************************************************************

111 --> Intelligent  summarization process is suited for batch  process as against to real time processing. 


 ---- TOKENS ----

 ['Intelligent', 'summarization', 'process', 'is', 'suited', 'for', 'batch', 'process', 'as', 'against', 'to', 'real', 'time', 'processing', '.'] 

 TOTAL TOKENS ==> 15

 ---- POST ----

 [('Intelligent', 'JJ'), ('summarization', 'NN'), ('process', 'NN'), ('is', 'VBZ'), ('suited', 'VBN'), ('for', 'IN'), ('batch', 'NN'), ('process', 'NN'), ('as', 'IN'), ('against', 'IN'), ('to', 'TO'), ('real', 'JJ'), ('time', 'NN'), ('processing', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Intelligent', 'summarization', 'process', 'suited', 'batch', 'process', 'real', 'time', 'processing', '.']

 TOTAL FILTERED TOKENS ==>  10

 ---- POST FOR FILTERED TOKENS ----

 [('Intelligent', 'JJ'), ('summarization', 'NN'), ('process', 'NN'), ('suited', 'VBD'), ('batch', 'NN'), ('process', 'NN'), ('real', 'JJ'), ('time', 'NN'), ('processing', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Intelligent summarization', 'summarization process', 'process suited', 'suited batch', 'batch process', 'process real', 'real time', 'time processing', 'processing .'] 

 TOTAL BIGRAMS --> 9 



 ---- TRI-GRAMS ---- 

 ['Intelligent summarization process', 'summarization process suited', 'process suited batch', 'suited batch process', 'batch process real', 'process real time', 'real time processing', 'time processing .'] 

 TOTAL TRIGRAMS --> 8 



 ---- NOUN PHRASES ---- 

 ['Intelligent summarization', 'process', 'batch', 'process', 'real time', 'processing'] 

 TOTAL NOUN PHRASES --> 6 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['intellig', 'summar', 'process', 'suit', 'batch', 'process', 'real', 'time', 'process', '.']

 TOTAL PORTER STEM WORDS ==> 10



 ---- SNOWBALL STEMMING ----

['intellig', 'summar', 'process', 'suit', 'batch', 'process', 'real', 'time', 'process', '.']

 TOTAL SNOWBALL STEM WORDS ==> 10



 ---- LEMMATIZATION ----

['Intelligent', 'summarization', 'process', 'suited', 'batch', 'process', 'real', 'time', 'processing', '.']

 TOTAL LEMMATIZE WORDS ==> 10

************************************************************************************************************************

112 --> 2.3.1.3 PII Identification Personally Identifiable Information belongs  to an individual and there are regulations,  such as GDPR, which mandates the  protection of PI information. 


 ---- TOKENS ----

 ['2.3.1.3', 'PII', 'Identification', 'Personally', 'Identifiable', 'Information', 'belongs', 'to', 'an', 'individual', 'and', 'there', 'are', 'regulations', ',', 'such', 'as', 'GDPR', ',', 'which', 'mandates', 'the', 'protection', 'of', 'PI', 'information', '.'] 

 TOTAL TOKENS ==> 27

 ---- POST ----

 [('2.3.1.3', 'CD'), ('PII', 'NNP'), ('Identification', 'NNP'), ('Personally', 'NNP'), ('Identifiable', 'NNP'), ('Information', 'NNP'), ('belongs', 'VBZ'), ('to', 'TO'), ('an', 'DT'), ('individual', 'NN'), ('and', 'CC'), ('there', 'EX'), ('are', 'VBP'), ('regulations', 'NNS'), (',', ','), ('such', 'JJ'), ('as', 'IN'), ('GDPR', 'NNP'), (',', ','), ('which', 'WDT'), ('mandates', 'VBZ'), ('the', 'DT'), ('protection', 'NN'), ('of', 'IN'), ('PI', 'NNP'), ('information', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['2.3.1.3', 'PII', 'Identification', 'Personally', 'Identifiable', 'Information', 'belongs', 'individual', 'regulations', ',', 'GDPR', ',', 'mandates', 'protection', 'PI', 'information', '.']

 TOTAL FILTERED TOKENS ==>  17

 ---- POST FOR FILTERED TOKENS ----

 [('2.3.1.3', 'CD'), ('PII', 'NNP'), ('Identification', 'NNP'), ('Personally', 'NNP'), ('Identifiable', 'NNP'), ('Information', 'NNP'), ('belongs', 'VBZ'), ('individual', 'JJ'), ('regulations', 'NNS'), (',', ','), ('GDPR', 'NNP'), (',', ','), ('mandates', 'VBZ'), ('protection', 'NN'), ('PI', 'NNP'), ('information', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['2.3.1.3 PII', 'PII Identification', 'Identification Personally', 'Personally Identifiable', 'Identifiable Information', 'Information belongs', 'belongs individual', 'individual regulations', 'regulations ,', ', GDPR', 'GDPR ,', ', mandates', 'mandates protection', 'protection PI', 'PI information', 'information .'] 

 TOTAL BIGRAMS --> 16 



 ---- TRI-GRAMS ---- 

 ['2.3.1.3 PII Identification', 'PII Identification Personally', 'Identification Personally Identifiable', 'Personally Identifiable Information', 'Identifiable Information belongs', 'Information belongs individual', 'belongs individual regulations', 'individual regulations ,', 'regulations , GDPR', ', GDPR ,', 'GDPR , mandates', ', mandates protection', 'mandates protection PI', 'protection PI information', 'PI information .'] 

 TOTAL TRIGRAMS --> 15 



 ---- NOUN PHRASES ---- 

 ['protection', 'information'] 

 TOTAL NOUN PHRASES --> 2 



 ---- NER ----

 
 ORGANIZATION ---> ['PII Identification Personally Identifiable Information', 'GDPR']
 TOTAL ORGANIZATION ENTITY --> 2 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['2.3.1.3', 'pii', 'identif', 'person', 'identifi', 'inform', 'belong', 'individu', 'regul', ',', 'gdpr', ',', 'mandat', 'protect', 'pi', 'inform', '.']

 TOTAL PORTER STEM WORDS ==> 17



 ---- SNOWBALL STEMMING ----

['2.3.1.3', 'pii', 'identif', 'person', 'identifi', 'inform', 'belong', 'individu', 'regul', ',', 'gdpr', ',', 'mandat', 'protect', 'pi', 'inform', '.']

 TOTAL SNOWBALL STEM WORDS ==> 17



 ---- LEMMATIZATION ----

['2.3.1.3', 'PII', 'Identification', 'Personally', 'Identifiable', 'Information', 'belongs', 'individual', 'regulation', ',', 'GDPR', ',', 'mandate', 'protection', 'PI', 'information', '.']

 TOTAL LEMMATIZE WORDS ==> 17

************************************************************************************************************************

113 --> If PII is not  protected it might lead to data breach, loss  of customer trust, and incur high penalty  and legal issues. 


 ---- TOKENS ----

 ['If', 'PII', 'is', 'not', 'protected', 'it', 'might', 'lead', 'to', 'data', 'breach', ',', 'loss', 'of', 'customer', 'trust', ',', 'and', 'incur', 'high', 'penalty', 'and', 'legal', 'issues', '.'] 

 TOTAL TOKENS ==> 25

 ---- POST ----

 [('If', 'IN'), ('PII', 'NNP'), ('is', 'VBZ'), ('not', 'RB'), ('protected', 'VBN'), ('it', 'PRP'), ('might', 'MD'), ('lead', 'VB'), ('to', 'TO'), ('data', 'NNS'), ('breach', 'NN'), (',', ','), ('loss', 'NN'), ('of', 'IN'), ('customer', 'NN'), ('trust', 'NN'), (',', ','), ('and', 'CC'), ('incur', 'VBP'), ('high', 'JJ'), ('penalty', 'NN'), ('and', 'CC'), ('legal', 'JJ'), ('issues', 'NNS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['PII', 'protected', 'might', 'lead', 'data', 'breach', ',', 'loss', 'customer', 'trust', ',', 'incur', 'high', 'penalty', 'legal', 'issues', '.']

 TOTAL FILTERED TOKENS ==>  17

 ---- POST FOR FILTERED TOKENS ----

 [('PII', 'NNP'), ('protected', 'VBD'), ('might', 'MD'), ('lead', 'VB'), ('data', 'NNS'), ('breach', 'NN'), (',', ','), ('loss', 'NN'), ('customer', 'NN'), ('trust', 'NN'), (',', ','), ('incur', 'VBP'), ('high', 'JJ'), ('penalty', 'NN'), ('legal', 'JJ'), ('issues', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['PII protected', 'protected might', 'might lead', 'lead data', 'data breach', 'breach ,', ', loss', 'loss customer', 'customer trust', 'trust ,', ', incur', 'incur high', 'high penalty', 'penalty legal', 'legal issues', 'issues .'] 

 TOTAL BIGRAMS --> 16 



 ---- TRI-GRAMS ---- 

 ['PII protected might', 'protected might lead', 'might lead data', 'lead data breach', 'data breach ,', 'breach , loss', ', loss customer', 'loss customer trust', 'customer trust ,', 'trust , incur', ', incur high', 'incur high penalty', 'high penalty legal', 'penalty legal issues', 'legal issues .'] 

 TOTAL TRIGRAMS --> 15 



 ---- NOUN PHRASES ---- 

 ['breach', 'loss', 'customer', 'trust', 'high penalty'] 

 TOTAL NOUN PHRASES --> 5 



 ---- NER ----

 
 ORGANIZATION ---> ['PII']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['pii', 'protect', 'might', 'lead', 'data', 'breach', ',', 'loss', 'custom', 'trust', ',', 'incur', 'high', 'penalti', 'legal', 'issu', '.']

 TOTAL PORTER STEM WORDS ==> 17



 ---- SNOWBALL STEMMING ----

['pii', 'protect', 'might', 'lead', 'data', 'breach', ',', 'loss', 'custom', 'trust', ',', 'incur', 'high', 'penalti', 'legal', 'issu', '.']

 TOTAL SNOWBALL STEM WORDS ==> 17



 ---- LEMMATIZATION ----

['PII', 'protected', 'might', 'lead', 'data', 'breach', ',', 'loss', 'customer', 'trust', ',', 'incur', 'high', 'penalty', 'legal', 'issue', '.']

 TOTAL LEMMATIZE WORDS ==> 17

************************************************************************************************************************

114 --> Identifying PI information  in unstructured content is a difficult and  challenging task. 


 ---- TOKENS ----

 ['Identifying', 'PI', 'information', 'in', 'unstructured', 'content', 'is', 'a', 'difficult', 'and', 'challenging', 'task', '.'] 

 TOTAL TOKENS ==> 13

 ---- POST ----

 [('Identifying', 'VBG'), ('PI', 'NNP'), ('information', 'NN'), ('in', 'IN'), ('unstructured', 'JJ'), ('content', 'NN'), ('is', 'VBZ'), ('a', 'DT'), ('difficult', 'JJ'), ('and', 'CC'), ('challenging', 'JJ'), ('task', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Identifying', 'PI', 'information', 'unstructured', 'content', 'difficult', 'challenging', 'task', '.']

 TOTAL FILTERED TOKENS ==>  9

 ---- POST FOR FILTERED TOKENS ----

 [('Identifying', 'VBG'), ('PI', 'NNP'), ('information', 'NN'), ('unstructured', 'VBD'), ('content', 'NN'), ('difficult', 'JJ'), ('challenging', 'VBG'), ('task', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Identifying PI', 'PI information', 'information unstructured', 'unstructured content', 'content difficult', 'difficult challenging', 'challenging task', 'task .'] 

 TOTAL BIGRAMS --> 8 



 ---- TRI-GRAMS ---- 

 ['Identifying PI information', 'PI information unstructured', 'information unstructured content', 'unstructured content difficult', 'content difficult challenging', 'difficult challenging task', 'challenging task .'] 

 TOTAL TRIGRAMS --> 7 



 ---- NOUN PHRASES ---- 

 ['information', 'content', 'task'] 

 TOTAL NOUN PHRASES --> 3 



 ---- NER ----

 
 ORGANIZATION ---> ['PI']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['identifi', 'pi', 'inform', 'unstructur', 'content', 'difficult', 'challeng', 'task', '.']

 TOTAL PORTER STEM WORDS ==> 9



 ---- SNOWBALL STEMMING ----

['identifi', 'pi', 'inform', 'unstructur', 'content', 'difficult', 'challeng', 'task', '.']

 TOTAL SNOWBALL STEM WORDS ==> 9



 ---- LEMMATIZATION ----

['Identifying', 'PI', 'information', 'unstructured', 'content', 'difficult', 'challenging', 'task', '.']

 TOTAL LEMMATIZE WORDS ==> 9

************************************************************************************************************************

115 --> An NLP model can be trained which  can identify PI information in a content. 


 ---- TOKENS ----

 ['An', 'NLP', 'model', 'can', 'be', 'trained', 'which', 'can', 'identify', 'PI', 'information', 'in', 'a', 'content', '.'] 

 TOTAL TOKENS ==> 15

 ---- POST ----

 [('An', 'DT'), ('NLP', 'NNP'), ('model', 'NN'), ('can', 'MD'), ('be', 'VB'), ('trained', 'VBN'), ('which', 'WDT'), ('can', 'MD'), ('identify', 'VB'), ('PI', 'NNP'), ('information', 'NN'), ('in', 'IN'), ('a', 'DT'), ('content', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['NLP', 'model', 'trained', 'identify', 'PI', 'information', 'content', '.']

 TOTAL FILTERED TOKENS ==>  8

 ---- POST FOR FILTERED TOKENS ----

 [('NLP', 'NNP'), ('model', 'NN'), ('trained', 'VBD'), ('identify', 'JJ'), ('PI', 'NNP'), ('information', 'NN'), ('content', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['NLP model', 'model trained', 'trained identify', 'identify PI', 'PI information', 'information content', 'content .'] 

 TOTAL BIGRAMS --> 7 



 ---- TRI-GRAMS ---- 

 ['NLP model trained', 'model trained identify', 'trained identify PI', 'identify PI information', 'PI information content', 'information content .'] 

 TOTAL TRIGRAMS --> 6 



 ---- NOUN PHRASES ---- 

 ['model', 'information', 'content'] 

 TOTAL NOUN PHRASES --> 3 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['nlp', 'model', 'train', 'identifi', 'pi', 'inform', 'content', '.']

 TOTAL PORTER STEM WORDS ==> 8



 ---- SNOWBALL STEMMING ----

['nlp', 'model', 'train', 'identifi', 'pi', 'inform', 'content', '.']

 TOTAL SNOWBALL STEM WORDS ==> 8



 ---- LEMMATIZATION ----

['NLP', 'model', 'trained', 'identify', 'PI', 'information', 'content', '.']

 TOTAL LEMMATIZE WORDS ==> 8

************************************************************************************************************************

116 --> Already, trained models are available,  which helps in identifying standard PI  information such as customer name,  zip code etc. 


 ---- TOKENS ----

 ['Already', ',', 'trained', 'models', 'are', 'available', ',', 'which', 'helps', 'in', 'identifying', 'standard', 'PI', 'information', 'such', 'as', 'customer', 'name', ',', 'zip', 'code', 'etc', '.'] 

 TOTAL TOKENS ==> 23

 ---- POST ----

 [('Already', 'RB'), (',', ','), ('trained', 'JJ'), ('models', 'NNS'), ('are', 'VBP'), ('available', 'JJ'), (',', ','), ('which', 'WDT'), ('helps', 'VBZ'), ('in', 'IN'), ('identifying', 'VBG'), ('standard', 'JJ'), ('PI', 'NNP'), ('information', 'NN'), ('such', 'JJ'), ('as', 'IN'), ('customer', 'NN'), ('name', 'NN'), (',', ','), ('zip', 'NN'), ('code', 'NN'), ('etc', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Already', ',', 'trained', 'models', 'available', ',', 'helps', 'identifying', 'standard', 'PI', 'information', 'customer', 'name', ',', 'zip', 'code', 'etc', '.']

 TOTAL FILTERED TOKENS ==>  18

 ---- POST FOR FILTERED TOKENS ----

 [('Already', 'RB'), (',', ','), ('trained', 'JJ'), ('models', 'NNS'), ('available', 'JJ'), (',', ','), ('helps', 'VBZ'), ('identifying', 'VBG'), ('standard', 'JJ'), ('PI', 'NNP'), ('information', 'NN'), ('customer', 'NN'), ('name', 'NN'), (',', ','), ('zip', 'NN'), ('code', 'NN'), ('etc', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Already ,', ', trained', 'trained models', 'models available', 'available ,', ', helps', 'helps identifying', 'identifying standard', 'standard PI', 'PI information', 'information customer', 'customer name', 'name ,', ', zip', 'zip code', 'code etc', 'etc .'] 

 TOTAL BIGRAMS --> 17 



 ---- TRI-GRAMS ---- 

 ['Already , trained', ', trained models', 'trained models available', 'models available ,', 'available , helps', ', helps identifying', 'helps identifying standard', 'identifying standard PI', 'standard PI information', 'PI information customer', 'information customer name', 'customer name ,', 'name , zip', ', zip code', 'zip code etc', 'code etc .'] 

 TOTAL TRIGRAMS --> 16 



 ---- NOUN PHRASES ---- 

 ['information', 'customer', 'name', 'zip', 'code', 'etc'] 

 TOTAL NOUN PHRASES --> 6 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['alreadi', ',', 'train', 'model', 'avail', ',', 'help', 'identifi', 'standard', 'pi', 'inform', 'custom', 'name', ',', 'zip', 'code', 'etc', '.']

 TOTAL PORTER STEM WORDS ==> 18



 ---- SNOWBALL STEMMING ----

['alreadi', ',', 'train', 'model', 'avail', ',', 'help', 'identifi', 'standard', 'pi', 'inform', 'custom', 'name', ',', 'zip', 'code', 'etc', '.']

 TOTAL SNOWBALL STEM WORDS ==> 18



 ---- LEMMATIZATION ----

['Already', ',', 'trained', 'model', 'available', ',', 'help', 'identifying', 'standard', 'PI', 'information', 'customer', 'name', ',', 'zip', 'code', 'etc', '.']

 TOTAL LEMMATIZE WORDS ==> 18

************************************************************************************************************************

117 --> These AI models can be  contextualized to identify PI information  from a content. 


 ---- TOKENS ----

 ['These', 'AI', 'models', 'can', 'be', 'contextualized', 'to', 'identify', 'PI', 'information', 'from', 'a', 'content', '.'] 

 TOTAL TOKENS ==> 14

 ---- POST ----

 [('These', 'DT'), ('AI', 'NNP'), ('models', 'NNS'), ('can', 'MD'), ('be', 'VB'), ('contextualized', 'VBN'), ('to', 'TO'), ('identify', 'VB'), ('PI', 'NNP'), ('information', 'NN'), ('from', 'IN'), ('a', 'DT'), ('content', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['AI', 'models', 'contextualized', 'identify', 'PI', 'information', 'content', '.']

 TOTAL FILTERED TOKENS ==>  8

 ---- POST FOR FILTERED TOKENS ----

 [('AI', 'NNP'), ('models', 'NNS'), ('contextualized', 'VBN'), ('identify', 'VB'), ('PI', 'NNP'), ('information', 'NN'), ('content', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['AI models', 'models contextualized', 'contextualized identify', 'identify PI', 'PI information', 'information content', 'content .'] 

 TOTAL BIGRAMS --> 7 



 ---- TRI-GRAMS ---- 

 ['AI models contextualized', 'models contextualized identify', 'contextualized identify PI', 'identify PI information', 'PI information content', 'information content .'] 

 TOTAL TRIGRAMS --> 6 



 ---- NOUN PHRASES ---- 

 ['information', 'content'] 

 TOTAL NOUN PHRASES --> 2 



 ---- NER ----

 
 ORGANIZATION ---> ['PI']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['ai', 'model', 'contextu', 'identifi', 'pi', 'inform', 'content', '.']

 TOTAL PORTER STEM WORDS ==> 8



 ---- SNOWBALL STEMMING ----

['ai', 'model', 'contextu', 'identifi', 'pi', 'inform', 'content', '.']

 TOTAL SNOWBALL STEM WORDS ==> 8



 ---- LEMMATIZATION ----

['AI', 'model', 'contextualized', 'identify', 'PI', 'information', 'content', '.']

 TOTAL LEMMATIZE WORDS ==> 8

************************************************************************************************************************

118 --> Once content with PI information is  identified, it can be encrypted or the  content with PI information can be  attached stringent ACL or the content can  be brought under retention or all of these  can be done to ensure the content with PI  information is properly protected. 


 ---- TOKENS ----

 ['Once', 'content', 'with', 'PI', 'information', 'is', 'identified', ',', 'it', 'can', 'be', 'encrypted', 'or', 'the', 'content', 'with', 'PI', 'information', 'can', 'be', 'attached', 'stringent', 'ACL', 'or', 'the', 'content', 'can', 'be', 'brought', 'under', 'retention', 'or', 'all', 'of', 'these', 'can', 'be', 'done', 'to', 'ensure', 'the', 'content', 'with', 'PI', 'information', 'is', 'properly', 'protected', '.'] 

 TOTAL TOKENS ==> 49

 ---- POST ----

 [('Once', 'RB'), ('content', 'JJ'), ('with', 'IN'), ('PI', 'NNP'), ('information', 'NN'), ('is', 'VBZ'), ('identified', 'VBN'), (',', ','), ('it', 'PRP'), ('can', 'MD'), ('be', 'VB'), ('encrypted', 'VBN'), ('or', 'CC'), ('the', 'DT'), ('content', 'NN'), ('with', 'IN'), ('PI', 'NNP'), ('information', 'NN'), ('can', 'MD'), ('be', 'VB'), ('attached', 'VBN'), ('stringent', 'JJ'), ('ACL', 'NNP'), ('or', 'CC'), ('the', 'DT'), ('content', 'NN'), ('can', 'MD'), ('be', 'VB'), ('brought', 'VBN'), ('under', 'IN'), ('retention', 'NN'), ('or', 'CC'), ('all', 'DT'), ('of', 'IN'), ('these', 'DT'), ('can', 'MD'), ('be', 'VB'), ('done', 'VBN'), ('to', 'TO'), ('ensure', 'VB'), ('the', 'DT'), ('content', 'NN'), ('with', 'IN'), ('PI', 'NNP'), ('information', 'NN'), ('is', 'VBZ'), ('properly', 'RB'), ('protected', 'VBN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['content', 'PI', 'information', 'identified', ',', 'encrypted', 'content', 'PI', 'information', 'attached', 'stringent', 'ACL', 'content', 'brought', 'retention', 'done', 'ensure', 'content', 'PI', 'information', 'properly', 'protected', '.']

 TOTAL FILTERED TOKENS ==>  23

 ---- POST FOR FILTERED TOKENS ----

 [('content', 'JJ'), ('PI', 'NNP'), ('information', 'NN'), ('identified', 'VBD'), (',', ','), ('encrypted', 'VBD'), ('content', 'JJ'), ('PI', 'NNP'), ('information', 'NN'), ('attached', 'VBD'), ('stringent', 'JJ'), ('ACL', 'NNP'), ('content', 'NN'), ('brought', 'VBD'), ('retention', 'NN'), ('done', 'VBN'), ('ensure', 'VB'), ('content', 'JJ'), ('PI', 'NNP'), ('information', 'NN'), ('properly', 'RB'), ('protected', 'VBN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['content PI', 'PI information', 'information identified', 'identified ,', ', encrypted', 'encrypted content', 'content PI', 'PI information', 'information attached', 'attached stringent', 'stringent ACL', 'ACL content', 'content brought', 'brought retention', 'retention done', 'done ensure', 'ensure content', 'content PI', 'PI information', 'information properly', 'properly protected', 'protected .'] 

 TOTAL BIGRAMS --> 22 



 ---- TRI-GRAMS ---- 

 ['content PI information', 'PI information identified', 'information identified ,', 'identified , encrypted', ', encrypted content', 'encrypted content PI', 'content PI information', 'PI information attached', 'information attached stringent', 'attached stringent ACL', 'stringent ACL content', 'ACL content brought', 'content brought retention', 'brought retention done', 'retention done ensure', 'done ensure content', 'ensure content PI', 'content PI information', 'PI information properly', 'information properly protected', 'properly protected .'] 

 TOTAL TRIGRAMS --> 21 



 ---- NOUN PHRASES ---- 

 ['information', 'information', 'content', 'retention', 'information'] 

 TOTAL NOUN PHRASES --> 5 



 ---- NER ----

 
 ORGANIZATION ---> ['ACL']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['content', 'pi', 'inform', 'identifi', ',', 'encrypt', 'content', 'pi', 'inform', 'attach', 'stringent', 'acl', 'content', 'brought', 'retent', 'done', 'ensur', 'content', 'pi', 'inform', 'properli', 'protect', '.']

 TOTAL PORTER STEM WORDS ==> 23



 ---- SNOWBALL STEMMING ----

['content', 'pi', 'inform', 'identifi', ',', 'encrypt', 'content', 'pi', 'inform', 'attach', 'stringent', 'acl', 'content', 'brought', 'retent', 'done', 'ensur', 'content', 'pi', 'inform', 'proper', 'protect', '.']

 TOTAL SNOWBALL STEM WORDS ==> 23



 ---- LEMMATIZATION ----

['content', 'PI', 'information', 'identified', ',', 'encrypted', 'content', 'PI', 'information', 'attached', 'stringent', 'ACL', 'content', 'brought', 'retention', 'done', 'ensure', 'content', 'PI', 'information', 'properly', 'protected', '.']

 TOTAL LEMMATIZE WORDS ==> 23

************************************************************************************************************************

119 --> 2.3.2 More Implementation Scenarios There can be additional NLP  implementation scenarios. 


 ---- TOKENS ----

 ['2.3.2', 'More', 'Implementation', 'Scenarios', 'There', 'can', 'be', 'additional', 'NLP', 'implementation', 'scenarios', '.'] 

 TOTAL TOKENS ==> 12

 ---- POST ----

 [('2.3.2', 'CD'), ('More', 'JJR'), ('Implementation', 'NN'), ('Scenarios', 'IN'), ('There', 'EX'), ('can', 'MD'), ('be', 'VB'), ('additional', 'JJ'), ('NLP', 'NNP'), ('implementation', 'NN'), ('scenarios', 'NNS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['2.3.2', 'Implementation', 'Scenarios', 'additional', 'NLP', 'implementation', 'scenarios', '.']

 TOTAL FILTERED TOKENS ==>  8

 ---- POST FOR FILTERED TOKENS ----

 [('2.3.2', 'CD'), ('Implementation', 'NNP'), ('Scenarios', 'NNP'), ('additional', 'JJ'), ('NLP', 'NNP'), ('implementation', 'NN'), ('scenarios', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['2.3.2 Implementation', 'Implementation Scenarios', 'Scenarios additional', 'additional NLP', 'NLP implementation', 'implementation scenarios', 'scenarios .'] 

 TOTAL BIGRAMS --> 7 



 ---- TRI-GRAMS ---- 

 ['2.3.2 Implementation Scenarios', 'Implementation Scenarios additional', 'Scenarios additional NLP', 'additional NLP implementation', 'NLP implementation scenarios', 'implementation scenarios .'] 

 TOTAL TRIGRAMS --> 6 



 ---- NOUN PHRASES ---- 

 ['implementation'] 

 TOTAL NOUN PHRASES --> 1 



 ---- NER ----

 
 ORGANIZATION ---> ['NLP']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['2.3.2', 'implement', 'scenario', 'addit', 'nlp', 'implement', 'scenario', '.']

 TOTAL PORTER STEM WORDS ==> 8



 ---- SNOWBALL STEMMING ----

['2.3.2', 'implement', 'scenario', 'addit', 'nlp', 'implement', 'scenario', '.']

 TOTAL SNOWBALL STEM WORDS ==> 8



 ---- LEMMATIZATION ----

['2.3.2', 'Implementation', 'Scenarios', 'additional', 'NLP', 'implementation', 'scenario', '.']

 TOTAL LEMMATIZE WORDS ==> 8

************************************************************************************************************************

120 --> Analyzing the  risk in contract or agreements is one such  scenario. 


 ---- TOKENS ----

 ['Analyzing', 'the', 'risk', 'in', 'contract', 'or', 'agreements', 'is', 'one', 'such', 'scenario', '.'] 

 TOTAL TOKENS ==> 12

 ---- POST ----

 [('Analyzing', 'VBG'), ('the', 'DT'), ('risk', 'NN'), ('in', 'IN'), ('contract', 'NN'), ('or', 'CC'), ('agreements', 'NNS'), ('is', 'VBZ'), ('one', 'CD'), ('such', 'JJ'), ('scenario', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Analyzing', 'risk', 'contract', 'agreements', 'one', 'scenario', '.']

 TOTAL FILTERED TOKENS ==>  7

 ---- POST FOR FILTERED TOKENS ----

 [('Analyzing', 'VBG'), ('risk', 'NN'), ('contract', 'NN'), ('agreements', 'NNS'), ('one', 'CD'), ('scenario', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Analyzing risk', 'risk contract', 'contract agreements', 'agreements one', 'one scenario', 'scenario .'] 

 TOTAL BIGRAMS --> 6 



 ---- TRI-GRAMS ---- 

 ['Analyzing risk contract', 'risk contract agreements', 'contract agreements one', 'agreements one scenario', 'one scenario .'] 

 TOTAL TRIGRAMS --> 5 



 ---- NOUN PHRASES ---- 

 ['risk', 'contract', 'scenario'] 

 TOTAL NOUN PHRASES --> 3 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['analyz', 'risk', 'contract', 'agreement', 'one', 'scenario', '.']

 TOTAL PORTER STEM WORDS ==> 7



 ---- SNOWBALL STEMMING ----

['analyz', 'risk', 'contract', 'agreement', 'one', 'scenario', '.']

 TOTAL SNOWBALL STEM WORDS ==> 7



 ---- LEMMATIZATION ----

['Analyzing', 'risk', 'contract', 'agreement', 'one', 'scenario', '.']

 TOTAL LEMMATIZE WORDS ==> 7

************************************************************************************************************************

121 --> NLP can be trained to identify  risky terms or phrases and based on the  occurrences of risky terms, the contract or  agreement can be classified as risky. 


 ---- TOKENS ----

 ['NLP', 'can', 'be', 'trained', 'to', 'identify', 'risky', 'terms', 'or', 'phrases', 'and', 'based', 'on', 'the', 'occurrences', 'of', 'risky', 'terms', ',', 'the', 'contract', 'or', 'agreement', 'can', 'be', 'classified', 'as', 'risky', '.'] 

 TOTAL TOKENS ==> 29

 ---- POST ----

 [('NLP', 'NN'), ('can', 'MD'), ('be', 'VB'), ('trained', 'VBN'), ('to', 'TO'), ('identify', 'VB'), ('risky', 'JJ'), ('terms', 'NNS'), ('or', 'CC'), ('phrases', 'NNS'), ('and', 'CC'), ('based', 'VBN'), ('on', 'IN'), ('the', 'DT'), ('occurrences', 'NNS'), ('of', 'IN'), ('risky', 'JJ'), ('terms', 'NNS'), (',', ','), ('the', 'DT'), ('contract', 'NN'), ('or', 'CC'), ('agreement', 'NN'), ('can', 'MD'), ('be', 'VB'), ('classified', 'VBN'), ('as', 'IN'), ('risky', 'JJ'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['NLP', 'trained', 'identify', 'risky', 'terms', 'phrases', 'based', 'occurrences', 'risky', 'terms', ',', 'contract', 'agreement', 'classified', 'risky', '.']

 TOTAL FILTERED TOKENS ==>  16

 ---- POST FOR FILTERED TOKENS ----

 [('NLP', 'NNP'), ('trained', 'VBD'), ('identify', 'VB'), ('risky', 'JJ'), ('terms', 'NNS'), ('phrases', 'NNS'), ('based', 'VBN'), ('occurrences', 'NNS'), ('risky', 'JJ'), ('terms', 'NNS'), (',', ','), ('contract', 'NN'), ('agreement', 'NN'), ('classified', 'VBD'), ('risky', 'JJ'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['NLP trained', 'trained identify', 'identify risky', 'risky terms', 'terms phrases', 'phrases based', 'based occurrences', 'occurrences risky', 'risky terms', 'terms ,', ', contract', 'contract agreement', 'agreement classified', 'classified risky', 'risky .'] 

 TOTAL BIGRAMS --> 15 



 ---- TRI-GRAMS ---- 

 ['NLP trained identify', 'trained identify risky', 'identify risky terms', 'risky terms phrases', 'terms phrases based', 'phrases based occurrences', 'based occurrences risky', 'occurrences risky terms', 'risky terms ,', 'terms , contract', ', contract agreement', 'contract agreement classified', 'agreement classified risky', 'classified risky .'] 

 TOTAL TRIGRAMS --> 14 



 ---- NOUN PHRASES ---- 

 ['contract', 'agreement'] 

 TOTAL NOUN PHRASES --> 2 



 ---- NER ----

 
 ORGANIZATION ---> ['NLP']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['nlp', 'train', 'identifi', 'riski', 'term', 'phrase', 'base', 'occurr', 'riski', 'term', ',', 'contract', 'agreement', 'classifi', 'riski', '.']

 TOTAL PORTER STEM WORDS ==> 16



 ---- SNOWBALL STEMMING ----

['nlp', 'train', 'identifi', 'riski', 'term', 'phrase', 'base', 'occurr', 'riski', 'term', ',', 'contract', 'agreement', 'classifi', 'riski', '.']

 TOTAL SNOWBALL STEM WORDS ==> 16



 ---- LEMMATIZATION ----

['NLP', 'trained', 'identify', 'risky', 'term', 'phrase', 'based', 'occurrence', 'risky', 'term', ',', 'contract', 'agreement', 'classified', 'risky', '.']

 TOTAL LEMMATIZE WORDS ==> 16

************************************************************************************************************************

122 --> Classifying a content in a particular  taxonomy is also one such implementation. 


 ---- TOKENS ----

 ['Classifying', 'a', 'content', 'in', 'a', 'particular', 'taxonomy', 'is', 'also', 'one', 'such', 'implementation', '.'] 

 TOTAL TOKENS ==> 13

 ---- POST ----

 [('Classifying', 'VBG'), ('a', 'DT'), ('content', 'NN'), ('in', 'IN'), ('a', 'DT'), ('particular', 'JJ'), ('taxonomy', 'NN'), ('is', 'VBZ'), ('also', 'RB'), ('one', 'CD'), ('such', 'JJ'), ('implementation', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Classifying', 'content', 'particular', 'taxonomy', 'also', 'one', 'implementation', '.']

 TOTAL FILTERED TOKENS ==>  8

 ---- POST FOR FILTERED TOKENS ----

 [('Classifying', 'VBG'), ('content', 'JJ'), ('particular', 'JJ'), ('taxonomy', 'NN'), ('also', 'RB'), ('one', 'CD'), ('implementation', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Classifying content', 'content particular', 'particular taxonomy', 'taxonomy also', 'also one', 'one implementation', 'implementation .'] 

 TOTAL BIGRAMS --> 7 



 ---- TRI-GRAMS ---- 

 ['Classifying content particular', 'content particular taxonomy', 'particular taxonomy also', 'taxonomy also one', 'also one implementation', 'one implementation .'] 

 TOTAL TRIGRAMS --> 6 



 ---- NOUN PHRASES ---- 

 ['content particular taxonomy', 'implementation'] 

 TOTAL NOUN PHRASES --> 2 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['classifi', 'content', 'particular', 'taxonomi', 'also', 'one', 'implement', '.']

 TOTAL PORTER STEM WORDS ==> 8



 ---- SNOWBALL STEMMING ----

['classifi', 'content', 'particular', 'taxonomi', 'also', 'one', 'implement', '.']

 TOTAL SNOWBALL STEM WORDS ==> 8



 ---- LEMMATIZATION ----

['Classifying', 'content', 'particular', 'taxonomy', 'also', 'one', 'implementation', '.']

 TOTAL LEMMATIZE WORDS ==> 8

************************************************************************************************************************

123 --> Each level of taxonomy can be identified  with specific terms or contextual phrases. 


 ---- TOKENS ----

 ['Each', 'level', 'of', 'taxonomy', 'can', 'be', 'identified', 'with', 'specific', 'terms', 'or', 'contextual', 'phrases', '.'] 

 TOTAL TOKENS ==> 14

 ---- POST ----

 [('Each', 'DT'), ('level', 'NN'), ('of', 'IN'), ('taxonomy', 'NN'), ('can', 'MD'), ('be', 'VB'), ('identified', 'VBN'), ('with', 'IN'), ('specific', 'JJ'), ('terms', 'NNS'), ('or', 'CC'), ('contextual', 'JJ'), ('phrases', 'NNS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['level', 'taxonomy', 'identified', 'specific', 'terms', 'contextual', 'phrases', '.']

 TOTAL FILTERED TOKENS ==>  8

 ---- POST FOR FILTERED TOKENS ----

 [('level', 'NN'), ('taxonomy', 'NN'), ('identified', 'VBN'), ('specific', 'JJ'), ('terms', 'NNS'), ('contextual', 'JJ'), ('phrases', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['level taxonomy', 'taxonomy identified', 'identified specific', 'specific terms', 'terms contextual', 'contextual phrases', 'phrases .'] 

 TOTAL BIGRAMS --> 7 



 ---- TRI-GRAMS ---- 

 ['level taxonomy identified', 'taxonomy identified specific', 'identified specific terms', 'specific terms contextual', 'terms contextual phrases', 'contextual phrases .'] 

 TOTAL TRIGRAMS --> 6 



 ---- NOUN PHRASES ---- 

 ['level', 'taxonomy'] 

 TOTAL NOUN PHRASES --> 2 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['level', 'taxonomi', 'identifi', 'specif', 'term', 'contextu', 'phrase', '.']

 TOTAL PORTER STEM WORDS ==> 8



 ---- SNOWBALL STEMMING ----

['level', 'taxonomi', 'identifi', 'specif', 'term', 'contextu', 'phrase', '.']

 TOTAL SNOWBALL STEM WORDS ==> 8



 ---- LEMMATIZATION ----

['level', 'taxonomy', 'identified', 'specific', 'term', 'contextual', 'phrase', '.']

 TOTAL LEMMATIZE WORDS ==> 8

************************************************************************************************************************

124 --> An NLP model can be trained which  will classify new or unseen content to a  taxonomy term. 


 ---- TOKENS ----

 ['An', 'NLP', 'model', 'can', 'be', 'trained', 'which', 'will', 'classify', 'new', 'or', 'unseen', 'content', 'to', 'a', 'taxonomy', 'term', '.'] 

 TOTAL TOKENS ==> 18

 ---- POST ----

 [('An', 'DT'), ('NLP', 'NNP'), ('model', 'NN'), ('can', 'MD'), ('be', 'VB'), ('trained', 'VBN'), ('which', 'WDT'), ('will', 'MD'), ('classify', 'VB'), ('new', 'JJ'), ('or', 'CC'), ('unseen', 'JJ'), ('content', 'NN'), ('to', 'TO'), ('a', 'DT'), ('taxonomy', 'JJ'), ('term', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['NLP', 'model', 'trained', 'classify', 'new', 'unseen', 'content', 'taxonomy', 'term', '.']

 TOTAL FILTERED TOKENS ==>  10

 ---- POST FOR FILTERED TOKENS ----

 [('NLP', 'NNP'), ('model', 'NN'), ('trained', 'VBD'), ('classify', 'JJ'), ('new', 'JJ'), ('unseen', 'JJ'), ('content', 'JJ'), ('taxonomy', 'JJ'), ('term', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['NLP model', 'model trained', 'trained classify', 'classify new', 'new unseen', 'unseen content', 'content taxonomy', 'taxonomy term', 'term .'] 

 TOTAL BIGRAMS --> 9 



 ---- TRI-GRAMS ---- 

 ['NLP model trained', 'model trained classify', 'trained classify new', 'classify new unseen', 'new unseen content', 'unseen content taxonomy', 'content taxonomy term', 'taxonomy term .'] 

 TOTAL TRIGRAMS --> 8 



 ---- NOUN PHRASES ---- 

 ['model', 'classify new unseen content taxonomy term'] 

 TOTAL NOUN PHRASES --> 2 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['nlp', 'model', 'train', 'classifi', 'new', 'unseen', 'content', 'taxonomi', 'term', '.']

 TOTAL PORTER STEM WORDS ==> 10



 ---- SNOWBALL STEMMING ----

['nlp', 'model', 'train', 'classifi', 'new', 'unseen', 'content', 'taxonomi', 'term', '.']

 TOTAL SNOWBALL STEM WORDS ==> 10



 ---- LEMMATIZATION ----

['NLP', 'model', 'trained', 'classify', 'new', 'unseen', 'content', 'taxonomy', 'term', '.']

 TOTAL LEMMATIZE WORDS ==> 10

************************************************************************************************************************

125 --> Similarly, relevancy of a search result can  be improved by query enrichment. 


 ---- TOKENS ----

 ['Similarly', ',', 'relevancy', 'of', 'a', 'search', 'result', 'can', 'be', 'improved', 'by', 'query', 'enrichment', '.'] 

 TOTAL TOKENS ==> 14

 ---- POST ----

 [('Similarly', 'RB'), (',', ','), ('relevancy', 'NN'), ('of', 'IN'), ('a', 'DT'), ('search', 'NN'), ('result', 'NN'), ('can', 'MD'), ('be', 'VB'), ('improved', 'VBN'), ('by', 'IN'), ('query', 'NN'), ('enrichment', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Similarly', ',', 'relevancy', 'search', 'result', 'improved', 'query', 'enrichment', '.']

 TOTAL FILTERED TOKENS ==>  9

 ---- POST FOR FILTERED TOKENS ----

 [('Similarly', 'RB'), (',', ','), ('relevancy', 'NN'), ('search', 'NN'), ('result', 'NN'), ('improved', 'VBD'), ('query', 'JJ'), ('enrichment', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Similarly ,', ', relevancy', 'relevancy search', 'search result', 'result improved', 'improved query', 'query enrichment', 'enrichment .'] 

 TOTAL BIGRAMS --> 8 



 ---- TRI-GRAMS ---- 

 ['Similarly , relevancy', ', relevancy search', 'relevancy search result', 'search result improved', 'result improved query', 'improved query enrichment', 'query enrichment .'] 

 TOTAL TRIGRAMS --> 7 



 ---- NOUN PHRASES ---- 

 ['relevancy', 'search', 'result', 'query enrichment'] 

 TOTAL NOUN PHRASES --> 4 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['similarli', ',', 'relev', 'search', 'result', 'improv', 'queri', 'enrich', '.']

 TOTAL PORTER STEM WORDS ==> 9



 ---- SNOWBALL STEMMING ----

['similar', ',', 'relev', 'search', 'result', 'improv', 'queri', 'enrich', '.']

 TOTAL SNOWBALL STEM WORDS ==> 9



 ---- LEMMATIZATION ----

['Similarly', ',', 'relevancy', 'search', 'result', 'improved', 'query', 'enrichment', '.']

 TOTAL LEMMATIZE WORDS ==> 9

************************************************************************************************************************

126 --> A  search query before executing against a  search engine, can be analyzed. 


 ---- TOKENS ----

 ['A', 'search', 'query', 'before', 'executing', 'against', 'a', 'search', 'engine', ',', 'can', 'be', 'analyzed', '.'] 

 TOTAL TOKENS ==> 14

 ---- POST ----

 [('A', 'DT'), ('search', 'NN'), ('query', 'NN'), ('before', 'IN'), ('executing', 'VBG'), ('against', 'IN'), ('a', 'DT'), ('search', 'NN'), ('engine', 'NN'), (',', ','), ('can', 'MD'), ('be', 'VB'), ('analyzed', 'VBN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['search', 'query', 'executing', 'search', 'engine', ',', 'analyzed', '.']

 TOTAL FILTERED TOKENS ==>  8

 ---- POST FOR FILTERED TOKENS ----

 [('search', 'NN'), ('query', 'NN'), ('executing', 'VBG'), ('search', 'NN'), ('engine', 'NN'), (',', ','), ('analyzed', 'VBN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['search query', 'query executing', 'executing search', 'search engine', 'engine ,', ', analyzed', 'analyzed .'] 

 TOTAL BIGRAMS --> 7 



 ---- TRI-GRAMS ---- 

 ['search query executing', 'query executing search', 'executing search engine', 'search engine ,', 'engine , analyzed', ', analyzed .'] 

 TOTAL TRIGRAMS --> 6 



 ---- NOUN PHRASES ---- 

 ['search', 'query', 'search', 'engine'] 

 TOTAL NOUN PHRASES --> 4 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['search', 'queri', 'execut', 'search', 'engin', ',', 'analyz', '.']

 TOTAL PORTER STEM WORDS ==> 8



 ---- SNOWBALL STEMMING ----

['search', 'queri', 'execut', 'search', 'engin', ',', 'analyz', '.']

 TOTAL SNOWBALL STEM WORDS ==> 8



 ---- LEMMATIZATION ----

['search', 'query', 'executing', 'search', 'engine', ',', 'analyzed', '.']

 TOTAL LEMMATIZE WORDS ==> 8

************************************************************************************************************************

127 --> Entities or  context from the query can be identified  and then the query can be enriched with  additional selective criteria to get relevant  search results. 


 ---- TOKENS ----

 ['Entities', 'or', 'context', 'from', 'the', 'query', 'can', 'be', 'identified', 'and', 'then', 'the', 'query', 'can', 'be', 'enriched', 'with', 'additional', 'selective', 'criteria', 'to', 'get', 'relevant', 'search', 'results', '.'] 

 TOTAL TOKENS ==> 26

 ---- POST ----

 [('Entities', 'NNS'), ('or', 'CC'), ('context', 'NN'), ('from', 'IN'), ('the', 'DT'), ('query', 'NN'), ('can', 'MD'), ('be', 'VB'), ('identified', 'VBN'), ('and', 'CC'), ('then', 'RB'), ('the', 'DT'), ('query', 'NN'), ('can', 'MD'), ('be', 'VB'), ('enriched', 'VBN'), ('with', 'IN'), ('additional', 'JJ'), ('selective', 'JJ'), ('criteria', 'NNS'), ('to', 'TO'), ('get', 'VB'), ('relevant', 'JJ'), ('search', 'NN'), ('results', 'NNS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Entities', 'context', 'query', 'identified', 'query', 'enriched', 'additional', 'selective', 'criteria', 'get', 'relevant', 'search', 'results', '.']

 TOTAL FILTERED TOKENS ==>  14

 ---- POST FOR FILTERED TOKENS ----

 [('Entities', 'NNS'), ('context', 'VBP'), ('query', 'RB'), ('identified', 'VBN'), ('query', 'NN'), ('enriched', 'VBD'), ('additional', 'JJ'), ('selective', 'NN'), ('criteria', 'NNS'), ('get', 'VBP'), ('relevant', 'JJ'), ('search', 'NN'), ('results', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Entities context', 'context query', 'query identified', 'identified query', 'query enriched', 'enriched additional', 'additional selective', 'selective criteria', 'criteria get', 'get relevant', 'relevant search', 'search results', 'results .'] 

 TOTAL BIGRAMS --> 13 



 ---- TRI-GRAMS ---- 

 ['Entities context query', 'context query identified', 'query identified query', 'identified query enriched', 'query enriched additional', 'enriched additional selective', 'additional selective criteria', 'selective criteria get', 'criteria get relevant', 'get relevant search', 'relevant search results', 'search results .'] 

 TOTAL TRIGRAMS --> 12 



 ---- NOUN PHRASES ---- 

 ['query', 'additional selective', 'relevant search'] 

 TOTAL NOUN PHRASES --> 3 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['entiti', 'context', 'queri', 'identifi', 'queri', 'enrich', 'addit', 'select', 'criteria', 'get', 'relev', 'search', 'result', '.']

 TOTAL PORTER STEM WORDS ==> 14



 ---- SNOWBALL STEMMING ----

['entiti', 'context', 'queri', 'identifi', 'queri', 'enrich', 'addit', 'select', 'criteria', 'get', 'relev', 'search', 'result', '.']

 TOTAL SNOWBALL STEM WORDS ==> 14



 ---- LEMMATIZATION ----

['Entities', 'context', 'query', 'identified', 'query', 'enriched', 'additional', 'selective', 'criterion', 'get', 'relevant', 'search', 'result', '.']

 TOTAL LEMMATIZE WORDS ==> 14

************************************************************************************************************************

128 --> There can be many other NLP  implementations in Content Services  Platforms. 


 ---- TOKENS ----

 ['There', 'can', 'be', 'many', 'other', 'NLP', 'implementations', 'in', 'Content', 'Services', 'Platforms', '.'] 

 TOTAL TOKENS ==> 12

 ---- POST ----

 [('There', 'EX'), ('can', 'MD'), ('be', 'VB'), ('many', 'JJ'), ('other', 'JJ'), ('NLP', 'NNP'), ('implementations', 'NNS'), ('in', 'IN'), ('Content', 'NNP'), ('Services', 'NNPS'), ('Platforms', 'NNP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['many', 'NLP', 'implementations', 'Content', 'Services', 'Platforms', '.']

 TOTAL FILTERED TOKENS ==>  7

 ---- POST FOR FILTERED TOKENS ----

 [('many', 'JJ'), ('NLP', 'NNP'), ('implementations', 'NNS'), ('Content', 'NNP'), ('Services', 'NNPS'), ('Platforms', 'NNP'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['many NLP', 'NLP implementations', 'implementations Content', 'Content Services', 'Services Platforms', 'Platforms .'] 

 TOTAL BIGRAMS --> 6 



 ---- TRI-GRAMS ---- 

 ['many NLP implementations', 'NLP implementations Content', 'implementations Content Services', 'Content Services Platforms', 'Services Platforms .'] 

 TOTAL TRIGRAMS --> 5 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> ['NLP', 'Content Services Platforms']
 TOTAL ORGANIZATION ENTITY --> 2 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['mani', 'nlp', 'implement', 'content', 'servic', 'platform', '.']

 TOTAL PORTER STEM WORDS ==> 7



 ---- SNOWBALL STEMMING ----

['mani', 'nlp', 'implement', 'content', 'servic', 'platform', '.']

 TOTAL SNOWBALL STEM WORDS ==> 7



 ---- LEMMATIZATION ----

['many', 'NLP', 'implementation', 'Content', 'Services', 'Platforms', '.']

 TOTAL LEMMATIZE WORDS ==> 7

************************************************************************************************************************

129 --> Only few are discussed here. 


 ---- TOKENS ----

 ['Only', 'few', 'are', 'discussed', 'here', '.'] 

 TOTAL TOKENS ==> 6

 ---- POST ----

 [('Only', 'RB'), ('few', 'JJ'), ('are', 'VBP'), ('discussed', 'VBN'), ('here', 'RB'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['discussed', '.']

 TOTAL FILTERED TOKENS ==>  2

 ---- POST FOR FILTERED TOKENS ----

 [('discussed', 'VBN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['discussed .'] 

 TOTAL BIGRAMS --> 1 



 ---- TRI-GRAMS ---- 

 [] 

 TOTAL TRIGRAMS --> 0 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['discuss', '.']

 TOTAL PORTER STEM WORDS ==> 2



 ---- SNOWBALL STEMMING ----

['discuss', '.']

 TOTAL SNOWBALL STEM WORDS ==> 2



 ---- LEMMATIZATION ----

['discussed', '.']

 TOTAL LEMMATIZE WORDS ==> 2

************************************************************************************************************************

130 --> External Document © 2019 Infosys Limited External Document © 2019 Infosys Limited  This section discusses a high level process  for implementing scenarios which are  discussed above. 


 ---- TOKENS ----

 ['External', 'Document', '©', '2019', 'Infosys', 'Limited', 'External', 'Document', '©', '2019', 'Infosys', 'Limited', 'This', 'section', 'discusses', 'a', 'high', 'level', 'process', 'for', 'implementing', 'scenarios', 'which', 'are', 'discussed', 'above', '.'] 

 TOTAL TOKENS ==> 27

 ---- POST ----

 [('External', 'JJ'), ('Document', 'NNP'), ('©', 'NN'), ('2019', 'CD'), ('Infosys', 'NNP'), ('Limited', 'NNP'), ('External', 'NNP'), ('Document', 'NNP'), ('©', 'NN'), ('2019', 'CD'), ('Infosys', 'NNP'), ('Limited', 'VBD'), ('This', 'DT'), ('section', 'NN'), ('discusses', 'VBZ'), ('a', 'DT'), ('high', 'JJ'), ('level', 'NN'), ('process', 'NN'), ('for', 'IN'), ('implementing', 'VBG'), ('scenarios', 'NNS'), ('which', 'WDT'), ('are', 'VBP'), ('discussed', 'VBN'), ('above', 'IN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['External', 'Document', '©', '2019', 'Infosys', 'Limited', 'External', 'Document', '©', '2019', 'Infosys', 'Limited', 'section', 'discusses', 'high', 'level', 'process', 'implementing', 'scenarios', 'discussed', '.']

 TOTAL FILTERED TOKENS ==>  21

 ---- POST FOR FILTERED TOKENS ----

 [('External', 'JJ'), ('Document', 'NNP'), ('©', 'NN'), ('2019', 'CD'), ('Infosys', 'NNP'), ('Limited', 'NNP'), ('External', 'NNP'), ('Document', 'NNP'), ('©', 'NN'), ('2019', 'CD'), ('Infosys', 'NNP'), ('Limited', 'NNP'), ('section', 'NN'), ('discusses', 'VBZ'), ('high', 'JJ'), ('level', 'NN'), ('process', 'NN'), ('implementing', 'VBG'), ('scenarios', 'NNS'), ('discussed', 'VBN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['External Document', 'Document ©', '© 2019', '2019 Infosys', 'Infosys Limited', 'Limited External', 'External Document', 'Document ©', '© 2019', '2019 Infosys', 'Infosys Limited', 'Limited section', 'section discusses', 'discusses high', 'high level', 'level process', 'process implementing', 'implementing scenarios', 'scenarios discussed', 'discussed .'] 

 TOTAL BIGRAMS --> 20 



 ---- TRI-GRAMS ---- 

 ['External Document ©', 'Document © 2019', '© 2019 Infosys', '2019 Infosys Limited', 'Infosys Limited External', 'Limited External Document', 'External Document ©', 'Document © 2019', '© 2019 Infosys', '2019 Infosys Limited', 'Infosys Limited section', 'Limited section discusses', 'section discusses high', 'discusses high level', 'high level process', 'level process implementing', 'process implementing scenarios', 'implementing scenarios discussed', 'scenarios discussed .'] 

 TOTAL TRIGRAMS --> 19 



 ---- NOUN PHRASES ---- 

 ['©', '©', 'section', 'high level', 'process'] 

 TOTAL NOUN PHRASES --> 5 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['extern', 'document', '©', '2019', 'infosi', 'limit', 'extern', 'document', '©', '2019', 'infosi', 'limit', 'section', 'discuss', 'high', 'level', 'process', 'implement', 'scenario', 'discuss', '.']

 TOTAL PORTER STEM WORDS ==> 21



 ---- SNOWBALL STEMMING ----

['extern', 'document', '©', '2019', 'infosi', 'limit', 'extern', 'document', '©', '2019', 'infosi', 'limit', 'section', 'discuss', 'high', 'level', 'process', 'implement', 'scenario', 'discuss', '.']

 TOTAL SNOWBALL STEM WORDS ==> 21



 ---- LEMMATIZATION ----

['External', 'Document', '©', '2019', 'Infosys', 'Limited', 'External', 'Document', '©', '2019', 'Infosys', 'Limited', 'section', 'discus', 'high', 'level', 'process', 'implementing', 'scenario', 'discussed', '.']

 TOTAL LEMMATIZE WORDS ==> 21

************************************************************************************************************************

131 --> 3.1 Getting Labeled Content  To train AI model, labeled content is  required. 


 ---- TOKENS ----

 ['3.1', 'Getting', 'Labeled', 'Content', 'To', 'train', 'AI', 'model', ',', 'labeled', 'content', 'is', 'required', '.'] 

 TOTAL TOKENS ==> 14

 ---- POST ----

 [('3.1', 'CD'), ('Getting', 'VBG'), ('Labeled', 'VBN'), ('Content', 'NNP'), ('To', 'TO'), ('train', 'VB'), ('AI', 'NNP'), ('model', 'NN'), (',', ','), ('labeled', 'VBN'), ('content', 'NN'), ('is', 'VBZ'), ('required', 'VBN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['3.1', 'Getting', 'Labeled', 'Content', 'train', 'AI', 'model', ',', 'labeled', 'content', 'required', '.']

 TOTAL FILTERED TOKENS ==>  12

 ---- POST FOR FILTERED TOKENS ----

 [('3.1', 'CD'), ('Getting', 'VBG'), ('Labeled', 'VBN'), ('Content', 'NNP'), ('train', 'NN'), ('AI', 'NNP'), ('model', 'NN'), (',', ','), ('labeled', 'VBN'), ('content', 'NN'), ('required', 'VBN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['3.1 Getting', 'Getting Labeled', 'Labeled Content', 'Content train', 'train AI', 'AI model', 'model ,', ', labeled', 'labeled content', 'content required', 'required .'] 

 TOTAL BIGRAMS --> 11 



 ---- TRI-GRAMS ---- 

 ['3.1 Getting Labeled', 'Getting Labeled Content', 'Labeled Content train', 'Content train AI', 'train AI model', 'AI model ,', 'model , labeled', ', labeled content', 'labeled content required', 'content required .'] 

 TOTAL TRIGRAMS --> 10 



 ---- NOUN PHRASES ---- 

 ['train', 'model', 'content'] 

 TOTAL NOUN PHRASES --> 3 



 ---- NER ----

 
 ORGANIZATION ---> ['Content']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['3.1', 'get', 'label', 'content', 'train', 'ai', 'model', ',', 'label', 'content', 'requir', '.']

 TOTAL PORTER STEM WORDS ==> 12



 ---- SNOWBALL STEMMING ----

['3.1', 'get', 'label', 'content', 'train', 'ai', 'model', ',', 'label', 'content', 'requir', '.']

 TOTAL SNOWBALL STEM WORDS ==> 12



 ---- LEMMATIZATION ----

['3.1', 'Getting', 'Labeled', 'Content', 'train', 'AI', 'model', ',', 'labeled', 'content', 'required', '.']

 TOTAL LEMMATIZE WORDS ==> 12

************************************************************************************************************************

132 --> Most of the time, labeled content  is collected from production environment  and because of this, analyzing the content  and building AI model from the content  needs to be done in secured way. 


 ---- TOKENS ----

 ['Most', 'of', 'the', 'time', ',', 'labeled', 'content', 'is', 'collected', 'from', 'production', 'environment', 'and', 'because', 'of', 'this', ',', 'analyzing', 'the', 'content', 'and', 'building', 'AI', 'model', 'from', 'the', 'content', 'needs', 'to', 'be', 'done', 'in', 'secured', 'way', '.'] 

 TOTAL TOKENS ==> 35

 ---- POST ----

 [('Most', 'JJS'), ('of', 'IN'), ('the', 'DT'), ('time', 'NN'), (',', ','), ('labeled', 'VBN'), ('content', 'NN'), ('is', 'VBZ'), ('collected', 'VBN'), ('from', 'IN'), ('production', 'NN'), ('environment', 'NN'), ('and', 'CC'), ('because', 'IN'), ('of', 'IN'), ('this', 'DT'), (',', ','), ('analyzing', 'VBG'), ('the', 'DT'), ('content', 'NN'), ('and', 'CC'), ('building', 'NN'), ('AI', 'NNP'), ('model', 'NN'), ('from', 'IN'), ('the', 'DT'), ('content', 'NN'), ('needs', 'VBZ'), ('to', 'TO'), ('be', 'VB'), ('done', 'VBN'), ('in', 'IN'), ('secured', 'JJ'), ('way', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['time', ',', 'labeled', 'content', 'collected', 'production', 'environment', ',', 'analyzing', 'content', 'building', 'AI', 'model', 'content', 'needs', 'done', 'secured', 'way', '.']

 TOTAL FILTERED TOKENS ==>  19

 ---- POST FOR FILTERED TOKENS ----

 [('time', 'NN'), (',', ','), ('labeled', 'VBD'), ('content', 'NN'), ('collected', 'VBN'), ('production', 'NN'), ('environment', 'NN'), (',', ','), ('analyzing', 'VBG'), ('content', 'NN'), ('building', 'NN'), ('AI', 'NNP'), ('model', 'NN'), ('content', 'NN'), ('needs', 'VBZ'), ('done', 'VBN'), ('secured', 'JJ'), ('way', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['time ,', ', labeled', 'labeled content', 'content collected', 'collected production', 'production environment', 'environment ,', ', analyzing', 'analyzing content', 'content building', 'building AI', 'AI model', 'model content', 'content needs', 'needs done', 'done secured', 'secured way', 'way .'] 

 TOTAL BIGRAMS --> 18 



 ---- TRI-GRAMS ---- 

 ['time , labeled', ', labeled content', 'labeled content collected', 'content collected production', 'collected production environment', 'production environment ,', 'environment , analyzing', ', analyzing content', 'analyzing content building', 'content building AI', 'building AI model', 'AI model content', 'model content needs', 'content needs done', 'needs done secured', 'done secured way', 'secured way .'] 

 TOTAL TRIGRAMS --> 17 



 ---- NOUN PHRASES ---- 

 ['time', 'content', 'production', 'environment', 'content', 'building', 'model', 'content', 'secured way'] 

 TOTAL NOUN PHRASES --> 9 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['time', ',', 'label', 'content', 'collect', 'product', 'environ', ',', 'analyz', 'content', 'build', 'ai', 'model', 'content', 'need', 'done', 'secur', 'way', '.']

 TOTAL PORTER STEM WORDS ==> 19



 ---- SNOWBALL STEMMING ----

['time', ',', 'label', 'content', 'collect', 'product', 'environ', ',', 'analyz', 'content', 'build', 'ai', 'model', 'content', 'need', 'done', 'secur', 'way', '.']

 TOTAL SNOWBALL STEM WORDS ==> 19



 ---- LEMMATIZATION ----

['time', ',', 'labeled', 'content', 'collected', 'production', 'environment', ',', 'analyzing', 'content', 'building', 'AI', 'model', 'content', 'need', 'done', 'secured', 'way', '.']

 TOTAL LEMMATIZE WORDS ==> 19

************************************************************************************************************************

133 --> Giving  access only to few developers, masking the  content (if possible), are some of the ways  to secure the labeled content. 


 ---- TOKENS ----

 ['Giving', 'access', 'only', 'to', 'few', 'developers', ',', 'masking', 'the', 'content', '(', 'if', 'possible', ')', ',', 'are', 'some', 'of', 'the', 'ways', 'to', 'secure', 'the', 'labeled', 'content', '.'] 

 TOTAL TOKENS ==> 26

 ---- POST ----

 [('Giving', 'VBG'), ('access', 'NN'), ('only', 'RB'), ('to', 'TO'), ('few', 'JJ'), ('developers', 'NNS'), (',', ','), ('masking', 'VBG'), ('the', 'DT'), ('content', 'NN'), ('(', '('), ('if', 'IN'), ('possible', 'JJ'), (')', ')'), (',', ','), ('are', 'VBP'), ('some', 'DT'), ('of', 'IN'), ('the', 'DT'), ('ways', 'NNS'), ('to', 'TO'), ('secure', 'VB'), ('the', 'DT'), ('labeled', 'JJ'), ('content', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Giving', 'access', 'developers', ',', 'masking', 'content', '(', 'possible', ')', ',', 'ways', 'secure', 'labeled', 'content', '.']

 TOTAL FILTERED TOKENS ==>  15

 ---- POST FOR FILTERED TOKENS ----

 [('Giving', 'VBG'), ('access', 'NN'), ('developers', 'NNS'), (',', ','), ('masking', 'VBG'), ('content', 'NN'), ('(', '('), ('possible', 'JJ'), (')', ')'), (',', ','), ('ways', 'NNS'), ('secure', 'VBP'), ('labeled', 'VBN'), ('content', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Giving access', 'access developers', 'developers ,', ', masking', 'masking content', 'content (', '( possible', 'possible )', ') ,', ', ways', 'ways secure', 'secure labeled', 'labeled content', 'content .'] 

 TOTAL BIGRAMS --> 14 



 ---- TRI-GRAMS ---- 

 ['Giving access developers', 'access developers ,', 'developers , masking', ', masking content', 'masking content (', 'content ( possible', '( possible )', 'possible ) ,', ') , ways', ', ways secure', 'ways secure labeled', 'secure labeled content', 'labeled content .'] 

 TOTAL TRIGRAMS --> 13 



 ---- NOUN PHRASES ---- 

 ['access', 'content', 'content'] 

 TOTAL NOUN PHRASES --> 3 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['give', 'access', 'develop', ',', 'mask', 'content', '(', 'possibl', ')', ',', 'way', 'secur', 'label', 'content', '.']

 TOTAL PORTER STEM WORDS ==> 15



 ---- SNOWBALL STEMMING ----

['give', 'access', 'develop', ',', 'mask', 'content', '(', 'possibl', ')', ',', 'way', 'secur', 'label', 'content', '.']

 TOTAL SNOWBALL STEM WORDS ==> 15



 ---- LEMMATIZATION ----

['Giving', 'access', 'developer', ',', 'masking', 'content', '(', 'possible', ')', ',', 'way', 'secure', 'labeled', 'content', '.']

 TOTAL LEMMATIZE WORDS ==> 15

************************************************************************************************************************

134 --> 3.2 Processing training data Labeled content needs to be processed  before it can be used for training. 


 ---- TOKENS ----

 ['3.2', 'Processing', 'training', 'data', 'Labeled', 'content', 'needs', 'to', 'be', 'processed', 'before', 'it', 'can', 'be', 'used', 'for', 'training', '.'] 

 TOTAL TOKENS ==> 18

 ---- POST ----

 [('3.2', 'CD'), ('Processing', 'VBG'), ('training', 'NN'), ('data', 'NNS'), ('Labeled', 'VBN'), ('content', 'JJ'), ('needs', 'NNS'), ('to', 'TO'), ('be', 'VB'), ('processed', 'VBN'), ('before', 'IN'), ('it', 'PRP'), ('can', 'MD'), ('be', 'VB'), ('used', 'VBN'), ('for', 'IN'), ('training', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['3.2', 'Processing', 'training', 'data', 'Labeled', 'content', 'needs', 'processed', 'used', 'training', '.']

 TOTAL FILTERED TOKENS ==>  11

 ---- POST FOR FILTERED TOKENS ----

 [('3.2', 'CD'), ('Processing', 'VBG'), ('training', 'NN'), ('data', 'NNS'), ('Labeled', 'VBN'), ('content', 'JJ'), ('needs', 'NNS'), ('processed', 'VBD'), ('used', 'JJ'), ('training', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['3.2 Processing', 'Processing training', 'training data', 'data Labeled', 'Labeled content', 'content needs', 'needs processed', 'processed used', 'used training', 'training .'] 

 TOTAL BIGRAMS --> 10 



 ---- TRI-GRAMS ---- 

 ['3.2 Processing training', 'Processing training data', 'training data Labeled', 'data Labeled content', 'Labeled content needs', 'content needs processed', 'needs processed used', 'processed used training', 'used training .'] 

 TOTAL TRIGRAMS --> 9 



 ---- NOUN PHRASES ---- 

 ['training', 'used training'] 

 TOTAL NOUN PHRASES --> 2 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['3.2', 'process', 'train', 'data', 'label', 'content', 'need', 'process', 'use', 'train', '.']

 TOTAL PORTER STEM WORDS ==> 11



 ---- SNOWBALL STEMMING ----

['3.2', 'process', 'train', 'data', 'label', 'content', 'need', 'process', 'use', 'train', '.']

 TOTAL SNOWBALL STEM WORDS ==> 11



 ---- LEMMATIZATION ----

['3.2', 'Processing', 'training', 'data', 'Labeled', 'content', 'need', 'processed', 'used', 'training', '.']

 TOTAL LEMMATIZE WORDS ==> 11

************************************************************************************************************************

135 --> What  processing is required, depends on which  AI model will be trained. 


 ---- TOKENS ----

 ['What', 'processing', 'is', 'required', ',', 'depends', 'on', 'which', 'AI', 'model', 'will', 'be', 'trained', '.'] 

 TOTAL TOKENS ==> 14

 ---- POST ----

 [('What', 'WP'), ('processing', 'NN'), ('is', 'VBZ'), ('required', 'VBN'), (',', ','), ('depends', 'VBZ'), ('on', 'IN'), ('which', 'WDT'), ('AI', 'NNP'), ('model', 'NN'), ('will', 'MD'), ('be', 'VB'), ('trained', 'VBN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['processing', 'required', ',', 'depends', 'AI', 'model', 'trained', '.']

 TOTAL FILTERED TOKENS ==>  8

 ---- POST FOR FILTERED TOKENS ----

 [('processing', 'NN'), ('required', 'VBN'), (',', ','), ('depends', 'VBZ'), ('AI', 'NNP'), ('model', 'NN'), ('trained', 'VBD'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['processing required', 'required ,', ', depends', 'depends AI', 'AI model', 'model trained', 'trained .'] 

 TOTAL BIGRAMS --> 7 



 ---- TRI-GRAMS ---- 

 ['processing required ,', 'required , depends', ', depends AI', 'depends AI model', 'AI model trained', 'model trained .'] 

 TOTAL TRIGRAMS --> 6 



 ---- NOUN PHRASES ---- 

 ['processing', 'model'] 

 TOTAL NOUN PHRASES --> 2 



 ---- NER ----

 
 ORGANIZATION ---> ['AI']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['process', 'requir', ',', 'depend', 'ai', 'model', 'train', '.']

 TOTAL PORTER STEM WORDS ==> 8



 ---- SNOWBALL STEMMING ----

['process', 'requir', ',', 'depend', 'ai', 'model', 'train', '.']

 TOTAL SNOWBALL STEM WORDS ==> 8



 ---- LEMMATIZATION ----

['processing', 'required', ',', 'depends', 'AI', 'model', 'trained', '.']

 TOTAL LEMMATIZE WORDS ==> 8

************************************************************************************************************************

136 --> For training NER model, an annotation tool  is required. 


 ---- TOKENS ----

 ['For', 'training', 'NER', 'model', ',', 'an', 'annotation', 'tool', 'is', 'required', '.'] 

 TOTAL TOKENS ==> 11

 ---- POST ----

 [('For', 'IN'), ('training', 'VBG'), ('NER', 'NNP'), ('model', 'NN'), (',', ','), ('an', 'DT'), ('annotation', 'NN'), ('tool', 'NN'), ('is', 'VBZ'), ('required', 'VBN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['training', 'NER', 'model', ',', 'annotation', 'tool', 'required', '.']

 TOTAL FILTERED TOKENS ==>  8

 ---- POST FOR FILTERED TOKENS ----

 [('training', 'NN'), ('NER', 'NNP'), ('model', 'NN'), (',', ','), ('annotation', 'NN'), ('tool', 'NN'), ('required', 'VBN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['training NER', 'NER model', 'model ,', ', annotation', 'annotation tool', 'tool required', 'required .'] 

 TOTAL BIGRAMS --> 7 



 ---- TRI-GRAMS ---- 

 ['training NER model', 'NER model ,', 'model , annotation', ', annotation tool', 'annotation tool required', 'tool required .'] 

 TOTAL TRIGRAMS --> 6 



 ---- NOUN PHRASES ---- 

 ['training', 'model', 'annotation', 'tool'] 

 TOTAL NOUN PHRASES --> 4 



 ---- NER ----

 
 ORGANIZATION ---> ['NER']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['train', 'ner', 'model', ',', 'annot', 'tool', 'requir', '.']

 TOTAL PORTER STEM WORDS ==> 8



 ---- SNOWBALL STEMMING ----

['train', 'ner', 'model', ',', 'annot', 'tool', 'requir', '.']

 TOTAL SNOWBALL STEM WORDS ==> 8



 ---- LEMMATIZATION ----

['training', 'NER', 'model', ',', 'annotation', 'tool', 'required', '.']

 TOTAL LEMMATIZE WORDS ==> 8

************************************************************************************************************************

137 --> There are annotation tools such  as Brat, tagtog, prodigy etc., using which  the entities in the labeled content, can  be annotated and relationship between  the different entities can be established. 


 ---- TOKENS ----

 ['There', 'are', 'annotation', 'tools', 'such', 'as', 'Brat', ',', 'tagtog', ',', 'prodigy', 'etc.', ',', 'using', 'which', 'the', 'entities', 'in', 'the', 'labeled', 'content', ',', 'can', 'be', 'annotated', 'and', 'relationship', 'between', 'the', 'different', 'entities', 'can', 'be', 'established', '.'] 

 TOTAL TOKENS ==> 35

 ---- POST ----

 [('There', 'EX'), ('are', 'VBP'), ('annotation', 'NN'), ('tools', 'NNS'), ('such', 'JJ'), ('as', 'IN'), ('Brat', 'NNP'), (',', ','), ('tagtog', 'NN'), (',', ','), ('prodigy', 'NN'), ('etc.', 'NN'), (',', ','), ('using', 'VBG'), ('which', 'WDT'), ('the', 'DT'), ('entities', 'NNS'), ('in', 'IN'), ('the', 'DT'), ('labeled', 'JJ'), ('content', 'NN'), (',', ','), ('can', 'MD'), ('be', 'VB'), ('annotated', 'VBN'), ('and', 'CC'), ('relationship', 'NN'), ('between', 'IN'), ('the', 'DT'), ('different', 'JJ'), ('entities', 'NNS'), ('can', 'MD'), ('be', 'VB'), ('established', 'VBN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['annotation', 'tools', 'Brat', ',', 'tagtog', ',', 'prodigy', 'etc.', ',', 'using', 'entities', 'labeled', 'content', ',', 'annotated', 'relationship', 'different', 'entities', 'established', '.']

 TOTAL FILTERED TOKENS ==>  20

 ---- POST FOR FILTERED TOKENS ----

 [('annotation', 'NN'), ('tools', 'NNS'), ('Brat', 'NNP'), (',', ','), ('tagtog', 'NN'), (',', ','), ('prodigy', 'NN'), ('etc.', 'NN'), (',', ','), ('using', 'VBG'), ('entities', 'NNS'), ('labeled', 'VBN'), ('content', 'NN'), (',', ','), ('annotated', 'VBD'), ('relationship', 'NN'), ('different', 'JJ'), ('entities', 'NNS'), ('established', 'VBN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['annotation tools', 'tools Brat', 'Brat ,', ', tagtog', 'tagtog ,', ', prodigy', 'prodigy etc.', 'etc. ,', ', using', 'using entities', 'entities labeled', 'labeled content', 'content ,', ', annotated', 'annotated relationship', 'relationship different', 'different entities', 'entities established', 'established .'] 

 TOTAL BIGRAMS --> 19 



 ---- TRI-GRAMS ---- 

 ['annotation tools Brat', 'tools Brat ,', 'Brat , tagtog', ', tagtog ,', 'tagtog , prodigy', ', prodigy etc.', 'prodigy etc. ,', 'etc. , using', ', using entities', 'using entities labeled', 'entities labeled content', 'labeled content ,', 'content , annotated', ', annotated relationship', 'annotated relationship different', 'relationship different entities', 'different entities established', 'entities established .'] 

 TOTAL TRIGRAMS --> 18 



 ---- NOUN PHRASES ---- 

 ['annotation', 'tagtog', 'prodigy', 'etc.', 'content', 'relationship'] 

 TOTAL NOUN PHRASES --> 6 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> ['Brat']
 TOTAL PERSON ENTITY --> 1 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['annot', 'tool', 'brat', ',', 'tagtog', ',', 'prodigi', 'etc.', ',', 'use', 'entiti', 'label', 'content', ',', 'annot', 'relationship', 'differ', 'entiti', 'establish', '.']

 TOTAL PORTER STEM WORDS ==> 20



 ---- SNOWBALL STEMMING ----

['annot', 'tool', 'brat', ',', 'tagtog', ',', 'prodigi', 'etc.', ',', 'use', 'entiti', 'label', 'content', ',', 'annot', 'relationship', 'differ', 'entiti', 'establish', '.']

 TOTAL SNOWBALL STEM WORDS ==> 20



 ---- LEMMATIZATION ----

['annotation', 'tool', 'Brat', ',', 'tagtog', ',', 'prodigy', 'etc.', ',', 'using', 'entity', 'labeled', 'content', ',', 'annotated', 'relationship', 'different', 'entity', 'established', '.']

 TOTAL LEMMATIZE WORDS ==> 20

************************************************************************************************************************

138 --> Depending on the file format supported  by the tool, the labeled content needs to  be converted into the different format. 


 ---- TOKENS ----

 ['Depending', 'on', 'the', 'file', 'format', 'supported', 'by', 'the', 'tool', ',', 'the', 'labeled', 'content', 'needs', 'to', 'be', 'converted', 'into', 'the', 'different', 'format', '.'] 

 TOTAL TOKENS ==> 22

 ---- POST ----

 [('Depending', 'VBG'), ('on', 'IN'), ('the', 'DT'), ('file', 'NN'), ('format', 'NN'), ('supported', 'VBN'), ('by', 'IN'), ('the', 'DT'), ('tool', 'NN'), (',', ','), ('the', 'DT'), ('labeled', 'JJ'), ('content', 'NN'), ('needs', 'NNS'), ('to', 'TO'), ('be', 'VB'), ('converted', 'VBN'), ('into', 'IN'), ('the', 'DT'), ('different', 'JJ'), ('format', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Depending', 'file', 'format', 'supported', 'tool', ',', 'labeled', 'content', 'needs', 'converted', 'different', 'format', '.']

 TOTAL FILTERED TOKENS ==>  13

 ---- POST FOR FILTERED TOKENS ----

 [('Depending', 'VBG'), ('file', 'NN'), ('format', 'NN'), ('supported', 'VBD'), ('tool', 'NN'), (',', ','), ('labeled', 'VBD'), ('content', 'JJ'), ('needs', 'NNS'), ('converted', 'VBN'), ('different', 'JJ'), ('format', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Depending file', 'file format', 'format supported', 'supported tool', 'tool ,', ', labeled', 'labeled content', 'content needs', 'needs converted', 'converted different', 'different format', 'format .'] 

 TOTAL BIGRAMS --> 12 



 ---- TRI-GRAMS ---- 

 ['Depending file format', 'file format supported', 'format supported tool', 'supported tool ,', 'tool , labeled', ', labeled content', 'labeled content needs', 'content needs converted', 'needs converted different', 'converted different format', 'different format .'] 

 TOTAL TRIGRAMS --> 11 



 ---- NOUN PHRASES ---- 

 ['file', 'format', 'tool', 'different format'] 

 TOTAL NOUN PHRASES --> 4 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['depend', 'file', 'format', 'support', 'tool', ',', 'label', 'content', 'need', 'convert', 'differ', 'format', '.']

 TOTAL PORTER STEM WORDS ==> 13



 ---- SNOWBALL STEMMING ----

['depend', 'file', 'format', 'support', 'tool', ',', 'label', 'content', 'need', 'convert', 'differ', 'format', '.']

 TOTAL SNOWBALL STEM WORDS ==> 13



 ---- LEMMATIZATION ----

['Depending', 'file', 'format', 'supported', 'tool', ',', 'labeled', 'content', 'need', 'converted', 'different', 'format', '.']

 TOTAL LEMMATIZE WORDS ==> 13

************************************************************************************************************************

139 --> For example, an audio file needs to be  converted into the text file using “speech- to-text” plug-in, and then it can be used for  annotation by the annotation tool. 


 ---- TOKENS ----

 ['For', 'example', ',', 'an', 'audio', 'file', 'needs', 'to', 'be', 'converted', 'into', 'the', 'text', 'file', 'using', '“', 'speech-', 'to-text', '”', 'plug-in', ',', 'and', 'then', 'it', 'can', 'be', 'used', 'for', 'annotation', 'by', 'the', 'annotation', 'tool', '.'] 

 TOTAL TOKENS ==> 34

 ---- POST ----

 [('For', 'IN'), ('example', 'NN'), (',', ','), ('an', 'DT'), ('audio', 'JJ'), ('file', 'NN'), ('needs', 'NNS'), ('to', 'TO'), ('be', 'VB'), ('converted', 'VBN'), ('into', 'IN'), ('the', 'DT'), ('text', 'NN'), ('file', 'NN'), ('using', 'VBG'), ('“', 'JJ'), ('speech-', 'JJ'), ('to-text', 'JJ'), ('”', 'NN'), ('plug-in', 'NN'), (',', ','), ('and', 'CC'), ('then', 'RB'), ('it', 'PRP'), ('can', 'MD'), ('be', 'VB'), ('used', 'VBN'), ('for', 'IN'), ('annotation', 'NN'), ('by', 'IN'), ('the', 'DT'), ('annotation', 'NN'), ('tool', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['example', ',', 'audio', 'file', 'needs', 'converted', 'text', 'file', 'using', '“', 'speech-', 'to-text', '”', 'plug-in', ',', 'used', 'annotation', 'annotation', 'tool', '.']

 TOTAL FILTERED TOKENS ==>  20

 ---- POST FOR FILTERED TOKENS ----

 [('example', 'NN'), (',', ','), ('audio', 'JJ'), ('file', 'NN'), ('needs', 'NNS'), ('converted', 'VBD'), ('text', 'NN'), ('file', 'NN'), ('using', 'VBG'), ('“', 'JJ'), ('speech-', 'JJ'), ('to-text', 'JJ'), ('”', 'NN'), ('plug-in', 'NN'), (',', ','), ('used', 'VBN'), ('annotation', 'NN'), ('annotation', 'NN'), ('tool', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['example ,', ', audio', 'audio file', 'file needs', 'needs converted', 'converted text', 'text file', 'file using', 'using “', '“ speech-', 'speech- to-text', 'to-text ”', '” plug-in', 'plug-in ,', ', used', 'used annotation', 'annotation annotation', 'annotation tool', 'tool .'] 

 TOTAL BIGRAMS --> 19 



 ---- TRI-GRAMS ---- 

 ['example , audio', ', audio file', 'audio file needs', 'file needs converted', 'needs converted text', 'converted text file', 'text file using', 'file using “', 'using “ speech-', '“ speech- to-text', 'speech- to-text ”', 'to-text ” plug-in', '” plug-in ,', 'plug-in , used', ', used annotation', 'used annotation annotation', 'annotation annotation tool', 'annotation tool .'] 

 TOTAL TRIGRAMS --> 18 



 ---- NOUN PHRASES ---- 

 ['example', 'audio file', 'text', 'file', '“ speech- to-text ”', 'plug-in', 'annotation', 'annotation', 'tool'] 

 TOTAL NOUN PHRASES --> 9 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['exampl', ',', 'audio', 'file', 'need', 'convert', 'text', 'file', 'use', '“', 'speech-', 'to-text', '”', 'plug-in', ',', 'use', 'annot', 'annot', 'tool', '.']

 TOTAL PORTER STEM WORDS ==> 20



 ---- SNOWBALL STEMMING ----

['exampl', ',', 'audio', 'file', 'need', 'convert', 'text', 'file', 'use', '“', 'speech-', 'to-text', '”', 'plug-in', ',', 'use', 'annot', 'annot', 'tool', '.']

 TOTAL SNOWBALL STEM WORDS ==> 20



 ---- LEMMATIZATION ----

['example', ',', 'audio', 'file', 'need', 'converted', 'text', 'file', 'using', '“', 'speech-', 'to-text', '”', 'plug-in', ',', 'used', 'annotation', 'annotation', 'tool', '.']

 TOTAL LEMMATIZE WORDS ==> 20

************************************************************************************************************************

140 --> During  the annotation, entities are identified and  tagged with an entity name. 


 ---- TOKENS ----

 ['During', 'the', 'annotation', ',', 'entities', 'are', 'identified', 'and', 'tagged', 'with', 'an', 'entity', 'name', '.'] 

 TOTAL TOKENS ==> 14

 ---- POST ----

 [('During', 'IN'), ('the', 'DT'), ('annotation', 'NN'), (',', ','), ('entities', 'NNS'), ('are', 'VBP'), ('identified', 'VBN'), ('and', 'CC'), ('tagged', 'VBN'), ('with', 'IN'), ('an', 'DT'), ('entity', 'NN'), ('name', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['annotation', ',', 'entities', 'identified', 'tagged', 'entity', 'name', '.']

 TOTAL FILTERED TOKENS ==>  8

 ---- POST FOR FILTERED TOKENS ----

 [('annotation', 'NN'), (',', ','), ('entities', 'NNS'), ('identified', 'VBD'), ('tagged', 'JJ'), ('entity', 'NN'), ('name', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['annotation ,', ', entities', 'entities identified', 'identified tagged', 'tagged entity', 'entity name', 'name .'] 

 TOTAL BIGRAMS --> 7 



 ---- TRI-GRAMS ---- 

 ['annotation , entities', ', entities identified', 'entities identified tagged', 'identified tagged entity', 'tagged entity name', 'entity name .'] 

 TOTAL TRIGRAMS --> 6 



 ---- NOUN PHRASES ---- 

 ['annotation', 'tagged entity', 'name'] 

 TOTAL NOUN PHRASES --> 3 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['annot', ',', 'entiti', 'identifi', 'tag', 'entiti', 'name', '.']

 TOTAL PORTER STEM WORDS ==> 8



 ---- SNOWBALL STEMMING ----

['annot', ',', 'entiti', 'identifi', 'tag', 'entiti', 'name', '.']

 TOTAL SNOWBALL STEM WORDS ==> 8



 ---- LEMMATIZATION ----

['annotation', ',', 'entity', 'identified', 'tagged', 'entity', 'name', '.']

 TOTAL LEMMATIZE WORDS ==> 8

************************************************************************************************************************

141 --> For example,  an employee name can be identified  and tagged to entity name “Person”. 


 ---- TOKENS ----

 ['For', 'example', ',', 'an', 'employee', 'name', 'can', 'be', 'identified', 'and', 'tagged', 'to', 'entity', 'name', '“', 'Person', '”', '.'] 

 TOTAL TOKENS ==> 18

 ---- POST ----

 [('For', 'IN'), ('example', 'NN'), (',', ','), ('an', 'DT'), ('employee', 'NN'), ('name', 'NN'), ('can', 'MD'), ('be', 'VB'), ('identified', 'VBN'), ('and', 'CC'), ('tagged', 'VBN'), ('to', 'TO'), ('entity', 'VB'), ('name', 'NN'), ('“', 'NNP'), ('Person', 'NNP'), ('”', 'NNP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['example', ',', 'employee', 'name', 'identified', 'tagged', 'entity', 'name', '“', 'Person', '”', '.']

 TOTAL FILTERED TOKENS ==>  12

 ---- POST FOR FILTERED TOKENS ----

 [('example', 'NN'), (',', ','), ('employee', 'NN'), ('name', 'NN'), ('identified', 'VBD'), ('tagged', 'JJ'), ('entity', 'NN'), ('name', 'NN'), ('“', 'NNP'), ('Person', 'NNP'), ('”', 'NNP'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['example ,', ', employee', 'employee name', 'name identified', 'identified tagged', 'tagged entity', 'entity name', 'name “', '“ Person', 'Person ”', '” .'] 

 TOTAL BIGRAMS --> 11 



 ---- TRI-GRAMS ---- 

 ['example , employee', ', employee name', 'employee name identified', 'name identified tagged', 'identified tagged entity', 'tagged entity name', 'entity name “', 'name “ Person', '“ Person ”', 'Person ” .'] 

 TOTAL TRIGRAMS --> 10 



 ---- NOUN PHRASES ---- 

 ['example', 'employee', 'name', 'tagged entity', 'name'] 

 TOTAL NOUN PHRASES --> 5 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> ['Person']
 TOTAL PERSON ENTITY --> 1 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['exampl', ',', 'employe', 'name', 'identifi', 'tag', 'entiti', 'name', '“', 'person', '”', '.']

 TOTAL PORTER STEM WORDS ==> 12



 ---- SNOWBALL STEMMING ----

['exampl', ',', 'employe', 'name', 'identifi', 'tag', 'entiti', 'name', '“', 'person', '”', '.']

 TOTAL SNOWBALL STEM WORDS ==> 12



 ---- LEMMATIZATION ----

['example', ',', 'employee', 'name', 'identified', 'tagged', 'entity', 'name', '“', 'Person', '”', '.']

 TOTAL LEMMATIZE WORDS ==> 12

************************************************************************************************************************

142 --> For  accurate results, relationship between the  entities can be established. 


 ---- TOKENS ----

 ['For', 'accurate', 'results', ',', 'relationship', 'between', 'the', 'entities', 'can', 'be', 'established', '.'] 

 TOTAL TOKENS ==> 12

 ---- POST ----

 [('For', 'IN'), ('accurate', 'JJ'), ('results', 'NNS'), (',', ','), ('relationship', 'NN'), ('between', 'IN'), ('the', 'DT'), ('entities', 'NNS'), ('can', 'MD'), ('be', 'VB'), ('established', 'VBN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['accurate', 'results', ',', 'relationship', 'entities', 'established', '.']

 TOTAL FILTERED TOKENS ==>  7

 ---- POST FOR FILTERED TOKENS ----

 [('accurate', 'JJ'), ('results', 'NNS'), (',', ','), ('relationship', 'NN'), ('entities', 'NNS'), ('established', 'VBN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['accurate results', 'results ,', ', relationship', 'relationship entities', 'entities established', 'established .'] 

 TOTAL BIGRAMS --> 6 



 ---- TRI-GRAMS ---- 

 ['accurate results ,', 'results , relationship', ', relationship entities', 'relationship entities established', 'entities established .'] 

 TOTAL TRIGRAMS --> 5 



 ---- NOUN PHRASES ---- 

 ['relationship'] 

 TOTAL NOUN PHRASES --> 1 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['accur', 'result', ',', 'relationship', 'entiti', 'establish', '.']

 TOTAL PORTER STEM WORDS ==> 7



 ---- SNOWBALL STEMMING ----

['accur', 'result', ',', 'relationship', 'entiti', 'establish', '.']

 TOTAL SNOWBALL STEM WORDS ==> 7



 ---- LEMMATIZATION ----

['accurate', 'result', ',', 'relationship', 'entity', 'established', '.']

 TOTAL LEMMATIZE WORDS ==> 7

************************************************************************************************************************

143 --> For example,  there are two entities, employee name as  “person” and a bank name as “organization”  in one sentence. 


 ---- TOKENS ----

 ['For', 'example', ',', 'there', 'are', 'two', 'entities', ',', 'employee', 'name', 'as', '“', 'person', '”', 'and', 'a', 'bank', 'name', 'as', '“', 'organization', '”', 'in', 'one', 'sentence', '.'] 

 TOTAL TOKENS ==> 26

 ---- POST ----

 [('For', 'IN'), ('example', 'NN'), (',', ','), ('there', 'EX'), ('are', 'VBP'), ('two', 'CD'), ('entities', 'NNS'), (',', ','), ('employee', 'NN'), ('name', 'NN'), ('as', 'IN'), ('“', 'NNP'), ('person', 'NN'), ('”', 'NNP'), ('and', 'CC'), ('a', 'DT'), ('bank', 'NN'), ('name', 'NN'), ('as', 'IN'), ('“', 'JJ'), ('organization', 'NN'), ('”', 'NN'), ('in', 'IN'), ('one', 'CD'), ('sentence', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['example', ',', 'two', 'entities', ',', 'employee', 'name', '“', 'person', '”', 'bank', 'name', '“', 'organization', '”', 'one', 'sentence', '.']

 TOTAL FILTERED TOKENS ==>  18

 ---- POST FOR FILTERED TOKENS ----

 [('example', 'NN'), (',', ','), ('two', 'CD'), ('entities', 'NNS'), (',', ','), ('employee', 'NN'), ('name', 'NN'), ('“', 'NNP'), ('person', 'NN'), ('”', 'NNP'), ('bank', 'NN'), ('name', 'NN'), ('“', 'NNP'), ('organization', 'NN'), ('”', 'NNP'), ('one', 'CD'), ('sentence', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['example ,', ', two', 'two entities', 'entities ,', ', employee', 'employee name', 'name “', '“ person', 'person ”', '” bank', 'bank name', 'name “', '“ organization', 'organization ”', '” one', 'one sentence', 'sentence .'] 

 TOTAL BIGRAMS --> 17 



 ---- TRI-GRAMS ---- 

 ['example , two', ', two entities', 'two entities ,', 'entities , employee', ', employee name', 'employee name “', 'name “ person', '“ person ”', 'person ” bank', '” bank name', 'bank name “', 'name “ organization', '“ organization ”', 'organization ” one', '” one sentence', 'one sentence .'] 

 TOTAL TRIGRAMS --> 16 



 ---- NOUN PHRASES ---- 

 ['example', 'employee', 'name', 'person', 'bank', 'name', 'organization', 'sentence'] 

 TOTAL NOUN PHRASES --> 8 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['exampl', ',', 'two', 'entiti', ',', 'employe', 'name', '“', 'person', '”', 'bank', 'name', '“', 'organ', '”', 'one', 'sentenc', '.']

 TOTAL PORTER STEM WORDS ==> 18



 ---- SNOWBALL STEMMING ----

['exampl', ',', 'two', 'entiti', ',', 'employe', 'name', '“', 'person', '”', 'bank', 'name', '“', 'organ', '”', 'one', 'sentenc', '.']

 TOTAL SNOWBALL STEM WORDS ==> 18



 ---- LEMMATIZATION ----

['example', ',', 'two', 'entity', ',', 'employee', 'name', '“', 'person', '”', 'bank', 'name', '“', 'organization', '”', 'one', 'sentence', '.']

 TOTAL LEMMATIZE WORDS ==> 18

************************************************************************************************************************

144 --> One of the possible  relationships between these entities,  can be “works at”. 


 ---- TOKENS ----

 ['One', 'of', 'the', 'possible', 'relationships', 'between', 'these', 'entities', ',', 'can', 'be', '“', 'works', 'at', '”', '.'] 

 TOTAL TOKENS ==> 16

 ---- POST ----

 [('One', 'CD'), ('of', 'IN'), ('the', 'DT'), ('possible', 'JJ'), ('relationships', 'NNS'), ('between', 'IN'), ('these', 'DT'), ('entities', 'NNS'), (',', ','), ('can', 'MD'), ('be', 'VB'), ('“', 'JJ'), ('works', 'NNS'), ('at', 'IN'), ('”', 'NNP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['One', 'possible', 'relationships', 'entities', ',', '“', 'works', '”', '.']

 TOTAL FILTERED TOKENS ==>  9

 ---- POST FOR FILTERED TOKENS ----

 [('One', 'CD'), ('possible', 'JJ'), ('relationships', 'NNS'), ('entities', 'NNS'), (',', ','), ('“', 'NNP'), ('works', 'VBZ'), ('”', 'NNP'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['One possible', 'possible relationships', 'relationships entities', 'entities ,', ', “', '“ works', 'works ”', '” .'] 

 TOTAL BIGRAMS --> 8 



 ---- TRI-GRAMS ---- 

 ['One possible relationships', 'possible relationships entities', 'relationships entities ,', 'entities , “', ', “ works', '“ works ”', 'works ” .'] 

 TOTAL TRIGRAMS --> 7 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['one', 'possibl', 'relationship', 'entiti', ',', '“', 'work', '”', '.']

 TOTAL PORTER STEM WORDS ==> 9



 ---- SNOWBALL STEMMING ----

['one', 'possibl', 'relationship', 'entiti', ',', '“', 'work', '”', '.']

 TOTAL SNOWBALL STEM WORDS ==> 9



 ---- LEMMATIZATION ----

['One', 'possible', 'relationship', 'entity', ',', '“', 'work', '”', '.']

 TOTAL LEMMATIZE WORDS ==> 9

************************************************************************************************************************

145 --> Relationship helps in  extracting the information accurately. 


 ---- TOKENS ----

 ['Relationship', 'helps', 'in', 'extracting', 'the', 'information', 'accurately', '.'] 

 TOTAL TOKENS ==> 8

 ---- POST ----

 [('Relationship', 'NNP'), ('helps', 'VBZ'), ('in', 'IN'), ('extracting', 'VBG'), ('the', 'DT'), ('information', 'NN'), ('accurately', 'RB'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Relationship', 'helps', 'extracting', 'information', 'accurately', '.']

 TOTAL FILTERED TOKENS ==>  6

 ---- POST FOR FILTERED TOKENS ----

 [('Relationship', 'NNP'), ('helps', 'VBZ'), ('extracting', 'VBG'), ('information', 'NN'), ('accurately', 'RB'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Relationship helps', 'helps extracting', 'extracting information', 'information accurately', 'accurately .'] 

 TOTAL BIGRAMS --> 5 



 ---- TRI-GRAMS ---- 

 ['Relationship helps extracting', 'helps extracting information', 'extracting information accurately', 'information accurately .'] 

 TOTAL TRIGRAMS --> 4 



 ---- NOUN PHRASES ---- 

 ['information'] 

 TOTAL NOUN PHRASES --> 1 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> ['Relationship']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['relationship', 'help', 'extract', 'inform', 'accur', '.']

 TOTAL PORTER STEM WORDS ==> 6



 ---- SNOWBALL STEMMING ----

['relationship', 'help', 'extract', 'inform', 'accur', '.']

 TOTAL SNOWBALL STEM WORDS ==> 6



 ---- LEMMATIZATION ----

['Relationship', 'help', 'extracting', 'information', 'accurately', '.']

 TOTAL LEMMATIZE WORDS ==> 6

************************************************************************************************************************

146 --> More  the annotation of content, more will the  training and hence, more will be accuracy. 


 ---- TOKENS ----

 ['More', 'the', 'annotation', 'of', 'content', ',', 'more', 'will', 'the', 'training', 'and', 'hence', ',', 'more', 'will', 'be', 'accuracy', '.'] 

 TOTAL TOKENS ==> 18

 ---- POST ----

 [('More', 'RBR'), ('the', 'DT'), ('annotation', 'NN'), ('of', 'IN'), ('content', 'NN'), (',', ','), ('more', 'JJR'), ('will', 'MD'), ('the', 'DT'), ('training', 'NN'), ('and', 'CC'), ('hence', 'NN'), (',', ','), ('more', 'JJR'), ('will', 'MD'), ('be', 'VB'), ('accuracy', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['annotation', 'content', ',', 'training', 'hence', ',', 'accuracy', '.']

 TOTAL FILTERED TOKENS ==>  8

 ---- POST FOR FILTERED TOKENS ----

 [('annotation', 'NN'), ('content', 'NN'), (',', ','), ('training', 'VBG'), ('hence', 'NN'), (',', ','), ('accuracy', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['annotation content', 'content ,', ', training', 'training hence', 'hence ,', ', accuracy', 'accuracy .'] 

 TOTAL BIGRAMS --> 7 



 ---- TRI-GRAMS ---- 

 ['annotation content ,', 'content , training', ', training hence', 'training hence ,', 'hence , accuracy', ', accuracy .'] 

 TOTAL TRIGRAMS --> 6 



 ---- NOUN PHRASES ---- 

 ['annotation', 'content', 'hence', 'accuracy'] 

 TOTAL NOUN PHRASES --> 4 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['annot', 'content', ',', 'train', 'henc', ',', 'accuraci', '.']

 TOTAL PORTER STEM WORDS ==> 8



 ---- SNOWBALL STEMMING ----

['annot', 'content', ',', 'train', 'henc', ',', 'accuraci', '.']

 TOTAL SNOWBALL STEM WORDS ==> 8



 ---- LEMMATIZATION ----

['annotation', 'content', ',', 'training', 'hence', ',', 'accuracy', '.']

 TOTAL LEMMATIZE WORDS ==> 8

************************************************************************************************************************

147 --> For training categorizer model, business  terms, phrases or sentences which helps  in classifying the content in that category  are identified. 


 ---- TOKENS ----

 ['For', 'training', 'categorizer', 'model', ',', 'business', 'terms', ',', 'phrases', 'or', 'sentences', 'which', 'helps', 'in', 'classifying', 'the', 'content', 'in', 'that', 'category', 'are', 'identified', '.'] 

 TOTAL TOKENS ==> 23

 ---- POST ----

 [('For', 'IN'), ('training', 'VBG'), ('categorizer', 'NN'), ('model', 'NN'), (',', ','), ('business', 'NN'), ('terms', 'NNS'), (',', ','), ('phrases', 'NNS'), ('or', 'CC'), ('sentences', 'NNS'), ('which', 'WDT'), ('helps', 'VBP'), ('in', 'IN'), ('classifying', 'VBG'), ('the', 'DT'), ('content', 'NN'), ('in', 'IN'), ('that', 'DT'), ('category', 'NN'), ('are', 'VBP'), ('identified', 'VBN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['training', 'categorizer', 'model', ',', 'business', 'terms', ',', 'phrases', 'sentences', 'helps', 'classifying', 'content', 'category', 'identified', '.']

 TOTAL FILTERED TOKENS ==>  15

 ---- POST FOR FILTERED TOKENS ----

 [('training', 'VBG'), ('categorizer', 'NN'), ('model', 'NN'), (',', ','), ('business', 'NN'), ('terms', 'NNS'), (',', ','), ('phrases', 'NNS'), ('sentences', 'NNS'), ('helps', 'VBP'), ('classifying', 'VBG'), ('content', 'NN'), ('category', 'NN'), ('identified', 'VBN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['training categorizer', 'categorizer model', 'model ,', ', business', 'business terms', 'terms ,', ', phrases', 'phrases sentences', 'sentences helps', 'helps classifying', 'classifying content', 'content category', 'category identified', 'identified .'] 

 TOTAL BIGRAMS --> 14 



 ---- TRI-GRAMS ---- 

 ['training categorizer model', 'categorizer model ,', 'model , business', ', business terms', 'business terms ,', 'terms , phrases', ', phrases sentences', 'phrases sentences helps', 'sentences helps classifying', 'helps classifying content', 'classifying content category', 'content category identified', 'category identified .'] 

 TOTAL TRIGRAMS --> 13 



 ---- NOUN PHRASES ---- 

 ['categorizer', 'model', 'business', 'content', 'category'] 

 TOTAL NOUN PHRASES --> 5 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['train', 'categor', 'model', ',', 'busi', 'term', ',', 'phrase', 'sentenc', 'help', 'classifi', 'content', 'categori', 'identifi', '.']

 TOTAL PORTER STEM WORDS ==> 15



 ---- SNOWBALL STEMMING ----

['train', 'categor', 'model', ',', 'busi', 'term', ',', 'phrase', 'sentenc', 'help', 'classifi', 'content', 'categori', 'identifi', '.']

 TOTAL SNOWBALL STEM WORDS ==> 15



 ---- LEMMATIZATION ----

['training', 'categorizer', 'model', ',', 'business', 'term', ',', 'phrase', 'sentence', 'help', 'classifying', 'content', 'category', 'identified', '.']

 TOTAL LEMMATIZE WORDS ==> 15

************************************************************************************************************************

148 --> Terms identified for different  categories are listed in plain text file. 


 ---- TOKENS ----

 ['Terms', 'identified', 'for', 'different', 'categories', 'are', 'listed', 'in', 'plain', 'text', 'file', '.'] 

 TOTAL TOKENS ==> 12

 ---- POST ----

 [('Terms', 'NNS'), ('identified', 'VBN'), ('for', 'IN'), ('different', 'JJ'), ('categories', 'NNS'), ('are', 'VBP'), ('listed', 'VBN'), ('in', 'IN'), ('plain', 'NN'), ('text', 'NN'), ('file', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Terms', 'identified', 'different', 'categories', 'listed', 'plain', 'text', 'file', '.']

 TOTAL FILTERED TOKENS ==>  9

 ---- POST FOR FILTERED TOKENS ----

 [('Terms', 'NNS'), ('identified', 'VBN'), ('different', 'JJ'), ('categories', 'NNS'), ('listed', 'VBN'), ('plain', 'VBP'), ('text', 'JJ'), ('file', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Terms identified', 'identified different', 'different categories', 'categories listed', 'listed plain', 'plain text', 'text file', 'file .'] 

 TOTAL BIGRAMS --> 8 



 ---- TRI-GRAMS ---- 

 ['Terms identified different', 'identified different categories', 'different categories listed', 'categories listed plain', 'listed plain text', 'plain text file', 'text file .'] 

 TOTAL TRIGRAMS --> 7 



 ---- NOUN PHRASES ---- 

 ['text file'] 

 TOTAL NOUN PHRASES --> 1 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['term', 'identifi', 'differ', 'categori', 'list', 'plain', 'text', 'file', '.']

 TOTAL PORTER STEM WORDS ==> 9



 ---- SNOWBALL STEMMING ----

['term', 'identifi', 'differ', 'categori', 'list', 'plain', 'text', 'file', '.']

 TOTAL SNOWBALL STEM WORDS ==> 9



 ---- LEMMATIZATION ----

['Terms', 'identified', 'different', 'category', 'listed', 'plain', 'text', 'file', '.']

 TOTAL LEMMATIZE WORDS ==> 9

************************************************************************************************************************

149 --> This  file is then used for training the categorizer  model. 


 ---- TOKENS ----

 ['This', 'file', 'is', 'then', 'used', 'for', 'training', 'the', 'categorizer', 'model', '.'] 

 TOTAL TOKENS ==> 11

 ---- POST ----

 [('This', 'DT'), ('file', 'NN'), ('is', 'VBZ'), ('then', 'RB'), ('used', 'VBN'), ('for', 'IN'), ('training', 'VBG'), ('the', 'DT'), ('categorizer', 'NN'), ('model', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['file', 'used', 'training', 'categorizer', 'model', '.']

 TOTAL FILTERED TOKENS ==>  6

 ---- POST FOR FILTERED TOKENS ----

 [('file', 'NN'), ('used', 'VBN'), ('training', 'NN'), ('categorizer', 'NN'), ('model', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['file used', 'used training', 'training categorizer', 'categorizer model', 'model .'] 

 TOTAL BIGRAMS --> 5 



 ---- TRI-GRAMS ---- 

 ['file used training', 'used training categorizer', 'training categorizer model', 'categorizer model .'] 

 TOTAL TRIGRAMS --> 4 



 ---- NOUN PHRASES ---- 

 ['file', 'training', 'categorizer', 'model'] 

 TOTAL NOUN PHRASES --> 4 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['file', 'use', 'train', 'categor', 'model', '.']

 TOTAL PORTER STEM WORDS ==> 6



 ---- SNOWBALL STEMMING ----

['file', 'use', 'train', 'categor', 'model', '.']

 TOTAL SNOWBALL STEM WORDS ==> 6



 ---- LEMMATIZATION ----

['file', 'used', 'training', 'categorizer', 'model', '.']

 TOTAL LEMMATIZE WORDS ==> 6

************************************************************************************************************************

150 --> If AI model need to be trained for more  than one language, then it is better to train  language specific AI models. 


 ---- TOKENS ----

 ['If', 'AI', 'model', 'need', 'to', 'be', 'trained', 'for', 'more', 'than', 'one', 'language', ',', 'then', 'it', 'is', 'better', 'to', 'train', 'language', 'specific', 'AI', 'models', '.'] 

 TOTAL TOKENS ==> 24

 ---- POST ----

 [('If', 'IN'), ('AI', 'NNP'), ('model', 'FW'), ('need', 'NN'), ('to', 'TO'), ('be', 'VB'), ('trained', 'VBN'), ('for', 'IN'), ('more', 'JJR'), ('than', 'IN'), ('one', 'CD'), ('language', 'NN'), (',', ','), ('then', 'RB'), ('it', 'PRP'), ('is', 'VBZ'), ('better', 'RBR'), ('to', 'TO'), ('train', 'VB'), ('language', 'NN'), ('specific', 'JJ'), ('AI', 'NNP'), ('models', 'NNS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['AI', 'model', 'need', 'trained', 'one', 'language', ',', 'better', 'train', 'language', 'specific', 'AI', 'models', '.']

 TOTAL FILTERED TOKENS ==>  14

 ---- POST FOR FILTERED TOKENS ----

 [('AI', 'NNP'), ('model', 'NN'), ('need', 'NN'), ('trained', 'VBD'), ('one', 'CD'), ('language', 'NN'), (',', ','), ('better', 'JJR'), ('train', 'NN'), ('language', 'NN'), ('specific', 'JJ'), ('AI', 'NNP'), ('models', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['AI model', 'model need', 'need trained', 'trained one', 'one language', 'language ,', ', better', 'better train', 'train language', 'language specific', 'specific AI', 'AI models', 'models .'] 

 TOTAL BIGRAMS --> 13 



 ---- TRI-GRAMS ---- 

 ['AI model need', 'model need trained', 'need trained one', 'trained one language', 'one language ,', 'language , better', ', better train', 'better train language', 'train language specific', 'language specific AI', 'specific AI models', 'AI models .'] 

 TOTAL TRIGRAMS --> 12 



 ---- NOUN PHRASES ---- 

 ['model', 'need', 'language', 'train', 'language'] 

 TOTAL NOUN PHRASES --> 5 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['ai', 'model', 'need', 'train', 'one', 'languag', ',', 'better', 'train', 'languag', 'specif', 'ai', 'model', '.']

 TOTAL PORTER STEM WORDS ==> 14



 ---- SNOWBALL STEMMING ----

['ai', 'model', 'need', 'train', 'one', 'languag', ',', 'better', 'train', 'languag', 'specif', 'ai', 'model', '.']

 TOTAL SNOWBALL STEM WORDS ==> 14



 ---- LEMMATIZATION ----

['AI', 'model', 'need', 'trained', 'one', 'language', ',', 'better', 'train', 'language', 'specific', 'AI', 'model', '.']

 TOTAL LEMMATIZE WORDS ==> 14

************************************************************************************************************************

151 --> 3.3 Building AI models For training NER model, once annotation  of labeled content is done, the files need  to be converted into a machine readable  format, which NLP engine understand. 


 ---- TOKENS ----

 ['3.3', 'Building', 'AI', 'models', 'For', 'training', 'NER', 'model', ',', 'once', 'annotation', 'of', 'labeled', 'content', 'is', 'done', ',', 'the', 'files', 'need', 'to', 'be', 'converted', 'into', 'a', 'machine', 'readable', 'format', ',', 'which', 'NLP', 'engine', 'understand', '.'] 

 TOTAL TOKENS ==> 34

 ---- POST ----

 [('3.3', 'CD'), ('Building', 'NNP'), ('AI', 'NNP'), ('models', 'NNS'), ('For', 'IN'), ('training', 'VBG'), ('NER', 'NNP'), ('model', 'NN'), (',', ','), ('once', 'RB'), ('annotation', 'NN'), ('of', 'IN'), ('labeled', 'JJ'), ('content', 'NN'), ('is', 'VBZ'), ('done', 'VBN'), (',', ','), ('the', 'DT'), ('files', 'NNS'), ('need', 'VBP'), ('to', 'TO'), ('be', 'VB'), ('converted', 'VBN'), ('into', 'IN'), ('a', 'DT'), ('machine', 'NN'), ('readable', 'JJ'), ('format', 'NN'), (',', ','), ('which', 'WDT'), ('NLP', 'NNP'), ('engine', 'NN'), ('understand', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['3.3', 'Building', 'AI', 'models', 'training', 'NER', 'model', ',', 'annotation', 'labeled', 'content', 'done', ',', 'files', 'need', 'converted', 'machine', 'readable', 'format', ',', 'NLP', 'engine', 'understand', '.']

 TOTAL FILTERED TOKENS ==>  24

 ---- POST FOR FILTERED TOKENS ----

 [('3.3', 'CD'), ('Building', 'NNP'), ('AI', 'NNP'), ('models', 'NNS'), ('training', 'VBG'), ('NER', 'NNP'), ('model', 'NN'), (',', ','), ('annotation', 'NN'), ('labeled', 'VBD'), ('content', 'JJ'), ('done', 'VBN'), (',', ','), ('files', 'NNS'), ('need', 'VBP'), ('converted', 'VBN'), ('machine', 'NN'), ('readable', 'JJ'), ('format', 'NN'), (',', ','), ('NLP', 'NNP'), ('engine', 'NN'), ('understand', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['3.3 Building', 'Building AI', 'AI models', 'models training', 'training NER', 'NER model', 'model ,', ', annotation', 'annotation labeled', 'labeled content', 'content done', 'done ,', ', files', 'files need', 'need converted', 'converted machine', 'machine readable', 'readable format', 'format ,', ', NLP', 'NLP engine', 'engine understand', 'understand .'] 

 TOTAL BIGRAMS --> 23 



 ---- TRI-GRAMS ---- 

 ['3.3 Building AI', 'Building AI models', 'AI models training', 'models training NER', 'training NER model', 'NER model ,', 'model , annotation', ', annotation labeled', 'annotation labeled content', 'labeled content done', 'content done ,', 'done , files', ', files need', 'files need converted', 'need converted machine', 'converted machine readable', 'machine readable format', 'readable format ,', 'format , NLP', ', NLP engine', 'NLP engine understand', 'engine understand .'] 

 TOTAL TRIGRAMS --> 22 



 ---- NOUN PHRASES ---- 

 ['model', 'annotation', 'machine', 'readable format', 'engine', 'understand'] 

 TOTAL NOUN PHRASES --> 6 



 ---- NER ----

 
 ORGANIZATION ---> ['NER', 'NLP']
 TOTAL ORGANIZATION ENTITY --> 2 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['3.3', 'build', 'ai', 'model', 'train', 'ner', 'model', ',', 'annot', 'label', 'content', 'done', ',', 'file', 'need', 'convert', 'machin', 'readabl', 'format', ',', 'nlp', 'engin', 'understand', '.']

 TOTAL PORTER STEM WORDS ==> 24



 ---- SNOWBALL STEMMING ----

['3.3', 'build', 'ai', 'model', 'train', 'ner', 'model', ',', 'annot', 'label', 'content', 'done', ',', 'file', 'need', 'convert', 'machin', 'readabl', 'format', ',', 'nlp', 'engin', 'understand', '.']

 TOTAL SNOWBALL STEM WORDS ==> 24



 ---- LEMMATIZATION ----

['3.3', 'Building', 'AI', 'model', 'training', 'NER', 'model', ',', 'annotation', 'labeled', 'content', 'done', ',', 'file', 'need', 'converted', 'machine', 'readable', 'format', ',', 'NLP', 'engine', 'understand', '.']

 TOTAL LEMMATIZE WORDS ==> 24

************************************************************************************************************************

152 --> Most of the annotation tools offers APIs  to convert the annotation format, into the  machine readable format. 


 ---- TOKENS ----

 ['Most', 'of', 'the', 'annotation', 'tools', 'offers', 'APIs', 'to', 'convert', 'the', 'annotation', 'format', ',', 'into', 'the', 'machine', 'readable', 'format', '.'] 

 TOTAL TOKENS ==> 19

 ---- POST ----

 [('Most', 'JJS'), ('of', 'IN'), ('the', 'DT'), ('annotation', 'NN'), ('tools', 'VBZ'), ('offers', 'NNS'), ('APIs', 'NNP'), ('to', 'TO'), ('convert', 'VB'), ('the', 'DT'), ('annotation', 'NN'), ('format', 'NN'), (',', ','), ('into', 'IN'), ('the', 'DT'), ('machine', 'NN'), ('readable', 'JJ'), ('format', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['annotation', 'tools', 'offers', 'APIs', 'convert', 'annotation', 'format', ',', 'machine', 'readable', 'format', '.']

 TOTAL FILTERED TOKENS ==>  12

 ---- POST FOR FILTERED TOKENS ----

 [('annotation', 'NN'), ('tools', 'VBZ'), ('offers', 'NNS'), ('APIs', 'NNP'), ('convert', 'NN'), ('annotation', 'NN'), ('format', 'NN'), (',', ','), ('machine', 'NN'), ('readable', 'JJ'), ('format', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['annotation tools', 'tools offers', 'offers APIs', 'APIs convert', 'convert annotation', 'annotation format', 'format ,', ', machine', 'machine readable', 'readable format', 'format .'] 

 TOTAL BIGRAMS --> 11 



 ---- TRI-GRAMS ---- 

 ['annotation tools offers', 'tools offers APIs', 'offers APIs convert', 'APIs convert annotation', 'convert annotation format', 'annotation format ,', 'format , machine', ', machine readable', 'machine readable format', 'readable format .'] 

 TOTAL TRIGRAMS --> 10 



 ---- NOUN PHRASES ---- 

 ['annotation', 'convert', 'annotation', 'format', 'machine', 'readable format'] 

 TOTAL NOUN PHRASES --> 6 



 ---- NER ----

 
 ORGANIZATION ---> ['APIs']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['annot', 'tool', 'offer', 'api', 'convert', 'annot', 'format', ',', 'machin', 'readabl', 'format', '.']

 TOTAL PORTER STEM WORDS ==> 12



 ---- SNOWBALL STEMMING ----

['annot', 'tool', 'offer', 'api', 'convert', 'annot', 'format', ',', 'machin', 'readabl', 'format', '.']

 TOTAL SNOWBALL STEM WORDS ==> 12



 ---- LEMMATIZATION ----

['annotation', 'tool', 'offer', 'APIs', 'convert', 'annotation', 'format', ',', 'machine', 'readable', 'format', '.']

 TOTAL LEMMATIZE WORDS ==> 12

************************************************************************************************************************

153 --> Once converted,  these files are then fed to NLP engine,  which in turn, internally build an AI model. 


 ---- TOKENS ----

 ['Once', 'converted', ',', 'these', 'files', 'are', 'then', 'fed', 'to', 'NLP', 'engine', ',', 'which', 'in', 'turn', ',', 'internally', 'build', 'an', 'AI', 'model', '.'] 

 TOTAL TOKENS ==> 22

 ---- POST ----

 [('Once', 'RB'), ('converted', 'VBN'), (',', ','), ('these', 'DT'), ('files', 'NNS'), ('are', 'VBP'), ('then', 'RB'), ('fed', 'VBN'), ('to', 'TO'), ('NLP', 'NNP'), ('engine', 'NN'), (',', ','), ('which', 'WDT'), ('in', 'IN'), ('turn', 'NN'), (',', ','), ('internally', 'RB'), ('build', 'VB'), ('an', 'DT'), ('AI', 'NNP'), ('model', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['converted', ',', 'files', 'fed', 'NLP', 'engine', ',', 'turn', ',', 'internally', 'build', 'AI', 'model', '.']

 TOTAL FILTERED TOKENS ==>  14

 ---- POST FOR FILTERED TOKENS ----

 [('converted', 'VBN'), (',', ','), ('files', 'NNS'), ('fed', 'VBP'), ('NLP', 'NNP'), ('engine', 'NN'), (',', ','), ('turn', 'NN'), (',', ','), ('internally', 'RB'), ('build', 'JJ'), ('AI', 'NNP'), ('model', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['converted ,', ', files', 'files fed', 'fed NLP', 'NLP engine', 'engine ,', ', turn', 'turn ,', ', internally', 'internally build', 'build AI', 'AI model', 'model .'] 

 TOTAL BIGRAMS --> 13 



 ---- TRI-GRAMS ---- 

 ['converted , files', ', files fed', 'files fed NLP', 'fed NLP engine', 'NLP engine ,', 'engine , turn', ', turn ,', 'turn , internally', ', internally build', 'internally build AI', 'build AI model', 'AI model .'] 

 TOTAL TRIGRAMS --> 12 



 ---- NOUN PHRASES ---- 

 ['engine', 'turn', 'model'] 

 TOTAL NOUN PHRASES --> 3 



 ---- NER ----

 
 ORGANIZATION ---> ['NLP']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['convert', ',', 'file', 'fed', 'nlp', 'engin', ',', 'turn', ',', 'intern', 'build', 'ai', 'model', '.']

 TOTAL PORTER STEM WORDS ==> 14



 ---- SNOWBALL STEMMING ----

['convert', ',', 'file', 'fed', 'nlp', 'engin', ',', 'turn', ',', 'intern', 'build', 'ai', 'model', '.']

 TOTAL SNOWBALL STEM WORDS ==> 14



 ---- LEMMATIZATION ----

['converted', ',', 'file', 'fed', 'NLP', 'engine', ',', 'turn', ',', 'internally', 'build', 'AI', 'model', '.']

 TOTAL LEMMATIZE WORDS ==> 14

************************************************************************************************************************

154 --> For training categorizer model, the text file,  listing category mapped to business terms,  phrases or sentences, is fed to NLP engine. 


 ---- TOKENS ----

 ['For', 'training', 'categorizer', 'model', ',', 'the', 'text', 'file', ',', 'listing', 'category', 'mapped', 'to', 'business', 'terms', ',', 'phrases', 'or', 'sentences', ',', 'is', 'fed', 'to', 'NLP', 'engine', '.'] 

 TOTAL TOKENS ==> 26

 ---- POST ----

 [('For', 'IN'), ('training', 'VBG'), ('categorizer', 'NN'), ('model', 'NN'), (',', ','), ('the', 'DT'), ('text', 'NN'), ('file', 'NN'), (',', ','), ('listing', 'VBG'), ('category', 'NN'), ('mapped', 'VBN'), ('to', 'TO'), ('business', 'NN'), ('terms', 'NNS'), (',', ','), ('phrases', 'NNS'), ('or', 'CC'), ('sentences', 'NNS'), (',', ','), ('is', 'VBZ'), ('fed', 'VBN'), ('to', 'TO'), ('NLP', 'NNP'), ('engine', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['training', 'categorizer', 'model', ',', 'text', 'file', ',', 'listing', 'category', 'mapped', 'business', 'terms', ',', 'phrases', 'sentences', ',', 'fed', 'NLP', 'engine', '.']

 TOTAL FILTERED TOKENS ==>  20

 ---- POST FOR FILTERED TOKENS ----

 [('training', 'VBG'), ('categorizer', 'NN'), ('model', 'NN'), (',', ','), ('text', 'NN'), ('file', 'NN'), (',', ','), ('listing', 'VBG'), ('category', 'NN'), ('mapped', 'VBD'), ('business', 'NN'), ('terms', 'NNS'), (',', ','), ('phrases', 'NNS'), ('sentences', 'NNS'), (',', ','), ('fed', 'VBN'), ('NLP', 'NNP'), ('engine', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['training categorizer', 'categorizer model', 'model ,', ', text', 'text file', 'file ,', ', listing', 'listing category', 'category mapped', 'mapped business', 'business terms', 'terms ,', ', phrases', 'phrases sentences', 'sentences ,', ', fed', 'fed NLP', 'NLP engine', 'engine .'] 

 TOTAL BIGRAMS --> 19 



 ---- TRI-GRAMS ---- 

 ['training categorizer model', 'categorizer model ,', 'model , text', ', text file', 'text file ,', 'file , listing', ', listing category', 'listing category mapped', 'category mapped business', 'mapped business terms', 'business terms ,', 'terms , phrases', ', phrases sentences', 'phrases sentences ,', 'sentences , fed', ', fed NLP', 'fed NLP engine', 'NLP engine .'] 

 TOTAL TRIGRAMS --> 18 



 ---- NOUN PHRASES ---- 

 ['categorizer', 'model', 'text', 'file', 'category', 'business', 'engine'] 

 TOTAL NOUN PHRASES --> 7 



 ---- NER ----

 
 ORGANIZATION ---> ['NLP']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['train', 'categor', 'model', ',', 'text', 'file', ',', 'list', 'categori', 'map', 'busi', 'term', ',', 'phrase', 'sentenc', ',', 'fed', 'nlp', 'engin', '.']

 TOTAL PORTER STEM WORDS ==> 20



 ---- SNOWBALL STEMMING ----

['train', 'categor', 'model', ',', 'text', 'file', ',', 'list', 'categori', 'map', 'busi', 'term', ',', 'phrase', 'sentenc', ',', 'fed', 'nlp', 'engin', '.']

 TOTAL SNOWBALL STEM WORDS ==> 20



 ---- LEMMATIZATION ----

['training', 'categorizer', 'model', ',', 'text', 'file', ',', 'listing', 'category', 'mapped', 'business', 'term', ',', 'phrase', 'sentence', ',', 'fed', 'NLP', 'engine', '.']

 TOTAL LEMMATIZE WORDS ==> 20

************************************************************************************************************************

155 --> The NLP engine then internally build an AI  model. 


 ---- TOKENS ----

 ['The', 'NLP', 'engine', 'then', 'internally', 'build', 'an', 'AI', 'model', '.'] 

 TOTAL TOKENS ==> 10

 ---- POST ----

 [('The', 'DT'), ('NLP', 'NNP'), ('engine', 'NN'), ('then', 'RB'), ('internally', 'RB'), ('build', 'VB'), ('an', 'DT'), ('AI', 'NNP'), ('model', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['NLP', 'engine', 'internally', 'build', 'AI', 'model', '.']

 TOTAL FILTERED TOKENS ==>  7

 ---- POST FOR FILTERED TOKENS ----

 [('NLP', 'NNP'), ('engine', 'NN'), ('internally', 'RB'), ('build', 'JJ'), ('AI', 'NNP'), ('model', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['NLP engine', 'engine internally', 'internally build', 'build AI', 'AI model', 'model .'] 

 TOTAL BIGRAMS --> 6 



 ---- TRI-GRAMS ---- 

 ['NLP engine internally', 'engine internally build', 'internally build AI', 'build AI model', 'AI model .'] 

 TOTAL TRIGRAMS --> 5 



 ---- NOUN PHRASES ---- 

 ['engine', 'model'] 

 TOTAL NOUN PHRASES --> 2 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['nlp', 'engin', 'intern', 'build', 'ai', 'model', '.']

 TOTAL PORTER STEM WORDS ==> 7



 ---- SNOWBALL STEMMING ----

['nlp', 'engin', 'intern', 'build', 'ai', 'model', '.']

 TOTAL SNOWBALL STEM WORDS ==> 7



 ---- LEMMATIZATION ----

['NLP', 'engine', 'internally', 'build', 'AI', 'model', '.']

 TOTAL LEMMATIZE WORDS ==> 7

************************************************************************************************************************

156 --> 3.4 Testing AI Model From the labeled content, a sub-set can be  used to test the accuracy of the trained AI  model. 


 ---- TOKENS ----

 ['3.4', 'Testing', 'AI', 'Model', 'From', 'the', 'labeled', 'content', ',', 'a', 'sub-set', 'can', 'be', 'used', 'to', 'test', 'the', 'accuracy', 'of', 'the', 'trained', 'AI', 'model', '.'] 

 TOTAL TOKENS ==> 24

 ---- POST ----

 [('3.4', 'CD'), ('Testing', 'VBG'), ('AI', 'NNP'), ('Model', 'NNP'), ('From', 'IN'), ('the', 'DT'), ('labeled', 'VBN'), ('content', 'NN'), (',', ','), ('a', 'DT'), ('sub-set', 'NN'), ('can', 'MD'), ('be', 'VB'), ('used', 'VBN'), ('to', 'TO'), ('test', 'VB'), ('the', 'DT'), ('accuracy', 'NN'), ('of', 'IN'), ('the', 'DT'), ('trained', 'JJ'), ('AI', 'NNP'), ('model', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['3.4', 'Testing', 'AI', 'Model', 'labeled', 'content', ',', 'sub-set', 'used', 'test', 'accuracy', 'trained', 'AI', 'model', '.']

 TOTAL FILTERED TOKENS ==>  15

 ---- POST FOR FILTERED TOKENS ----

 [('3.4', 'CD'), ('Testing', 'VBG'), ('AI', 'NNP'), ('Model', 'NNP'), ('labeled', 'VBD'), ('content', 'JJ'), (',', ','), ('sub-set', 'JJ'), ('used', 'JJ'), ('test', 'NN'), ('accuracy', 'NN'), ('trained', 'VBD'), ('AI', 'NNP'), ('model', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['3.4 Testing', 'Testing AI', 'AI Model', 'Model labeled', 'labeled content', 'content ,', ', sub-set', 'sub-set used', 'used test', 'test accuracy', 'accuracy trained', 'trained AI', 'AI model', 'model .'] 

 TOTAL BIGRAMS --> 14 



 ---- TRI-GRAMS ---- 

 ['3.4 Testing AI', 'Testing AI Model', 'AI Model labeled', 'Model labeled content', 'labeled content ,', 'content , sub-set', ', sub-set used', 'sub-set used test', 'used test accuracy', 'test accuracy trained', 'accuracy trained AI', 'trained AI model', 'AI model .'] 

 TOTAL TRIGRAMS --> 13 



 ---- NOUN PHRASES ---- 

 ['sub-set used test', 'accuracy', 'model'] 

 TOTAL NOUN PHRASES --> 3 



 ---- NER ----

 
 ORGANIZATION ---> ['AI Model', 'AI']
 TOTAL ORGANIZATION ENTITY --> 2 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['3.4', 'test', 'ai', 'model', 'label', 'content', ',', 'sub-set', 'use', 'test', 'accuraci', 'train', 'ai', 'model', '.']

 TOTAL PORTER STEM WORDS ==> 15



 ---- SNOWBALL STEMMING ----

['3.4', 'test', 'ai', 'model', 'label', 'content', ',', 'sub-set', 'use', 'test', 'accuraci', 'train', 'ai', 'model', '.']

 TOTAL SNOWBALL STEM WORDS ==> 15



 ---- LEMMATIZATION ----

['3.4', 'Testing', 'AI', 'Model', 'labeled', 'content', ',', 'sub-set', 'used', 'test', 'accuracy', 'trained', 'AI', 'model', '.']

 TOTAL LEMMATIZE WORDS ==> 15

************************************************************************************************************************

157 --> The content whose classification or  entities to be extracted, is already known  and is subjected to the AI model. 


 ---- TOKENS ----

 ['The', 'content', 'whose', 'classification', 'or', 'entities', 'to', 'be', 'extracted', ',', 'is', 'already', 'known', 'and', 'is', 'subjected', 'to', 'the', 'AI', 'model', '.'] 

 TOTAL TOKENS ==> 21

 ---- POST ----

 [('The', 'DT'), ('content', 'NN'), ('whose', 'WP$'), ('classification', 'NN'), ('or', 'CC'), ('entities', 'NNS'), ('to', 'TO'), ('be', 'VB'), ('extracted', 'VBN'), (',', ','), ('is', 'VBZ'), ('already', 'RB'), ('known', 'VBN'), ('and', 'CC'), ('is', 'VBZ'), ('subjected', 'VBN'), ('to', 'TO'), ('the', 'DT'), ('AI', 'NNP'), ('model', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['content', 'whose', 'classification', 'entities', 'extracted', ',', 'already', 'known', 'subjected', 'AI', 'model', '.']

 TOTAL FILTERED TOKENS ==>  12

 ---- POST FOR FILTERED TOKENS ----

 [('content', 'NN'), ('whose', 'WP$'), ('classification', 'NN'), ('entities', 'NNS'), ('extracted', 'VBD'), (',', ','), ('already', 'RB'), ('known', 'VBN'), ('subjected', 'VBN'), ('AI', 'NNP'), ('model', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['content whose', 'whose classification', 'classification entities', 'entities extracted', 'extracted ,', ', already', 'already known', 'known subjected', 'subjected AI', 'AI model', 'model .'] 

 TOTAL BIGRAMS --> 11 



 ---- TRI-GRAMS ---- 

 ['content whose classification', 'whose classification entities', 'classification entities extracted', 'entities extracted ,', 'extracted , already', ', already known', 'already known subjected', 'known subjected AI', 'subjected AI model', 'AI model .'] 

 TOTAL TRIGRAMS --> 10 



 ---- NOUN PHRASES ---- 

 ['content', 'classification', 'model'] 

 TOTAL NOUN PHRASES --> 3 



 ---- NER ----

 
 ORGANIZATION ---> ['AI']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['content', 'whose', 'classif', 'entiti', 'extract', ',', 'alreadi', 'known', 'subject', 'ai', 'model', '.']

 TOTAL PORTER STEM WORDS ==> 12



 ---- SNOWBALL STEMMING ----

['content', 'whose', 'classif', 'entiti', 'extract', ',', 'alreadi', 'known', 'subject', 'ai', 'model', '.']

 TOTAL SNOWBALL STEM WORDS ==> 12



 ---- LEMMATIZATION ----

['content', 'whose', 'classification', 'entity', 'extracted', ',', 'already', 'known', 'subjected', 'AI', 'model', '.']

 TOTAL LEMMATIZE WORDS ==> 12

************************************************************************************************************************

158 --> If the  AI model is able to correctly identify the  category or extract the information, the AI  model is trained with sufficient accuracy  and hence can be deployed for production  use. 


 ---- TOKENS ----

 ['If', 'the', 'AI', 'model', 'is', 'able', 'to', 'correctly', 'identify', 'the', 'category', 'or', 'extract', 'the', 'information', ',', 'the', 'AI', 'model', 'is', 'trained', 'with', 'sufficient', 'accuracy', 'and', 'hence', 'can', 'be', 'deployed', 'for', 'production', 'use', '.'] 

 TOTAL TOKENS ==> 33

 ---- POST ----

 [('If', 'IN'), ('the', 'DT'), ('AI', 'NNP'), ('model', 'NN'), ('is', 'VBZ'), ('able', 'JJ'), ('to', 'TO'), ('correctly', 'RB'), ('identify', 'VB'), ('the', 'DT'), ('category', 'NN'), ('or', 'CC'), ('extract', 'VB'), ('the', 'DT'), ('information', 'NN'), (',', ','), ('the', 'DT'), ('AI', 'NNP'), ('model', 'NN'), ('is', 'VBZ'), ('trained', 'VBN'), ('with', 'IN'), ('sufficient', 'JJ'), ('accuracy', 'NN'), ('and', 'CC'), ('hence', 'NN'), ('can', 'MD'), ('be', 'VB'), ('deployed', 'VBN'), ('for', 'IN'), ('production', 'NN'), ('use', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['AI', 'model', 'able', 'correctly', 'identify', 'category', 'extract', 'information', ',', 'AI', 'model', 'trained', 'sufficient', 'accuracy', 'hence', 'deployed', 'production', 'use', '.']

 TOTAL FILTERED TOKENS ==>  19

 ---- POST FOR FILTERED TOKENS ----

 [('AI', 'NNP'), ('model', 'NN'), ('able', 'JJ'), ('correctly', 'RB'), ('identify', 'VBP'), ('category', 'JJ'), ('extract', 'JJ'), ('information', 'NN'), (',', ','), ('AI', 'NNP'), ('model', 'NN'), ('trained', 'VBD'), ('sufficient', 'JJ'), ('accuracy', 'NN'), ('hence', 'NN'), ('deployed', 'VBN'), ('production', 'NN'), ('use', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['AI model', 'model able', 'able correctly', 'correctly identify', 'identify category', 'category extract', 'extract information', 'information ,', ', AI', 'AI model', 'model trained', 'trained sufficient', 'sufficient accuracy', 'accuracy hence', 'hence deployed', 'deployed production', 'production use', 'use .'] 

 TOTAL BIGRAMS --> 18 



 ---- TRI-GRAMS ---- 

 ['AI model able', 'model able correctly', 'able correctly identify', 'correctly identify category', 'identify category extract', 'category extract information', 'extract information ,', 'information , AI', ', AI model', 'AI model trained', 'model trained sufficient', 'trained sufficient accuracy', 'sufficient accuracy hence', 'accuracy hence deployed', 'hence deployed production', 'deployed production use', 'production use .'] 

 TOTAL TRIGRAMS --> 17 



 ---- NOUN PHRASES ---- 

 ['model', 'category extract information', 'model', 'sufficient accuracy', 'hence', 'production', 'use'] 

 TOTAL NOUN PHRASES --> 7 



 ---- NER ----

 
 ORGANIZATION ---> ['AI']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['ai', 'model', 'abl', 'correctli', 'identifi', 'categori', 'extract', 'inform', ',', 'ai', 'model', 'train', 'suffici', 'accuraci', 'henc', 'deploy', 'product', 'use', '.']

 TOTAL PORTER STEM WORDS ==> 19



 ---- SNOWBALL STEMMING ----

['ai', 'model', 'abl', 'correct', 'identifi', 'categori', 'extract', 'inform', ',', 'ai', 'model', 'train', 'suffici', 'accuraci', 'henc', 'deploy', 'product', 'use', '.']

 TOTAL SNOWBALL STEM WORDS ==> 19



 ---- LEMMATIZATION ----

['AI', 'model', 'able', 'correctly', 'identify', 'category', 'extract', 'information', ',', 'AI', 'model', 'trained', 'sufficient', 'accuracy', 'hence', 'deployed', 'production', 'use', '.']

 TOTAL LEMMATIZE WORDS ==> 19

************************************************************************************************************************

159 --> If the AI model does not perform  adequately in this test run, then the AI  model need to be re-trained with the  additional labeled content. 


 ---- TOKENS ----

 ['If', 'the', 'AI', 'model', 'does', 'not', 'perform', 'adequately', 'in', 'this', 'test', 'run', ',', 'then', 'the', 'AI', 'model', 'need', 'to', 'be', 're-trained', 'with', 'the', 'additional', 'labeled', 'content', '.'] 

 TOTAL TOKENS ==> 27

 ---- POST ----

 [('If', 'IN'), ('the', 'DT'), ('AI', 'NNP'), ('model', 'NN'), ('does', 'VBZ'), ('not', 'RB'), ('perform', 'VB'), ('adequately', 'RB'), ('in', 'IN'), ('this', 'DT'), ('test', 'NN'), ('run', 'NN'), (',', ','), ('then', 'RB'), ('the', 'DT'), ('AI', 'NNP'), ('model', 'NN'), ('need', 'NN'), ('to', 'TO'), ('be', 'VB'), ('re-trained', 'JJ'), ('with', 'IN'), ('the', 'DT'), ('additional', 'JJ'), ('labeled', 'JJ'), ('content', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['AI', 'model', 'perform', 'adequately', 'test', 'run', ',', 'AI', 'model', 'need', 're-trained', 'additional', 'labeled', 'content', '.']

 TOTAL FILTERED TOKENS ==>  15

 ---- POST FOR FILTERED TOKENS ----

 [('AI', 'NNP'), ('model', 'NN'), ('perform', 'NN'), ('adequately', 'RB'), ('test', 'JJ'), ('run', 'NN'), (',', ','), ('AI', 'NNP'), ('model', 'FW'), ('need', 'VBP'), ('re-trained', 'JJ'), ('additional', 'JJ'), ('labeled', 'VBN'), ('content', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['AI model', 'model perform', 'perform adequately', 'adequately test', 'test run', 'run ,', ', AI', 'AI model', 'model need', 'need re-trained', 're-trained additional', 'additional labeled', 'labeled content', 'content .'] 

 TOTAL BIGRAMS --> 14 



 ---- TRI-GRAMS ---- 

 ['AI model perform', 'model perform adequately', 'perform adequately test', 'adequately test run', 'test run ,', 'run , AI', ', AI model', 'AI model need', 'model need re-trained', 'need re-trained additional', 're-trained additional labeled', 'additional labeled content', 'labeled content .'] 

 TOTAL TRIGRAMS --> 13 



 ---- NOUN PHRASES ---- 

 ['model', 'perform', 'test run', 'content'] 

 TOTAL NOUN PHRASES --> 4 



 ---- NER ----

 
 ORGANIZATION ---> ['AI']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['ai', 'model', 'perform', 'adequ', 'test', 'run', ',', 'ai', 'model', 'need', 're-train', 'addit', 'label', 'content', '.']

 TOTAL PORTER STEM WORDS ==> 15



 ---- SNOWBALL STEMMING ----

['ai', 'model', 'perform', 'adequ', 'test', 'run', ',', 'ai', 'model', 'need', 're-train', 'addit', 'label', 'content', '.']

 TOTAL SNOWBALL STEM WORDS ==> 15



 ---- LEMMATIZATION ----

['AI', 'model', 'perform', 'adequately', 'test', 'run', ',', 'AI', 'model', 'need', 're-trained', 'additional', 'labeled', 'content', '.']

 TOTAL LEMMATIZE WORDS ==> 15

************************************************************************************************************************

160 --> 3.5 Post Processor NLP engine, with trained AL model,  can classify or extract the entities from  new or unseen content. 


 ---- TOKENS ----

 ['3.5', 'Post', 'Processor', 'NLP', 'engine', ',', 'with', 'trained', 'AL', 'model', ',', 'can', 'classify', 'or', 'extract', 'the', 'entities', 'from', 'new', 'or', 'unseen', 'content', '.'] 

 TOTAL TOKENS ==> 23

 ---- POST ----

 [('3.5', 'CD'), ('Post', 'NNP'), ('Processor', 'NNP'), ('NLP', 'NNP'), ('engine', 'NN'), (',', ','), ('with', 'IN'), ('trained', 'JJ'), ('AL', 'NNP'), ('model', 'NN'), (',', ','), ('can', 'MD'), ('classify', 'VB'), ('or', 'CC'), ('extract', 'VB'), ('the', 'DT'), ('entities', 'NNS'), ('from', 'IN'), ('new', 'JJ'), ('or', 'CC'), ('unseen', 'JJ'), ('content', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['3.5', 'Post', 'Processor', 'NLP', 'engine', ',', 'trained', 'AL', 'model', ',', 'classify', 'extract', 'entities', 'new', 'unseen', 'content', '.']

 TOTAL FILTERED TOKENS ==>  17

 ---- POST FOR FILTERED TOKENS ----

 [('3.5', 'CD'), ('Post', 'NNP'), ('Processor', 'NNP'), ('NLP', 'NNP'), ('engine', 'NN'), (',', ','), ('trained', 'VBD'), ('AL', 'NNP'), ('model', 'NN'), (',', ','), ('classify', 'VB'), ('extract', 'JJ'), ('entities', 'NNS'), ('new', 'JJ'), ('unseen', 'JJ'), ('content', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['3.5 Post', 'Post Processor', 'Processor NLP', 'NLP engine', 'engine ,', ', trained', 'trained AL', 'AL model', 'model ,', ', classify', 'classify extract', 'extract entities', 'entities new', 'new unseen', 'unseen content', 'content .'] 

 TOTAL BIGRAMS --> 16 



 ---- TRI-GRAMS ---- 

 ['3.5 Post Processor', 'Post Processor NLP', 'Processor NLP engine', 'NLP engine ,', 'engine , trained', ', trained AL', 'trained AL model', 'AL model ,', 'model , classify', ', classify extract', 'classify extract entities', 'extract entities new', 'entities new unseen', 'new unseen content', 'unseen content .'] 

 TOTAL TRIGRAMS --> 15 



 ---- NOUN PHRASES ---- 

 ['engine', 'model', 'new unseen content'] 

 TOTAL NOUN PHRASES --> 3 



 ---- NER ----

 
 ORGANIZATION ---> ['AL']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['3.5', 'post', 'processor', 'nlp', 'engin', ',', 'train', 'al', 'model', ',', 'classifi', 'extract', 'entiti', 'new', 'unseen', 'content', '.']

 TOTAL PORTER STEM WORDS ==> 17



 ---- SNOWBALL STEMMING ----

['3.5', 'post', 'processor', 'nlp', 'engin', ',', 'train', 'al', 'model', ',', 'classifi', 'extract', 'entiti', 'new', 'unseen', 'content', '.']

 TOTAL SNOWBALL STEM WORDS ==> 17



 ---- LEMMATIZATION ----

['3.5', 'Post', 'Processor', 'NLP', 'engine', ',', 'trained', 'AL', 'model', ',', 'classify', 'extract', 'entity', 'new', 'unseen', 'content', '.']

 TOTAL LEMMATIZE WORDS ==> 17

************************************************************************************************************************

161 --> To process the  classification or extraction result of the  NLP, a post processer is required. 


 ---- TOKENS ----

 ['To', 'process', 'the', 'classification', 'or', 'extraction', 'result', 'of', 'the', 'NLP', ',', 'a', 'post', 'processer', 'is', 'required', '.'] 

 TOTAL TOKENS ==> 17

 ---- POST ----

 [('To', 'TO'), ('process', 'VB'), ('the', 'DT'), ('classification', 'NN'), ('or', 'CC'), ('extraction', 'NN'), ('result', 'NN'), ('of', 'IN'), ('the', 'DT'), ('NLP', 'NNP'), (',', ','), ('a', 'DT'), ('post', 'NN'), ('processer', 'NN'), ('is', 'VBZ'), ('required', 'VBN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['process', 'classification', 'extraction', 'result', 'NLP', ',', 'post', 'processer', 'required', '.']

 TOTAL FILTERED TOKENS ==>  10

 ---- POST FOR FILTERED TOKENS ----

 [('process', 'NN'), ('classification', 'NN'), ('extraction', 'NN'), ('result', 'NN'), ('NLP', 'NNP'), (',', ','), ('post', 'NN'), ('processer', 'NN'), ('required', 'VBN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['process classification', 'classification extraction', 'extraction result', 'result NLP', 'NLP ,', ', post', 'post processer', 'processer required', 'required .'] 

 TOTAL BIGRAMS --> 9 



 ---- TRI-GRAMS ---- 

 ['process classification extraction', 'classification extraction result', 'extraction result NLP', 'result NLP ,', 'NLP , post', ', post processer', 'post processer required', 'processer required .'] 

 TOTAL TRIGRAMS --> 8 



 ---- NOUN PHRASES ---- 

 ['process', 'classification', 'extraction', 'result', 'post', 'processer'] 

 TOTAL NOUN PHRASES --> 6 



 ---- NER ----

 
 ORGANIZATION ---> ['NLP']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['process', 'classif', 'extract', 'result', 'nlp', ',', 'post', 'process', 'requir', '.']

 TOTAL PORTER STEM WORDS ==> 10



 ---- SNOWBALL STEMMING ----

['process', 'classif', 'extract', 'result', 'nlp', ',', 'post', 'process', 'requir', '.']

 TOTAL SNOWBALL STEM WORDS ==> 10



 ---- LEMMATIZATION ----

['process', 'classification', 'extraction', 'result', 'NLP', ',', 'post', 'processer', 'required', '.']

 TOTAL LEMMATIZE WORDS ==> 10

************************************************************************************************************************

162 --> For  example, in case of metadata identification,  a categorizer model will give the list of  metadata value with their probabilities. 


 ---- TOKENS ----

 ['For', 'example', ',', 'in', 'case', 'of', 'metadata', 'identification', ',', 'a', 'categorizer', 'model', 'will', 'give', 'the', 'list', 'of', 'metadata', 'value', 'with', 'their', 'probabilities', '.'] 

 TOTAL TOKENS ==> 23

 ---- POST ----

 [('For', 'IN'), ('example', 'NN'), (',', ','), ('in', 'IN'), ('case', 'NN'), ('of', 'IN'), ('metadata', 'NN'), ('identification', 'NN'), (',', ','), ('a', 'DT'), ('categorizer', 'NN'), ('model', 'NN'), ('will', 'MD'), ('give', 'VB'), ('the', 'DT'), ('list', 'NN'), ('of', 'IN'), ('metadata', 'NNS'), ('value', 'NN'), ('with', 'IN'), ('their', 'PRP$'), ('probabilities', 'NNS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['example', ',', 'case', 'metadata', 'identification', ',', 'categorizer', 'model', 'give', 'list', 'metadata', 'value', 'probabilities', '.']

 TOTAL FILTERED TOKENS ==>  14

 ---- POST FOR FILTERED TOKENS ----

 [('example', 'NN'), (',', ','), ('case', 'NN'), ('metadata', 'NN'), ('identification', 'NN'), (',', ','), ('categorizer', 'NN'), ('model', 'NN'), ('give', 'JJ'), ('list', 'NN'), ('metadata', 'NN'), ('value', 'NN'), ('probabilities', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['example ,', ', case', 'case metadata', 'metadata identification', 'identification ,', ', categorizer', 'categorizer model', 'model give', 'give list', 'list metadata', 'metadata value', 'value probabilities', 'probabilities .'] 

 TOTAL BIGRAMS --> 13 



 ---- TRI-GRAMS ---- 

 ['example , case', ', case metadata', 'case metadata identification', 'metadata identification ,', 'identification , categorizer', ', categorizer model', 'categorizer model give', 'model give list', 'give list metadata', 'list metadata value', 'metadata value probabilities', 'value probabilities .'] 

 TOTAL TRIGRAMS --> 12 



 ---- NOUN PHRASES ---- 

 ['example', 'case', 'metadata', 'identification', 'categorizer', 'model', 'give list', 'metadata', 'value'] 

 TOTAL NOUN PHRASES --> 9 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['exampl', ',', 'case', 'metadata', 'identif', ',', 'categor', 'model', 'give', 'list', 'metadata', 'valu', 'probabl', '.']

 TOTAL PORTER STEM WORDS ==> 14



 ---- SNOWBALL STEMMING ----

['exampl', ',', 'case', 'metadata', 'identif', ',', 'categor', 'model', 'give', 'list', 'metadata', 'valu', 'probabl', '.']

 TOTAL SNOWBALL STEM WORDS ==> 14



 ---- LEMMATIZATION ----

['example', ',', 'case', 'metadata', 'identification', ',', 'categorizer', 'model', 'give', 'list', 'metadata', 'value', 'probability', '.']

 TOTAL LEMMATIZE WORDS ==> 14

************************************************************************************************************************

163 --> Post processor should select the metadata  value with highest probability. 


 ---- TOKENS ----

 ['Post', 'processor', 'should', 'select', 'the', 'metadata', 'value', 'with', 'highest', 'probability', '.'] 

 TOTAL TOKENS ==> 11

 ---- POST ----

 [('Post', 'NN'), ('processor', 'NN'), ('should', 'MD'), ('select', 'VB'), ('the', 'DT'), ('metadata', 'NNS'), ('value', 'NN'), ('with', 'IN'), ('highest', 'JJS'), ('probability', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Post', 'processor', 'select', 'metadata', 'value', 'highest', 'probability', '.']

 TOTAL FILTERED TOKENS ==>  8

 ---- POST FOR FILTERED TOKENS ----

 [('Post', 'NNP'), ('processor', 'NN'), ('select', 'NN'), ('metadata', 'NN'), ('value', 'NN'), ('highest', 'JJS'), ('probability', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Post processor', 'processor select', 'select metadata', 'metadata value', 'value highest', 'highest probability', 'probability .'] 

 TOTAL BIGRAMS --> 7 



 ---- TRI-GRAMS ---- 

 ['Post processor select', 'processor select metadata', 'select metadata value', 'metadata value highest', 'value highest probability', 'highest probability .'] 

 TOTAL TRIGRAMS --> 6 



 ---- NOUN PHRASES ---- 

 ['processor', 'select', 'metadata', 'value', 'probability'] 

 TOTAL NOUN PHRASES --> 5 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> ['Post']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['post', 'processor', 'select', 'metadata', 'valu', 'highest', 'probabl', '.']

 TOTAL PORTER STEM WORDS ==> 8



 ---- SNOWBALL STEMMING ----

['post', 'processor', 'select', 'metadata', 'valu', 'highest', 'probabl', '.']

 TOTAL SNOWBALL STEM WORDS ==> 8



 ---- LEMMATIZATION ----

['Post', 'processor', 'select', 'metadata', 'value', 'highest', 'probability', '.']

 TOTAL LEMMATIZE WORDS ==> 8

************************************************************************************************************************

164 --> Processing  logic of Post processor depends on the  implementation scenario. 


 ---- TOKENS ----

 ['Processing', 'logic', 'of', 'Post', 'processor', 'depends', 'on', 'the', 'implementation', 'scenario', '.'] 

 TOTAL TOKENS ==> 11

 ---- POST ----

 [('Processing', 'VBG'), ('logic', 'NN'), ('of', 'IN'), ('Post', 'NNP'), ('processor', 'NN'), ('depends', 'VBZ'), ('on', 'IN'), ('the', 'DT'), ('implementation', 'NN'), ('scenario', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Processing', 'logic', 'Post', 'processor', 'depends', 'implementation', 'scenario', '.']

 TOTAL FILTERED TOKENS ==>  8

 ---- POST FOR FILTERED TOKENS ----

 [('Processing', 'VBG'), ('logic', 'JJ'), ('Post', 'NNP'), ('processor', 'NN'), ('depends', 'VBZ'), ('implementation', 'NN'), ('scenario', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Processing logic', 'logic Post', 'Post processor', 'processor depends', 'depends implementation', 'implementation scenario', 'scenario .'] 

 TOTAL BIGRAMS --> 7 



 ---- TRI-GRAMS ---- 

 ['Processing logic Post', 'logic Post processor', 'Post processor depends', 'processor depends implementation', 'depends implementation scenario', 'implementation scenario .'] 

 TOTAL TRIGRAMS --> 6 



 ---- NOUN PHRASES ---- 

 ['processor', 'implementation', 'scenario'] 

 TOTAL NOUN PHRASES --> 3 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['process', 'logic', 'post', 'processor', 'depend', 'implement', 'scenario', '.']

 TOTAL PORTER STEM WORDS ==> 8



 ---- SNOWBALL STEMMING ----

['process', 'logic', 'post', 'processor', 'depend', 'implement', 'scenario', '.']

 TOTAL SNOWBALL STEM WORDS ==> 8



 ---- LEMMATIZATION ----

['Processing', 'logic', 'Post', 'processor', 'depends', 'implementation', 'scenario', '.']

 TOTAL LEMMATIZE WORDS ==> 8

************************************************************************************************************************

165 --> 3.6 Execution Once the AI model is trained and tested  and post processor is developed, the  pipeline is ready to classify or extract  the information from new or unseen  documents. 


 ---- TOKENS ----

 ['3.6', 'Execution', 'Once', 'the', 'AI', 'model', 'is', 'trained', 'and', 'tested', 'and', 'post', 'processor', 'is', 'developed', ',', 'the', 'pipeline', 'is', 'ready', 'to', 'classify', 'or', 'extract', 'the', 'information', 'from', 'new', 'or', 'unseen', 'documents', '.'] 

 TOTAL TOKENS ==> 32

 ---- POST ----

 [('3.6', 'CD'), ('Execution', 'NN'), ('Once', 'IN'), ('the', 'DT'), ('AI', 'NNP'), ('model', 'NN'), ('is', 'VBZ'), ('trained', 'VBN'), ('and', 'CC'), ('tested', 'VBN'), ('and', 'CC'), ('post', 'NN'), ('processor', 'NN'), ('is', 'VBZ'), ('developed', 'VBN'), (',', ','), ('the', 'DT'), ('pipeline', 'NN'), ('is', 'VBZ'), ('ready', 'JJ'), ('to', 'TO'), ('classify', 'VB'), ('or', 'CC'), ('extract', 'VB'), ('the', 'DT'), ('information', 'NN'), ('from', 'IN'), ('new', 'JJ'), ('or', 'CC'), ('unseen', 'JJ'), ('documents', 'NNS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['3.6', 'Execution', 'AI', 'model', 'trained', 'tested', 'post', 'processor', 'developed', ',', 'pipeline', 'ready', 'classify', 'extract', 'information', 'new', 'unseen', 'documents', '.']

 TOTAL FILTERED TOKENS ==>  19

 ---- POST FOR FILTERED TOKENS ----

 [('3.6', 'CD'), ('Execution', 'NNP'), ('AI', 'NNP'), ('model', 'NN'), ('trained', 'VBD'), ('tested', 'VBN'), ('post', 'NN'), ('processor', 'NN'), ('developed', 'VBD'), (',', ','), ('pipeline', 'NN'), ('ready', 'JJ'), ('classify', 'VB'), ('extract', 'JJ'), ('information', 'NN'), ('new', 'JJ'), ('unseen', 'JJ'), ('documents', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['3.6 Execution', 'Execution AI', 'AI model', 'model trained', 'trained tested', 'tested post', 'post processor', 'processor developed', 'developed ,', ', pipeline', 'pipeline ready', 'ready classify', 'classify extract', 'extract information', 'information new', 'new unseen', 'unseen documents', 'documents .'] 

 TOTAL BIGRAMS --> 18 



 ---- TRI-GRAMS ---- 

 ['3.6 Execution AI', 'Execution AI model', 'AI model trained', 'model trained tested', 'trained tested post', 'tested post processor', 'post processor developed', 'processor developed ,', 'developed , pipeline', ', pipeline ready', 'pipeline ready classify', 'ready classify extract', 'classify extract information', 'extract information new', 'information new unseen', 'new unseen documents', 'unseen documents .'] 

 TOTAL TRIGRAMS --> 17 



 ---- NOUN PHRASES ---- 

 ['model', 'post', 'processor', 'pipeline', 'extract information'] 

 TOTAL NOUN PHRASES --> 5 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['3.6', 'execut', 'ai', 'model', 'train', 'test', 'post', 'processor', 'develop', ',', 'pipelin', 'readi', 'classifi', 'extract', 'inform', 'new', 'unseen', 'document', '.']

 TOTAL PORTER STEM WORDS ==> 19



 ---- SNOWBALL STEMMING ----

['3.6', 'execut', 'ai', 'model', 'train', 'test', 'post', 'processor', 'develop', ',', 'pipelin', 'readi', 'classifi', 'extract', 'inform', 'new', 'unseen', 'document', '.']

 TOTAL SNOWBALL STEM WORDS ==> 19



 ---- LEMMATIZATION ----

['3.6', 'Execution', 'AI', 'model', 'trained', 'tested', 'post', 'processor', 'developed', ',', 'pipeline', 'ready', 'classify', 'extract', 'information', 'new', 'unseen', 'document', '.']

 TOTAL LEMMATIZE WORDS ==> 19

************************************************************************************************************************

166 --> The new content can be  subjected to NER model or categorizer  model or both depending upon the use  case. 


 ---- TOKENS ----

 ['The', 'new', 'content', 'can', 'be', 'subjected', 'to', 'NER', 'model', 'or', 'categorizer', 'model', 'or', 'both', 'depending', 'upon', 'the', 'use', 'case', '.'] 

 TOTAL TOKENS ==> 20

 ---- POST ----

 [('The', 'DT'), ('new', 'JJ'), ('content', 'NN'), ('can', 'MD'), ('be', 'VB'), ('subjected', 'VBN'), ('to', 'TO'), ('NER', 'NNP'), ('model', 'NN'), ('or', 'CC'), ('categorizer', 'NN'), ('model', 'NN'), ('or', 'CC'), ('both', 'DT'), ('depending', 'VBG'), ('upon', 'IN'), ('the', 'DT'), ('use', 'NN'), ('case', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['new', 'content', 'subjected', 'NER', 'model', 'categorizer', 'model', 'depending', 'upon', 'use', 'case', '.']

 TOTAL FILTERED TOKENS ==>  12

 ---- POST FOR FILTERED TOKENS ----

 [('new', 'JJ'), ('content', 'NN'), ('subjected', 'VBN'), ('NER', 'NNP'), ('model', 'FW'), ('categorizer', 'NN'), ('model', 'NN'), ('depending', 'VBG'), ('upon', 'IN'), ('use', 'NN'), ('case', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['new content', 'content subjected', 'subjected NER', 'NER model', 'model categorizer', 'categorizer model', 'model depending', 'depending upon', 'upon use', 'use case', 'case .'] 

 TOTAL BIGRAMS --> 11 



 ---- TRI-GRAMS ---- 

 ['new content subjected', 'content subjected NER', 'subjected NER model', 'NER model categorizer', 'model categorizer model', 'categorizer model depending', 'model depending upon', 'depending upon use', 'upon use case', 'use case .'] 

 TOTAL TRIGRAMS --> 10 



 ---- NOUN PHRASES ---- 

 ['new content', 'categorizer', 'model', 'use', 'case'] 

 TOTAL NOUN PHRASES --> 5 



 ---- NER ----

 
 ORGANIZATION ---> ['NER']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['new', 'content', 'subject', 'ner', 'model', 'categor', 'model', 'depend', 'upon', 'use', 'case', '.']

 TOTAL PORTER STEM WORDS ==> 12



 ---- SNOWBALL STEMMING ----

['new', 'content', 'subject', 'ner', 'model', 'categor', 'model', 'depend', 'upon', 'use', 'case', '.']

 TOTAL SNOWBALL STEM WORDS ==> 12



 ---- LEMMATIZATION ----

['new', 'content', 'subjected', 'NER', 'model', 'categorizer', 'model', 'depending', 'upon', 'use', 'case', '.']

 TOTAL LEMMATIZE WORDS ==> 12

************************************************************************************************************************

167 --> 3 High Level Implementation Steps External Document © 2019 Infosys Limited External Document © 2019 Infosys Limited  © 2019 Infosys Limited, Bengaluru, India. 


 ---- TOKENS ----

 ['3', 'High', 'Level', 'Implementation', 'Steps', 'External', 'Document', '©', '2019', 'Infosys', 'Limited', 'External', 'Document', '©', '2019', 'Infosys', 'Limited', '©', '2019', 'Infosys', 'Limited', ',', 'Bengaluru', ',', 'India', '.'] 

 TOTAL TOKENS ==> 26

 ---- POST ----

 [('3', 'CD'), ('High', 'NNP'), ('Level', 'NNP'), ('Implementation', 'NNP'), ('Steps', 'NNP'), ('External', 'NNP'), ('Document', 'NNP'), ('©', 'NN'), ('2019', 'CD'), ('Infosys', 'NNP'), ('Limited', 'NNP'), ('External', 'NNP'), ('Document', 'NNP'), ('©', 'NN'), ('2019', 'CD'), ('Infosys', 'NNP'), ('Limited', 'NNP'), ('©', 'NN'), ('2019', 'CD'), ('Infosys', 'NNP'), ('Limited', 'NNP'), (',', ','), ('Bengaluru', 'NNP'), (',', ','), ('India', 'NNP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['3', 'High', 'Level', 'Implementation', 'Steps', 'External', 'Document', '©', '2019', 'Infosys', 'Limited', 'External', 'Document', '©', '2019', 'Infosys', 'Limited', '©', '2019', 'Infosys', 'Limited', ',', 'Bengaluru', ',', 'India', '.']

 TOTAL FILTERED TOKENS ==>  26

 ---- POST FOR FILTERED TOKENS ----

 [('3', 'CD'), ('High', 'NNP'), ('Level', 'NNP'), ('Implementation', 'NNP'), ('Steps', 'NNP'), ('External', 'NNP'), ('Document', 'NNP'), ('©', 'NN'), ('2019', 'CD'), ('Infosys', 'NNP'), ('Limited', 'NNP'), ('External', 'NNP'), ('Document', 'NNP'), ('©', 'NN'), ('2019', 'CD'), ('Infosys', 'NNP'), ('Limited', 'NNP'), ('©', 'NN'), ('2019', 'CD'), ('Infosys', 'NNP'), ('Limited', 'NNP'), (',', ','), ('Bengaluru', 'NNP'), (',', ','), ('India', 'NNP'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['3 High', 'High Level', 'Level Implementation', 'Implementation Steps', 'Steps External', 'External Document', 'Document ©', '© 2019', '2019 Infosys', 'Infosys Limited', 'Limited External', 'External Document', 'Document ©', '© 2019', '2019 Infosys', 'Infosys Limited', 'Limited ©', '© 2019', '2019 Infosys', 'Infosys Limited', 'Limited ,', ', Bengaluru', 'Bengaluru ,', ', India', 'India .'] 

 TOTAL BIGRAMS --> 25 



 ---- TRI-GRAMS ---- 

 ['3 High Level', 'High Level Implementation', 'Level Implementation Steps', 'Implementation Steps External', 'Steps External Document', 'External Document ©', 'Document © 2019', '© 2019 Infosys', '2019 Infosys Limited', 'Infosys Limited External', 'Limited External Document', 'External Document ©', 'Document © 2019', '© 2019 Infosys', '2019 Infosys Limited', 'Infosys Limited ©', 'Limited © 2019', '© 2019 Infosys', '2019 Infosys Limited', 'Infosys Limited ,', 'Limited , Bengaluru', ', Bengaluru ,', 'Bengaluru , India', ', India .'] 

 TOTAL TRIGRAMS --> 24 



 ---- NOUN PHRASES ---- 

 ['©', '©', '©'] 

 TOTAL NOUN PHRASES --> 3 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> ['Level Implementation Steps External Document', 'Bengaluru']
 TOTAL PERSON ENTITY --> 2 


 GPE ---> ['India']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['3', 'high', 'level', 'implement', 'step', 'extern', 'document', '©', '2019', 'infosi', 'limit', 'extern', 'document', '©', '2019', 'infosi', 'limit', '©', '2019', 'infosi', 'limit', ',', 'bengaluru', ',', 'india', '.']

 TOTAL PORTER STEM WORDS ==> 26



 ---- SNOWBALL STEMMING ----

['3', 'high', 'level', 'implement', 'step', 'extern', 'document', '©', '2019', 'infosi', 'limit', 'extern', 'document', '©', '2019', 'infosi', 'limit', '©', '2019', 'infosi', 'limit', ',', 'bengaluru', ',', 'india', '.']

 TOTAL SNOWBALL STEM WORDS ==> 26



 ---- LEMMATIZATION ----

['3', 'High', 'Level', 'Implementation', 'Steps', 'External', 'Document', '©', '2019', 'Infosys', 'Limited', 'External', 'Document', '©', '2019', 'Infosys', 'Limited', '©', '2019', 'Infosys', 'Limited', ',', 'Bengaluru', ',', 'India', '.']

 TOTAL LEMMATIZE WORDS ==> 26

************************************************************************************************************************

168 --> All Rights Reserved. 


 ---- TOKENS ----

 ['All', 'Rights', 'Reserved', '.'] 

 TOTAL TOKENS ==> 4

 ---- POST ----

 [('All', 'DT'), ('Rights', 'NNPS'), ('Reserved', 'NNP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Rights', 'Reserved', '.']

 TOTAL FILTERED TOKENS ==>  3

 ---- POST FOR FILTERED TOKENS ----

 [('Rights', 'NNS'), ('Reserved', 'VBN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Rights Reserved', 'Reserved .'] 

 TOTAL BIGRAMS --> 2 



 ---- TRI-GRAMS ---- 

 ['Rights Reserved .'] 

 TOTAL TRIGRAMS --> 1 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['right', 'reserv', '.']

 TOTAL PORTER STEM WORDS ==> 3



 ---- SNOWBALL STEMMING ----

['right', 'reserv', '.']

 TOTAL SNOWBALL STEM WORDS ==> 3



 ---- LEMMATIZATION ----

['Rights', 'Reserved', '.']

 TOTAL LEMMATIZE WORDS ==> 3

************************************************************************************************************************

169 --> Infosys believes the information in this document is accurate as of its publication date; such information is subject to change without notice. 


 ---- TOKENS ----

 ['Infosys', 'believes', 'the', 'information', 'in', 'this', 'document', 'is', 'accurate', 'as', 'of', 'its', 'publication', 'date', ';', 'such', 'information', 'is', 'subject', 'to', 'change', 'without', 'notice', '.'] 

 TOTAL TOKENS ==> 24

 ---- POST ----

 [('Infosys', 'NNP'), ('believes', 'VBZ'), ('the', 'DT'), ('information', 'NN'), ('in', 'IN'), ('this', 'DT'), ('document', 'NN'), ('is', 'VBZ'), ('accurate', 'JJ'), ('as', 'IN'), ('of', 'IN'), ('its', 'PRP$'), ('publication', 'NN'), ('date', 'NN'), (';', ':'), ('such', 'JJ'), ('information', 'NN'), ('is', 'VBZ'), ('subject', 'JJ'), ('to', 'TO'), ('change', 'VB'), ('without', 'IN'), ('notice', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Infosys', 'believes', 'information', 'document', 'accurate', 'publication', 'date', ';', 'information', 'subject', 'change', 'without', 'notice', '.']

 TOTAL FILTERED TOKENS ==>  14

 ---- POST FOR FILTERED TOKENS ----

 [('Infosys', 'NNP'), ('believes', 'VBZ'), ('information', 'NN'), ('document', 'NN'), ('accurate', 'NN'), ('publication', 'NN'), ('date', 'NN'), (';', ':'), ('information', 'NN'), ('subject', 'JJ'), ('change', 'NN'), ('without', 'IN'), ('notice', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Infosys believes', 'believes information', 'information document', 'document accurate', 'accurate publication', 'publication date', 'date ;', '; information', 'information subject', 'subject change', 'change without', 'without notice', 'notice .'] 

 TOTAL BIGRAMS --> 13 



 ---- TRI-GRAMS ---- 

 ['Infosys believes information', 'believes information document', 'information document accurate', 'document accurate publication', 'accurate publication date', 'publication date ;', 'date ; information', '; information subject', 'information subject change', 'subject change without', 'change without notice', 'without notice .'] 

 TOTAL TRIGRAMS --> 12 



 ---- NOUN PHRASES ---- 

 ['information', 'document', 'accurate', 'publication', 'date', 'information', 'subject change', 'notice'] 

 TOTAL NOUN PHRASES --> 8 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> ['Infosys']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['infosi', 'believ', 'inform', 'document', 'accur', 'public', 'date', ';', 'inform', 'subject', 'chang', 'without', 'notic', '.']

 TOTAL PORTER STEM WORDS ==> 14



 ---- SNOWBALL STEMMING ----

['infosi', 'believ', 'inform', 'document', 'accur', 'public', 'date', ';', 'inform', 'subject', 'chang', 'without', 'notic', '.']

 TOTAL SNOWBALL STEM WORDS ==> 14



 ---- LEMMATIZATION ----

['Infosys', 'belief', 'information', 'document', 'accurate', 'publication', 'date', ';', 'information', 'subject', 'change', 'without', 'notice', '.']

 TOTAL LEMMATIZE WORDS ==> 14

************************************************************************************************************************

170 --> Infosys  acknowledges the proprietary rights of other companies to the trademarks, product names and such other intellectual property rights mentioned in this document. 


 ---- TOKENS ----

 ['Infosys', 'acknowledges', 'the', 'proprietary', 'rights', 'of', 'other', 'companies', 'to', 'the', 'trademarks', ',', 'product', 'names', 'and', 'such', 'other', 'intellectual', 'property', 'rights', 'mentioned', 'in', 'this', 'document', '.'] 

 TOTAL TOKENS ==> 25

 ---- POST ----

 [('Infosys', 'NNP'), ('acknowledges', 'VBZ'), ('the', 'DT'), ('proprietary', 'JJ'), ('rights', 'NNS'), ('of', 'IN'), ('other', 'JJ'), ('companies', 'NNS'), ('to', 'TO'), ('the', 'DT'), ('trademarks', 'NNS'), (',', ','), ('product', 'NN'), ('names', 'NNS'), ('and', 'CC'), ('such', 'JJ'), ('other', 'JJ'), ('intellectual', 'JJ'), ('property', 'NN'), ('rights', 'NNS'), ('mentioned', 'VBN'), ('in', 'IN'), ('this', 'DT'), ('document', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Infosys', 'acknowledges', 'proprietary', 'rights', 'companies', 'trademarks', ',', 'product', 'names', 'intellectual', 'property', 'rights', 'mentioned', 'document', '.']

 TOTAL FILTERED TOKENS ==>  15

 ---- POST FOR FILTERED TOKENS ----

 [('Infosys', 'NNP'), ('acknowledges', 'VBZ'), ('proprietary', 'JJ'), ('rights', 'NNS'), ('companies', 'NNS'), ('trademarks', 'NNS'), (',', ','), ('product', 'NN'), ('names', 'NNS'), ('intellectual', 'JJ'), ('property', 'NN'), ('rights', 'NNS'), ('mentioned', 'VBN'), ('document', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Infosys acknowledges', 'acknowledges proprietary', 'proprietary rights', 'rights companies', 'companies trademarks', 'trademarks ,', ', product', 'product names', 'names intellectual', 'intellectual property', 'property rights', 'rights mentioned', 'mentioned document', 'document .'] 

 TOTAL BIGRAMS --> 14 



 ---- TRI-GRAMS ---- 

 ['Infosys acknowledges proprietary', 'acknowledges proprietary rights', 'proprietary rights companies', 'rights companies trademarks', 'companies trademarks ,', 'trademarks , product', ', product names', 'product names intellectual', 'names intellectual property', 'intellectual property rights', 'property rights mentioned', 'rights mentioned document', 'mentioned document .'] 

 TOTAL TRIGRAMS --> 13 



 ---- NOUN PHRASES ---- 

 ['product', 'intellectual property', 'document'] 

 TOTAL NOUN PHRASES --> 3 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> ['Infosys']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['infosi', 'acknowledg', 'proprietari', 'right', 'compani', 'trademark', ',', 'product', 'name', 'intellectu', 'properti', 'right', 'mention', 'document', '.']

 TOTAL PORTER STEM WORDS ==> 15



 ---- SNOWBALL STEMMING ----

['infosi', 'acknowledg', 'proprietari', 'right', 'compani', 'trademark', ',', 'product', 'name', 'intellectu', 'properti', 'right', 'mention', 'document', '.']

 TOTAL SNOWBALL STEM WORDS ==> 15



 ---- LEMMATIZATION ----

['Infosys', 'acknowledges', 'proprietary', 'right', 'company', 'trademark', ',', 'product', 'name', 'intellectual', 'property', 'right', 'mentioned', 'document', '.']

 TOTAL LEMMATIZE WORDS ==> 15

************************************************************************************************************************

171 --> Except as expressly permitted, neither this  documentation nor any part of it may be reproduced, stored in a retrieval system, or transmitted in any form or by any means, electronic, mechanical, printing, photocopying, recording or otherwise, without the  prior permission of Infosys Limited and/ or any named intellectual property rights holders under this document. 


 ---- TOKENS ----

 ['Except', 'as', 'expressly', 'permitted', ',', 'neither', 'this', 'documentation', 'nor', 'any', 'part', 'of', 'it', 'may', 'be', 'reproduced', ',', 'stored', 'in', 'a', 'retrieval', 'system', ',', 'or', 'transmitted', 'in', 'any', 'form', 'or', 'by', 'any', 'means', ',', 'electronic', ',', 'mechanical', ',', 'printing', ',', 'photocopying', ',', 'recording', 'or', 'otherwise', ',', 'without', 'the', 'prior', 'permission', 'of', 'Infosys', 'Limited', 'and/', 'or', 'any', 'named', 'intellectual', 'property', 'rights', 'holders', 'under', 'this', 'document', '.'] 

 TOTAL TOKENS ==> 64

 ---- POST ----

 [('Except', 'IN'), ('as', 'RB'), ('expressly', 'RB'), ('permitted', 'VBN'), (',', ','), ('neither', 'CC'), ('this', 'DT'), ('documentation', 'NN'), ('nor', 'CC'), ('any', 'DT'), ('part', 'NN'), ('of', 'IN'), ('it', 'PRP'), ('may', 'MD'), ('be', 'VB'), ('reproduced', 'VBN'), (',', ','), ('stored', 'VBN'), ('in', 'IN'), ('a', 'DT'), ('retrieval', 'NN'), ('system', 'NN'), (',', ','), ('or', 'CC'), ('transmitted', 'VBN'), ('in', 'IN'), ('any', 'DT'), ('form', 'NN'), ('or', 'CC'), ('by', 'IN'), ('any', 'DT'), ('means', 'NNS'), (',', ','), ('electronic', 'JJ'), (',', ','), ('mechanical', 'JJ'), (',', ','), ('printing', 'NN'), (',', ','), ('photocopying', 'NN'), (',', ','), ('recording', 'VBG'), ('or', 'CC'), ('otherwise', 'RB'), (',', ','), ('without', 'IN'), ('the', 'DT'), ('prior', 'JJ'), ('permission', 'NN'), ('of', 'IN'), ('Infosys', 'NNP'), ('Limited', 'NNP'), ('and/', 'NN'), ('or', 'CC'), ('any', 'DT'), ('named', 'VBN'), ('intellectual', 'JJ'), ('property', 'NN'), ('rights', 'NNS'), ('holders', 'NNS'), ('under', 'IN'), ('this', 'DT'), ('document', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Except', 'expressly', 'permitted', ',', 'neither', 'documentation', 'part', 'may', 'reproduced', ',', 'stored', 'retrieval', 'system', ',', 'transmitted', 'form', 'means', ',', 'electronic', ',', 'mechanical', ',', 'printing', ',', 'photocopying', ',', 'recording', 'otherwise', ',', 'without', 'prior', 'permission', 'Infosys', 'Limited', 'and/', 'named', 'intellectual', 'property', 'rights', 'holders', 'document', '.']

 TOTAL FILTERED TOKENS ==>  42

 ---- POST FOR FILTERED TOKENS ----

 [('Except', 'IN'), ('expressly', 'RB'), ('permitted', 'VBN'), (',', ','), ('neither', 'DT'), ('documentation', 'NN'), ('part', 'NN'), ('may', 'MD'), ('reproduced', 'VB'), (',', ','), ('stored', 'VBN'), ('retrieval', 'NN'), ('system', 'NN'), (',', ','), ('transmitted', 'VBN'), ('form', 'NN'), ('means', 'VBZ'), (',', ','), ('electronic', 'JJ'), (',', ','), ('mechanical', 'JJ'), (',', ','), ('printing', 'NN'), (',', ','), ('photocopying', 'NN'), (',', ','), ('recording', 'VBG'), ('otherwise', 'RB'), (',', ','), ('without', 'IN'), ('prior', 'JJ'), ('permission', 'NN'), ('Infosys', 'NNP'), ('Limited', 'NNP'), ('and/', 'NN'), ('named', 'VBN'), ('intellectual', 'JJ'), ('property', 'NN'), ('rights', 'NNS'), ('holders', 'NNS'), ('document', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Except expressly', 'expressly permitted', 'permitted ,', ', neither', 'neither documentation', 'documentation part', 'part may', 'may reproduced', 'reproduced ,', ', stored', 'stored retrieval', 'retrieval system', 'system ,', ', transmitted', 'transmitted form', 'form means', 'means ,', ', electronic', 'electronic ,', ', mechanical', 'mechanical ,', ', printing', 'printing ,', ', photocopying', 'photocopying ,', ', recording', 'recording otherwise', 'otherwise ,', ', without', 'without prior', 'prior permission', 'permission Infosys', 'Infosys Limited', 'Limited and/', 'and/ named', 'named intellectual', 'intellectual property', 'property rights', 'rights holders', 'holders document', 'document .'] 

 TOTAL BIGRAMS --> 41 



 ---- TRI-GRAMS ---- 

 ['Except expressly permitted', 'expressly permitted ,', 'permitted , neither', ', neither documentation', 'neither documentation part', 'documentation part may', 'part may reproduced', 'may reproduced ,', 'reproduced , stored', ', stored retrieval', 'stored retrieval system', 'retrieval system ,', 'system , transmitted', ', transmitted form', 'transmitted form means', 'form means ,', 'means , electronic', ', electronic ,', 'electronic , mechanical', ', mechanical ,', 'mechanical , printing', ', printing ,', 'printing , photocopying', ', photocopying ,', 'photocopying , recording', ', recording otherwise', 'recording otherwise ,', 'otherwise , without', ', without prior', 'without prior permission', 'prior permission Infosys', 'permission Infosys Limited', 'Infosys Limited and/', 'Limited and/ named', 'and/ named intellectual', 'named intellectual property', 'intellectual property rights', 'property rights holders', 'rights holders document', 'holders document .'] 

 TOTAL TRIGRAMS --> 40 



 ---- NOUN PHRASES ---- 

 ['neither documentation', 'part', 'retrieval', 'system', 'form', 'printing', 'photocopying', 'prior permission', 'and', 'intellectual property', 'document'] 

 TOTAL NOUN PHRASES --> 11 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> ['Infosys Limited']
 TOTAL PERSON ENTITY --> 1 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['except', 'expressli', 'permit', ',', 'neither', 'document', 'part', 'may', 'reproduc', ',', 'store', 'retriev', 'system', ',', 'transmit', 'form', 'mean', ',', 'electron', ',', 'mechan', ',', 'print', ',', 'photocopi', ',', 'record', 'otherwis', ',', 'without', 'prior', 'permiss', 'infosi', 'limit', 'and/', 'name', 'intellectu', 'properti', 'right', 'holder', 'document', '.']

 TOTAL PORTER STEM WORDS ==> 42



 ---- SNOWBALL STEMMING ----

['except', 'expressli', 'permit', ',', 'neither', 'document', 'part', 'may', 'reproduc', ',', 'store', 'retriev', 'system', ',', 'transmit', 'form', 'mean', ',', 'electron', ',', 'mechan', ',', 'print', ',', 'photocopi', ',', 'record', 'otherwis', ',', 'without', 'prior', 'permiss', 'infosi', 'limit', 'and/', 'name', 'intellectu', 'properti', 'right', 'holder', 'document', '.']

 TOTAL SNOWBALL STEM WORDS ==> 42



 ---- LEMMATIZATION ----

['Except', 'expressly', 'permitted', ',', 'neither', 'documentation', 'part', 'may', 'reproduced', ',', 'stored', 'retrieval', 'system', ',', 'transmitted', 'form', 'mean', ',', 'electronic', ',', 'mechanical', ',', 'printing', ',', 'photocopying', ',', 'recording', 'otherwise', ',', 'without', 'prior', 'permission', 'Infosys', 'Limited', 'and/', 'named', 'intellectual', 'property', 'right', 'holder', 'document', '.']

 TOTAL LEMMATIZE WORDS ==> 42

************************************************************************************************************************

172 --> For more information, contact askus@infosys.com Infosys.com | NYSE: INFY Stay Connected Authors: A senior technology architect within the Digital practice of financial unit at Infosys, Girish Pande, has around 19 years  of experience in Information Technology. 


 ---- TOKENS ----

 ['For', 'more', 'information', ',', 'contact', 'askus', '@', 'infosys.com', 'Infosys.com', '|', 'NYSE', ':', 'INFY', 'Stay', 'Connected', 'Authors', ':', 'A', 'senior', 'technology', 'architect', 'within', 'the', 'Digital', 'practice', 'of', 'financial', 'unit', 'at', 'Infosys', ',', 'Girish', 'Pande', ',', 'has', 'around', '19', 'years', 'of', 'experience', 'in', 'Information', 'Technology', '.'] 

 TOTAL TOKENS ==> 44

 ---- POST ----

 [('For', 'IN'), ('more', 'JJR'), ('information', 'NN'), (',', ','), ('contact', 'NN'), ('askus', 'NN'), ('@', 'NNP'), ('infosys.com', 'NN'), ('Infosys.com', 'NNP'), ('|', 'NNP'), ('NYSE', 'NNP'), (':', ':'), ('INFY', 'NNP'), ('Stay', 'NNP'), ('Connected', 'NNP'), ('Authors', 'NNS'), (':', ':'), ('A', 'DT'), ('senior', 'JJ'), ('technology', 'NN'), ('architect', 'NN'), ('within', 'IN'), ('the', 'DT'), ('Digital', 'NNP'), ('practice', 'NN'), ('of', 'IN'), ('financial', 'JJ'), ('unit', 'NN'), ('at', 'IN'), ('Infosys', 'NNP'), (',', ','), ('Girish', 'NNP'), ('Pande', 'NNP'), (',', ','), ('has', 'VBZ'), ('around', 'RB'), ('19', 'CD'), ('years', 'NNS'), ('of', 'IN'), ('experience', 'NN'), ('in', 'IN'), ('Information', 'NNP'), ('Technology', 'NNP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['information', ',', 'contact', 'askus', '@', 'infosys.com', 'Infosys.com', '|', 'NYSE', ':', 'INFY', 'Stay', 'Connected', 'Authors', ':', 'senior', 'technology', 'architect', 'within', 'Digital', 'practice', 'financial', 'unit', 'Infosys', ',', 'Girish', 'Pande', ',', 'around', '19', 'years', 'experience', 'Information', 'Technology', '.']

 TOTAL FILTERED TOKENS ==>  35

 ---- POST FOR FILTERED TOKENS ----

 [('information', 'NN'), (',', ','), ('contact', 'NN'), ('askus', 'NN'), ('@', 'NNP'), ('infosys.com', 'NN'), ('Infosys.com', 'NNP'), ('|', 'NNP'), ('NYSE', 'NNP'), (':', ':'), ('INFY', 'NNP'), ('Stay', 'NNP'), ('Connected', 'NNP'), ('Authors', 'NNS'), (':', ':'), ('senior', 'JJ'), ('technology', 'NN'), ('architect', 'NN'), ('within', 'IN'), ('Digital', 'NNP'), ('practice', 'NN'), ('financial', 'JJ'), ('unit', 'NN'), ('Infosys', 'NNP'), (',', ','), ('Girish', 'NNP'), ('Pande', 'NNP'), (',', ','), ('around', 'IN'), ('19', 'CD'), ('years', 'NNS'), ('experience', 'JJ'), ('Information', 'NNP'), ('Technology', 'NNP'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['information ,', ', contact', 'contact askus', 'askus @', '@ infosys.com', 'infosys.com Infosys.com', 'Infosys.com |', '| NYSE', 'NYSE :', ': INFY', 'INFY Stay', 'Stay Connected', 'Connected Authors', 'Authors :', ': senior', 'senior technology', 'technology architect', 'architect within', 'within Digital', 'Digital practice', 'practice financial', 'financial unit', 'unit Infosys', 'Infosys ,', ', Girish', 'Girish Pande', 'Pande ,', ', around', 'around 19', '19 years', 'years experience', 'experience Information', 'Information Technology', 'Technology .'] 

 TOTAL BIGRAMS --> 34 



 ---- TRI-GRAMS ---- 

 ['information , contact', ', contact askus', 'contact askus @', 'askus @ infosys.com', '@ infosys.com Infosys.com', 'infosys.com Infosys.com |', 'Infosys.com | NYSE', '| NYSE :', 'NYSE : INFY', ': INFY Stay', 'INFY Stay Connected', 'Stay Connected Authors', 'Connected Authors :', 'Authors : senior', ': senior technology', 'senior technology architect', 'technology architect within', 'architect within Digital', 'within Digital practice', 'Digital practice financial', 'practice financial unit', 'financial unit Infosys', 'unit Infosys ,', 'Infosys , Girish', ', Girish Pande', 'Girish Pande ,', 'Pande , around', ', around 19', 'around 19 years', '19 years experience', 'years experience Information', 'experience Information Technology', 'Information Technology .'] 

 TOTAL TRIGRAMS --> 33 



 ---- NOUN PHRASES ---- 

 ['information', 'contact', 'askus', 'infosys.com', 'senior technology', 'architect', 'practice', 'financial unit'] 

 TOTAL NOUN PHRASES --> 8 



 ---- NER ----

 
 ORGANIZATION ---> ['INFY Stay', 'Digital']
 TOTAL ORGANIZATION ENTITY --> 2 


 PERSON ---> ['Infosys', 'Girish Pande']
 TOTAL PERSON ENTITY --> 2 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['inform', ',', 'contact', 'asku', '@', 'infosys.com', 'infosys.com', '|', 'nyse', ':', 'infi', 'stay', 'connect', 'author', ':', 'senior', 'technolog', 'architect', 'within', 'digit', 'practic', 'financi', 'unit', 'infosi', ',', 'girish', 'pand', ',', 'around', '19', 'year', 'experi', 'inform', 'technolog', '.']

 TOTAL PORTER STEM WORDS ==> 35



 ---- SNOWBALL STEMMING ----

['inform', ',', 'contact', 'askus', '@', 'infosys.com', 'infosys.com', '|', 'nyse', ':', 'infi', 'stay', 'connect', 'author', ':', 'senior', 'technolog', 'architect', 'within', 'digit', 'practic', 'financi', 'unit', 'infosi', ',', 'girish', 'pand', ',', 'around', '19', 'year', 'experi', 'inform', 'technolog', '.']

 TOTAL SNOWBALL STEM WORDS ==> 35



 ---- LEMMATIZATION ----

['information', ',', 'contact', 'askus', '@', 'infosys.com', 'Infosys.com', '|', 'NYSE', ':', 'INFY', 'Stay', 'Connected', 'Authors', ':', 'senior', 'technology', 'architect', 'within', 'Digital', 'practice', 'financial', 'unit', 'Infosys', ',', 'Girish', 'Pande', ',', 'around', '19', 'year', 'experience', 'Information', 'Technology', '.']

 TOTAL LEMMATIZE WORDS ==> 35

************************************************************************************************************************

173 --> He has played key role in architecting and implementing end to end Content  Services, Enterprise Search, NLP and Automation solutions for various clients across the globe. 


 ---- TOKENS ----

 ['He', 'has', 'played', 'key', 'role', 'in', 'architecting', 'and', 'implementing', 'end', 'to', 'end', 'Content', 'Services', ',', 'Enterprise', 'Search', ',', 'NLP', 'and', 'Automation', 'solutions', 'for', 'various', 'clients', 'across', 'the', 'globe', '.'] 

 TOTAL TOKENS ==> 29

 ---- POST ----

 [('He', 'PRP'), ('has', 'VBZ'), ('played', 'VBN'), ('key', 'JJ'), ('role', 'NN'), ('in', 'IN'), ('architecting', 'VBG'), ('and', 'CC'), ('implementing', 'VBG'), ('end', 'NN'), ('to', 'TO'), ('end', 'VB'), ('Content', 'NNP'), ('Services', 'NNPS'), (',', ','), ('Enterprise', 'NNP'), ('Search', 'NNP'), (',', ','), ('NLP', 'NNP'), ('and', 'CC'), ('Automation', 'NNP'), ('solutions', 'NNS'), ('for', 'IN'), ('various', 'JJ'), ('clients', 'NNS'), ('across', 'IN'), ('the', 'DT'), ('globe', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['played', 'key', 'role', 'architecting', 'implementing', 'end', 'end', 'Content', 'Services', ',', 'Enterprise', 'Search', ',', 'NLP', 'Automation', 'solutions', 'various', 'clients', 'across', 'globe', '.']

 TOTAL FILTERED TOKENS ==>  21

 ---- POST FOR FILTERED TOKENS ----

 [('played', 'VBN'), ('key', 'JJ'), ('role', 'NN'), ('architecting', 'VBG'), ('implementing', 'VBG'), ('end', 'JJ'), ('end', 'NN'), ('Content', 'NNP'), ('Services', 'NNPS'), (',', ','), ('Enterprise', 'NNP'), ('Search', 'NNP'), (',', ','), ('NLP', 'NNP'), ('Automation', 'NNP'), ('solutions', 'NNS'), ('various', 'JJ'), ('clients', 'NNS'), ('across', 'IN'), ('globe', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['played key', 'key role', 'role architecting', 'architecting implementing', 'implementing end', 'end end', 'end Content', 'Content Services', 'Services ,', ', Enterprise', 'Enterprise Search', 'Search ,', ', NLP', 'NLP Automation', 'Automation solutions', 'solutions various', 'various clients', 'clients across', 'across globe', 'globe .'] 

 TOTAL BIGRAMS --> 20 



 ---- TRI-GRAMS ---- 

 ['played key role', 'key role architecting', 'role architecting implementing', 'architecting implementing end', 'implementing end end', 'end end Content', 'end Content Services', 'Content Services ,', 'Services , Enterprise', ', Enterprise Search', 'Enterprise Search ,', 'Search , NLP', ', NLP Automation', 'NLP Automation solutions', 'Automation solutions various', 'solutions various clients', 'various clients across', 'clients across globe', 'across globe .'] 

 TOTAL TRIGRAMS --> 19 



 ---- NOUN PHRASES ---- 

 ['key role', 'end end', 'globe'] 

 TOTAL NOUN PHRASES --> 3 



 ---- NER ----

 
 ORGANIZATION ---> ['Content Services', 'NLP Automation']
 TOTAL ORGANIZATION ENTITY --> 2 


 PERSON ---> ['Enterprise Search']
 TOTAL PERSON ENTITY --> 1 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['play', 'key', 'role', 'architect', 'implement', 'end', 'end', 'content', 'servic', ',', 'enterpris', 'search', ',', 'nlp', 'autom', 'solut', 'variou', 'client', 'across', 'globe', '.']

 TOTAL PORTER STEM WORDS ==> 21



 ---- SNOWBALL STEMMING ----

['play', 'key', 'role', 'architect', 'implement', 'end', 'end', 'content', 'servic', ',', 'enterpris', 'search', ',', 'nlp', 'autom', 'solut', 'various', 'client', 'across', 'globe', '.']

 TOTAL SNOWBALL STEM WORDS ==> 21



 ---- LEMMATIZATION ----

['played', 'key', 'role', 'architecting', 'implementing', 'end', 'end', 'Content', 'Services', ',', 'Enterprise', 'Search', ',', 'NLP', 'Automation', 'solution', 'various', 'client', 'across', 'globe', '.']

 TOTAL LEMMATIZE WORDS ==> 21

************************************************************************************************************************

174 --> He is an M.Tech. 


 ---- TOKENS ----

 ['He', 'is', 'an', 'M.Tech', '.'] 

 TOTAL TOKENS ==> 5

 ---- POST ----

 [('He', 'PRP'), ('is', 'VBZ'), ('an', 'DT'), ('M.Tech', 'NNP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['M.Tech', '.']

 TOTAL FILTERED TOKENS ==>  2

 ---- POST FOR FILTERED TOKENS ----

 [('M.Tech', 'NNP'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['M.Tech .'] 

 TOTAL BIGRAMS --> 1 



 ---- TRI-GRAMS ---- 

 [] 

 TOTAL TRIGRAMS --> 0 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['m.tech', '.']

 TOTAL PORTER STEM WORDS ==> 2



 ---- SNOWBALL STEMMING ----

['m.tech', '.']

 TOTAL SNOWBALL STEM WORDS ==> 2



 ---- LEMMATIZATION ----

['M.Tech', '.']

 TOTAL LEMMATIZE WORDS ==> 2

************************************************************************************************************************

175 --> in  Industrial Engineering and Operations Research from IIT Bombay. 


 ---- TOKENS ----

 ['in', 'Industrial', 'Engineering', 'and', 'Operations', 'Research', 'from', 'IIT', 'Bombay', '.'] 

 TOTAL TOKENS ==> 10

 ---- POST ----

 [('in', 'IN'), ('Industrial', 'NNP'), ('Engineering', 'NNP'), ('and', 'CC'), ('Operations', 'NNP'), ('Research', 'NNP'), ('from', 'IN'), ('IIT', 'NNP'), ('Bombay', 'NNP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Industrial', 'Engineering', 'Operations', 'Research', 'IIT', 'Bombay', '.']

 TOTAL FILTERED TOKENS ==>  7

 ---- POST FOR FILTERED TOKENS ----

 [('Industrial', 'NNP'), ('Engineering', 'NNP'), ('Operations', 'NNP'), ('Research', 'NNP'), ('IIT', 'NNP'), ('Bombay', 'NNP'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Industrial Engineering', 'Engineering Operations', 'Operations Research', 'Research IIT', 'IIT Bombay', 'Bombay .'] 

 TOTAL BIGRAMS --> 6 



 ---- TRI-GRAMS ---- 

 ['Industrial Engineering Operations', 'Engineering Operations Research', 'Operations Research IIT', 'Research IIT Bombay', 'IIT Bombay .'] 

 TOTAL TRIGRAMS --> 5 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> ['Industrial']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['industri', 'engin', 'oper', 'research', 'iit', 'bombay', '.']

 TOTAL PORTER STEM WORDS ==> 7



 ---- SNOWBALL STEMMING ----

['industri', 'engin', 'oper', 'research', 'iit', 'bombay', '.']

 TOTAL SNOWBALL STEM WORDS ==> 7



 ---- LEMMATIZATION ----

['Industrial', 'Engineering', 'Operations', 'Research', 'IIT', 'Bombay', '.']

 TOTAL LEMMATIZE WORDS ==> 7

************************************************************************************************************************

176 --> He can be reached at girish_pande@Infosys.com A Technology Architect with the Digital Practice unit at Infosys, Yamuna Sri Kannaian, has around 13 years of experience  in Information Technology. 


 ---- TOKENS ----

 ['He', 'can', 'be', 'reached', 'at', 'girish_pande', '@', 'Infosys.com', 'A', 'Technology', 'Architect', 'with', 'the', 'Digital', 'Practice', 'unit', 'at', 'Infosys', ',', 'Yamuna', 'Sri', 'Kannaian', ',', 'has', 'around', '13', 'years', 'of', 'experience', 'in', 'Information', 'Technology', '.'] 

 TOTAL TOKENS ==> 33

 ---- POST ----

 [('He', 'PRP'), ('can', 'MD'), ('be', 'VB'), ('reached', 'VBN'), ('at', 'IN'), ('girish_pande', 'NN'), ('@', 'NNP'), ('Infosys.com', 'NNP'), ('A', 'NNP'), ('Technology', 'NNP'), ('Architect', 'NNP'), ('with', 'IN'), ('the', 'DT'), ('Digital', 'NNP'), ('Practice', 'NNP'), ('unit', 'NN'), ('at', 'IN'), ('Infosys', 'NNP'), (',', ','), ('Yamuna', 'NNP'), ('Sri', 'NNP'), ('Kannaian', 'NNP'), (',', ','), ('has', 'VBZ'), ('around', 'RB'), ('13', 'CD'), ('years', 'NNS'), ('of', 'IN'), ('experience', 'NN'), ('in', 'IN'), ('Information', 'NNP'), ('Technology', 'NNP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['reached', 'girish_pande', '@', 'Infosys.com', 'Technology', 'Architect', 'Digital', 'Practice', 'unit', 'Infosys', ',', 'Yamuna', 'Sri', 'Kannaian', ',', 'around', '13', 'years', 'experience', 'Information', 'Technology', '.']

 TOTAL FILTERED TOKENS ==>  22

 ---- POST FOR FILTERED TOKENS ----

 [('reached', 'VBN'), ('girish_pande', 'NN'), ('@', 'NN'), ('Infosys.com', 'NNP'), ('Technology', 'NNP'), ('Architect', 'NNP'), ('Digital', 'NNP'), ('Practice', 'NNP'), ('unit', 'NN'), ('Infosys', 'NNP'), (',', ','), ('Yamuna', 'NNP'), ('Sri', 'NNP'), ('Kannaian', 'NNP'), (',', ','), ('around', 'IN'), ('13', 'CD'), ('years', 'NNS'), ('experience', 'JJ'), ('Information', 'NNP'), ('Technology', 'NNP'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['reached girish_pande', 'girish_pande @', '@ Infosys.com', 'Infosys.com Technology', 'Technology Architect', 'Architect Digital', 'Digital Practice', 'Practice unit', 'unit Infosys', 'Infosys ,', ', Yamuna', 'Yamuna Sri', 'Sri Kannaian', 'Kannaian ,', ', around', 'around 13', '13 years', 'years experience', 'experience Information', 'Information Technology', 'Technology .'] 

 TOTAL BIGRAMS --> 21 



 ---- TRI-GRAMS ---- 

 ['reached girish_pande @', 'girish_pande @ Infosys.com', '@ Infosys.com Technology', 'Infosys.com Technology Architect', 'Technology Architect Digital', 'Architect Digital Practice', 'Digital Practice unit', 'Practice unit Infosys', 'unit Infosys ,', 'Infosys , Yamuna', ', Yamuna Sri', 'Yamuna Sri Kannaian', 'Sri Kannaian ,', 'Kannaian , around', ', around 13', 'around 13 years', '13 years experience', 'years experience Information', 'experience Information Technology', 'Information Technology .'] 

 TOTAL TRIGRAMS --> 20 



 ---- NOUN PHRASES ---- 

 ['girish_pande', '@', 'unit'] 

 TOTAL NOUN PHRASES --> 3 



 ---- NER ----

 
 ORGANIZATION ---> ['Digital']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> ['Infosys', 'Yamuna Sri Kannaian']
 TOTAL PERSON ENTITY --> 2 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['reach', 'girish_pand', '@', 'infosys.com', 'technolog', 'architect', 'digit', 'practic', 'unit', 'infosi', ',', 'yamuna', 'sri', 'kannaian', ',', 'around', '13', 'year', 'experi', 'inform', 'technolog', '.']

 TOTAL PORTER STEM WORDS ==> 22



 ---- SNOWBALL STEMMING ----

['reach', 'girish_pand', '@', 'infosys.com', 'technolog', 'architect', 'digit', 'practic', 'unit', 'infosi', ',', 'yamuna', 'sri', 'kannaian', ',', 'around', '13', 'year', 'experi', 'inform', 'technolog', '.']

 TOTAL SNOWBALL STEM WORDS ==> 22



 ---- LEMMATIZATION ----

['reached', 'girish_pande', '@', 'Infosys.com', 'Technology', 'Architect', 'Digital', 'Practice', 'unit', 'Infosys', ',', 'Yamuna', 'Sri', 'Kannaian', ',', 'around', '13', 'year', 'experience', 'Information', 'Technology', '.']

 TOTAL LEMMATIZE WORDS ==> 22

************************************************************************************************************************

177 --> She has played various key roles and has wide experience in architecting, designing and  implementing Web applications, Content Services and NLP solutions. 


 ---- TOKENS ----

 ['She', 'has', 'played', 'various', 'key', 'roles', 'and', 'has', 'wide', 'experience', 'in', 'architecting', ',', 'designing', 'and', 'implementing', 'Web', 'applications', ',', 'Content', 'Services', 'and', 'NLP', 'solutions', '.'] 

 TOTAL TOKENS ==> 25

 ---- POST ----

 [('She', 'PRP'), ('has', 'VBZ'), ('played', 'VBN'), ('various', 'JJ'), ('key', 'JJ'), ('roles', 'NNS'), ('and', 'CC'), ('has', 'VBZ'), ('wide', 'JJ'), ('experience', 'NN'), ('in', 'IN'), ('architecting', 'VBG'), (',', ','), ('designing', 'VBG'), ('and', 'CC'), ('implementing', 'VBG'), ('Web', 'NNP'), ('applications', 'NNS'), (',', ','), ('Content', 'NNP'), ('Services', 'NNPS'), ('and', 'CC'), ('NLP', 'NNP'), ('solutions', 'NNS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['played', 'various', 'key', 'roles', 'wide', 'experience', 'architecting', ',', 'designing', 'implementing', 'Web', 'applications', ',', 'Content', 'Services', 'NLP', 'solutions', '.']

 TOTAL FILTERED TOKENS ==>  18

 ---- POST FOR FILTERED TOKENS ----

 [('played', 'VBN'), ('various', 'JJ'), ('key', 'JJ'), ('roles', 'NNS'), ('wide', 'JJ'), ('experience', 'NN'), ('architecting', 'NN'), (',', ','), ('designing', 'VBG'), ('implementing', 'VBG'), ('Web', 'NNP'), ('applications', 'NNS'), (',', ','), ('Content', 'NNP'), ('Services', 'NNPS'), ('NLP', 'NNP'), ('solutions', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['played various', 'various key', 'key roles', 'roles wide', 'wide experience', 'experience architecting', 'architecting ,', ', designing', 'designing implementing', 'implementing Web', 'Web applications', 'applications ,', ', Content', 'Content Services', 'Services NLP', 'NLP solutions', 'solutions .'] 

 TOTAL BIGRAMS --> 17 



 ---- TRI-GRAMS ---- 

 ['played various key', 'various key roles', 'key roles wide', 'roles wide experience', 'wide experience architecting', 'experience architecting ,', 'architecting , designing', ', designing implementing', 'designing implementing Web', 'implementing Web applications', 'Web applications ,', 'applications , Content', ', Content Services', 'Content Services NLP', 'Services NLP solutions', 'NLP solutions .'] 

 TOTAL TRIGRAMS --> 16 



 ---- NOUN PHRASES ---- 

 ['wide experience', 'architecting'] 

 TOTAL NOUN PHRASES --> 2 



 ---- NER ----

 
 ORGANIZATION ---> ['Content Services', 'NLP']
 TOTAL ORGANIZATION ENTITY --> 2 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['play', 'variou', 'key', 'role', 'wide', 'experi', 'architect', ',', 'design', 'implement', 'web', 'applic', ',', 'content', 'servic', 'nlp', 'solut', '.']

 TOTAL PORTER STEM WORDS ==> 18



 ---- SNOWBALL STEMMING ----

['play', 'various', 'key', 'role', 'wide', 'experi', 'architect', ',', 'design', 'implement', 'web', 'applic', ',', 'content', 'servic', 'nlp', 'solut', '.']

 TOTAL SNOWBALL STEM WORDS ==> 18



 ---- LEMMATIZATION ----

['played', 'various', 'key', 'role', 'wide', 'experience', 'architecting', ',', 'designing', 'implementing', 'Web', 'application', ',', 'Content', 'Services', 'NLP', 'solution', '.']

 TOTAL LEMMATIZE WORDS ==> 18

************************************************************************************************************************

178 --> She holds a B. 


 ---- TOKENS ----

 ['She', 'holds', 'a', 'B', '.'] 

 TOTAL TOKENS ==> 5

 ---- POST ----

 [('She', 'PRP'), ('holds', 'VBZ'), ('a', 'DT'), ('B', 'NNP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['holds', 'B', '.']

 TOTAL FILTERED TOKENS ==>  3

 ---- POST FOR FILTERED TOKENS ----

 [('holds', 'VBZ'), ('B', 'NNP'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['holds B', 'B .'] 

 TOTAL BIGRAMS --> 2 



 ---- TRI-GRAMS ---- 

 ['holds B .'] 

 TOTAL TRIGRAMS --> 1 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['hold', 'b', '.']

 TOTAL PORTER STEM WORDS ==> 3



 ---- SNOWBALL STEMMING ----

['hold', 'b', '.']

 TOTAL SNOWBALL STEM WORDS ==> 3



 ---- LEMMATIZATION ----

['hold', 'B', '.']

 TOTAL LEMMATIZE WORDS ==> 3

************************************************************************************************************************

179 --> Tech. 


 ---- TOKENS ----

 ['Tech', '.'] 

 TOTAL TOKENS ==> 2

 ---- POST ----

 [('Tech', 'NNP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Tech', '.']

 TOTAL FILTERED TOKENS ==>  2

 ---- POST FOR FILTERED TOKENS ----

 [('Tech', 'NNP'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Tech .'] 

 TOTAL BIGRAMS --> 1 



 ---- TRI-GRAMS ---- 

 [] 

 TOTAL TRIGRAMS --> 0 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> ['Tech']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['tech', '.']

 TOTAL PORTER STEM WORDS ==> 2



 ---- SNOWBALL STEMMING ----

['tech', '.']

 TOTAL SNOWBALL STEM WORDS ==> 2



 ---- LEMMATIZATION ----

['Tech', '.']

 TOTAL LEMMATIZE WORDS ==> 2

************************************************************************************************************************

180 --> degree on Information  Technology from Anna University. 


 ---- TOKENS ----

 ['degree', 'on', 'Information', 'Technology', 'from', 'Anna', 'University', '.'] 

 TOTAL TOKENS ==> 8

 ---- POST ----

 [('degree', 'NN'), ('on', 'IN'), ('Information', 'NNP'), ('Technology', 'NNP'), ('from', 'IN'), ('Anna', 'NNP'), ('University', 'NNP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['degree', 'Information', 'Technology', 'Anna', 'University', '.']

 TOTAL FILTERED TOKENS ==>  6

 ---- POST FOR FILTERED TOKENS ----

 [('degree', 'JJ'), ('Information', 'NNP'), ('Technology', 'NNP'), ('Anna', 'NNP'), ('University', 'NNP'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['degree Information', 'Information Technology', 'Technology Anna', 'Anna University', 'University .'] 

 TOTAL BIGRAMS --> 5 



 ---- TRI-GRAMS ---- 

 ['degree Information Technology', 'Information Technology Anna', 'Technology Anna University', 'Anna University .'] 

 TOTAL TRIGRAMS --> 4 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> ['Anna University']
 TOTAL PERSON ENTITY --> 1 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['degre', 'inform', 'technolog', 'anna', 'univers', '.']

 TOTAL PORTER STEM WORDS ==> 6



 ---- SNOWBALL STEMMING ----

['degre', 'inform', 'technolog', 'anna', 'univers', '.']

 TOTAL SNOWBALL STEM WORDS ==> 6



 ---- LEMMATIZATION ----

['degree', 'Information', 'Technology', 'Anna', 'University', '.']

 TOTAL LEMMATIZE WORDS ==> 6

************************************************************************************************************************

181 --> She can be reached at Yamuna_kannaian@infosys.com To know more about this paper, please reach out to digital@infosys.com  Conclusion With above discussion, it is evident that  NLP can play very crucial role in addressing  some of the key challenges encountered  in Content Services Platform. 


 ---- TOKENS ----

 ['She', 'can', 'be', 'reached', 'at', 'Yamuna_kannaian', '@', 'infosys.com', 'To', 'know', 'more', 'about', 'this', 'paper', ',', 'please', 'reach', 'out', 'to', 'digital', '@', 'infosys.com', 'Conclusion', 'With', 'above', 'discussion', ',', 'it', 'is', 'evident', 'that', 'NLP', 'can', 'play', 'very', 'crucial', 'role', 'in', 'addressing', 'some', 'of', 'the', 'key', 'challenges', 'encountered', 'in', 'Content', 'Services', 'Platform', '.'] 

 TOTAL TOKENS ==> 50

 ---- POST ----

 [('She', 'PRP'), ('can', 'MD'), ('be', 'VB'), ('reached', 'VBN'), ('at', 'IN'), ('Yamuna_kannaian', 'JJ'), ('@', 'NNP'), ('infosys.com', 'NN'), ('To', 'TO'), ('know', 'VB'), ('more', 'JJR'), ('about', 'IN'), ('this', 'DT'), ('paper', 'NN'), (',', ','), ('please', 'VB'), ('reach', 'VB'), ('out', 'RP'), ('to', 'TO'), ('digital', 'JJ'), ('@', 'NNP'), ('infosys.com', 'NN'), ('Conclusion', 'NNP'), ('With', 'IN'), ('above', 'JJ'), ('discussion', 'NN'), (',', ','), ('it', 'PRP'), ('is', 'VBZ'), ('evident', 'JJ'), ('that', 'IN'), ('NLP', 'NNP'), ('can', 'MD'), ('play', 'VB'), ('very', 'RB'), ('crucial', 'JJ'), ('role', 'NN'), ('in', 'IN'), ('addressing', 'VBG'), ('some', 'DT'), ('of', 'IN'), ('the', 'DT'), ('key', 'JJ'), ('challenges', 'NNS'), ('encountered', 'VBN'), ('in', 'IN'), ('Content', 'NNP'), ('Services', 'NNPS'), ('Platform', 'NNP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['reached', 'Yamuna_kannaian', '@', 'infosys.com', 'know', 'paper', ',', 'please', 'reach', 'digital', '@', 'infosys.com', 'Conclusion', 'discussion', ',', 'evident', 'NLP', 'play', 'crucial', 'role', 'addressing', 'key', 'challenges', 'encountered', 'Content', 'Services', 'Platform', '.']

 TOTAL FILTERED TOKENS ==>  28

 ---- POST FOR FILTERED TOKENS ----

 [('reached', 'VBN'), ('Yamuna_kannaian', 'JJ'), ('@', 'NNP'), ('infosys.com', 'NN'), ('know', 'VBP'), ('paper', 'NN'), (',', ','), ('please', 'VB'), ('reach', 'VB'), ('digital', 'JJ'), ('@', 'NNP'), ('infosys.com', 'NN'), ('Conclusion', 'NNP'), ('discussion', 'NN'), (',', ','), ('evident', 'JJ'), ('NLP', 'NNP'), ('play', 'NN'), ('crucial', 'JJ'), ('role', 'NN'), ('addressing', 'VBG'), ('key', 'JJ'), ('challenges', 'NNS'), ('encountered', 'VBN'), ('Content', 'NNP'), ('Services', 'NNPS'), ('Platform', 'NNP'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['reached Yamuna_kannaian', 'Yamuna_kannaian @', '@ infosys.com', 'infosys.com know', 'know paper', 'paper ,', ', please', 'please reach', 'reach digital', 'digital @', '@ infosys.com', 'infosys.com Conclusion', 'Conclusion discussion', 'discussion ,', ', evident', 'evident NLP', 'NLP play', 'play crucial', 'crucial role', 'role addressing', 'addressing key', 'key challenges', 'challenges encountered', 'encountered Content', 'Content Services', 'Services Platform', 'Platform .'] 

 TOTAL BIGRAMS --> 27 



 ---- TRI-GRAMS ---- 

 ['reached Yamuna_kannaian @', 'Yamuna_kannaian @ infosys.com', '@ infosys.com know', 'infosys.com know paper', 'know paper ,', 'paper , please', ', please reach', 'please reach digital', 'reach digital @', 'digital @ infosys.com', '@ infosys.com Conclusion', 'infosys.com Conclusion discussion', 'Conclusion discussion ,', 'discussion , evident', ', evident NLP', 'evident NLP play', 'NLP play crucial', 'play crucial role', 'crucial role addressing', 'role addressing key', 'addressing key challenges', 'key challenges encountered', 'challenges encountered Content', 'encountered Content Services', 'Content Services Platform', 'Services Platform .'] 

 TOTAL TRIGRAMS --> 26 



 ---- NOUN PHRASES ---- 

 ['infosys.com', 'paper', 'infosys.com', 'discussion', 'play', 'crucial role'] 

 TOTAL NOUN PHRASES --> 6 



 ---- NER ----

 
 ORGANIZATION ---> ['Yamuna_kannaian', 'NLP', 'Content Services Platform']
 TOTAL ORGANIZATION ENTITY --> 3 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['reach', 'yamuna_kannaian', '@', 'infosys.com', 'know', 'paper', ',', 'pleas', 'reach', 'digit', '@', 'infosys.com', 'conclus', 'discuss', ',', 'evid', 'nlp', 'play', 'crucial', 'role', 'address', 'key', 'challeng', 'encount', 'content', 'servic', 'platform', '.']

 TOTAL PORTER STEM WORDS ==> 28



 ---- SNOWBALL STEMMING ----

['reach', 'yamuna_kannaian', '@', 'infosys.com', 'know', 'paper', ',', 'pleas', 'reach', 'digit', '@', 'infosys.com', 'conclus', 'discuss', ',', 'evid', 'nlp', 'play', 'crucial', 'role', 'address', 'key', 'challeng', 'encount', 'content', 'servic', 'platform', '.']

 TOTAL SNOWBALL STEM WORDS ==> 28



 ---- LEMMATIZATION ----

['reached', 'Yamuna_kannaian', '@', 'infosys.com', 'know', 'paper', ',', 'please', 'reach', 'digital', '@', 'infosys.com', 'Conclusion', 'discussion', ',', 'evident', 'NLP', 'play', 'crucial', 'role', 'addressing', 'key', 'challenge', 'encountered', 'Content', 'Services', 'Platform', '.']

 TOTAL LEMMATIZE WORDS ==> 28

************************************************************************************************************************

182 --> There can be  many other challenges, not discussed here,    whichcan be addressed by NLP. 


 ---- TOKENS ----

 ['There', 'can', 'be', 'many', 'other', 'challenges', ',', 'not', 'discussed', 'here', ',', 'whichcan', 'be', 'addressed', 'by', 'NLP', '.'] 

 TOTAL TOKENS ==> 17

 ---- POST ----

 [('There', 'EX'), ('can', 'MD'), ('be', 'VB'), ('many', 'JJ'), ('other', 'JJ'), ('challenges', 'NNS'), (',', ','), ('not', 'RB'), ('discussed', 'VBN'), ('here', 'RB'), (',', ','), ('whichcan', 'JJ'), ('be', 'VB'), ('addressed', 'VBN'), ('by', 'IN'), ('NLP', 'NNP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['many', 'challenges', ',', 'discussed', ',', 'whichcan', 'addressed', 'NLP', '.']

 TOTAL FILTERED TOKENS ==>  9

 ---- POST FOR FILTERED TOKENS ----

 [('many', 'JJ'), ('challenges', 'NNS'), (',', ','), ('discussed', 'VBN'), (',', ','), ('whichcan', 'JJ'), ('addressed', 'VBD'), ('NLP', 'NNP'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['many challenges', 'challenges ,', ', discussed', 'discussed ,', ', whichcan', 'whichcan addressed', 'addressed NLP', 'NLP .'] 

 TOTAL BIGRAMS --> 8 



 ---- TRI-GRAMS ---- 

 ['many challenges ,', 'challenges , discussed', ', discussed ,', 'discussed , whichcan', ', whichcan addressed', 'whichcan addressed NLP', 'addressed NLP .'] 

 TOTAL TRIGRAMS --> 7 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> ['NLP']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['mani', 'challeng', ',', 'discuss', ',', 'whichcan', 'address', 'nlp', '.']

 TOTAL PORTER STEM WORDS ==> 9



 ---- SNOWBALL STEMMING ----

['mani', 'challeng', ',', 'discuss', ',', 'whichcan', 'address', 'nlp', '.']

 TOTAL SNOWBALL STEM WORDS ==> 9



 ---- LEMMATIZATION ----

['many', 'challenge', ',', 'discussed', ',', 'whichcan', 'addressed', 'NLP', '.']

 TOTAL LEMMATIZE WORDS ==> 9

************************************************************************************************************************

183 --> Using  the capabilities offered by NLP, Content  Services can help users not only save their  time but also, accurately locating and  reusing the content. 


 ---- TOKENS ----

 ['Using', 'the', 'capabilities', 'offered', 'by', 'NLP', ',', 'Content', 'Services', 'can', 'help', 'users', 'not', 'only', 'save', 'their', 'time', 'but', 'also', ',', 'accurately', 'locating', 'and', 'reusing', 'the', 'content', '.'] 

 TOTAL TOKENS ==> 27

 ---- POST ----

 [('Using', 'VBG'), ('the', 'DT'), ('capabilities', 'NNS'), ('offered', 'VBN'), ('by', 'IN'), ('NLP', 'NNP'), (',', ','), ('Content', 'NNP'), ('Services', 'NNPS'), ('can', 'MD'), ('help', 'VB'), ('users', 'NNS'), ('not', 'RB'), ('only', 'RB'), ('save', 'VB'), ('their', 'PRP$'), ('time', 'NN'), ('but', 'CC'), ('also', 'RB'), (',', ','), ('accurately', 'RB'), ('locating', 'VBG'), ('and', 'CC'), ('reusing', 'VBG'), ('the', 'DT'), ('content', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Using', 'capabilities', 'offered', 'NLP', ',', 'Content', 'Services', 'help', 'users', 'save', 'time', 'also', ',', 'accurately', 'locating', 'reusing', 'content', '.']

 TOTAL FILTERED TOKENS ==>  18

 ---- POST FOR FILTERED TOKENS ----

 [('Using', 'VBG'), ('capabilities', 'NNS'), ('offered', 'VBN'), ('NLP', 'NNP'), (',', ','), ('Content', 'NNP'), ('Services', 'NNPS'), ('help', 'NN'), ('users', 'NNS'), ('save', 'VBP'), ('time', 'NN'), ('also', 'RB'), (',', ','), ('accurately', 'RB'), ('locating', 'VBG'), ('reusing', 'VBG'), ('content', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Using capabilities', 'capabilities offered', 'offered NLP', 'NLP ,', ', Content', 'Content Services', 'Services help', 'help users', 'users save', 'save time', 'time also', 'also ,', ', accurately', 'accurately locating', 'locating reusing', 'reusing content', 'content .'] 

 TOTAL BIGRAMS --> 17 



 ---- TRI-GRAMS ---- 

 ['Using capabilities offered', 'capabilities offered NLP', 'offered NLP ,', 'NLP , Content', ', Content Services', 'Content Services help', 'Services help users', 'help users save', 'users save time', 'save time also', 'time also ,', 'also , accurately', ', accurately locating', 'accurately locating reusing', 'locating reusing content', 'reusing content .'] 

 TOTAL TRIGRAMS --> 16 



 ---- NOUN PHRASES ---- 

 ['help', 'time', 'content'] 

 TOTAL NOUN PHRASES --> 3 



 ---- NER ----

 
 ORGANIZATION ---> ['NLP', 'Content Services']
 TOTAL ORGANIZATION ENTITY --> 2 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['use', 'capabl', 'offer', 'nlp', ',', 'content', 'servic', 'help', 'user', 'save', 'time', 'also', ',', 'accur', 'locat', 'reus', 'content', '.']

 TOTAL PORTER STEM WORDS ==> 18



 ---- SNOWBALL STEMMING ----

['use', 'capabl', 'offer', 'nlp', ',', 'content', 'servic', 'help', 'user', 'save', 'time', 'also', ',', 'accur', 'locat', 'reus', 'content', '.']

 TOTAL SNOWBALL STEM WORDS ==> 18



 ---- LEMMATIZATION ----

['Using', 'capability', 'offered', 'NLP', ',', 'Content', 'Services', 'help', 'user', 'save', 'time', 'also', ',', 'accurately', 'locating', 'reusing', 'content', '.']

 TOTAL LEMMATIZE WORDS ==> 18

************************************************************************************************************************

184 --> https://twitter.com/infosys https://www.linkedin.com/company/infosys https://www.youtube.com/Infosys http://www.slideshare.net/Infosys https://www.infosys.com/ 


 ---- TOKENS ----

 ['https', ':', '//twitter.com/infosys', 'https', ':', '//www.linkedin.com/company/infosys', 'https', ':', '//www.youtube.com/Infosys', 'http', ':', '//www.slideshare.net/Infosys', 'https', ':', '//www.infosys.com/'] 

 TOTAL TOKENS ==> 15

 ---- POST ----

 [('https', 'NN'), (':', ':'), ('//twitter.com/infosys', 'JJ'), ('https', 'NN'), (':', ':'), ('//www.linkedin.com/company/infosys', 'JJ'), ('https', 'NN'), (':', ':'), ('//www.youtube.com/Infosys', 'JJ'), ('http', 'NN'), (':', ':'), ('//www.slideshare.net/Infosys', 'JJ'), ('https', 'NN'), (':', ':'), ('//www.infosys.com/', 'NN')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['https', ':', '//twitter.com/infosys', 'https', ':', '//www.linkedin.com/company/infosys', 'https', ':', '//www.youtube.com/Infosys', 'http', ':', '//www.slideshare.net/Infosys', 'https', ':', '//www.infosys.com/']

 TOTAL FILTERED TOKENS ==>  15

 ---- POST FOR FILTERED TOKENS ----

 [('https', 'NN'), (':', ':'), ('//twitter.com/infosys', 'JJ'), ('https', 'NN'), (':', ':'), ('//www.linkedin.com/company/infosys', 'JJ'), ('https', 'NN'), (':', ':'), ('//www.youtube.com/Infosys', 'JJ'), ('http', 'NN'), (':', ':'), ('//www.slideshare.net/Infosys', 'JJ'), ('https', 'NN'), (':', ':'), ('//www.infosys.com/', 'NN')] 



 ---- BI-GRAMS ---- 

 ['https :', ': //twitter.com/infosys', '//twitter.com/infosys https', 'https :', ': //www.linkedin.com/company/infosys', '//www.linkedin.com/company/infosys https', 'https :', ': //www.youtube.com/Infosys', '//www.youtube.com/Infosys http', 'http :', ': //www.slideshare.net/Infosys', '//www.slideshare.net/Infosys https', 'https :', ': //www.infosys.com/'] 

 TOTAL BIGRAMS --> 14 



 ---- TRI-GRAMS ---- 

 ['https : //twitter.com/infosys', ': //twitter.com/infosys https', '//twitter.com/infosys https :', 'https : //www.linkedin.com/company/infosys', ': //www.linkedin.com/company/infosys https', '//www.linkedin.com/company/infosys https :', 'https : //www.youtube.com/Infosys', ': //www.youtube.com/Infosys http', '//www.youtube.com/Infosys http :', 'http : //www.slideshare.net/Infosys', ': //www.slideshare.net/Infosys https', '//www.slideshare.net/Infosys https :', 'https : //www.infosys.com/'] 

 TOTAL TRIGRAMS --> 13 



 ---- NOUN PHRASES ---- 

 ['https', ' https', ' https', ' http', ' https', ''] 

 TOTAL NOUN PHRASES --> 6 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['http', ':', '//twitter.com/infosi', 'http', ':', '//www.linkedin.com/company/infosi', 'http', ':', '//www.youtube.com/infosi', 'http', ':', '//www.slideshare.net/infosi', 'http', ':', '//www.infosys.com/']

 TOTAL PORTER STEM WORDS ==> 15



 ---- SNOWBALL STEMMING ----

['https', ':', '//twitter.com/infosi', 'https', ':', '//www.linkedin.com/company/infosi', 'https', ':', '//www.youtube.com/infosi', 'http', ':', '//www.slideshare.net/infosi', 'https', ':', '//www.infosys.com/']

 TOTAL SNOWBALL STEM WORDS ==> 15



 ---- LEMMATIZATION ----

['http', ':', '//twitter.com/infosys', 'http', ':', '//www.linkedin.com/company/infosys', 'http', ':', '//www.youtube.com/Infosys', 'http', ':', '//www.slideshare.net/Infosys', 'http', ':', '//www.infosys.com/']

 TOTAL LEMMATIZE WORDS ==> 15

************************************************************************************************************************

