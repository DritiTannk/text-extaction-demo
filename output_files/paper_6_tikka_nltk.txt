1 --> Proving the Art of the Possible  with Natural Language  Processing Research at the Dell Technologies HPC & AI Innovation Lab is  showcasing the art of the possible with deep learning for   language-to-language translation and text-to-voice translation. 


 ---- TOKENS ----

 ['Proving', 'the', 'Art', 'of', 'the', 'Possible', 'with', 'Natural', 'Language', 'Processing', 'Research', 'at', 'the', 'Dell', 'Technologies', 'HPC', '&', 'AI', 'Innovation', 'Lab', 'is', 'showcasing', 'the', 'art', 'of', 'the', 'possible', 'with', 'deep', 'learning', 'for', 'language-to-language', 'translation', 'and', 'text-to-voice', 'translation', '.'] 

 TOTAL TOKENS ==> 37

 ---- POST ----

 [('Proving', 'VBG'), ('the', 'DT'), ('Art', 'NN'), ('of', 'IN'), ('the', 'DT'), ('Possible', 'NNP'), ('with', 'IN'), ('Natural', 'NNP'), ('Language', 'NNP'), ('Processing', 'NNP'), ('Research', 'NNP'), ('at', 'IN'), ('the', 'DT'), ('Dell', 'NNP'), ('Technologies', 'NNPS'), ('HPC', 'NNP'), ('&', 'CC'), ('AI', 'NNP'), ('Innovation', 'NNP'), ('Lab', 'NNP'), ('is', 'VBZ'), ('showcasing', 'VBG'), ('the', 'DT'), ('art', 'NN'), ('of', 'IN'), ('the', 'DT'), ('possible', 'JJ'), ('with', 'IN'), ('deep', 'JJ'), ('learning', 'NN'), ('for', 'IN'), ('language-to-language', 'JJ'), ('translation', 'NN'), ('and', 'CC'), ('text-to-voice', 'JJ'), ('translation', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Proving', 'Art', 'Possible', 'Natural', 'Language', 'Processing', 'Research', 'Dell', 'Technologies', 'HPC', '&', 'AI', 'Innovation', 'Lab', 'showcasing', 'art', 'possible', 'deep', 'learning', 'language-to-language', 'translation', 'text-to-voice', 'translation', '.']

 TOTAL FILTERED TOKENS ==>  24

 ---- POST FOR FILTERED TOKENS ----

 [('Proving', 'VBG'), ('Art', 'NNP'), ('Possible', 'NNP'), ('Natural', 'NNP'), ('Language', 'NNP'), ('Processing', 'NNP'), ('Research', 'NNP'), ('Dell', 'NNP'), ('Technologies', 'NNP'), ('HPC', 'NNP'), ('&', 'CC'), ('AI', 'NNP'), ('Innovation', 'NNP'), ('Lab', 'NNP'), ('showcasing', 'VBG'), ('art', 'NN'), ('possible', 'JJ'), ('deep', 'JJ'), ('learning', 'VBG'), ('language-to-language', 'JJ'), ('translation', 'NN'), ('text-to-voice', 'JJ'), ('translation', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Proving Art', 'Art Possible', 'Possible Natural', 'Natural Language', 'Language Processing', 'Processing Research', 'Research Dell', 'Dell Technologies', 'Technologies HPC', 'HPC &', '& AI', 'AI Innovation', 'Innovation Lab', 'Lab showcasing', 'showcasing art', 'art possible', 'possible deep', 'deep learning', 'learning language-to-language', 'language-to-language translation', 'translation text-to-voice', 'text-to-voice translation', 'translation .'] 

 TOTAL BIGRAMS --> 23 



 ---- TRI-GRAMS ---- 

 ['Proving Art Possible', 'Art Possible Natural', 'Possible Natural Language', 'Natural Language Processing', 'Language Processing Research', 'Processing Research Dell', 'Research Dell Technologies', 'Dell Technologies HPC', 'Technologies HPC &', 'HPC & AI', '& AI Innovation', 'AI Innovation Lab', 'Innovation Lab showcasing', 'Lab showcasing art', 'showcasing art possible', 'art possible deep', 'possible deep learning', 'deep learning language-to-language', 'learning language-to-language translation', 'language-to-language translation text-to-voice', 'translation text-to-voice translation', 'text-to-voice translation .'] 

 TOTAL TRIGRAMS --> 22 



 ---- NOUN PHRASES ---- 

 ['art', 'language-to-language translation', 'text-to-voice translation'] 

 TOTAL NOUN PHRASES --> 3 



 ---- NER ----

 
 ORGANIZATION ---> ['AI Innovation']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> ['Art Possible Natural Language']
 TOTAL PERSON ENTITY --> 1 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['prove', 'art', 'possibl', 'natur', 'languag', 'process', 'research', 'dell', 'technolog', 'hpc', '&', 'ai', 'innov', 'lab', 'showcas', 'art', 'possibl', 'deep', 'learn', 'language-to-languag', 'translat', 'text-to-voic', 'translat', '.']

 TOTAL PORTER STEM WORDS ==> 24



 ---- SNOWBALL STEMMING ----

['prove', 'art', 'possibl', 'natur', 'languag', 'process', 'research', 'dell', 'technolog', 'hpc', '&', 'ai', 'innov', 'lab', 'showcas', 'art', 'possibl', 'deep', 'learn', 'language-to-languag', 'translat', 'text-to-voic', 'translat', '.']

 TOTAL SNOWBALL STEM WORDS ==> 24



 ---- LEMMATIZATION ----

['Proving', 'Art', 'Possible', 'Natural', 'Language', 'Processing', 'Research', 'Dell', 'Technologies', 'HPC', '&', 'AI', 'Innovation', 'Lab', 'showcasing', 'art', 'possible', 'deep', 'learning', 'language-to-language', 'translation', 'text-to-voice', 'translation', '.']

 TOTAL LEMMATIZE WORDS ==> 24

************************************************************************************************************************

2 --> ABSTRACT Dell Technologies has an active research program focused on helping organizations  explore, develop and adopt natural language processing applications. 


 ---- TOKENS ----

 ['ABSTRACT', 'Dell', 'Technologies', 'has', 'an', 'active', 'research', 'program', 'focused', 'on', 'helping', 'organizations', 'explore', ',', 'develop', 'and', 'adopt', 'natural', 'language', 'processing', 'applications', '.'] 

 TOTAL TOKENS ==> 22

 ---- POST ----

 [('ABSTRACT', 'NNP'), ('Dell', 'NNP'), ('Technologies', 'NNP'), ('has', 'VBZ'), ('an', 'DT'), ('active', 'JJ'), ('research', 'NN'), ('program', 'NN'), ('focused', 'VBD'), ('on', 'IN'), ('helping', 'VBG'), ('organizations', 'NNS'), ('explore', 'RB'), (',', ','), ('develop', 'VB'), ('and', 'CC'), ('adopt', 'VB'), ('natural', 'JJ'), ('language', 'NN'), ('processing', 'NN'), ('applications', 'NNS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['ABSTRACT', 'Dell', 'Technologies', 'active', 'research', 'program', 'focused', 'helping', 'organizations', 'explore', ',', 'develop', 'adopt', 'natural', 'language', 'processing', 'applications', '.']

 TOTAL FILTERED TOKENS ==>  18

 ---- POST FOR FILTERED TOKENS ----

 [('ABSTRACT', 'NNP'), ('Dell', 'NNP'), ('Technologies', 'NNPS'), ('active', 'JJ'), ('research', 'NN'), ('program', 'NN'), ('focused', 'VBD'), ('helping', 'VBG'), ('organizations', 'NNS'), ('explore', 'RB'), (',', ','), ('develop', 'VB'), ('adopt', 'JJ'), ('natural', 'JJ'), ('language', 'NN'), ('processing', 'NN'), ('applications', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['ABSTRACT Dell', 'Dell Technologies', 'Technologies active', 'active research', 'research program', 'program focused', 'focused helping', 'helping organizations', 'organizations explore', 'explore ,', ', develop', 'develop adopt', 'adopt natural', 'natural language', 'language processing', 'processing applications', 'applications .'] 

 TOTAL BIGRAMS --> 17 



 ---- TRI-GRAMS ---- 

 ['ABSTRACT Dell Technologies', 'Dell Technologies active', 'Technologies active research', 'active research program', 'research program focused', 'program focused helping', 'focused helping organizations', 'helping organizations explore', 'organizations explore ,', 'explore , develop', ', develop adopt', 'develop adopt natural', 'adopt natural language', 'natural language processing', 'language processing applications', 'processing applications .'] 

 TOTAL TRIGRAMS --> 16 



 ---- NOUN PHRASES ---- 

 ['active research', 'program', 'adopt natural language', 'processing'] 

 TOTAL NOUN PHRASES --> 4 



 ---- NER ----

 
 ORGANIZATION ---> ['ABSTRACT', 'Dell Technologies']
 TOTAL ORGANIZATION ENTITY --> 2 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['abstract', 'dell', 'technolog', 'activ', 'research', 'program', 'focus', 'help', 'organ', 'explor', ',', 'develop', 'adopt', 'natur', 'languag', 'process', 'applic', '.']

 TOTAL PORTER STEM WORDS ==> 18



 ---- SNOWBALL STEMMING ----

['abstract', 'dell', 'technolog', 'activ', 'research', 'program', 'focus', 'help', 'organ', 'explor', ',', 'develop', 'adopt', 'natur', 'languag', 'process', 'applic', '.']

 TOTAL SNOWBALL STEM WORDS ==> 18



 ---- LEMMATIZATION ----

['ABSTRACT', 'Dell', 'Technologies', 'active', 'research', 'program', 'focused', 'helping', 'organization', 'explore', ',', 'develop', 'adopt', 'natural', 'language', 'processing', 'application', '.']

 TOTAL LEMMATIZE WORDS ==> 18

************************************************************************************************************************

3 --> This research  is carried out by a data sciences team in the Dell Technologies HPC & AI Innovation  Lab in Austin, Texas. 


 ---- TOKENS ----

 ['This', 'research', 'is', 'carried', 'out', 'by', 'a', 'data', 'sciences', 'team', 'in', 'the', 'Dell', 'Technologies', 'HPC', '&', 'AI', 'Innovation', 'Lab', 'in', 'Austin', ',', 'Texas', '.'] 

 TOTAL TOKENS ==> 24

 ---- POST ----

 [('This', 'DT'), ('research', 'NN'), ('is', 'VBZ'), ('carried', 'VBN'), ('out', 'RP'), ('by', 'IN'), ('a', 'DT'), ('data', 'NN'), ('sciences', 'NNS'), ('team', 'NN'), ('in', 'IN'), ('the', 'DT'), ('Dell', 'NNP'), ('Technologies', 'NNPS'), ('HPC', 'NNP'), ('&', 'CC'), ('AI', 'NNP'), ('Innovation', 'NNP'), ('Lab', 'NNP'), ('in', 'IN'), ('Austin', 'NNP'), (',', ','), ('Texas', 'NNP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['research', 'carried', 'data', 'sciences', 'team', 'Dell', 'Technologies', 'HPC', '&', 'AI', 'Innovation', 'Lab', 'Austin', ',', 'Texas', '.']

 TOTAL FILTERED TOKENS ==>  16

 ---- POST FOR FILTERED TOKENS ----

 [('research', 'NN'), ('carried', 'VBD'), ('data', 'NNS'), ('sciences', 'NNS'), ('team', 'NN'), ('Dell', 'NNP'), ('Technologies', 'NNP'), ('HPC', 'NNP'), ('&', 'CC'), ('AI', 'NNP'), ('Innovation', 'NNP'), ('Lab', 'NNP'), ('Austin', 'NNP'), (',', ','), ('Texas', 'NNP'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['research carried', 'carried data', 'data sciences', 'sciences team', 'team Dell', 'Dell Technologies', 'Technologies HPC', 'HPC &', '& AI', 'AI Innovation', 'Innovation Lab', 'Lab Austin', 'Austin ,', ', Texas', 'Texas .'] 

 TOTAL BIGRAMS --> 15 



 ---- TRI-GRAMS ---- 

 ['research carried data', 'carried data sciences', 'data sciences team', 'sciences team Dell', 'team Dell Technologies', 'Dell Technologies HPC', 'Technologies HPC &', 'HPC & AI', '& AI Innovation', 'AI Innovation Lab', 'Innovation Lab Austin', 'Lab Austin ,', 'Austin , Texas', ', Texas .'] 

 TOTAL TRIGRAMS --> 14 



 ---- NOUN PHRASES ---- 

 ['research', 'team'] 

 TOTAL NOUN PHRASES --> 2 



 ---- NER ----

 
 ORGANIZATION ---> ['Dell Technologies', 'AI Innovation Lab Austin']
 TOTAL ORGANIZATION ENTITY --> 2 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> ['Texas']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['research', 'carri', 'data', 'scienc', 'team', 'dell', 'technolog', 'hpc', '&', 'ai', 'innov', 'lab', 'austin', ',', 'texa', '.']

 TOTAL PORTER STEM WORDS ==> 16



 ---- SNOWBALL STEMMING ----

['research', 'carri', 'data', 'scienc', 'team', 'dell', 'technolog', 'hpc', '&', 'ai', 'innov', 'lab', 'austin', ',', 'texa', '.']

 TOTAL SNOWBALL STEM WORDS ==> 16



 ---- LEMMATIZATION ----

['research', 'carried', 'data', 'science', 'team', 'Dell', 'Technologies', 'HPC', '&', 'AI', 'Innovation', 'Lab', 'Austin', ',', 'Texas', '.']

 TOTAL LEMMATIZE WORDS ==> 16

************************************************************************************************************************

4 --> This white paper explores two groundbreaking projects now  under way in the lab. 


 ---- TOKENS ----

 ['This', 'white', 'paper', 'explores', 'two', 'groundbreaking', 'projects', 'now', 'under', 'way', 'in', 'the', 'lab', '.'] 

 TOTAL TOKENS ==> 14

 ---- POST ----

 [('This', 'DT'), ('white', 'JJ'), ('paper', 'NN'), ('explores', 'VBZ'), ('two', 'CD'), ('groundbreaking', 'VBG'), ('projects', 'NNS'), ('now', 'RB'), ('under', 'IN'), ('way', 'NN'), ('in', 'IN'), ('the', 'DT'), ('lab', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['white', 'paper', 'explores', 'two', 'groundbreaking', 'projects', 'way', 'lab', '.']

 TOTAL FILTERED TOKENS ==>  9

 ---- POST FOR FILTERED TOKENS ----

 [('white', 'JJ'), ('paper', 'NN'), ('explores', 'VBZ'), ('two', 'CD'), ('groundbreaking', 'VBG'), ('projects', 'NNS'), ('way', 'NN'), ('lab', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['white paper', 'paper explores', 'explores two', 'two groundbreaking', 'groundbreaking projects', 'projects way', 'way lab', 'lab .'] 

 TOTAL BIGRAMS --> 8 



 ---- TRI-GRAMS ---- 

 ['white paper explores', 'paper explores two', 'explores two groundbreaking', 'two groundbreaking projects', 'groundbreaking projects way', 'projects way lab', 'way lab .'] 

 TOTAL TRIGRAMS --> 7 



 ---- NOUN PHRASES ---- 

 ['white paper', 'way', 'lab'] 

 TOTAL NOUN PHRASES --> 3 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['white', 'paper', 'explor', 'two', 'groundbreak', 'project', 'way', 'lab', '.']

 TOTAL PORTER STEM WORDS ==> 9



 ---- SNOWBALL STEMMING ----

['white', 'paper', 'explor', 'two', 'groundbreak', 'project', 'way', 'lab', '.']

 TOTAL SNOWBALL STEM WORDS ==> 9



 ---- LEMMATIZATION ----

['white', 'paper', 'explores', 'two', 'groundbreaking', 'project', 'way', 'lab', '.']

 TOTAL LEMMATIZE WORDS ==> 9

************************************************************************************************************************

5 --> One focuses on language-to-language translation and the  other focuses on text-to-voice translation. 


 ---- TOKENS ----

 ['One', 'focuses', 'on', 'language-to-language', 'translation', 'and', 'the', 'other', 'focuses', 'on', 'text-to-voice', 'translation', '.'] 

 TOTAL TOKENS ==> 13

 ---- POST ----

 [('One', 'CD'), ('focuses', 'VBZ'), ('on', 'IN'), ('language-to-language', 'JJ'), ('translation', 'NN'), ('and', 'CC'), ('the', 'DT'), ('other', 'JJ'), ('focuses', 'NNS'), ('on', 'IN'), ('text-to-voice', 'JJ'), ('translation', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['One', 'focuses', 'language-to-language', 'translation', 'focuses', 'text-to-voice', 'translation', '.']

 TOTAL FILTERED TOKENS ==>  8

 ---- POST FOR FILTERED TOKENS ----

 [('One', 'CD'), ('focuses', 'VBZ'), ('language-to-language', 'NN'), ('translation', 'NN'), ('focuses', 'VBZ'), ('text-to-voice', 'JJ'), ('translation', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['One focuses', 'focuses language-to-language', 'language-to-language translation', 'translation focuses', 'focuses text-to-voice', 'text-to-voice translation', 'translation .'] 

 TOTAL BIGRAMS --> 7 



 ---- TRI-GRAMS ---- 

 ['One focuses language-to-language', 'focuses language-to-language translation', 'language-to-language translation focuses', 'translation focuses text-to-voice', 'focuses text-to-voice translation', 'text-to-voice translation .'] 

 TOTAL TRIGRAMS --> 6 



 ---- NOUN PHRASES ---- 

 ['language-to-language', 'translation', 'text-to-voice translation'] 

 TOTAL NOUN PHRASES --> 3 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['one', 'focus', 'language-to-languag', 'translat', 'focus', 'text-to-voic', 'translat', '.']

 TOTAL PORTER STEM WORDS ==> 8



 ---- SNOWBALL STEMMING ----

['one', 'focus', 'language-to-languag', 'translat', 'focus', 'text-to-voic', 'translat', '.']

 TOTAL SNOWBALL STEM WORDS ==> 8



 ---- LEMMATIZATION ----

['One', 'focus', 'language-to-language', 'translation', 'focus', 'text-to-voice', 'translation', '.']

 TOTAL LEMMATIZE WORDS ==> 8

************************************************************************************************************************

6 --> September 2020 DELL TECHNOLOGIES WHITE PAPER  DELL TECHNOLOGIES WHITE PAPER  The information in this publication is provided “as is.” Dell Inc. makes no representations or warranties of any kind with respect to the  information in this publication, and specifically disclaims implied warranties of merchantability or fitness for a particular purpose. 


 ---- TOKENS ----

 ['September', '2020', 'DELL', 'TECHNOLOGIES', 'WHITE', 'PAPER', 'DELL', 'TECHNOLOGIES', 'WHITE', 'PAPER', 'The', 'information', 'in', 'this', 'publication', 'is', 'provided', '“', 'as', 'is.', '”', 'Dell', 'Inc.', 'makes', 'no', 'representations', 'or', 'warranties', 'of', 'any', 'kind', 'with', 'respect', 'to', 'the', 'information', 'in', 'this', 'publication', ',', 'and', 'specifically', 'disclaims', 'implied', 'warranties', 'of', 'merchantability', 'or', 'fitness', 'for', 'a', 'particular', 'purpose', '.'] 

 TOTAL TOKENS ==> 54

 ---- POST ----

 [('September', 'NNP'), ('2020', 'CD'), ('DELL', 'NNP'), ('TECHNOLOGIES', 'NNP'), ('WHITE', 'NNP'), ('PAPER', 'NNP'), ('DELL', 'NNP'), ('TECHNOLOGIES', 'NNP'), ('WHITE', 'NNP'), ('PAPER', 'NNP'), ('The', 'DT'), ('information', 'NN'), ('in', 'IN'), ('this', 'DT'), ('publication', 'NN'), ('is', 'VBZ'), ('provided', 'VBN'), ('“', 'RB'), ('as', 'IN'), ('is.', 'JJ'), ('”', 'NNP'), ('Dell', 'NNP'), ('Inc.', 'NNP'), ('makes', 'VBZ'), ('no', 'DT'), ('representations', 'NNS'), ('or', 'CC'), ('warranties', 'NNS'), ('of', 'IN'), ('any', 'DT'), ('kind', 'NN'), ('with', 'IN'), ('respect', 'NN'), ('to', 'TO'), ('the', 'DT'), ('information', 'NN'), ('in', 'IN'), ('this', 'DT'), ('publication', 'NN'), (',', ','), ('and', 'CC'), ('specifically', 'RB'), ('disclaims', 'NNS'), ('implied', 'VBD'), ('warranties', 'NNS'), ('of', 'IN'), ('merchantability', 'NN'), ('or', 'CC'), ('fitness', 'NN'), ('for', 'IN'), ('a', 'DT'), ('particular', 'JJ'), ('purpose', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['September', '2020', 'DELL', 'TECHNOLOGIES', 'WHITE', 'PAPER', 'DELL', 'TECHNOLOGIES', 'WHITE', 'PAPER', 'information', 'publication', 'provided', '“', 'is.', '”', 'Dell', 'Inc.', 'makes', 'representations', 'warranties', 'kind', 'respect', 'information', 'publication', ',', 'specifically', 'disclaims', 'implied', 'warranties', 'merchantability', 'fitness', 'particular', 'purpose', '.']

 TOTAL FILTERED TOKENS ==>  35

 ---- POST FOR FILTERED TOKENS ----

 [('September', 'NNP'), ('2020', 'CD'), ('DELL', 'NNP'), ('TECHNOLOGIES', 'NNP'), ('WHITE', 'NNP'), ('PAPER', 'NNP'), ('DELL', 'NNP'), ('TECHNOLOGIES', 'NNP'), ('WHITE', 'NNP'), ('PAPER', 'NNP'), ('information', 'NN'), ('publication', 'NN'), ('provided', 'VBD'), ('“', 'NNP'), ('is.', 'JJ'), ('”', 'NNP'), ('Dell', 'NNP'), ('Inc.', 'NNP'), ('makes', 'VBZ'), ('representations', 'NNS'), ('warranties', 'NNS'), ('kind', 'NN'), ('respect', 'NN'), ('information', 'NN'), ('publication', 'NN'), (',', ','), ('specifically', 'RB'), ('disclaims', 'VBZ'), ('implied', 'JJ'), ('warranties', 'NNS'), ('merchantability', 'NN'), ('fitness', 'NN'), ('particular', 'JJ'), ('purpose', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['September 2020', '2020 DELL', 'DELL TECHNOLOGIES', 'TECHNOLOGIES WHITE', 'WHITE PAPER', 'PAPER DELL', 'DELL TECHNOLOGIES', 'TECHNOLOGIES WHITE', 'WHITE PAPER', 'PAPER information', 'information publication', 'publication provided', 'provided “', '“ is.', 'is. ”', '” Dell', 'Dell Inc.', 'Inc. makes', 'makes representations', 'representations warranties', 'warranties kind', 'kind respect', 'respect information', 'information publication', 'publication ,', ', specifically', 'specifically disclaims', 'disclaims implied', 'implied warranties', 'warranties merchantability', 'merchantability fitness', 'fitness particular', 'particular purpose', 'purpose .'] 

 TOTAL BIGRAMS --> 34 



 ---- TRI-GRAMS ---- 

 ['September 2020 DELL', '2020 DELL TECHNOLOGIES', 'DELL TECHNOLOGIES WHITE', 'TECHNOLOGIES WHITE PAPER', 'WHITE PAPER DELL', 'PAPER DELL TECHNOLOGIES', 'DELL TECHNOLOGIES WHITE', 'TECHNOLOGIES WHITE PAPER', 'WHITE PAPER information', 'PAPER information publication', 'information publication provided', 'publication provided “', 'provided “ is.', '“ is. ”', 'is. ” Dell', '” Dell Inc.', 'Dell Inc. makes', 'Inc. makes representations', 'makes representations warranties', 'representations warranties kind', 'warranties kind respect', 'kind respect information', 'respect information publication', 'information publication ,', 'publication , specifically', ', specifically disclaims', 'specifically disclaims implied', 'disclaims implied warranties', 'implied warranties merchantability', 'warranties merchantability fitness', 'merchantability fitness particular', 'fitness particular purpose', 'particular purpose .'] 

 TOTAL TRIGRAMS --> 33 



 ---- NOUN PHRASES ---- 

 ['information', 'publication', 'kind', 'respect', 'information', 'publication', 'merchantability', 'fitness', 'particular purpose'] 

 TOTAL NOUN PHRASES --> 9 



 ---- NER ----

 
 ORGANIZATION ---> ['DELL', 'WHITE', 'DELL', 'WHITE', 'Dell Inc.']
 TOTAL ORGANIZATION ENTITY --> 5 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['septemb', '2020', 'dell', 'technolog', 'white', 'paper', 'dell', 'technolog', 'white', 'paper', 'inform', 'public', 'provid', '“', 'is.', '”', 'dell', 'inc.', 'make', 'represent', 'warranti', 'kind', 'respect', 'inform', 'public', ',', 'specif', 'disclaim', 'impli', 'warranti', 'merchant', 'fit', 'particular', 'purpos', '.']

 TOTAL PORTER STEM WORDS ==> 35



 ---- SNOWBALL STEMMING ----

['septemb', '2020', 'dell', 'technolog', 'white', 'paper', 'dell', 'technolog', 'white', 'paper', 'inform', 'public', 'provid', '“', 'is.', '”', 'dell', 'inc.', 'make', 'represent', 'warranti', 'kind', 'respect', 'inform', 'public', ',', 'specif', 'disclaim', 'impli', 'warranti', 'merchant', 'fit', 'particular', 'purpos', '.']

 TOTAL SNOWBALL STEM WORDS ==> 35



 ---- LEMMATIZATION ----

['September', '2020', 'DELL', 'TECHNOLOGIES', 'WHITE', 'PAPER', 'DELL', 'TECHNOLOGIES', 'WHITE', 'PAPER', 'information', 'publication', 'provided', '“', 'is.', '”', 'Dell', 'Inc.', 'make', 'representation', 'warranty', 'kind', 'respect', 'information', 'publication', ',', 'specifically', 'disclaims', 'implied', 'warranty', 'merchantability', 'fitness', 'particular', 'purpose', '.']

 TOTAL LEMMATIZE WORDS ==> 35

************************************************************************************************************************

7 --> Use, copying and distribution of any software described in this publication require an applicable software license. 


 ---- TOKENS ----

 ['Use', ',', 'copying', 'and', 'distribution', 'of', 'any', 'software', 'described', 'in', 'this', 'publication', 'require', 'an', 'applicable', 'software', 'license', '.'] 

 TOTAL TOKENS ==> 18

 ---- POST ----

 [('Use', 'NNP'), (',', ','), ('copying', 'NN'), ('and', 'CC'), ('distribution', 'NN'), ('of', 'IN'), ('any', 'DT'), ('software', 'NN'), ('described', 'VBN'), ('in', 'IN'), ('this', 'DT'), ('publication', 'NN'), ('require', 'VB'), ('an', 'DT'), ('applicable', 'JJ'), ('software', 'NN'), ('license', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Use', ',', 'copying', 'distribution', 'software', 'described', 'publication', 'require', 'applicable', 'software', 'license', '.']

 TOTAL FILTERED TOKENS ==>  12

 ---- POST FOR FILTERED TOKENS ----

 [('Use', 'NNP'), (',', ','), ('copying', 'VBG'), ('distribution', 'NN'), ('software', 'NN'), ('described', 'VBN'), ('publication', 'NN'), ('require', 'NN'), ('applicable', 'JJ'), ('software', 'NN'), ('license', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Use ,', ', copying', 'copying distribution', 'distribution software', 'software described', 'described publication', 'publication require', 'require applicable', 'applicable software', 'software license', 'license .'] 

 TOTAL BIGRAMS --> 11 



 ---- TRI-GRAMS ---- 

 ['Use , copying', ', copying distribution', 'copying distribution software', 'distribution software described', 'software described publication', 'described publication require', 'publication require applicable', 'require applicable software', 'applicable software license', 'software license .'] 

 TOTAL TRIGRAMS --> 10 



 ---- NOUN PHRASES ---- 

 ['distribution', 'software', 'publication', 'require', 'applicable software', 'license'] 

 TOTAL NOUN PHRASES --> 6 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> ['Use']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['use', ',', 'copi', 'distribut', 'softwar', 'describ', 'public', 'requir', 'applic', 'softwar', 'licens', '.']

 TOTAL PORTER STEM WORDS ==> 12



 ---- SNOWBALL STEMMING ----

['use', ',', 'copi', 'distribut', 'softwar', 'describ', 'public', 'requir', 'applic', 'softwar', 'licens', '.']

 TOTAL SNOWBALL STEM WORDS ==> 12



 ---- LEMMATIZATION ----

['Use', ',', 'copying', 'distribution', 'software', 'described', 'publication', 'require', 'applicable', 'software', 'license', '.']

 TOTAL LEMMATIZE WORDS ==> 12

************************************************************************************************************************

8 --> Copyright © 2020 Dell Inc. or its subsidiaries. 


 ---- TOKENS ----

 ['Copyright', '©', '2020', 'Dell', 'Inc.', 'or', 'its', 'subsidiaries', '.'] 

 TOTAL TOKENS ==> 9

 ---- POST ----

 [('Copyright', 'NNP'), ('©', 'NN'), ('2020', 'CD'), ('Dell', 'NNP'), ('Inc.', 'NNP'), ('or', 'CC'), ('its', 'PRP$'), ('subsidiaries', 'NNS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Copyright', '©', '2020', 'Dell', 'Inc.', 'subsidiaries', '.']

 TOTAL FILTERED TOKENS ==>  7

 ---- POST FOR FILTERED TOKENS ----

 [('Copyright', 'NNP'), ('©', 'NN'), ('2020', 'CD'), ('Dell', 'NNP'), ('Inc.', 'NNP'), ('subsidiaries', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Copyright ©', '© 2020', '2020 Dell', 'Dell Inc.', 'Inc. subsidiaries', 'subsidiaries .'] 

 TOTAL BIGRAMS --> 6 



 ---- TRI-GRAMS ---- 

 ['Copyright © 2020', '© 2020 Dell', '2020 Dell Inc.', 'Dell Inc. subsidiaries', 'Inc. subsidiaries .'] 

 TOTAL TRIGRAMS --> 5 



 ---- NOUN PHRASES ---- 

 ['©'] 

 TOTAL NOUN PHRASES --> 1 



 ---- NER ----

 
 ORGANIZATION ---> ['Dell Inc.']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> ['Copyright']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['copyright', '©', '2020', 'dell', 'inc.', 'subsidiari', '.']

 TOTAL PORTER STEM WORDS ==> 7



 ---- SNOWBALL STEMMING ----

['copyright', '©', '2020', 'dell', 'inc.', 'subsidiari', '.']

 TOTAL SNOWBALL STEM WORDS ==> 7



 ---- LEMMATIZATION ----

['Copyright', '©', '2020', 'Dell', 'Inc.', 'subsidiary', '.']

 TOTAL LEMMATIZE WORDS ==> 7

************************************************************************************************************************

9 --> All Rights Reserved. 


 ---- TOKENS ----

 ['All', 'Rights', 'Reserved', '.'] 

 TOTAL TOKENS ==> 4

 ---- POST ----

 [('All', 'DT'), ('Rights', 'NNPS'), ('Reserved', 'NNP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Rights', 'Reserved', '.']

 TOTAL FILTERED TOKENS ==>  3

 ---- POST FOR FILTERED TOKENS ----

 [('Rights', 'NNS'), ('Reserved', 'VBN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Rights Reserved', 'Reserved .'] 

 TOTAL BIGRAMS --> 2 



 ---- TRI-GRAMS ---- 

 ['Rights Reserved .'] 

 TOTAL TRIGRAMS --> 1 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['right', 'reserv', '.']

 TOTAL PORTER STEM WORDS ==> 3



 ---- SNOWBALL STEMMING ----

['right', 'reserv', '.']

 TOTAL SNOWBALL STEM WORDS ==> 3



 ---- LEMMATIZATION ----

['Rights', 'Reserved', '.']

 TOTAL LEMMATIZE WORDS ==> 3

************************************************************************************************************************

10 --> Dell, Dell Technologies, EMC and other trademarks are  trademarks of Dell Inc. or its subsidiaries. 


 ---- TOKENS ----

 ['Dell', ',', 'Dell', 'Technologies', ',', 'EMC', 'and', 'other', 'trademarks', 'are', 'trademarks', 'of', 'Dell', 'Inc.', 'or', 'its', 'subsidiaries', '.'] 

 TOTAL TOKENS ==> 18

 ---- POST ----

 [('Dell', 'NNP'), (',', ','), ('Dell', 'NNP'), ('Technologies', 'NNPS'), (',', ','), ('EMC', 'NNP'), ('and', 'CC'), ('other', 'JJ'), ('trademarks', 'NNS'), ('are', 'VBP'), ('trademarks', 'NNS'), ('of', 'IN'), ('Dell', 'NNP'), ('Inc.', 'NNP'), ('or', 'CC'), ('its', 'PRP$'), ('subsidiaries', 'NNS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Dell', ',', 'Dell', 'Technologies', ',', 'EMC', 'trademarks', 'trademarks', 'Dell', 'Inc.', 'subsidiaries', '.']

 TOTAL FILTERED TOKENS ==>  12

 ---- POST FOR FILTERED TOKENS ----

 [('Dell', 'NNP'), (',', ','), ('Dell', 'NNP'), ('Technologies', 'NNPS'), (',', ','), ('EMC', 'NNP'), ('trademarks', 'VBZ'), ('trademarks', 'NNS'), ('Dell', 'NNP'), ('Inc.', 'NNP'), ('subsidiaries', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Dell ,', ', Dell', 'Dell Technologies', 'Technologies ,', ', EMC', 'EMC trademarks', 'trademarks trademarks', 'trademarks Dell', 'Dell Inc.', 'Inc. subsidiaries', 'subsidiaries .'] 

 TOTAL BIGRAMS --> 11 



 ---- TRI-GRAMS ---- 

 ['Dell , Dell', ', Dell Technologies', 'Dell Technologies ,', 'Technologies , EMC', ', EMC trademarks', 'EMC trademarks trademarks', 'trademarks trademarks Dell', 'trademarks Dell Inc.', 'Dell Inc. subsidiaries', 'Inc. subsidiaries .'] 

 TOTAL TRIGRAMS --> 10 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> ['Dell Technologies', 'EMC', 'Dell Inc.']
 TOTAL ORGANIZATION ENTITY --> 3 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> ['Dell']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['dell', ',', 'dell', 'technolog', ',', 'emc', 'trademark', 'trademark', 'dell', 'inc.', 'subsidiari', '.']

 TOTAL PORTER STEM WORDS ==> 12



 ---- SNOWBALL STEMMING ----

['dell', ',', 'dell', 'technolog', ',', 'emc', 'trademark', 'trademark', 'dell', 'inc.', 'subsidiari', '.']

 TOTAL SNOWBALL STEM WORDS ==> 12



 ---- LEMMATIZATION ----

['Dell', ',', 'Dell', 'Technologies', ',', 'EMC', 'trademark', 'trademark', 'Dell', 'Inc.', 'subsidiary', '.']

 TOTAL LEMMATIZE WORDS ==> 12

************************************************************************************************************************

11 --> Other trademarks may be the property of their respective owners. 


 ---- TOKENS ----

 ['Other', 'trademarks', 'may', 'be', 'the', 'property', 'of', 'their', 'respective', 'owners', '.'] 

 TOTAL TOKENS ==> 11

 ---- POST ----

 [('Other', 'JJ'), ('trademarks', 'NNS'), ('may', 'MD'), ('be', 'VB'), ('the', 'DT'), ('property', 'NN'), ('of', 'IN'), ('their', 'PRP$'), ('respective', 'JJ'), ('owners', 'NNS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['trademarks', 'may', 'property', 'respective', 'owners', '.']

 TOTAL FILTERED TOKENS ==>  6

 ---- POST FOR FILTERED TOKENS ----

 [('trademarks', 'NNS'), ('may', 'MD'), ('property', 'NN'), ('respective', 'NN'), ('owners', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['trademarks may', 'may property', 'property respective', 'respective owners', 'owners .'] 

 TOTAL BIGRAMS --> 5 



 ---- TRI-GRAMS ---- 

 ['trademarks may property', 'may property respective', 'property respective owners', 'respective owners .'] 

 TOTAL TRIGRAMS --> 4 



 ---- NOUN PHRASES ---- 

 ['property', 'respective'] 

 TOTAL NOUN PHRASES --> 2 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['trademark', 'may', 'properti', 'respect', 'owner', '.']

 TOTAL PORTER STEM WORDS ==> 6



 ---- SNOWBALL STEMMING ----

['trademark', 'may', 'properti', 'respect', 'owner', '.']

 TOTAL SNOWBALL STEM WORDS ==> 6



 ---- LEMMATIZATION ----

['trademark', 'may', 'property', 'respective', 'owner', '.']

 TOTAL LEMMATIZE WORDS ==> 6

************************************************************************************************************************

12 --> Dell Technologies believes the information in this document is accurate as of its publication date. 


 ---- TOKENS ----

 ['Dell', 'Technologies', 'believes', 'the', 'information', 'in', 'this', 'document', 'is', 'accurate', 'as', 'of', 'its', 'publication', 'date', '.'] 

 TOTAL TOKENS ==> 16

 ---- POST ----

 [('Dell', 'NNP'), ('Technologies', 'NNPS'), ('believes', 'VBZ'), ('the', 'DT'), ('information', 'NN'), ('in', 'IN'), ('this', 'DT'), ('document', 'NN'), ('is', 'VBZ'), ('accurate', 'JJ'), ('as', 'IN'), ('of', 'IN'), ('its', 'PRP$'), ('publication', 'NN'), ('date', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Dell', 'Technologies', 'believes', 'information', 'document', 'accurate', 'publication', 'date', '.']

 TOTAL FILTERED TOKENS ==>  9

 ---- POST FOR FILTERED TOKENS ----

 [('Dell', 'NNP'), ('Technologies', 'NNPS'), ('believes', 'VBZ'), ('information', 'NN'), ('document', 'NN'), ('accurate', 'NN'), ('publication', 'NN'), ('date', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Dell Technologies', 'Technologies believes', 'believes information', 'information document', 'document accurate', 'accurate publication', 'publication date', 'date .'] 

 TOTAL BIGRAMS --> 8 



 ---- TRI-GRAMS ---- 

 ['Dell Technologies believes', 'Technologies believes information', 'believes information document', 'information document accurate', 'document accurate publication', 'accurate publication date', 'publication date .'] 

 TOTAL TRIGRAMS --> 7 



 ---- NOUN PHRASES ---- 

 ['information', 'document', 'accurate', 'publication', 'date'] 

 TOTAL NOUN PHRASES --> 5 



 ---- NER ----

 
 ORGANIZATION ---> ['Dell Technologies']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['dell', 'technolog', 'believ', 'inform', 'document', 'accur', 'public', 'date', '.']

 TOTAL PORTER STEM WORDS ==> 9



 ---- SNOWBALL STEMMING ----

['dell', 'technolog', 'believ', 'inform', 'document', 'accur', 'public', 'date', '.']

 TOTAL SNOWBALL STEM WORDS ==> 9



 ---- LEMMATIZATION ----

['Dell', 'Technologies', 'belief', 'information', 'document', 'accurate', 'publication', 'date', '.']

 TOTAL LEMMATIZE WORDS ==> 9

************************************************************************************************************************

13 --> The information is subject to  change without notice. 


 ---- TOKENS ----

 ['The', 'information', 'is', 'subject', 'to', 'change', 'without', 'notice', '.'] 

 TOTAL TOKENS ==> 9

 ---- POST ----

 [('The', 'DT'), ('information', 'NN'), ('is', 'VBZ'), ('subject', 'JJ'), ('to', 'TO'), ('change', 'VB'), ('without', 'IN'), ('notice', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['information', 'subject', 'change', 'without', 'notice', '.']

 TOTAL FILTERED TOKENS ==>  6

 ---- POST FOR FILTERED TOKENS ----

 [('information', 'NN'), ('subject', 'JJ'), ('change', 'NN'), ('without', 'IN'), ('notice', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['information subject', 'subject change', 'change without', 'without notice', 'notice .'] 

 TOTAL BIGRAMS --> 5 



 ---- TRI-GRAMS ---- 

 ['information subject change', 'subject change without', 'change without notice', 'without notice .'] 

 TOTAL TRIGRAMS --> 4 



 ---- NOUN PHRASES ---- 

 ['information', 'subject change', 'notice'] 

 TOTAL NOUN PHRASES --> 3 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['inform', 'subject', 'chang', 'without', 'notic', '.']

 TOTAL PORTER STEM WORDS ==> 6



 ---- SNOWBALL STEMMING ----

['inform', 'subject', 'chang', 'without', 'notic', '.']

 TOTAL SNOWBALL STEM WORDS ==> 6



 ---- LEMMATIZATION ----

['information', 'subject', 'change', 'without', 'notice', '.']

 TOTAL LEMMATIZE WORDS ==> 6

************************************************************************************************************************

14 --> Published in the USA 09/20. 


 ---- TOKENS ----

 ['Published', 'in', 'the', 'USA', '09/20', '.'] 

 TOTAL TOKENS ==> 6

 ---- POST ----

 [('Published', 'VBN'), ('in', 'IN'), ('the', 'DT'), ('USA', 'NNP'), ('09/20', 'CD'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Published', 'USA', '09/20', '.']

 TOTAL FILTERED TOKENS ==>  4

 ---- POST FOR FILTERED TOKENS ----

 [('Published', 'VBN'), ('USA', 'NNP'), ('09/20', 'CD'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Published USA', 'USA 09/20', '09/20 .'] 

 TOTAL BIGRAMS --> 3 



 ---- TRI-GRAMS ---- 

 ['Published USA 09/20', 'USA 09/20 .'] 

 TOTAL TRIGRAMS --> 2 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> ['USA']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['publish', 'usa', '09/20', '.']

 TOTAL PORTER STEM WORDS ==> 4



 ---- SNOWBALL STEMMING ----

['publish', 'usa', '09/20', '.']

 TOTAL SNOWBALL STEM WORDS ==> 4



 ---- LEMMATIZATION ----

['Published', 'USA', '09/20', '.']

 TOTAL LEMMATIZE WORDS ==> 4

************************************************************************************************************************

15 --> TABLE OF CONTENTS NATURAL LANGUAGE PROCESSING                                  1 LANGUAGE-TO-LANGUAGE TRANSLATION                           1  Computing resources . 


 ---- TOKENS ----

 ['TABLE', 'OF', 'CONTENTS', 'NATURAL', 'LANGUAGE', 'PROCESSING', '1', 'LANGUAGE-TO-LANGUAGE', 'TRANSLATION', '1', 'Computing', 'resources', '.'] 

 TOTAL TOKENS ==> 13

 ---- POST ----

 [('TABLE', 'NN'), ('OF', 'IN'), ('CONTENTS', 'NNP'), ('NATURAL', 'NNP'), ('LANGUAGE', 'NNP'), ('PROCESSING', 'VBD'), ('1', 'CD'), ('LANGUAGE-TO-LANGUAGE', 'JJ'), ('TRANSLATION', 'NNP'), ('1', 'CD'), ('Computing', 'NNP'), ('resources', 'NNS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['TABLE', 'CONTENTS', 'NATURAL', 'LANGUAGE', 'PROCESSING', '1', 'LANGUAGE-TO-LANGUAGE', 'TRANSLATION', '1', 'Computing', 'resources', '.']

 TOTAL FILTERED TOKENS ==>  12

 ---- POST FOR FILTERED TOKENS ----

 [('TABLE', 'NN'), ('CONTENTS', 'NNP'), ('NATURAL', 'NNP'), ('LANGUAGE', 'NNP'), ('PROCESSING', 'VBD'), ('1', 'CD'), ('LANGUAGE-TO-LANGUAGE', 'JJ'), ('TRANSLATION', 'NNP'), ('1', 'CD'), ('Computing', 'NNP'), ('resources', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['TABLE CONTENTS', 'CONTENTS NATURAL', 'NATURAL LANGUAGE', 'LANGUAGE PROCESSING', 'PROCESSING 1', '1 LANGUAGE-TO-LANGUAGE', 'LANGUAGE-TO-LANGUAGE TRANSLATION', 'TRANSLATION 1', '1 Computing', 'Computing resources', 'resources .'] 

 TOTAL BIGRAMS --> 11 



 ---- TRI-GRAMS ---- 

 ['TABLE CONTENTS NATURAL', 'CONTENTS NATURAL LANGUAGE', 'NATURAL LANGUAGE PROCESSING', 'LANGUAGE PROCESSING 1', 'PROCESSING 1 LANGUAGE-TO-LANGUAGE', '1 LANGUAGE-TO-LANGUAGE TRANSLATION', 'LANGUAGE-TO-LANGUAGE TRANSLATION 1', 'TRANSLATION 1 Computing', '1 Computing resources', 'Computing resources .'] 

 TOTAL TRIGRAMS --> 10 



 ---- NOUN PHRASES ---- 

 ['TABLE'] 

 TOTAL NOUN PHRASES --> 1 



 ---- NER ----

 
 ORGANIZATION ---> ['TABLE', 'CONTENTS']
 TOTAL ORGANIZATION ENTITY --> 2 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['tabl', 'content', 'natur', 'languag', 'process', '1', 'language-to-languag', 'translat', '1', 'comput', 'resourc', '.']

 TOTAL PORTER STEM WORDS ==> 12



 ---- SNOWBALL STEMMING ----

['tabl', 'content', 'natur', 'languag', 'process', '1', 'language-to-languag', 'translat', '1', 'comput', 'resourc', '.']

 TOTAL SNOWBALL STEM WORDS ==> 12



 ---- LEMMATIZATION ----

['TABLE', 'CONTENTS', 'NATURAL', 'LANGUAGE', 'PROCESSING', '1', 'LANGUAGE-TO-LANGUAGE', 'TRANSLATION', '1', 'Computing', 'resource', '.']

 TOTAL LEMMATIZE WORDS ==> 12

************************************************************************************************************************

16 --> . 


 ---- TOKENS ----

 ['.'] 

 TOTAL TOKENS ==> 1

 ---- POST ----

 [('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['.']

 TOTAL FILTERED TOKENS ==>  1

 ---- POST FOR FILTERED TOKENS ----

 [('.', '.')] 



 ---- BI-GRAMS ---- 

 [] 

 TOTAL BIGRAMS --> 0 



 ---- TRI-GRAMS ---- 

 [] 

 TOTAL TRIGRAMS --> 0 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['.']

 TOTAL PORTER STEM WORDS ==> 1



 ---- SNOWBALL STEMMING ----

['.']

 TOTAL SNOWBALL STEM WORDS ==> 1



 ---- LEMMATIZATION ----

['.']

 TOTAL LEMMATIZE WORDS ==> 1

************************************************************************************************************************

17 --> . 


 ---- TOKENS ----

 ['.'] 

 TOTAL TOKENS ==> 1

 ---- POST ----

 [('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['.']

 TOTAL FILTERED TOKENS ==>  1

 ---- POST FOR FILTERED TOKENS ----

 [('.', '.')] 



 ---- BI-GRAMS ---- 

 [] 

 TOTAL BIGRAMS --> 0 



 ---- TRI-GRAMS ---- 

 [] 

 TOTAL TRIGRAMS --> 0 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['.']

 TOTAL PORTER STEM WORDS ==> 1



 ---- SNOWBALL STEMMING ----

['.']

 TOTAL SNOWBALL STEM WORDS ==> 1



 ---- LEMMATIZATION ----

['.']

 TOTAL LEMMATIZE WORDS ==> 1

************************************************************************************************************************

18 --> . 


 ---- TOKENS ----

 ['.'] 

 TOTAL TOKENS ==> 1

 ---- POST ----

 [('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['.']

 TOTAL FILTERED TOKENS ==>  1

 ---- POST FOR FILTERED TOKENS ----

 [('.', '.')] 



 ---- BI-GRAMS ---- 

 [] 

 TOTAL BIGRAMS --> 0 



 ---- TRI-GRAMS ---- 

 [] 

 TOTAL TRIGRAMS --> 0 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['.']

 TOTAL PORTER STEM WORDS ==> 1



 ---- SNOWBALL STEMMING ----

['.']

 TOTAL SNOWBALL STEM WORDS ==> 1



 ---- LEMMATIZATION ----

['.']

 TOTAL LEMMATIZE WORDS ==> 1

************************************************************************************************************************

19 --> . 


 ---- TOKENS ----

 ['.'] 

 TOTAL TOKENS ==> 1

 ---- POST ----

 [('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['.']

 TOTAL FILTERED TOKENS ==>  1

 ---- POST FOR FILTERED TOKENS ----

 [('.', '.')] 



 ---- BI-GRAMS ---- 

 [] 

 TOTAL BIGRAMS --> 0 



 ---- TRI-GRAMS ---- 

 [] 

 TOTAL TRIGRAMS --> 0 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['.']

 TOTAL PORTER STEM WORDS ==> 1



 ---- SNOWBALL STEMMING ----

['.']

 TOTAL SNOWBALL STEM WORDS ==> 1



 ---- LEMMATIZATION ----

['.']

 TOTAL LEMMATIZE WORDS ==> 1

************************************************************************************************************************

20 --> . 


 ---- TOKENS ----

 ['.'] 

 TOTAL TOKENS ==> 1

 ---- POST ----

 [('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['.']

 TOTAL FILTERED TOKENS ==>  1

 ---- POST FOR FILTERED TOKENS ----

 [('.', '.')] 



 ---- BI-GRAMS ---- 

 [] 

 TOTAL BIGRAMS --> 0 



 ---- TRI-GRAMS ---- 

 [] 

 TOTAL TRIGRAMS --> 0 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['.']

 TOTAL PORTER STEM WORDS ==> 1



 ---- SNOWBALL STEMMING ----

['.']

 TOTAL SNOWBALL STEM WORDS ==> 1



 ---- LEMMATIZATION ----

['.']

 TOTAL LEMMATIZE WORDS ==> 1

************************************************************************************************************************

21 --> . 


 ---- TOKENS ----

 ['.'] 

 TOTAL TOKENS ==> 1

 ---- POST ----

 [('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['.']

 TOTAL FILTERED TOKENS ==>  1

 ---- POST FOR FILTERED TOKENS ----

 [('.', '.')] 



 ---- BI-GRAMS ---- 

 [] 

 TOTAL BIGRAMS --> 0 



 ---- TRI-GRAMS ---- 

 [] 

 TOTAL TRIGRAMS --> 0 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['.']

 TOTAL PORTER STEM WORDS ==> 1



 ---- SNOWBALL STEMMING ----

['.']

 TOTAL SNOWBALL STEM WORDS ==> 1



 ---- LEMMATIZATION ----

['.']

 TOTAL LEMMATIZE WORDS ==> 1

************************************************************************************************************************

22 --> . 


 ---- TOKENS ----

 ['.'] 

 TOTAL TOKENS ==> 1

 ---- POST ----

 [('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['.']

 TOTAL FILTERED TOKENS ==>  1

 ---- POST FOR FILTERED TOKENS ----

 [('.', '.')] 



 ---- BI-GRAMS ---- 

 [] 

 TOTAL BIGRAMS --> 0 



 ---- TRI-GRAMS ---- 

 [] 

 TOTAL TRIGRAMS --> 0 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['.']

 TOTAL PORTER STEM WORDS ==> 1



 ---- SNOWBALL STEMMING ----

['.']

 TOTAL SNOWBALL STEM WORDS ==> 1



 ---- LEMMATIZATION ----

['.']

 TOTAL LEMMATIZE WORDS ==> 1

************************************************************************************************************************

23 --> . 


 ---- TOKENS ----

 ['.'] 

 TOTAL TOKENS ==> 1

 ---- POST ----

 [('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['.']

 TOTAL FILTERED TOKENS ==>  1

 ---- POST FOR FILTERED TOKENS ----

 [('.', '.')] 



 ---- BI-GRAMS ---- 

 [] 

 TOTAL BIGRAMS --> 0 



 ---- TRI-GRAMS ---- 

 [] 

 TOTAL TRIGRAMS --> 0 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['.']

 TOTAL PORTER STEM WORDS ==> 1



 ---- SNOWBALL STEMMING ----

['.']

 TOTAL SNOWBALL STEM WORDS ==> 1



 ---- LEMMATIZATION ----

['.']

 TOTAL LEMMATIZE WORDS ==> 1

************************************************************************************************************************

24 --> . 


 ---- TOKENS ----

 ['.'] 

 TOTAL TOKENS ==> 1

 ---- POST ----

 [('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['.']

 TOTAL FILTERED TOKENS ==>  1

 ---- POST FOR FILTERED TOKENS ----

 [('.', '.')] 



 ---- BI-GRAMS ---- 

 [] 

 TOTAL BIGRAMS --> 0 



 ---- TRI-GRAMS ---- 

 [] 

 TOTAL TRIGRAMS --> 0 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['.']

 TOTAL PORTER STEM WORDS ==> 1



 ---- SNOWBALL STEMMING ----

['.']

 TOTAL SNOWBALL STEM WORDS ==> 1



 ---- LEMMATIZATION ----

['.']

 TOTAL LEMMATIZE WORDS ==> 1

************************************************************************************************************************

25 --> . 


 ---- TOKENS ----

 ['.'] 

 TOTAL TOKENS ==> 1

 ---- POST ----

 [('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['.']

 TOTAL FILTERED TOKENS ==>  1

 ---- POST FOR FILTERED TOKENS ----

 [('.', '.')] 



 ---- BI-GRAMS ---- 

 [] 

 TOTAL BIGRAMS --> 0 



 ---- TRI-GRAMS ---- 

 [] 

 TOTAL TRIGRAMS --> 0 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['.']

 TOTAL PORTER STEM WORDS ==> 1



 ---- SNOWBALL STEMMING ----

['.']

 TOTAL SNOWBALL STEM WORDS ==> 1



 ---- LEMMATIZATION ----

['.']

 TOTAL LEMMATIZE WORDS ==> 1

************************************************************************************************************************

26 --> . 


 ---- TOKENS ----

 ['.'] 

 TOTAL TOKENS ==> 1

 ---- POST ----

 [('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['.']

 TOTAL FILTERED TOKENS ==>  1

 ---- POST FOR FILTERED TOKENS ----

 [('.', '.')] 



 ---- BI-GRAMS ---- 

 [] 

 TOTAL BIGRAMS --> 0 



 ---- TRI-GRAMS ---- 

 [] 

 TOTAL TRIGRAMS --> 0 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['.']

 TOTAL PORTER STEM WORDS ==> 1



 ---- SNOWBALL STEMMING ----

['.']

 TOTAL SNOWBALL STEM WORDS ==> 1



 ---- LEMMATIZATION ----

['.']

 TOTAL LEMMATIZE WORDS ==> 1

************************************************************************************************************************

27 --> . 


 ---- TOKENS ----

 ['.'] 

 TOTAL TOKENS ==> 1

 ---- POST ----

 [('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['.']

 TOTAL FILTERED TOKENS ==>  1

 ---- POST FOR FILTERED TOKENS ----

 [('.', '.')] 



 ---- BI-GRAMS ---- 

 [] 

 TOTAL BIGRAMS --> 0 



 ---- TRI-GRAMS ---- 

 [] 

 TOTAL TRIGRAMS --> 0 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['.']

 TOTAL PORTER STEM WORDS ==> 1



 ---- SNOWBALL STEMMING ----

['.']

 TOTAL SNOWBALL STEM WORDS ==> 1



 ---- LEMMATIZATION ----

['.']

 TOTAL LEMMATIZE WORDS ==> 1

************************************************************************************************************************

28 --> . 


 ---- TOKENS ----

 ['.'] 

 TOTAL TOKENS ==> 1

 ---- POST ----

 [('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['.']

 TOTAL FILTERED TOKENS ==>  1

 ---- POST FOR FILTERED TOKENS ----

 [('.', '.')] 



 ---- BI-GRAMS ---- 

 [] 

 TOTAL BIGRAMS --> 0 



 ---- TRI-GRAMS ---- 

 [] 

 TOTAL TRIGRAMS --> 0 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['.']

 TOTAL PORTER STEM WORDS ==> 1



 ---- SNOWBALL STEMMING ----

['.']

 TOTAL SNOWBALL STEM WORDS ==> 1



 ---- LEMMATIZATION ----

['.']

 TOTAL LEMMATIZE WORDS ==> 1

************************************************************************************************************************

29 --> . 


 ---- TOKENS ----

 ['.'] 

 TOTAL TOKENS ==> 1

 ---- POST ----

 [('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['.']

 TOTAL FILTERED TOKENS ==>  1

 ---- POST FOR FILTERED TOKENS ----

 [('.', '.')] 



 ---- BI-GRAMS ---- 

 [] 

 TOTAL BIGRAMS --> 0 



 ---- TRI-GRAMS ---- 

 [] 

 TOTAL TRIGRAMS --> 0 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['.']

 TOTAL PORTER STEM WORDS ==> 1



 ---- SNOWBALL STEMMING ----

['.']

 TOTAL SNOWBALL STEM WORDS ==> 1



 ---- LEMMATIZATION ----

['.']

 TOTAL LEMMATIZE WORDS ==> 1

************************************************************************************************************************

30 --> . 


 ---- TOKENS ----

 ['.'] 

 TOTAL TOKENS ==> 1

 ---- POST ----

 [('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['.']

 TOTAL FILTERED TOKENS ==>  1

 ---- POST FOR FILTERED TOKENS ----

 [('.', '.')] 



 ---- BI-GRAMS ---- 

 [] 

 TOTAL BIGRAMS --> 0 



 ---- TRI-GRAMS ---- 

 [] 

 TOTAL TRIGRAMS --> 0 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['.']

 TOTAL PORTER STEM WORDS ==> 1



 ---- SNOWBALL STEMMING ----

['.']

 TOTAL SNOWBALL STEM WORDS ==> 1



 ---- LEMMATIZATION ----

['.']

 TOTAL LEMMATIZE WORDS ==> 1

************************************************************************************************************************

31 --> . 


 ---- TOKENS ----

 ['.'] 

 TOTAL TOKENS ==> 1

 ---- POST ----

 [('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['.']

 TOTAL FILTERED TOKENS ==>  1

 ---- POST FOR FILTERED TOKENS ----

 [('.', '.')] 



 ---- BI-GRAMS ---- 

 [] 

 TOTAL BIGRAMS --> 0 



 ---- TRI-GRAMS ---- 

 [] 

 TOTAL TRIGRAMS --> 0 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['.']

 TOTAL PORTER STEM WORDS ==> 1



 ---- SNOWBALL STEMMING ----

['.']

 TOTAL SNOWBALL STEM WORDS ==> 1



 ---- LEMMATIZATION ----

['.']

 TOTAL LEMMATIZE WORDS ==> 1

************************************************************************************************************************

32 --> . 


 ---- TOKENS ----

 ['.'] 

 TOTAL TOKENS ==> 1

 ---- POST ----

 [('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['.']

 TOTAL FILTERED TOKENS ==>  1

 ---- POST FOR FILTERED TOKENS ----

 [('.', '.')] 



 ---- BI-GRAMS ---- 

 [] 

 TOTAL BIGRAMS --> 0 



 ---- TRI-GRAMS ---- 

 [] 

 TOTAL TRIGRAMS --> 0 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['.']

 TOTAL PORTER STEM WORDS ==> 1



 ---- SNOWBALL STEMMING ----

['.']

 TOTAL SNOWBALL STEM WORDS ==> 1



 ---- LEMMATIZATION ----

['.']

 TOTAL LEMMATIZE WORDS ==> 1

************************************************************************************************************************

33 --> . 


 ---- TOKENS ----

 ['.'] 

 TOTAL TOKENS ==> 1

 ---- POST ----

 [('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['.']

 TOTAL FILTERED TOKENS ==>  1

 ---- POST FOR FILTERED TOKENS ----

 [('.', '.')] 



 ---- BI-GRAMS ---- 

 [] 

 TOTAL BIGRAMS --> 0 



 ---- TRI-GRAMS ---- 

 [] 

 TOTAL TRIGRAMS --> 0 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['.']

 TOTAL PORTER STEM WORDS ==> 1



 ---- SNOWBALL STEMMING ----

['.']

 TOTAL SNOWBALL STEM WORDS ==> 1



 ---- LEMMATIZATION ----

['.']

 TOTAL LEMMATIZE WORDS ==> 1

************************************************************************************************************************

34 --> . 


 ---- TOKENS ----

 ['.'] 

 TOTAL TOKENS ==> 1

 ---- POST ----

 [('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['.']

 TOTAL FILTERED TOKENS ==>  1

 ---- POST FOR FILTERED TOKENS ----

 [('.', '.')] 



 ---- BI-GRAMS ---- 

 [] 

 TOTAL BIGRAMS --> 0 



 ---- TRI-GRAMS ---- 

 [] 

 TOTAL TRIGRAMS --> 0 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['.']

 TOTAL PORTER STEM WORDS ==> 1



 ---- SNOWBALL STEMMING ----

['.']

 TOTAL SNOWBALL STEM WORDS ==> 1



 ---- LEMMATIZATION ----

['.']

 TOTAL LEMMATIZE WORDS ==> 1

************************************************************************************************************************

35 --> . 


 ---- TOKENS ----

 ['.'] 

 TOTAL TOKENS ==> 1

 ---- POST ----

 [('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['.']

 TOTAL FILTERED TOKENS ==>  1

 ---- POST FOR FILTERED TOKENS ----

 [('.', '.')] 



 ---- BI-GRAMS ---- 

 [] 

 TOTAL BIGRAMS --> 0 



 ---- TRI-GRAMS ---- 

 [] 

 TOTAL TRIGRAMS --> 0 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['.']

 TOTAL PORTER STEM WORDS ==> 1



 ---- SNOWBALL STEMMING ----

['.']

 TOTAL SNOWBALL STEM WORDS ==> 1



 ---- LEMMATIZATION ----

['.']

 TOTAL LEMMATIZE WORDS ==> 1

************************************************************************************************************************

36 --> . 


 ---- TOKENS ----

 ['.'] 

 TOTAL TOKENS ==> 1

 ---- POST ----

 [('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['.']

 TOTAL FILTERED TOKENS ==>  1

 ---- POST FOR FILTERED TOKENS ----

 [('.', '.')] 



 ---- BI-GRAMS ---- 

 [] 

 TOTAL BIGRAMS --> 0 



 ---- TRI-GRAMS ---- 

 [] 

 TOTAL TRIGRAMS --> 0 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['.']

 TOTAL PORTER STEM WORDS ==> 1



 ---- SNOWBALL STEMMING ----

['.']

 TOTAL SNOWBALL STEM WORDS ==> 1



 ---- LEMMATIZATION ----

['.']

 TOTAL LEMMATIZE WORDS ==> 1

************************************************************************************************************************

37 --> . 


 ---- TOKENS ----

 ['.'] 

 TOTAL TOKENS ==> 1

 ---- POST ----

 [('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['.']

 TOTAL FILTERED TOKENS ==>  1

 ---- POST FOR FILTERED TOKENS ----

 [('.', '.')] 



 ---- BI-GRAMS ---- 

 [] 

 TOTAL BIGRAMS --> 0 



 ---- TRI-GRAMS ---- 

 [] 

 TOTAL TRIGRAMS --> 0 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['.']

 TOTAL PORTER STEM WORDS ==> 1



 ---- SNOWBALL STEMMING ----

['.']

 TOTAL SNOWBALL STEM WORDS ==> 1



 ---- LEMMATIZATION ----

['.']

 TOTAL LEMMATIZE WORDS ==> 1

************************************************************************************************************************

38 --> . 


 ---- TOKENS ----

 ['.'] 

 TOTAL TOKENS ==> 1

 ---- POST ----

 [('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['.']

 TOTAL FILTERED TOKENS ==>  1

 ---- POST FOR FILTERED TOKENS ----

 [('.', '.')] 



 ---- BI-GRAMS ---- 

 [] 

 TOTAL BIGRAMS --> 0 



 ---- TRI-GRAMS ---- 

 [] 

 TOTAL TRIGRAMS --> 0 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['.']

 TOTAL PORTER STEM WORDS ==> 1



 ---- SNOWBALL STEMMING ----

['.']

 TOTAL SNOWBALL STEM WORDS ==> 1



 ---- LEMMATIZATION ----

['.']

 TOTAL LEMMATIZE WORDS ==> 1

************************************************************************************************************************

39 --> . 


 ---- TOKENS ----

 ['.'] 

 TOTAL TOKENS ==> 1

 ---- POST ----

 [('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['.']

 TOTAL FILTERED TOKENS ==>  1

 ---- POST FOR FILTERED TOKENS ----

 [('.', '.')] 



 ---- BI-GRAMS ---- 

 [] 

 TOTAL BIGRAMS --> 0 



 ---- TRI-GRAMS ---- 

 [] 

 TOTAL TRIGRAMS --> 0 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['.']

 TOTAL PORTER STEM WORDS ==> 1



 ---- SNOWBALL STEMMING ----

['.']

 TOTAL SNOWBALL STEM WORDS ==> 1



 ---- LEMMATIZATION ----

['.']

 TOTAL LEMMATIZE WORDS ==> 1

************************************************************************************************************************

40 --> . 


 ---- TOKENS ----

 ['.'] 

 TOTAL TOKENS ==> 1

 ---- POST ----

 [('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['.']

 TOTAL FILTERED TOKENS ==>  1

 ---- POST FOR FILTERED TOKENS ----

 [('.', '.')] 



 ---- BI-GRAMS ---- 

 [] 

 TOTAL BIGRAMS --> 0 



 ---- TRI-GRAMS ---- 

 [] 

 TOTAL TRIGRAMS --> 0 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['.']

 TOTAL PORTER STEM WORDS ==> 1



 ---- SNOWBALL STEMMING ----

['.']

 TOTAL SNOWBALL STEM WORDS ==> 1



 ---- LEMMATIZATION ----

['.']

 TOTAL LEMMATIZE WORDS ==> 1

************************************************************************************************************************

41 --> 2  Results . 


 ---- TOKENS ----

 ['2', 'Results', '.'] 

 TOTAL TOKENS ==> 3

 ---- POST ----

 [('2', 'CD'), ('Results', 'NNS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['2', 'Results', '.']

 TOTAL FILTERED TOKENS ==>  3

 ---- POST FOR FILTERED TOKENS ----

 [('2', 'CD'), ('Results', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['2 Results', 'Results .'] 

 TOTAL BIGRAMS --> 2 



 ---- TRI-GRAMS ---- 

 ['2 Results .'] 

 TOTAL TRIGRAMS --> 1 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['2', 'result', '.']

 TOTAL PORTER STEM WORDS ==> 3



 ---- SNOWBALL STEMMING ----

['2', 'result', '.']

 TOTAL SNOWBALL STEM WORDS ==> 3



 ---- LEMMATIZATION ----

['2', 'Results', '.']

 TOTAL LEMMATIZE WORDS ==> 3

************************************************************************************************************************

42 --> . 


 ---- TOKENS ----

 ['.'] 

 TOTAL TOKENS ==> 1

 ---- POST ----

 [('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['.']

 TOTAL FILTERED TOKENS ==>  1

 ---- POST FOR FILTERED TOKENS ----

 [('.', '.')] 



 ---- BI-GRAMS ---- 

 [] 

 TOTAL BIGRAMS --> 0 



 ---- TRI-GRAMS ---- 

 [] 

 TOTAL TRIGRAMS --> 0 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['.']

 TOTAL PORTER STEM WORDS ==> 1



 ---- SNOWBALL STEMMING ----

['.']

 TOTAL SNOWBALL STEM WORDS ==> 1



 ---- LEMMATIZATION ----

['.']

 TOTAL LEMMATIZE WORDS ==> 1

************************************************************************************************************************

43 --> . 


 ---- TOKENS ----

 ['.'] 

 TOTAL TOKENS ==> 1

 ---- POST ----

 [('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['.']

 TOTAL FILTERED TOKENS ==>  1

 ---- POST FOR FILTERED TOKENS ----

 [('.', '.')] 



 ---- BI-GRAMS ---- 

 [] 

 TOTAL BIGRAMS --> 0 



 ---- TRI-GRAMS ---- 

 [] 

 TOTAL TRIGRAMS --> 0 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['.']

 TOTAL PORTER STEM WORDS ==> 1



 ---- SNOWBALL STEMMING ----

['.']

 TOTAL SNOWBALL STEM WORDS ==> 1



 ---- LEMMATIZATION ----

['.']

 TOTAL LEMMATIZE WORDS ==> 1

************************************************************************************************************************

44 --> . 


 ---- TOKENS ----

 ['.'] 

 TOTAL TOKENS ==> 1

 ---- POST ----

 [('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['.']

 TOTAL FILTERED TOKENS ==>  1

 ---- POST FOR FILTERED TOKENS ----

 [('.', '.')] 



 ---- BI-GRAMS ---- 

 [] 

 TOTAL BIGRAMS --> 0 



 ---- TRI-GRAMS ---- 

 [] 

 TOTAL TRIGRAMS --> 0 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['.']

 TOTAL PORTER STEM WORDS ==> 1



 ---- SNOWBALL STEMMING ----

['.']

 TOTAL SNOWBALL STEM WORDS ==> 1



 ---- LEMMATIZATION ----

['.']

 TOTAL LEMMATIZE WORDS ==> 1

************************************************************************************************************************

45 --> . 


 ---- TOKENS ----

 ['.'] 

 TOTAL TOKENS ==> 1

 ---- POST ----

 [('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['.']

 TOTAL FILTERED TOKENS ==>  1

 ---- POST FOR FILTERED TOKENS ----

 [('.', '.')] 



 ---- BI-GRAMS ---- 

 [] 

 TOTAL BIGRAMS --> 0 



 ---- TRI-GRAMS ---- 

 [] 

 TOTAL TRIGRAMS --> 0 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['.']

 TOTAL PORTER STEM WORDS ==> 1



 ---- SNOWBALL STEMMING ----

['.']

 TOTAL SNOWBALL STEM WORDS ==> 1



 ---- LEMMATIZATION ----

['.']

 TOTAL LEMMATIZE WORDS ==> 1

************************************************************************************************************************

46 --> . 


 ---- TOKENS ----

 ['.'] 

 TOTAL TOKENS ==> 1

 ---- POST ----

 [('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['.']

 TOTAL FILTERED TOKENS ==>  1

 ---- POST FOR FILTERED TOKENS ----

 [('.', '.')] 



 ---- BI-GRAMS ---- 

 [] 

 TOTAL BIGRAMS --> 0 



 ---- TRI-GRAMS ---- 

 [] 

 TOTAL TRIGRAMS --> 0 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['.']

 TOTAL PORTER STEM WORDS ==> 1



 ---- SNOWBALL STEMMING ----

['.']

 TOTAL SNOWBALL STEM WORDS ==> 1



 ---- LEMMATIZATION ----

['.']

 TOTAL LEMMATIZE WORDS ==> 1

************************************************************************************************************************

47 --> . 


 ---- TOKENS ----

 ['.'] 

 TOTAL TOKENS ==> 1

 ---- POST ----

 [('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['.']

 TOTAL FILTERED TOKENS ==>  1

 ---- POST FOR FILTERED TOKENS ----

 [('.', '.')] 



 ---- BI-GRAMS ---- 

 [] 

 TOTAL BIGRAMS --> 0 



 ---- TRI-GRAMS ---- 

 [] 

 TOTAL TRIGRAMS --> 0 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['.']

 TOTAL PORTER STEM WORDS ==> 1



 ---- SNOWBALL STEMMING ----

['.']

 TOTAL SNOWBALL STEM WORDS ==> 1



 ---- LEMMATIZATION ----

['.']

 TOTAL LEMMATIZE WORDS ==> 1

************************************************************************************************************************

48 --> . 


 ---- TOKENS ----

 ['.'] 

 TOTAL TOKENS ==> 1

 ---- POST ----

 [('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['.']

 TOTAL FILTERED TOKENS ==>  1

 ---- POST FOR FILTERED TOKENS ----

 [('.', '.')] 



 ---- BI-GRAMS ---- 

 [] 

 TOTAL BIGRAMS --> 0 



 ---- TRI-GRAMS ---- 

 [] 

 TOTAL TRIGRAMS --> 0 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['.']

 TOTAL PORTER STEM WORDS ==> 1



 ---- SNOWBALL STEMMING ----

['.']

 TOTAL SNOWBALL STEM WORDS ==> 1



 ---- LEMMATIZATION ----

['.']

 TOTAL LEMMATIZE WORDS ==> 1

************************************************************************************************************************

49 --> . 


 ---- TOKENS ----

 ['.'] 

 TOTAL TOKENS ==> 1

 ---- POST ----

 [('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['.']

 TOTAL FILTERED TOKENS ==>  1

 ---- POST FOR FILTERED TOKENS ----

 [('.', '.')] 



 ---- BI-GRAMS ---- 

 [] 

 TOTAL BIGRAMS --> 0 



 ---- TRI-GRAMS ---- 

 [] 

 TOTAL TRIGRAMS --> 0 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['.']

 TOTAL PORTER STEM WORDS ==> 1



 ---- SNOWBALL STEMMING ----

['.']

 TOTAL SNOWBALL STEM WORDS ==> 1



 ---- LEMMATIZATION ----

['.']

 TOTAL LEMMATIZE WORDS ==> 1

************************************************************************************************************************

50 --> . 


 ---- TOKENS ----

 ['.'] 

 TOTAL TOKENS ==> 1

 ---- POST ----

 [('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['.']

 TOTAL FILTERED TOKENS ==>  1

 ---- POST FOR FILTERED TOKENS ----

 [('.', '.')] 



 ---- BI-GRAMS ---- 

 [] 

 TOTAL BIGRAMS --> 0 



 ---- TRI-GRAMS ---- 

 [] 

 TOTAL TRIGRAMS --> 0 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['.']

 TOTAL PORTER STEM WORDS ==> 1



 ---- SNOWBALL STEMMING ----

['.']

 TOTAL SNOWBALL STEM WORDS ==> 1



 ---- LEMMATIZATION ----

['.']

 TOTAL LEMMATIZE WORDS ==> 1

************************************************************************************************************************

51 --> . 


 ---- TOKENS ----

 ['.'] 

 TOTAL TOKENS ==> 1

 ---- POST ----

 [('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['.']

 TOTAL FILTERED TOKENS ==>  1

 ---- POST FOR FILTERED TOKENS ----

 [('.', '.')] 



 ---- BI-GRAMS ---- 

 [] 

 TOTAL BIGRAMS --> 0 



 ---- TRI-GRAMS ---- 

 [] 

 TOTAL TRIGRAMS --> 0 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['.']

 TOTAL PORTER STEM WORDS ==> 1



 ---- SNOWBALL STEMMING ----

['.']

 TOTAL SNOWBALL STEM WORDS ==> 1



 ---- LEMMATIZATION ----

['.']

 TOTAL LEMMATIZE WORDS ==> 1

************************************************************************************************************************

52 --> . 


 ---- TOKENS ----

 ['.'] 

 TOTAL TOKENS ==> 1

 ---- POST ----

 [('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['.']

 TOTAL FILTERED TOKENS ==>  1

 ---- POST FOR FILTERED TOKENS ----

 [('.', '.')] 



 ---- BI-GRAMS ---- 

 [] 

 TOTAL BIGRAMS --> 0 



 ---- TRI-GRAMS ---- 

 [] 

 TOTAL TRIGRAMS --> 0 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['.']

 TOTAL PORTER STEM WORDS ==> 1



 ---- SNOWBALL STEMMING ----

['.']

 TOTAL SNOWBALL STEM WORDS ==> 1



 ---- LEMMATIZATION ----

['.']

 TOTAL LEMMATIZE WORDS ==> 1

************************************************************************************************************************

53 --> . 


 ---- TOKENS ----

 ['.'] 

 TOTAL TOKENS ==> 1

 ---- POST ----

 [('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['.']

 TOTAL FILTERED TOKENS ==>  1

 ---- POST FOR FILTERED TOKENS ----

 [('.', '.')] 



 ---- BI-GRAMS ---- 

 [] 

 TOTAL BIGRAMS --> 0 



 ---- TRI-GRAMS ---- 

 [] 

 TOTAL TRIGRAMS --> 0 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['.']

 TOTAL PORTER STEM WORDS ==> 1



 ---- SNOWBALL STEMMING ----

['.']

 TOTAL SNOWBALL STEM WORDS ==> 1



 ---- LEMMATIZATION ----

['.']

 TOTAL LEMMATIZE WORDS ==> 1

************************************************************************************************************************

54 --> . 


 ---- TOKENS ----

 ['.'] 

 TOTAL TOKENS ==> 1

 ---- POST ----

 [('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['.']

 TOTAL FILTERED TOKENS ==>  1

 ---- POST FOR FILTERED TOKENS ----

 [('.', '.')] 



 ---- BI-GRAMS ---- 

 [] 

 TOTAL BIGRAMS --> 0 



 ---- TRI-GRAMS ---- 

 [] 

 TOTAL TRIGRAMS --> 0 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['.']

 TOTAL PORTER STEM WORDS ==> 1



 ---- SNOWBALL STEMMING ----

['.']

 TOTAL SNOWBALL STEM WORDS ==> 1



 ---- LEMMATIZATION ----

['.']

 TOTAL LEMMATIZE WORDS ==> 1

************************************************************************************************************************

55 --> . 


 ---- TOKENS ----

 ['.'] 

 TOTAL TOKENS ==> 1

 ---- POST ----

 [('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['.']

 TOTAL FILTERED TOKENS ==>  1

 ---- POST FOR FILTERED TOKENS ----

 [('.', '.')] 



 ---- BI-GRAMS ---- 

 [] 

 TOTAL BIGRAMS --> 0 



 ---- TRI-GRAMS ---- 

 [] 

 TOTAL TRIGRAMS --> 0 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['.']

 TOTAL PORTER STEM WORDS ==> 1



 ---- SNOWBALL STEMMING ----

['.']

 TOTAL SNOWBALL STEM WORDS ==> 1



 ---- LEMMATIZATION ----

['.']

 TOTAL LEMMATIZE WORDS ==> 1

************************************************************************************************************************

56 --> . 


 ---- TOKENS ----

 ['.'] 

 TOTAL TOKENS ==> 1

 ---- POST ----

 [('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['.']

 TOTAL FILTERED TOKENS ==>  1

 ---- POST FOR FILTERED TOKENS ----

 [('.', '.')] 



 ---- BI-GRAMS ---- 

 [] 

 TOTAL BIGRAMS --> 0 



 ---- TRI-GRAMS ---- 

 [] 

 TOTAL TRIGRAMS --> 0 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['.']

 TOTAL PORTER STEM WORDS ==> 1



 ---- SNOWBALL STEMMING ----

['.']

 TOTAL SNOWBALL STEM WORDS ==> 1



 ---- LEMMATIZATION ----

['.']

 TOTAL LEMMATIZE WORDS ==> 1

************************************************************************************************************************

57 --> . 


 ---- TOKENS ----

 ['.'] 

 TOTAL TOKENS ==> 1

 ---- POST ----

 [('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['.']

 TOTAL FILTERED TOKENS ==>  1

 ---- POST FOR FILTERED TOKENS ----

 [('.', '.')] 



 ---- BI-GRAMS ---- 

 [] 

 TOTAL BIGRAMS --> 0 



 ---- TRI-GRAMS ---- 

 [] 

 TOTAL TRIGRAMS --> 0 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['.']

 TOTAL PORTER STEM WORDS ==> 1



 ---- SNOWBALL STEMMING ----

['.']

 TOTAL SNOWBALL STEM WORDS ==> 1



 ---- LEMMATIZATION ----

['.']

 TOTAL LEMMATIZE WORDS ==> 1

************************************************************************************************************************

58 --> . 


 ---- TOKENS ----

 ['.'] 

 TOTAL TOKENS ==> 1

 ---- POST ----

 [('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['.']

 TOTAL FILTERED TOKENS ==>  1

 ---- POST FOR FILTERED TOKENS ----

 [('.', '.')] 



 ---- BI-GRAMS ---- 

 [] 

 TOTAL BIGRAMS --> 0 



 ---- TRI-GRAMS ---- 

 [] 

 TOTAL TRIGRAMS --> 0 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['.']

 TOTAL PORTER STEM WORDS ==> 1



 ---- SNOWBALL STEMMING ----

['.']

 TOTAL SNOWBALL STEM WORDS ==> 1



 ---- LEMMATIZATION ----

['.']

 TOTAL LEMMATIZE WORDS ==> 1

************************************************************************************************************************

59 --> . 


 ---- TOKENS ----

 ['.'] 

 TOTAL TOKENS ==> 1

 ---- POST ----

 [('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['.']

 TOTAL FILTERED TOKENS ==>  1

 ---- POST FOR FILTERED TOKENS ----

 [('.', '.')] 



 ---- BI-GRAMS ---- 

 [] 

 TOTAL BIGRAMS --> 0 



 ---- TRI-GRAMS ---- 

 [] 

 TOTAL TRIGRAMS --> 0 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['.']

 TOTAL PORTER STEM WORDS ==> 1



 ---- SNOWBALL STEMMING ----

['.']

 TOTAL SNOWBALL STEM WORDS ==> 1



 ---- LEMMATIZATION ----

['.']

 TOTAL LEMMATIZE WORDS ==> 1

************************************************************************************************************************

60 --> . 


 ---- TOKENS ----

 ['.'] 

 TOTAL TOKENS ==> 1

 ---- POST ----

 [('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['.']

 TOTAL FILTERED TOKENS ==>  1

 ---- POST FOR FILTERED TOKENS ----

 [('.', '.')] 



 ---- BI-GRAMS ---- 

 [] 

 TOTAL BIGRAMS --> 0 



 ---- TRI-GRAMS ---- 

 [] 

 TOTAL TRIGRAMS --> 0 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['.']

 TOTAL PORTER STEM WORDS ==> 1



 ---- SNOWBALL STEMMING ----

['.']

 TOTAL SNOWBALL STEM WORDS ==> 1



 ---- LEMMATIZATION ----

['.']

 TOTAL LEMMATIZE WORDS ==> 1

************************************************************************************************************************

61 --> . 


 ---- TOKENS ----

 ['.'] 

 TOTAL TOKENS ==> 1

 ---- POST ----

 [('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['.']

 TOTAL FILTERED TOKENS ==>  1

 ---- POST FOR FILTERED TOKENS ----

 [('.', '.')] 



 ---- BI-GRAMS ---- 

 [] 

 TOTAL BIGRAMS --> 0 



 ---- TRI-GRAMS ---- 

 [] 

 TOTAL TRIGRAMS --> 0 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['.']

 TOTAL PORTER STEM WORDS ==> 1



 ---- SNOWBALL STEMMING ----

['.']

 TOTAL SNOWBALL STEM WORDS ==> 1



 ---- LEMMATIZATION ----

['.']

 TOTAL LEMMATIZE WORDS ==> 1

************************************************************************************************************************

62 --> . 


 ---- TOKENS ----

 ['.'] 

 TOTAL TOKENS ==> 1

 ---- POST ----

 [('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['.']

 TOTAL FILTERED TOKENS ==>  1

 ---- POST FOR FILTERED TOKENS ----

 [('.', '.')] 



 ---- BI-GRAMS ---- 

 [] 

 TOTAL BIGRAMS --> 0 



 ---- TRI-GRAMS ---- 

 [] 

 TOTAL TRIGRAMS --> 0 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['.']

 TOTAL PORTER STEM WORDS ==> 1



 ---- SNOWBALL STEMMING ----

['.']

 TOTAL SNOWBALL STEM WORDS ==> 1



 ---- LEMMATIZATION ----

['.']

 TOTAL LEMMATIZE WORDS ==> 1

************************************************************************************************************************

63 --> . 


 ---- TOKENS ----

 ['.'] 

 TOTAL TOKENS ==> 1

 ---- POST ----

 [('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['.']

 TOTAL FILTERED TOKENS ==>  1

 ---- POST FOR FILTERED TOKENS ----

 [('.', '.')] 



 ---- BI-GRAMS ---- 

 [] 

 TOTAL BIGRAMS --> 0 



 ---- TRI-GRAMS ---- 

 [] 

 TOTAL TRIGRAMS --> 0 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['.']

 TOTAL PORTER STEM WORDS ==> 1



 ---- SNOWBALL STEMMING ----

['.']

 TOTAL SNOWBALL STEM WORDS ==> 1



 ---- LEMMATIZATION ----

['.']

 TOTAL LEMMATIZE WORDS ==> 1

************************************************************************************************************************

64 --> . 


 ---- TOKENS ----

 ['.'] 

 TOTAL TOKENS ==> 1

 ---- POST ----

 [('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['.']

 TOTAL FILTERED TOKENS ==>  1

 ---- POST FOR FILTERED TOKENS ----

 [('.', '.')] 



 ---- BI-GRAMS ---- 

 [] 

 TOTAL BIGRAMS --> 0 



 ---- TRI-GRAMS ---- 

 [] 

 TOTAL TRIGRAMS --> 0 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['.']

 TOTAL PORTER STEM WORDS ==> 1



 ---- SNOWBALL STEMMING ----

['.']

 TOTAL SNOWBALL STEM WORDS ==> 1



 ---- LEMMATIZATION ----

['.']

 TOTAL LEMMATIZE WORDS ==> 1

************************************************************************************************************************

65 --> . 


 ---- TOKENS ----

 ['.'] 

 TOTAL TOKENS ==> 1

 ---- POST ----

 [('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['.']

 TOTAL FILTERED TOKENS ==>  1

 ---- POST FOR FILTERED TOKENS ----

 [('.', '.')] 



 ---- BI-GRAMS ---- 

 [] 

 TOTAL BIGRAMS --> 0 



 ---- TRI-GRAMS ---- 

 [] 

 TOTAL TRIGRAMS --> 0 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['.']

 TOTAL PORTER STEM WORDS ==> 1



 ---- SNOWBALL STEMMING ----

['.']

 TOTAL SNOWBALL STEM WORDS ==> 1



 ---- LEMMATIZATION ----

['.']

 TOTAL LEMMATIZE WORDS ==> 1

************************************************************************************************************************

66 --> . 


 ---- TOKENS ----

 ['.'] 

 TOTAL TOKENS ==> 1

 ---- POST ----

 [('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['.']

 TOTAL FILTERED TOKENS ==>  1

 ---- POST FOR FILTERED TOKENS ----

 [('.', '.')] 



 ---- BI-GRAMS ---- 

 [] 

 TOTAL BIGRAMS --> 0 



 ---- TRI-GRAMS ---- 

 [] 

 TOTAL TRIGRAMS --> 0 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['.']

 TOTAL PORTER STEM WORDS ==> 1



 ---- SNOWBALL STEMMING ----

['.']

 TOTAL SNOWBALL STEM WORDS ==> 1



 ---- LEMMATIZATION ----

['.']

 TOTAL LEMMATIZE WORDS ==> 1

************************************************************************************************************************

67 --> . 


 ---- TOKENS ----

 ['.'] 

 TOTAL TOKENS ==> 1

 ---- POST ----

 [('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['.']

 TOTAL FILTERED TOKENS ==>  1

 ---- POST FOR FILTERED TOKENS ----

 [('.', '.')] 



 ---- BI-GRAMS ---- 

 [] 

 TOTAL BIGRAMS --> 0 



 ---- TRI-GRAMS ---- 

 [] 

 TOTAL TRIGRAMS --> 0 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['.']

 TOTAL PORTER STEM WORDS ==> 1



 ---- SNOWBALL STEMMING ----

['.']

 TOTAL SNOWBALL STEM WORDS ==> 1



 ---- LEMMATIZATION ----

['.']

 TOTAL LEMMATIZE WORDS ==> 1

************************************************************************************************************************

68 --> . 


 ---- TOKENS ----

 ['.'] 

 TOTAL TOKENS ==> 1

 ---- POST ----

 [('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['.']

 TOTAL FILTERED TOKENS ==>  1

 ---- POST FOR FILTERED TOKENS ----

 [('.', '.')] 



 ---- BI-GRAMS ---- 

 [] 

 TOTAL BIGRAMS --> 0 



 ---- TRI-GRAMS ---- 

 [] 

 TOTAL TRIGRAMS --> 0 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['.']

 TOTAL PORTER STEM WORDS ==> 1



 ---- SNOWBALL STEMMING ----

['.']

 TOTAL SNOWBALL STEM WORDS ==> 1



 ---- LEMMATIZATION ----

['.']

 TOTAL LEMMATIZE WORDS ==> 1

************************************************************************************************************************

69 --> . 


 ---- TOKENS ----

 ['.'] 

 TOTAL TOKENS ==> 1

 ---- POST ----

 [('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['.']

 TOTAL FILTERED TOKENS ==>  1

 ---- POST FOR FILTERED TOKENS ----

 [('.', '.')] 



 ---- BI-GRAMS ---- 

 [] 

 TOTAL BIGRAMS --> 0 



 ---- TRI-GRAMS ---- 

 [] 

 TOTAL TRIGRAMS --> 0 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['.']

 TOTAL PORTER STEM WORDS ==> 1



 ---- SNOWBALL STEMMING ----

['.']

 TOTAL SNOWBALL STEM WORDS ==> 1



 ---- LEMMATIZATION ----

['.']

 TOTAL LEMMATIZE WORDS ==> 1

************************************************************************************************************************

70 --> . 


 ---- TOKENS ----

 ['.'] 

 TOTAL TOKENS ==> 1

 ---- POST ----

 [('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['.']

 TOTAL FILTERED TOKENS ==>  1

 ---- POST FOR FILTERED TOKENS ----

 [('.', '.')] 



 ---- BI-GRAMS ---- 

 [] 

 TOTAL BIGRAMS --> 0 



 ---- TRI-GRAMS ---- 

 [] 

 TOTAL TRIGRAMS --> 0 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['.']

 TOTAL PORTER STEM WORDS ==> 1



 ---- SNOWBALL STEMMING ----

['.']

 TOTAL SNOWBALL STEM WORDS ==> 1



 ---- LEMMATIZATION ----

['.']

 TOTAL LEMMATIZE WORDS ==> 1

************************************************************************************************************************

71 --> . 


 ---- TOKENS ----

 ['.'] 

 TOTAL TOKENS ==> 1

 ---- POST ----

 [('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['.']

 TOTAL FILTERED TOKENS ==>  1

 ---- POST FOR FILTERED TOKENS ----

 [('.', '.')] 



 ---- BI-GRAMS ---- 

 [] 

 TOTAL BIGRAMS --> 0 



 ---- TRI-GRAMS ---- 

 [] 

 TOTAL TRIGRAMS --> 0 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['.']

 TOTAL PORTER STEM WORDS ==> 1



 ---- SNOWBALL STEMMING ----

['.']

 TOTAL SNOWBALL STEM WORDS ==> 1



 ---- LEMMATIZATION ----

['.']

 TOTAL LEMMATIZE WORDS ==> 1

************************************************************************************************************************

72 --> . 


 ---- TOKENS ----

 ['.'] 

 TOTAL TOKENS ==> 1

 ---- POST ----

 [('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['.']

 TOTAL FILTERED TOKENS ==>  1

 ---- POST FOR FILTERED TOKENS ----

 [('.', '.')] 



 ---- BI-GRAMS ---- 

 [] 

 TOTAL BIGRAMS --> 0 



 ---- TRI-GRAMS ---- 

 [] 

 TOTAL TRIGRAMS --> 0 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['.']

 TOTAL PORTER STEM WORDS ==> 1



 ---- SNOWBALL STEMMING ----

['.']

 TOTAL SNOWBALL STEM WORDS ==> 1



 ---- LEMMATIZATION ----

['.']

 TOTAL LEMMATIZE WORDS ==> 1

************************************************************************************************************************

73 --> . 


 ---- TOKENS ----

 ['.'] 

 TOTAL TOKENS ==> 1

 ---- POST ----

 [('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['.']

 TOTAL FILTERED TOKENS ==>  1

 ---- POST FOR FILTERED TOKENS ----

 [('.', '.')] 



 ---- BI-GRAMS ---- 

 [] 

 TOTAL BIGRAMS --> 0 



 ---- TRI-GRAMS ---- 

 [] 

 TOTAL TRIGRAMS --> 0 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['.']

 TOTAL PORTER STEM WORDS ==> 1



 ---- SNOWBALL STEMMING ----

['.']

 TOTAL SNOWBALL STEM WORDS ==> 1



 ---- LEMMATIZATION ----

['.']

 TOTAL LEMMATIZE WORDS ==> 1

************************************************************************************************************************

74 --> 2 TEXT-TO-VOICE TRANSLATION                                        2  Computing resources . 


 ---- TOKENS ----

 ['2', 'TEXT-TO-VOICE', 'TRANSLATION', '2', 'Computing', 'resources', '.'] 

 TOTAL TOKENS ==> 7

 ---- POST ----

 [('2', 'CD'), ('TEXT-TO-VOICE', 'JJ'), ('TRANSLATION', 'NNP'), ('2', 'CD'), ('Computing', 'NNP'), ('resources', 'NNS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['2', 'TEXT-TO-VOICE', 'TRANSLATION', '2', 'Computing', 'resources', '.']

 TOTAL FILTERED TOKENS ==>  7

 ---- POST FOR FILTERED TOKENS ----

 [('2', 'CD'), ('TEXT-TO-VOICE', 'JJ'), ('TRANSLATION', 'NNP'), ('2', 'CD'), ('Computing', 'NNP'), ('resources', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['2 TEXT-TO-VOICE', 'TEXT-TO-VOICE TRANSLATION', 'TRANSLATION 2', '2 Computing', 'Computing resources', 'resources .'] 

 TOTAL BIGRAMS --> 6 



 ---- TRI-GRAMS ---- 

 ['2 TEXT-TO-VOICE TRANSLATION', 'TEXT-TO-VOICE TRANSLATION 2', 'TRANSLATION 2 Computing', '2 Computing resources', 'Computing resources .'] 

 TOTAL TRIGRAMS --> 5 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['2', 'text-to-voic', 'translat', '2', 'comput', 'resourc', '.']

 TOTAL PORTER STEM WORDS ==> 7



 ---- SNOWBALL STEMMING ----

['2', 'text-to-voic', 'translat', '2', 'comput', 'resourc', '.']

 TOTAL SNOWBALL STEM WORDS ==> 7



 ---- LEMMATIZATION ----

['2', 'TEXT-TO-VOICE', 'TRANSLATION', '2', 'Computing', 'resource', '.']

 TOTAL LEMMATIZE WORDS ==> 7

************************************************************************************************************************

75 --> . 


 ---- TOKENS ----

 ['.'] 

 TOTAL TOKENS ==> 1

 ---- POST ----

 [('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['.']

 TOTAL FILTERED TOKENS ==>  1

 ---- POST FOR FILTERED TOKENS ----

 [('.', '.')] 



 ---- BI-GRAMS ---- 

 [] 

 TOTAL BIGRAMS --> 0 



 ---- TRI-GRAMS ---- 

 [] 

 TOTAL TRIGRAMS --> 0 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['.']

 TOTAL PORTER STEM WORDS ==> 1



 ---- SNOWBALL STEMMING ----

['.']

 TOTAL SNOWBALL STEM WORDS ==> 1



 ---- LEMMATIZATION ----

['.']

 TOTAL LEMMATIZE WORDS ==> 1

************************************************************************************************************************

76 --> . 


 ---- TOKENS ----

 ['.'] 

 TOTAL TOKENS ==> 1

 ---- POST ----

 [('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['.']

 TOTAL FILTERED TOKENS ==>  1

 ---- POST FOR FILTERED TOKENS ----

 [('.', '.')] 



 ---- BI-GRAMS ---- 

 [] 

 TOTAL BIGRAMS --> 0 



 ---- TRI-GRAMS ---- 

 [] 

 TOTAL TRIGRAMS --> 0 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['.']

 TOTAL PORTER STEM WORDS ==> 1



 ---- SNOWBALL STEMMING ----

['.']

 TOTAL SNOWBALL STEM WORDS ==> 1



 ---- LEMMATIZATION ----

['.']

 TOTAL LEMMATIZE WORDS ==> 1

************************************************************************************************************************

77 --> . 


 ---- TOKENS ----

 ['.'] 

 TOTAL TOKENS ==> 1

 ---- POST ----

 [('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['.']

 TOTAL FILTERED TOKENS ==>  1

 ---- POST FOR FILTERED TOKENS ----

 [('.', '.')] 



 ---- BI-GRAMS ---- 

 [] 

 TOTAL BIGRAMS --> 0 



 ---- TRI-GRAMS ---- 

 [] 

 TOTAL TRIGRAMS --> 0 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['.']

 TOTAL PORTER STEM WORDS ==> 1



 ---- SNOWBALL STEMMING ----

['.']

 TOTAL SNOWBALL STEM WORDS ==> 1



 ---- LEMMATIZATION ----

['.']

 TOTAL LEMMATIZE WORDS ==> 1

************************************************************************************************************************

78 --> . 


 ---- TOKENS ----

 ['.'] 

 TOTAL TOKENS ==> 1

 ---- POST ----

 [('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['.']

 TOTAL FILTERED TOKENS ==>  1

 ---- POST FOR FILTERED TOKENS ----

 [('.', '.')] 



 ---- BI-GRAMS ---- 

 [] 

 TOTAL BIGRAMS --> 0 



 ---- TRI-GRAMS ---- 

 [] 

 TOTAL TRIGRAMS --> 0 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['.']

 TOTAL PORTER STEM WORDS ==> 1



 ---- SNOWBALL STEMMING ----

['.']

 TOTAL SNOWBALL STEM WORDS ==> 1



 ---- LEMMATIZATION ----

['.']

 TOTAL LEMMATIZE WORDS ==> 1

************************************************************************************************************************

79 --> . 


 ---- TOKENS ----

 ['.'] 

 TOTAL TOKENS ==> 1

 ---- POST ----

 [('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['.']

 TOTAL FILTERED TOKENS ==>  1

 ---- POST FOR FILTERED TOKENS ----

 [('.', '.')] 



 ---- BI-GRAMS ---- 

 [] 

 TOTAL BIGRAMS --> 0 



 ---- TRI-GRAMS ---- 

 [] 

 TOTAL TRIGRAMS --> 0 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['.']

 TOTAL PORTER STEM WORDS ==> 1



 ---- SNOWBALL STEMMING ----

['.']

 TOTAL SNOWBALL STEM WORDS ==> 1



 ---- LEMMATIZATION ----

['.']

 TOTAL LEMMATIZE WORDS ==> 1

************************************************************************************************************************

80 --> . 


 ---- TOKENS ----

 ['.'] 

 TOTAL TOKENS ==> 1

 ---- POST ----

 [('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['.']

 TOTAL FILTERED TOKENS ==>  1

 ---- POST FOR FILTERED TOKENS ----

 [('.', '.')] 



 ---- BI-GRAMS ---- 

 [] 

 TOTAL BIGRAMS --> 0 



 ---- TRI-GRAMS ---- 

 [] 

 TOTAL TRIGRAMS --> 0 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['.']

 TOTAL PORTER STEM WORDS ==> 1



 ---- SNOWBALL STEMMING ----

['.']

 TOTAL SNOWBALL STEM WORDS ==> 1



 ---- LEMMATIZATION ----

['.']

 TOTAL LEMMATIZE WORDS ==> 1

************************************************************************************************************************

81 --> . 


 ---- TOKENS ----

 ['.'] 

 TOTAL TOKENS ==> 1

 ---- POST ----

 [('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['.']

 TOTAL FILTERED TOKENS ==>  1

 ---- POST FOR FILTERED TOKENS ----

 [('.', '.')] 



 ---- BI-GRAMS ---- 

 [] 

 TOTAL BIGRAMS --> 0 



 ---- TRI-GRAMS ---- 

 [] 

 TOTAL TRIGRAMS --> 0 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['.']

 TOTAL PORTER STEM WORDS ==> 1



 ---- SNOWBALL STEMMING ----

['.']

 TOTAL SNOWBALL STEM WORDS ==> 1



 ---- LEMMATIZATION ----

['.']

 TOTAL LEMMATIZE WORDS ==> 1

************************************************************************************************************************

82 --> . 


 ---- TOKENS ----

 ['.'] 

 TOTAL TOKENS ==> 1

 ---- POST ----

 [('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['.']

 TOTAL FILTERED TOKENS ==>  1

 ---- POST FOR FILTERED TOKENS ----

 [('.', '.')] 



 ---- BI-GRAMS ---- 

 [] 

 TOTAL BIGRAMS --> 0 



 ---- TRI-GRAMS ---- 

 [] 

 TOTAL TRIGRAMS --> 0 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['.']

 TOTAL PORTER STEM WORDS ==> 1



 ---- SNOWBALL STEMMING ----

['.']

 TOTAL SNOWBALL STEM WORDS ==> 1



 ---- LEMMATIZATION ----

['.']

 TOTAL LEMMATIZE WORDS ==> 1

************************************************************************************************************************

83 --> . 


 ---- TOKENS ----

 ['.'] 

 TOTAL TOKENS ==> 1

 ---- POST ----

 [('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['.']

 TOTAL FILTERED TOKENS ==>  1

 ---- POST FOR FILTERED TOKENS ----

 [('.', '.')] 



 ---- BI-GRAMS ---- 

 [] 

 TOTAL BIGRAMS --> 0 



 ---- TRI-GRAMS ---- 

 [] 

 TOTAL TRIGRAMS --> 0 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['.']

 TOTAL PORTER STEM WORDS ==> 1



 ---- SNOWBALL STEMMING ----

['.']

 TOTAL SNOWBALL STEM WORDS ==> 1



 ---- LEMMATIZATION ----

['.']

 TOTAL LEMMATIZE WORDS ==> 1

************************************************************************************************************************

84 --> . 


 ---- TOKENS ----

 ['.'] 

 TOTAL TOKENS ==> 1

 ---- POST ----

 [('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['.']

 TOTAL FILTERED TOKENS ==>  1

 ---- POST FOR FILTERED TOKENS ----

 [('.', '.')] 



 ---- BI-GRAMS ---- 

 [] 

 TOTAL BIGRAMS --> 0 



 ---- TRI-GRAMS ---- 

 [] 

 TOTAL TRIGRAMS --> 0 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['.']

 TOTAL PORTER STEM WORDS ==> 1



 ---- SNOWBALL STEMMING ----

['.']

 TOTAL SNOWBALL STEM WORDS ==> 1



 ---- LEMMATIZATION ----

['.']

 TOTAL LEMMATIZE WORDS ==> 1

************************************************************************************************************************

85 --> . 


 ---- TOKENS ----

 ['.'] 

 TOTAL TOKENS ==> 1

 ---- POST ----

 [('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['.']

 TOTAL FILTERED TOKENS ==>  1

 ---- POST FOR FILTERED TOKENS ----

 [('.', '.')] 



 ---- BI-GRAMS ---- 

 [] 

 TOTAL BIGRAMS --> 0 



 ---- TRI-GRAMS ---- 

 [] 

 TOTAL TRIGRAMS --> 0 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['.']

 TOTAL PORTER STEM WORDS ==> 1



 ---- SNOWBALL STEMMING ----

['.']

 TOTAL SNOWBALL STEM WORDS ==> 1



 ---- LEMMATIZATION ----

['.']

 TOTAL LEMMATIZE WORDS ==> 1

************************************************************************************************************************

86 --> . 


 ---- TOKENS ----

 ['.'] 

 TOTAL TOKENS ==> 1

 ---- POST ----

 [('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['.']

 TOTAL FILTERED TOKENS ==>  1

 ---- POST FOR FILTERED TOKENS ----

 [('.', '.')] 



 ---- BI-GRAMS ---- 

 [] 

 TOTAL BIGRAMS --> 0 



 ---- TRI-GRAMS ---- 

 [] 

 TOTAL TRIGRAMS --> 0 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['.']

 TOTAL PORTER STEM WORDS ==> 1



 ---- SNOWBALL STEMMING ----

['.']

 TOTAL SNOWBALL STEM WORDS ==> 1



 ---- LEMMATIZATION ----

['.']

 TOTAL LEMMATIZE WORDS ==> 1

************************************************************************************************************************

87 --> . 


 ---- TOKENS ----

 ['.'] 

 TOTAL TOKENS ==> 1

 ---- POST ----

 [('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['.']

 TOTAL FILTERED TOKENS ==>  1

 ---- POST FOR FILTERED TOKENS ----

 [('.', '.')] 



 ---- BI-GRAMS ---- 

 [] 

 TOTAL BIGRAMS --> 0 



 ---- TRI-GRAMS ---- 

 [] 

 TOTAL TRIGRAMS --> 0 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['.']

 TOTAL PORTER STEM WORDS ==> 1



 ---- SNOWBALL STEMMING ----

['.']

 TOTAL SNOWBALL STEM WORDS ==> 1



 ---- LEMMATIZATION ----

['.']

 TOTAL LEMMATIZE WORDS ==> 1

************************************************************************************************************************

88 --> . 


 ---- TOKENS ----

 ['.'] 

 TOTAL TOKENS ==> 1

 ---- POST ----

 [('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['.']

 TOTAL FILTERED TOKENS ==>  1

 ---- POST FOR FILTERED TOKENS ----

 [('.', '.')] 



 ---- BI-GRAMS ---- 

 [] 

 TOTAL BIGRAMS --> 0 



 ---- TRI-GRAMS ---- 

 [] 

 TOTAL TRIGRAMS --> 0 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['.']

 TOTAL PORTER STEM WORDS ==> 1



 ---- SNOWBALL STEMMING ----

['.']

 TOTAL SNOWBALL STEM WORDS ==> 1



 ---- LEMMATIZATION ----

['.']

 TOTAL LEMMATIZE WORDS ==> 1

************************************************************************************************************************

89 --> . 


 ---- TOKENS ----

 ['.'] 

 TOTAL TOKENS ==> 1

 ---- POST ----

 [('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['.']

 TOTAL FILTERED TOKENS ==>  1

 ---- POST FOR FILTERED TOKENS ----

 [('.', '.')] 



 ---- BI-GRAMS ---- 

 [] 

 TOTAL BIGRAMS --> 0 



 ---- TRI-GRAMS ---- 

 [] 

 TOTAL TRIGRAMS --> 0 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['.']

 TOTAL PORTER STEM WORDS ==> 1



 ---- SNOWBALL STEMMING ----

['.']

 TOTAL SNOWBALL STEM WORDS ==> 1



 ---- LEMMATIZATION ----

['.']

 TOTAL LEMMATIZE WORDS ==> 1

************************************************************************************************************************

90 --> . 


 ---- TOKENS ----

 ['.'] 

 TOTAL TOKENS ==> 1

 ---- POST ----

 [('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['.']

 TOTAL FILTERED TOKENS ==>  1

 ---- POST FOR FILTERED TOKENS ----

 [('.', '.')] 



 ---- BI-GRAMS ---- 

 [] 

 TOTAL BIGRAMS --> 0 



 ---- TRI-GRAMS ---- 

 [] 

 TOTAL TRIGRAMS --> 0 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['.']

 TOTAL PORTER STEM WORDS ==> 1



 ---- SNOWBALL STEMMING ----

['.']

 TOTAL SNOWBALL STEM WORDS ==> 1



 ---- LEMMATIZATION ----

['.']

 TOTAL LEMMATIZE WORDS ==> 1

************************************************************************************************************************

91 --> . 


 ---- TOKENS ----

 ['.'] 

 TOTAL TOKENS ==> 1

 ---- POST ----

 [('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['.']

 TOTAL FILTERED TOKENS ==>  1

 ---- POST FOR FILTERED TOKENS ----

 [('.', '.')] 



 ---- BI-GRAMS ---- 

 [] 

 TOTAL BIGRAMS --> 0 



 ---- TRI-GRAMS ---- 

 [] 

 TOTAL TRIGRAMS --> 0 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['.']

 TOTAL PORTER STEM WORDS ==> 1



 ---- SNOWBALL STEMMING ----

['.']

 TOTAL SNOWBALL STEM WORDS ==> 1



 ---- LEMMATIZATION ----

['.']

 TOTAL LEMMATIZE WORDS ==> 1

************************************************************************************************************************

92 --> . 


 ---- TOKENS ----

 ['.'] 

 TOTAL TOKENS ==> 1

 ---- POST ----

 [('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['.']

 TOTAL FILTERED TOKENS ==>  1

 ---- POST FOR FILTERED TOKENS ----

 [('.', '.')] 



 ---- BI-GRAMS ---- 

 [] 

 TOTAL BIGRAMS --> 0 



 ---- TRI-GRAMS ---- 

 [] 

 TOTAL TRIGRAMS --> 0 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['.']

 TOTAL PORTER STEM WORDS ==> 1



 ---- SNOWBALL STEMMING ----

['.']

 TOTAL SNOWBALL STEM WORDS ==> 1



 ---- LEMMATIZATION ----

['.']

 TOTAL LEMMATIZE WORDS ==> 1

************************************************************************************************************************

93 --> . 


 ---- TOKENS ----

 ['.'] 

 TOTAL TOKENS ==> 1

 ---- POST ----

 [('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['.']

 TOTAL FILTERED TOKENS ==>  1

 ---- POST FOR FILTERED TOKENS ----

 [('.', '.')] 



 ---- BI-GRAMS ---- 

 [] 

 TOTAL BIGRAMS --> 0 



 ---- TRI-GRAMS ---- 

 [] 

 TOTAL TRIGRAMS --> 0 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['.']

 TOTAL PORTER STEM WORDS ==> 1



 ---- SNOWBALL STEMMING ----

['.']

 TOTAL SNOWBALL STEM WORDS ==> 1



 ---- LEMMATIZATION ----

['.']

 TOTAL LEMMATIZE WORDS ==> 1

************************************************************************************************************************

94 --> . 


 ---- TOKENS ----

 ['.'] 

 TOTAL TOKENS ==> 1

 ---- POST ----

 [('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['.']

 TOTAL FILTERED TOKENS ==>  1

 ---- POST FOR FILTERED TOKENS ----

 [('.', '.')] 



 ---- BI-GRAMS ---- 

 [] 

 TOTAL BIGRAMS --> 0 



 ---- TRI-GRAMS ---- 

 [] 

 TOTAL TRIGRAMS --> 0 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['.']

 TOTAL PORTER STEM WORDS ==> 1



 ---- SNOWBALL STEMMING ----

['.']

 TOTAL SNOWBALL STEM WORDS ==> 1



 ---- LEMMATIZATION ----

['.']

 TOTAL LEMMATIZE WORDS ==> 1

************************************************************************************************************************

95 --> . 


 ---- TOKENS ----

 ['.'] 

 TOTAL TOKENS ==> 1

 ---- POST ----

 [('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['.']

 TOTAL FILTERED TOKENS ==>  1

 ---- POST FOR FILTERED TOKENS ----

 [('.', '.')] 



 ---- BI-GRAMS ---- 

 [] 

 TOTAL BIGRAMS --> 0 



 ---- TRI-GRAMS ---- 

 [] 

 TOTAL TRIGRAMS --> 0 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['.']

 TOTAL PORTER STEM WORDS ==> 1



 ---- SNOWBALL STEMMING ----

['.']

 TOTAL SNOWBALL STEM WORDS ==> 1



 ---- LEMMATIZATION ----

['.']

 TOTAL LEMMATIZE WORDS ==> 1

************************************************************************************************************************

96 --> . 


 ---- TOKENS ----

 ['.'] 

 TOTAL TOKENS ==> 1

 ---- POST ----

 [('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['.']

 TOTAL FILTERED TOKENS ==>  1

 ---- POST FOR FILTERED TOKENS ----

 [('.', '.')] 



 ---- BI-GRAMS ---- 

 [] 

 TOTAL BIGRAMS --> 0 



 ---- TRI-GRAMS ---- 

 [] 

 TOTAL TRIGRAMS --> 0 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['.']

 TOTAL PORTER STEM WORDS ==> 1



 ---- SNOWBALL STEMMING ----

['.']

 TOTAL SNOWBALL STEM WORDS ==> 1



 ---- LEMMATIZATION ----

['.']

 TOTAL LEMMATIZE WORDS ==> 1

************************************************************************************************************************

97 --> . 


 ---- TOKENS ----

 ['.'] 

 TOTAL TOKENS ==> 1

 ---- POST ----

 [('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['.']

 TOTAL FILTERED TOKENS ==>  1

 ---- POST FOR FILTERED TOKENS ----

 [('.', '.')] 



 ---- BI-GRAMS ---- 

 [] 

 TOTAL BIGRAMS --> 0 



 ---- TRI-GRAMS ---- 

 [] 

 TOTAL TRIGRAMS --> 0 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['.']

 TOTAL PORTER STEM WORDS ==> 1



 ---- SNOWBALL STEMMING ----

['.']

 TOTAL SNOWBALL STEM WORDS ==> 1



 ---- LEMMATIZATION ----

['.']

 TOTAL LEMMATIZE WORDS ==> 1

************************************************************************************************************************

98 --> . 


 ---- TOKENS ----

 ['.'] 

 TOTAL TOKENS ==> 1

 ---- POST ----

 [('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['.']

 TOTAL FILTERED TOKENS ==>  1

 ---- POST FOR FILTERED TOKENS ----

 [('.', '.')] 



 ---- BI-GRAMS ---- 

 [] 

 TOTAL BIGRAMS --> 0 



 ---- TRI-GRAMS ---- 

 [] 

 TOTAL TRIGRAMS --> 0 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['.']

 TOTAL PORTER STEM WORDS ==> 1



 ---- SNOWBALL STEMMING ----

['.']

 TOTAL SNOWBALL STEM WORDS ==> 1



 ---- LEMMATIZATION ----

['.']

 TOTAL LEMMATIZE WORDS ==> 1

************************************************************************************************************************

99 --> . 


 ---- TOKENS ----

 ['.'] 

 TOTAL TOKENS ==> 1

 ---- POST ----

 [('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['.']

 TOTAL FILTERED TOKENS ==>  1

 ---- POST FOR FILTERED TOKENS ----

 [('.', '.')] 



 ---- BI-GRAMS ---- 

 [] 

 TOTAL BIGRAMS --> 0 



 ---- TRI-GRAMS ---- 

 [] 

 TOTAL TRIGRAMS --> 0 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['.']

 TOTAL PORTER STEM WORDS ==> 1



 ---- SNOWBALL STEMMING ----

['.']

 TOTAL SNOWBALL STEM WORDS ==> 1



 ---- LEMMATIZATION ----

['.']

 TOTAL LEMMATIZE WORDS ==> 1

************************************************************************************************************************

100 --> 3  Results . 


 ---- TOKENS ----

 ['3', 'Results', '.'] 

 TOTAL TOKENS ==> 3

 ---- POST ----

 [('3', 'CD'), ('Results', 'NNS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['3', 'Results', '.']

 TOTAL FILTERED TOKENS ==>  3

 ---- POST FOR FILTERED TOKENS ----

 [('3', 'CD'), ('Results', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['3 Results', 'Results .'] 

 TOTAL BIGRAMS --> 2 



 ---- TRI-GRAMS ---- 

 ['3 Results .'] 

 TOTAL TRIGRAMS --> 1 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['3', 'result', '.']

 TOTAL PORTER STEM WORDS ==> 3



 ---- SNOWBALL STEMMING ----

['3', 'result', '.']

 TOTAL SNOWBALL STEM WORDS ==> 3



 ---- LEMMATIZATION ----

['3', 'Results', '.']

 TOTAL LEMMATIZE WORDS ==> 3

************************************************************************************************************************

101 --> . 


 ---- TOKENS ----

 ['.'] 

 TOTAL TOKENS ==> 1

 ---- POST ----

 [('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['.']

 TOTAL FILTERED TOKENS ==>  1

 ---- POST FOR FILTERED TOKENS ----

 [('.', '.')] 



 ---- BI-GRAMS ---- 

 [] 

 TOTAL BIGRAMS --> 0 



 ---- TRI-GRAMS ---- 

 [] 

 TOTAL TRIGRAMS --> 0 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['.']

 TOTAL PORTER STEM WORDS ==> 1



 ---- SNOWBALL STEMMING ----

['.']

 TOTAL SNOWBALL STEM WORDS ==> 1



 ---- LEMMATIZATION ----

['.']

 TOTAL LEMMATIZE WORDS ==> 1

************************************************************************************************************************

102 --> . 


 ---- TOKENS ----

 ['.'] 

 TOTAL TOKENS ==> 1

 ---- POST ----

 [('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['.']

 TOTAL FILTERED TOKENS ==>  1

 ---- POST FOR FILTERED TOKENS ----

 [('.', '.')] 



 ---- BI-GRAMS ---- 

 [] 

 TOTAL BIGRAMS --> 0 



 ---- TRI-GRAMS ---- 

 [] 

 TOTAL TRIGRAMS --> 0 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['.']

 TOTAL PORTER STEM WORDS ==> 1



 ---- SNOWBALL STEMMING ----

['.']

 TOTAL SNOWBALL STEM WORDS ==> 1



 ---- LEMMATIZATION ----

['.']

 TOTAL LEMMATIZE WORDS ==> 1

************************************************************************************************************************

103 --> . 


 ---- TOKENS ----

 ['.'] 

 TOTAL TOKENS ==> 1

 ---- POST ----

 [('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['.']

 TOTAL FILTERED TOKENS ==>  1

 ---- POST FOR FILTERED TOKENS ----

 [('.', '.')] 



 ---- BI-GRAMS ---- 

 [] 

 TOTAL BIGRAMS --> 0 



 ---- TRI-GRAMS ---- 

 [] 

 TOTAL TRIGRAMS --> 0 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['.']

 TOTAL PORTER STEM WORDS ==> 1



 ---- SNOWBALL STEMMING ----

['.']

 TOTAL SNOWBALL STEM WORDS ==> 1



 ---- LEMMATIZATION ----

['.']

 TOTAL LEMMATIZE WORDS ==> 1

************************************************************************************************************************

104 --> . 


 ---- TOKENS ----

 ['.'] 

 TOTAL TOKENS ==> 1

 ---- POST ----

 [('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['.']

 TOTAL FILTERED TOKENS ==>  1

 ---- POST FOR FILTERED TOKENS ----

 [('.', '.')] 



 ---- BI-GRAMS ---- 

 [] 

 TOTAL BIGRAMS --> 0 



 ---- TRI-GRAMS ---- 

 [] 

 TOTAL TRIGRAMS --> 0 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['.']

 TOTAL PORTER STEM WORDS ==> 1



 ---- SNOWBALL STEMMING ----

['.']

 TOTAL SNOWBALL STEM WORDS ==> 1



 ---- LEMMATIZATION ----

['.']

 TOTAL LEMMATIZE WORDS ==> 1

************************************************************************************************************************

105 --> . 


 ---- TOKENS ----

 ['.'] 

 TOTAL TOKENS ==> 1

 ---- POST ----

 [('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['.']

 TOTAL FILTERED TOKENS ==>  1

 ---- POST FOR FILTERED TOKENS ----

 [('.', '.')] 



 ---- BI-GRAMS ---- 

 [] 

 TOTAL BIGRAMS --> 0 



 ---- TRI-GRAMS ---- 

 [] 

 TOTAL TRIGRAMS --> 0 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['.']

 TOTAL PORTER STEM WORDS ==> 1



 ---- SNOWBALL STEMMING ----

['.']

 TOTAL SNOWBALL STEM WORDS ==> 1



 ---- LEMMATIZATION ----

['.']

 TOTAL LEMMATIZE WORDS ==> 1

************************************************************************************************************************

106 --> . 


 ---- TOKENS ----

 ['.'] 

 TOTAL TOKENS ==> 1

 ---- POST ----

 [('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['.']

 TOTAL FILTERED TOKENS ==>  1

 ---- POST FOR FILTERED TOKENS ----

 [('.', '.')] 



 ---- BI-GRAMS ---- 

 [] 

 TOTAL BIGRAMS --> 0 



 ---- TRI-GRAMS ---- 

 [] 

 TOTAL TRIGRAMS --> 0 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['.']

 TOTAL PORTER STEM WORDS ==> 1



 ---- SNOWBALL STEMMING ----

['.']

 TOTAL SNOWBALL STEM WORDS ==> 1



 ---- LEMMATIZATION ----

['.']

 TOTAL LEMMATIZE WORDS ==> 1

************************************************************************************************************************

107 --> . 


 ---- TOKENS ----

 ['.'] 

 TOTAL TOKENS ==> 1

 ---- POST ----

 [('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['.']

 TOTAL FILTERED TOKENS ==>  1

 ---- POST FOR FILTERED TOKENS ----

 [('.', '.')] 



 ---- BI-GRAMS ---- 

 [] 

 TOTAL BIGRAMS --> 0 



 ---- TRI-GRAMS ---- 

 [] 

 TOTAL TRIGRAMS --> 0 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['.']

 TOTAL PORTER STEM WORDS ==> 1



 ---- SNOWBALL STEMMING ----

['.']

 TOTAL SNOWBALL STEM WORDS ==> 1



 ---- LEMMATIZATION ----

['.']

 TOTAL LEMMATIZE WORDS ==> 1

************************************************************************************************************************

108 --> . 


 ---- TOKENS ----

 ['.'] 

 TOTAL TOKENS ==> 1

 ---- POST ----

 [('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['.']

 TOTAL FILTERED TOKENS ==>  1

 ---- POST FOR FILTERED TOKENS ----

 [('.', '.')] 



 ---- BI-GRAMS ---- 

 [] 

 TOTAL BIGRAMS --> 0 



 ---- TRI-GRAMS ---- 

 [] 

 TOTAL TRIGRAMS --> 0 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['.']

 TOTAL PORTER STEM WORDS ==> 1



 ---- SNOWBALL STEMMING ----

['.']

 TOTAL SNOWBALL STEM WORDS ==> 1



 ---- LEMMATIZATION ----

['.']

 TOTAL LEMMATIZE WORDS ==> 1

************************************************************************************************************************

109 --> . 


 ---- TOKENS ----

 ['.'] 

 TOTAL TOKENS ==> 1

 ---- POST ----

 [('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['.']

 TOTAL FILTERED TOKENS ==>  1

 ---- POST FOR FILTERED TOKENS ----

 [('.', '.')] 



 ---- BI-GRAMS ---- 

 [] 

 TOTAL BIGRAMS --> 0 



 ---- TRI-GRAMS ---- 

 [] 

 TOTAL TRIGRAMS --> 0 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['.']

 TOTAL PORTER STEM WORDS ==> 1



 ---- SNOWBALL STEMMING ----

['.']

 TOTAL SNOWBALL STEM WORDS ==> 1



 ---- LEMMATIZATION ----

['.']

 TOTAL LEMMATIZE WORDS ==> 1

************************************************************************************************************************

110 --> . 


 ---- TOKENS ----

 ['.'] 

 TOTAL TOKENS ==> 1

 ---- POST ----

 [('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['.']

 TOTAL FILTERED TOKENS ==>  1

 ---- POST FOR FILTERED TOKENS ----

 [('.', '.')] 



 ---- BI-GRAMS ---- 

 [] 

 TOTAL BIGRAMS --> 0 



 ---- TRI-GRAMS ---- 

 [] 

 TOTAL TRIGRAMS --> 0 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['.']

 TOTAL PORTER STEM WORDS ==> 1



 ---- SNOWBALL STEMMING ----

['.']

 TOTAL SNOWBALL STEM WORDS ==> 1



 ---- LEMMATIZATION ----

['.']

 TOTAL LEMMATIZE WORDS ==> 1

************************************************************************************************************************

111 --> . 


 ---- TOKENS ----

 ['.'] 

 TOTAL TOKENS ==> 1

 ---- POST ----

 [('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['.']

 TOTAL FILTERED TOKENS ==>  1

 ---- POST FOR FILTERED TOKENS ----

 [('.', '.')] 



 ---- BI-GRAMS ---- 

 [] 

 TOTAL BIGRAMS --> 0 



 ---- TRI-GRAMS ---- 

 [] 

 TOTAL TRIGRAMS --> 0 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['.']

 TOTAL PORTER STEM WORDS ==> 1



 ---- SNOWBALL STEMMING ----

['.']

 TOTAL SNOWBALL STEM WORDS ==> 1



 ---- LEMMATIZATION ----

['.']

 TOTAL LEMMATIZE WORDS ==> 1

************************************************************************************************************************

112 --> . 


 ---- TOKENS ----

 ['.'] 

 TOTAL TOKENS ==> 1

 ---- POST ----

 [('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['.']

 TOTAL FILTERED TOKENS ==>  1

 ---- POST FOR FILTERED TOKENS ----

 [('.', '.')] 



 ---- BI-GRAMS ---- 

 [] 

 TOTAL BIGRAMS --> 0 



 ---- TRI-GRAMS ---- 

 [] 

 TOTAL TRIGRAMS --> 0 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['.']

 TOTAL PORTER STEM WORDS ==> 1



 ---- SNOWBALL STEMMING ----

['.']

 TOTAL SNOWBALL STEM WORDS ==> 1



 ---- LEMMATIZATION ----

['.']

 TOTAL LEMMATIZE WORDS ==> 1

************************************************************************************************************************

113 --> . 


 ---- TOKENS ----

 ['.'] 

 TOTAL TOKENS ==> 1

 ---- POST ----

 [('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['.']

 TOTAL FILTERED TOKENS ==>  1

 ---- POST FOR FILTERED TOKENS ----

 [('.', '.')] 



 ---- BI-GRAMS ---- 

 [] 

 TOTAL BIGRAMS --> 0 



 ---- TRI-GRAMS ---- 

 [] 

 TOTAL TRIGRAMS --> 0 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['.']

 TOTAL PORTER STEM WORDS ==> 1



 ---- SNOWBALL STEMMING ----

['.']

 TOTAL SNOWBALL STEM WORDS ==> 1



 ---- LEMMATIZATION ----

['.']

 TOTAL LEMMATIZE WORDS ==> 1

************************************************************************************************************************

114 --> . 


 ---- TOKENS ----

 ['.'] 

 TOTAL TOKENS ==> 1

 ---- POST ----

 [('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['.']

 TOTAL FILTERED TOKENS ==>  1

 ---- POST FOR FILTERED TOKENS ----

 [('.', '.')] 



 ---- BI-GRAMS ---- 

 [] 

 TOTAL BIGRAMS --> 0 



 ---- TRI-GRAMS ---- 

 [] 

 TOTAL TRIGRAMS --> 0 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['.']

 TOTAL PORTER STEM WORDS ==> 1



 ---- SNOWBALL STEMMING ----

['.']

 TOTAL SNOWBALL STEM WORDS ==> 1



 ---- LEMMATIZATION ----

['.']

 TOTAL LEMMATIZE WORDS ==> 1

************************************************************************************************************************

115 --> . 


 ---- TOKENS ----

 ['.'] 

 TOTAL TOKENS ==> 1

 ---- POST ----

 [('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['.']

 TOTAL FILTERED TOKENS ==>  1

 ---- POST FOR FILTERED TOKENS ----

 [('.', '.')] 



 ---- BI-GRAMS ---- 

 [] 

 TOTAL BIGRAMS --> 0 



 ---- TRI-GRAMS ---- 

 [] 

 TOTAL TRIGRAMS --> 0 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['.']

 TOTAL PORTER STEM WORDS ==> 1



 ---- SNOWBALL STEMMING ----

['.']

 TOTAL SNOWBALL STEM WORDS ==> 1



 ---- LEMMATIZATION ----

['.']

 TOTAL LEMMATIZE WORDS ==> 1

************************************************************************************************************************

116 --> . 


 ---- TOKENS ----

 ['.'] 

 TOTAL TOKENS ==> 1

 ---- POST ----

 [('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['.']

 TOTAL FILTERED TOKENS ==>  1

 ---- POST FOR FILTERED TOKENS ----

 [('.', '.')] 



 ---- BI-GRAMS ---- 

 [] 

 TOTAL BIGRAMS --> 0 



 ---- TRI-GRAMS ---- 

 [] 

 TOTAL TRIGRAMS --> 0 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['.']

 TOTAL PORTER STEM WORDS ==> 1



 ---- SNOWBALL STEMMING ----

['.']

 TOTAL SNOWBALL STEM WORDS ==> 1



 ---- LEMMATIZATION ----

['.']

 TOTAL LEMMATIZE WORDS ==> 1

************************************************************************************************************************

117 --> . 


 ---- TOKENS ----

 ['.'] 

 TOTAL TOKENS ==> 1

 ---- POST ----

 [('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['.']

 TOTAL FILTERED TOKENS ==>  1

 ---- POST FOR FILTERED TOKENS ----

 [('.', '.')] 



 ---- BI-GRAMS ---- 

 [] 

 TOTAL BIGRAMS --> 0 



 ---- TRI-GRAMS ---- 

 [] 

 TOTAL TRIGRAMS --> 0 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['.']

 TOTAL PORTER STEM WORDS ==> 1



 ---- SNOWBALL STEMMING ----

['.']

 TOTAL SNOWBALL STEM WORDS ==> 1



 ---- LEMMATIZATION ----

['.']

 TOTAL LEMMATIZE WORDS ==> 1

************************************************************************************************************************

118 --> . 


 ---- TOKENS ----

 ['.'] 

 TOTAL TOKENS ==> 1

 ---- POST ----

 [('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['.']

 TOTAL FILTERED TOKENS ==>  1

 ---- POST FOR FILTERED TOKENS ----

 [('.', '.')] 



 ---- BI-GRAMS ---- 

 [] 

 TOTAL BIGRAMS --> 0 



 ---- TRI-GRAMS ---- 

 [] 

 TOTAL TRIGRAMS --> 0 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['.']

 TOTAL PORTER STEM WORDS ==> 1



 ---- SNOWBALL STEMMING ----

['.']

 TOTAL SNOWBALL STEM WORDS ==> 1



 ---- LEMMATIZATION ----

['.']

 TOTAL LEMMATIZE WORDS ==> 1

************************************************************************************************************************

119 --> . 


 ---- TOKENS ----

 ['.'] 

 TOTAL TOKENS ==> 1

 ---- POST ----

 [('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['.']

 TOTAL FILTERED TOKENS ==>  1

 ---- POST FOR FILTERED TOKENS ----

 [('.', '.')] 



 ---- BI-GRAMS ---- 

 [] 

 TOTAL BIGRAMS --> 0 



 ---- TRI-GRAMS ---- 

 [] 

 TOTAL TRIGRAMS --> 0 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['.']

 TOTAL PORTER STEM WORDS ==> 1



 ---- SNOWBALL STEMMING ----

['.']

 TOTAL SNOWBALL STEM WORDS ==> 1



 ---- LEMMATIZATION ----

['.']

 TOTAL LEMMATIZE WORDS ==> 1

************************************************************************************************************************

120 --> . 


 ---- TOKENS ----

 ['.'] 

 TOTAL TOKENS ==> 1

 ---- POST ----

 [('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['.']

 TOTAL FILTERED TOKENS ==>  1

 ---- POST FOR FILTERED TOKENS ----

 [('.', '.')] 



 ---- BI-GRAMS ---- 

 [] 

 TOTAL BIGRAMS --> 0 



 ---- TRI-GRAMS ---- 

 [] 

 TOTAL TRIGRAMS --> 0 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['.']

 TOTAL PORTER STEM WORDS ==> 1



 ---- SNOWBALL STEMMING ----

['.']

 TOTAL SNOWBALL STEM WORDS ==> 1



 ---- LEMMATIZATION ----

['.']

 TOTAL LEMMATIZE WORDS ==> 1

************************************************************************************************************************

121 --> . 


 ---- TOKENS ----

 ['.'] 

 TOTAL TOKENS ==> 1

 ---- POST ----

 [('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['.']

 TOTAL FILTERED TOKENS ==>  1

 ---- POST FOR FILTERED TOKENS ----

 [('.', '.')] 



 ---- BI-GRAMS ---- 

 [] 

 TOTAL BIGRAMS --> 0 



 ---- TRI-GRAMS ---- 

 [] 

 TOTAL TRIGRAMS --> 0 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['.']

 TOTAL PORTER STEM WORDS ==> 1



 ---- SNOWBALL STEMMING ----

['.']

 TOTAL SNOWBALL STEM WORDS ==> 1



 ---- LEMMATIZATION ----

['.']

 TOTAL LEMMATIZE WORDS ==> 1

************************************************************************************************************************

122 --> . 


 ---- TOKENS ----

 ['.'] 

 TOTAL TOKENS ==> 1

 ---- POST ----

 [('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['.']

 TOTAL FILTERED TOKENS ==>  1

 ---- POST FOR FILTERED TOKENS ----

 [('.', '.')] 



 ---- BI-GRAMS ---- 

 [] 

 TOTAL BIGRAMS --> 0 



 ---- TRI-GRAMS ---- 

 [] 

 TOTAL TRIGRAMS --> 0 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['.']

 TOTAL PORTER STEM WORDS ==> 1



 ---- SNOWBALL STEMMING ----

['.']

 TOTAL SNOWBALL STEM WORDS ==> 1



 ---- LEMMATIZATION ----

['.']

 TOTAL LEMMATIZE WORDS ==> 1

************************************************************************************************************************

123 --> . 


 ---- TOKENS ----

 ['.'] 

 TOTAL TOKENS ==> 1

 ---- POST ----

 [('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['.']

 TOTAL FILTERED TOKENS ==>  1

 ---- POST FOR FILTERED TOKENS ----

 [('.', '.')] 



 ---- BI-GRAMS ---- 

 [] 

 TOTAL BIGRAMS --> 0 



 ---- TRI-GRAMS ---- 

 [] 

 TOTAL TRIGRAMS --> 0 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['.']

 TOTAL PORTER STEM WORDS ==> 1



 ---- SNOWBALL STEMMING ----

['.']

 TOTAL SNOWBALL STEM WORDS ==> 1



 ---- LEMMATIZATION ----

['.']

 TOTAL LEMMATIZE WORDS ==> 1

************************************************************************************************************************

124 --> . 


 ---- TOKENS ----

 ['.'] 

 TOTAL TOKENS ==> 1

 ---- POST ----

 [('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['.']

 TOTAL FILTERED TOKENS ==>  1

 ---- POST FOR FILTERED TOKENS ----

 [('.', '.')] 



 ---- BI-GRAMS ---- 

 [] 

 TOTAL BIGRAMS --> 0 



 ---- TRI-GRAMS ---- 

 [] 

 TOTAL TRIGRAMS --> 0 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['.']

 TOTAL PORTER STEM WORDS ==> 1



 ---- SNOWBALL STEMMING ----

['.']

 TOTAL SNOWBALL STEM WORDS ==> 1



 ---- LEMMATIZATION ----

['.']

 TOTAL LEMMATIZE WORDS ==> 1

************************************************************************************************************************

125 --> . 


 ---- TOKENS ----

 ['.'] 

 TOTAL TOKENS ==> 1

 ---- POST ----

 [('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['.']

 TOTAL FILTERED TOKENS ==>  1

 ---- POST FOR FILTERED TOKENS ----

 [('.', '.')] 



 ---- BI-GRAMS ---- 

 [] 

 TOTAL BIGRAMS --> 0 



 ---- TRI-GRAMS ---- 

 [] 

 TOTAL TRIGRAMS --> 0 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['.']

 TOTAL PORTER STEM WORDS ==> 1



 ---- SNOWBALL STEMMING ----

['.']

 TOTAL SNOWBALL STEM WORDS ==> 1



 ---- LEMMATIZATION ----

['.']

 TOTAL LEMMATIZE WORDS ==> 1

************************************************************************************************************************

126 --> . 


 ---- TOKENS ----

 ['.'] 

 TOTAL TOKENS ==> 1

 ---- POST ----

 [('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['.']

 TOTAL FILTERED TOKENS ==>  1

 ---- POST FOR FILTERED TOKENS ----

 [('.', '.')] 



 ---- BI-GRAMS ---- 

 [] 

 TOTAL BIGRAMS --> 0 



 ---- TRI-GRAMS ---- 

 [] 

 TOTAL TRIGRAMS --> 0 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['.']

 TOTAL PORTER STEM WORDS ==> 1



 ---- SNOWBALL STEMMING ----

['.']

 TOTAL SNOWBALL STEM WORDS ==> 1



 ---- LEMMATIZATION ----

['.']

 TOTAL LEMMATIZE WORDS ==> 1

************************************************************************************************************************

127 --> . 


 ---- TOKENS ----

 ['.'] 

 TOTAL TOKENS ==> 1

 ---- POST ----

 [('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['.']

 TOTAL FILTERED TOKENS ==>  1

 ---- POST FOR FILTERED TOKENS ----

 [('.', '.')] 



 ---- BI-GRAMS ---- 

 [] 

 TOTAL BIGRAMS --> 0 



 ---- TRI-GRAMS ---- 

 [] 

 TOTAL TRIGRAMS --> 0 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['.']

 TOTAL PORTER STEM WORDS ==> 1



 ---- SNOWBALL STEMMING ----

['.']

 TOTAL SNOWBALL STEM WORDS ==> 1



 ---- LEMMATIZATION ----

['.']

 TOTAL LEMMATIZE WORDS ==> 1

************************************************************************************************************************

128 --> . 


 ---- TOKENS ----

 ['.'] 

 TOTAL TOKENS ==> 1

 ---- POST ----

 [('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['.']

 TOTAL FILTERED TOKENS ==>  1

 ---- POST FOR FILTERED TOKENS ----

 [('.', '.')] 



 ---- BI-GRAMS ---- 

 [] 

 TOTAL BIGRAMS --> 0 



 ---- TRI-GRAMS ---- 

 [] 

 TOTAL TRIGRAMS --> 0 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['.']

 TOTAL PORTER STEM WORDS ==> 1



 ---- SNOWBALL STEMMING ----

['.']

 TOTAL SNOWBALL STEM WORDS ==> 1



 ---- LEMMATIZATION ----

['.']

 TOTAL LEMMATIZE WORDS ==> 1

************************************************************************************************************************

129 --> . 


 ---- TOKENS ----

 ['.'] 

 TOTAL TOKENS ==> 1

 ---- POST ----

 [('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['.']

 TOTAL FILTERED TOKENS ==>  1

 ---- POST FOR FILTERED TOKENS ----

 [('.', '.')] 



 ---- BI-GRAMS ---- 

 [] 

 TOTAL BIGRAMS --> 0 



 ---- TRI-GRAMS ---- 

 [] 

 TOTAL TRIGRAMS --> 0 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['.']

 TOTAL PORTER STEM WORDS ==> 1



 ---- SNOWBALL STEMMING ----

['.']

 TOTAL SNOWBALL STEM WORDS ==> 1



 ---- LEMMATIZATION ----

['.']

 TOTAL LEMMATIZE WORDS ==> 1

************************************************************************************************************************

130 --> . 


 ---- TOKENS ----

 ['.'] 

 TOTAL TOKENS ==> 1

 ---- POST ----

 [('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['.']

 TOTAL FILTERED TOKENS ==>  1

 ---- POST FOR FILTERED TOKENS ----

 [('.', '.')] 



 ---- BI-GRAMS ---- 

 [] 

 TOTAL BIGRAMS --> 0 



 ---- TRI-GRAMS ---- 

 [] 

 TOTAL TRIGRAMS --> 0 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['.']

 TOTAL PORTER STEM WORDS ==> 1



 ---- SNOWBALL STEMMING ----

['.']

 TOTAL SNOWBALL STEM WORDS ==> 1



 ---- LEMMATIZATION ----

['.']

 TOTAL LEMMATIZE WORDS ==> 1

************************************************************************************************************************

131 --> . 


 ---- TOKENS ----

 ['.'] 

 TOTAL TOKENS ==> 1

 ---- POST ----

 [('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['.']

 TOTAL FILTERED TOKENS ==>  1

 ---- POST FOR FILTERED TOKENS ----

 [('.', '.')] 



 ---- BI-GRAMS ---- 

 [] 

 TOTAL BIGRAMS --> 0 



 ---- TRI-GRAMS ---- 

 [] 

 TOTAL TRIGRAMS --> 0 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['.']

 TOTAL PORTER STEM WORDS ==> 1



 ---- SNOWBALL STEMMING ----

['.']

 TOTAL SNOWBALL STEM WORDS ==> 1



 ---- LEMMATIZATION ----

['.']

 TOTAL LEMMATIZE WORDS ==> 1

************************************************************************************************************************

132 --> . 


 ---- TOKENS ----

 ['.'] 

 TOTAL TOKENS ==> 1

 ---- POST ----

 [('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['.']

 TOTAL FILTERED TOKENS ==>  1

 ---- POST FOR FILTERED TOKENS ----

 [('.', '.')] 



 ---- BI-GRAMS ---- 

 [] 

 TOTAL BIGRAMS --> 0 



 ---- TRI-GRAMS ---- 

 [] 

 TOTAL TRIGRAMS --> 0 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['.']

 TOTAL PORTER STEM WORDS ==> 1



 ---- SNOWBALL STEMMING ----

['.']

 TOTAL SNOWBALL STEM WORDS ==> 1



 ---- LEMMATIZATION ----

['.']

 TOTAL LEMMATIZE WORDS ==> 1

************************************************************************************************************************

133 --> 3 TIPS FOR YOUR PROJECT                                              3 KEY TAKEAWAYS                                                        4 TO LEARN MORE                                                         4  1 DELL TECHNOLOGIES WHITE PAPER NATURAL LANGUAGE PROCESSING Natural language processing is a form of artificial intelligence that allows a computer  application to understand human language, either spoken or written. 


 ---- TOKENS ----

 ['3', 'TIPS', 'FOR', 'YOUR', 'PROJECT', '3', 'KEY', 'TAKEAWAYS', '4', 'TO', 'LEARN', 'MORE', '4', '1', 'DELL', 'TECHNOLOGIES', 'WHITE', 'PAPER', 'NATURAL', 'LANGUAGE', 'PROCESSING', 'Natural', 'language', 'processing', 'is', 'a', 'form', 'of', 'artificial', 'intelligence', 'that', 'allows', 'a', 'computer', 'application', 'to', 'understand', 'human', 'language', ',', 'either', 'spoken', 'or', 'written', '.'] 

 TOTAL TOKENS ==> 45

 ---- POST ----

 [('3', 'CD'), ('TIPS', 'NNP'), ('FOR', 'NNP'), ('YOUR', 'NNP'), ('PROJECT', 'NNP'), ('3', 'CD'), ('KEY', 'NNP'), ('TAKEAWAYS', 'NNP'), ('4', 'CD'), ('TO', 'NNP'), ('LEARN', 'NNP'), ('MORE', 'NNP'), ('4', 'CD'), ('1', 'CD'), ('DELL', 'NNP'), ('TECHNOLOGIES', 'NNP'), ('WHITE', 'NNP'), ('PAPER', 'NNP'), ('NATURAL', 'NNP'), ('LANGUAGE', 'NNP'), ('PROCESSING', 'NNP'), ('Natural', 'NNP'), ('language', 'NN'), ('processing', 'NN'), ('is', 'VBZ'), ('a', 'DT'), ('form', 'NN'), ('of', 'IN'), ('artificial', 'JJ'), ('intelligence', 'NN'), ('that', 'WDT'), ('allows', 'VBZ'), ('a', 'DT'), ('computer', 'NN'), ('application', 'NN'), ('to', 'TO'), ('understand', 'VB'), ('human', 'JJ'), ('language', 'NN'), (',', ','), ('either', 'RB'), ('spoken', 'VBD'), ('or', 'CC'), ('written', 'VBN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['3', 'TIPS', 'PROJECT', '3', 'KEY', 'TAKEAWAYS', '4', 'LEARN', '4', '1', 'DELL', 'TECHNOLOGIES', 'WHITE', 'PAPER', 'NATURAL', 'LANGUAGE', 'PROCESSING', 'Natural', 'language', 'processing', 'form', 'artificial', 'intelligence', 'allows', 'computer', 'application', 'understand', 'human', 'language', ',', 'either', 'spoken', 'written', '.']

 TOTAL FILTERED TOKENS ==>  34

 ---- POST FOR FILTERED TOKENS ----

 [('3', 'CD'), ('TIPS', 'NNP'), ('PROJECT', 'NNP'), ('3', 'CD'), ('KEY', 'NNP'), ('TAKEAWAYS', 'NNP'), ('4', 'CD'), ('LEARN', 'NNP'), ('4', 'CD'), ('1', 'CD'), ('DELL', 'NNP'), ('TECHNOLOGIES', 'NNP'), ('WHITE', 'NNP'), ('PAPER', 'NNP'), ('NATURAL', 'NNP'), ('LANGUAGE', 'NNP'), ('PROCESSING', 'NNP'), ('Natural', 'NNP'), ('language', 'NN'), ('processing', 'NN'), ('form', 'NN'), ('artificial', 'JJ'), ('intelligence', 'NN'), ('allows', 'VBZ'), ('computer', 'NN'), ('application', 'NN'), ('understand', 'VBP'), ('human', 'JJ'), ('language', 'NN'), (',', ','), ('either', 'RB'), ('spoken', 'JJ'), ('written', 'VBN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['3 TIPS', 'TIPS PROJECT', 'PROJECT 3', '3 KEY', 'KEY TAKEAWAYS', 'TAKEAWAYS 4', '4 LEARN', 'LEARN 4', '4 1', '1 DELL', 'DELL TECHNOLOGIES', 'TECHNOLOGIES WHITE', 'WHITE PAPER', 'PAPER NATURAL', 'NATURAL LANGUAGE', 'LANGUAGE PROCESSING', 'PROCESSING Natural', 'Natural language', 'language processing', 'processing form', 'form artificial', 'artificial intelligence', 'intelligence allows', 'allows computer', 'computer application', 'application understand', 'understand human', 'human language', 'language ,', ', either', 'either spoken', 'spoken written', 'written .'] 

 TOTAL BIGRAMS --> 33 



 ---- TRI-GRAMS ---- 

 ['3 TIPS PROJECT', 'TIPS PROJECT 3', 'PROJECT 3 KEY', '3 KEY TAKEAWAYS', 'KEY TAKEAWAYS 4', 'TAKEAWAYS 4 LEARN', '4 LEARN 4', 'LEARN 4 1', '4 1 DELL', '1 DELL TECHNOLOGIES', 'DELL TECHNOLOGIES WHITE', 'TECHNOLOGIES WHITE PAPER', 'WHITE PAPER NATURAL', 'PAPER NATURAL LANGUAGE', 'NATURAL LANGUAGE PROCESSING', 'LANGUAGE PROCESSING Natural', 'PROCESSING Natural language', 'Natural language processing', 'language processing form', 'processing form artificial', 'form artificial intelligence', 'artificial intelligence allows', 'intelligence allows computer', 'allows computer application', 'computer application understand', 'application understand human', 'understand human language', 'human language ,', 'language , either', ', either spoken', 'either spoken written', 'spoken written .'] 

 TOTAL TRIGRAMS --> 32 



 ---- NOUN PHRASES ---- 

 ['language', 'processing', 'form', 'artificial intelligence', 'computer', 'application', 'human language'] 

 TOTAL NOUN PHRASES --> 7 



 ---- NER ----

 
 ORGANIZATION ---> ['TIPS', 'KEY', 'DELL', 'WHITE']
 TOTAL ORGANIZATION ENTITY --> 4 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['3', 'tip', 'project', '3', 'key', 'takeaway', '4', 'learn', '4', '1', 'dell', 'technolog', 'white', 'paper', 'natur', 'languag', 'process', 'natur', 'languag', 'process', 'form', 'artifici', 'intellig', 'allow', 'comput', 'applic', 'understand', 'human', 'languag', ',', 'either', 'spoken', 'written', '.']

 TOTAL PORTER STEM WORDS ==> 34



 ---- SNOWBALL STEMMING ----

['3', 'tip', 'project', '3', 'key', 'takeaway', '4', 'learn', '4', '1', 'dell', 'technolog', 'white', 'paper', 'natur', 'languag', 'process', 'natur', 'languag', 'process', 'form', 'artifici', 'intellig', 'allow', 'comput', 'applic', 'understand', 'human', 'languag', ',', 'either', 'spoken', 'written', '.']

 TOTAL SNOWBALL STEM WORDS ==> 34



 ---- LEMMATIZATION ----

['3', 'TIPS', 'PROJECT', '3', 'KEY', 'TAKEAWAYS', '4', 'LEARN', '4', '1', 'DELL', 'TECHNOLOGIES', 'WHITE', 'PAPER', 'NATURAL', 'LANGUAGE', 'PROCESSING', 'Natural', 'language', 'processing', 'form', 'artificial', 'intelligence', 'allows', 'computer', 'application', 'understand', 'human', 'language', ',', 'either', 'spoken', 'written', '.']

 TOTAL LEMMATIZE WORDS ==> 34

************************************************************************************************************************

134 --> The concept of NLP  encompasses coding, understanding, interpreting and manipulating human language. 


 ---- TOKENS ----

 ['The', 'concept', 'of', 'NLP', 'encompasses', 'coding', ',', 'understanding', ',', 'interpreting', 'and', 'manipulating', 'human', 'language', '.'] 

 TOTAL TOKENS ==> 15

 ---- POST ----

 [('The', 'DT'), ('concept', 'NN'), ('of', 'IN'), ('NLP', 'NNP'), ('encompasses', 'VBZ'), ('coding', 'VBG'), (',', ','), ('understanding', 'VBG'), (',', ','), ('interpreting', 'VBG'), ('and', 'CC'), ('manipulating', 'VBG'), ('human', 'JJ'), ('language', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['concept', 'NLP', 'encompasses', 'coding', ',', 'understanding', ',', 'interpreting', 'manipulating', 'human', 'language', '.']

 TOTAL FILTERED TOKENS ==>  12

 ---- POST FOR FILTERED TOKENS ----

 [('concept', 'NN'), ('NLP', 'NNP'), ('encompasses', 'VBZ'), ('coding', 'VBG'), (',', ','), ('understanding', 'VBG'), (',', ','), ('interpreting', 'VBG'), ('manipulating', 'VBG'), ('human', 'JJ'), ('language', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['concept NLP', 'NLP encompasses', 'encompasses coding', 'coding ,', ', understanding', 'understanding ,', ', interpreting', 'interpreting manipulating', 'manipulating human', 'human language', 'language .'] 

 TOTAL BIGRAMS --> 11 



 ---- TRI-GRAMS ---- 

 ['concept NLP encompasses', 'NLP encompasses coding', 'encompasses coding ,', 'coding , understanding', ', understanding ,', 'understanding , interpreting', ', interpreting manipulating', 'interpreting manipulating human', 'manipulating human language', 'human language .'] 

 TOTAL TRIGRAMS --> 10 



 ---- NOUN PHRASES ---- 

 ['concept', 'human language'] 

 TOTAL NOUN PHRASES --> 2 



 ---- NER ----

 
 ORGANIZATION ---> ['NLP']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['concept', 'nlp', 'encompass', 'code', ',', 'understand', ',', 'interpret', 'manipul', 'human', 'languag', '.']

 TOTAL PORTER STEM WORDS ==> 12



 ---- SNOWBALL STEMMING ----

['concept', 'nlp', 'encompass', 'code', ',', 'understand', ',', 'interpret', 'manipul', 'human', 'languag', '.']

 TOTAL SNOWBALL STEM WORDS ==> 12



 ---- LEMMATIZATION ----

['concept', 'NLP', 'encompasses', 'coding', ',', 'understanding', ',', 'interpreting', 'manipulating', 'human', 'language', '.']

 TOTAL LEMMATIZE WORDS ==> 12

************************************************************************************************************************

135 --> NLP  applications use computers to translate languages, convert voice to text and text to voice,  and create human-like conversational agents to help customers, employees and others  deal with issues, questions and concerns. 


 ---- TOKENS ----

 ['NLP', 'applications', 'use', 'computers', 'to', 'translate', 'languages', ',', 'convert', 'voice', 'to', 'text', 'and', 'text', 'to', 'voice', ',', 'and', 'create', 'human-like', 'conversational', 'agents', 'to', 'help', 'customers', ',', 'employees', 'and', 'others', 'deal', 'with', 'issues', ',', 'questions', 'and', 'concerns', '.'] 

 TOTAL TOKENS ==> 37

 ---- POST ----

 [('NLP', 'NNP'), ('applications', 'NNS'), ('use', 'VBP'), ('computers', 'NNS'), ('to', 'TO'), ('translate', 'VB'), ('languages', 'NNS'), (',', ','), ('convert', 'JJ'), ('voice', 'NN'), ('to', 'TO'), ('text', 'VB'), ('and', 'CC'), ('text', 'VB'), ('to', 'TO'), ('voice', 'VB'), (',', ','), ('and', 'CC'), ('create', 'VB'), ('human-like', 'JJ'), ('conversational', 'JJ'), ('agents', 'NNS'), ('to', 'TO'), ('help', 'VB'), ('customers', 'NNS'), (',', ','), ('employees', 'NNS'), ('and', 'CC'), ('others', 'NNS'), ('deal', 'VBP'), ('with', 'IN'), ('issues', 'NNS'), (',', ','), ('questions', 'NNS'), ('and', 'CC'), ('concerns', 'NNS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['NLP', 'applications', 'use', 'computers', 'translate', 'languages', ',', 'convert', 'voice', 'text', 'text', 'voice', ',', 'create', 'human-like', 'conversational', 'agents', 'help', 'customers', ',', 'employees', 'others', 'deal', 'issues', ',', 'questions', 'concerns', '.']

 TOTAL FILTERED TOKENS ==>  28

 ---- POST FOR FILTERED TOKENS ----

 [('NLP', 'NNP'), ('applications', 'NNS'), ('use', 'VBP'), ('computers', 'NNS'), ('translate', 'JJ'), ('languages', 'NNS'), (',', ','), ('convert', 'JJ'), ('voice', 'NN'), ('text', 'NN'), ('text', 'JJ'), ('voice', 'NN'), (',', ','), ('create', 'VB'), ('human-like', 'JJ'), ('conversational', 'JJ'), ('agents', 'NNS'), ('help', 'NN'), ('customers', 'NNS'), (',', ','), ('employees', 'NNS'), ('others', 'NNS'), ('deal', 'VBP'), ('issues', 'NNS'), (',', ','), ('questions', 'NNS'), ('concerns', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['NLP applications', 'applications use', 'use computers', 'computers translate', 'translate languages', 'languages ,', ', convert', 'convert voice', 'voice text', 'text text', 'text voice', 'voice ,', ', create', 'create human-like', 'human-like conversational', 'conversational agents', 'agents help', 'help customers', 'customers ,', ', employees', 'employees others', 'others deal', 'deal issues', 'issues ,', ', questions', 'questions concerns', 'concerns .'] 

 TOTAL BIGRAMS --> 27 



 ---- TRI-GRAMS ---- 

 ['NLP applications use', 'applications use computers', 'use computers translate', 'computers translate languages', 'translate languages ,', 'languages , convert', ', convert voice', 'convert voice text', 'voice text text', 'text text voice', 'text voice ,', 'voice , create', ', create human-like', 'create human-like conversational', 'human-like conversational agents', 'conversational agents help', 'agents help customers', 'help customers ,', 'customers , employees', ', employees others', 'employees others deal', 'others deal issues', 'deal issues ,', 'issues , questions', ', questions concerns', 'questions concerns .'] 

 TOTAL TRIGRAMS --> 26 



 ---- NOUN PHRASES ---- 

 ['convert voice', 'text', 'text voice', 'help'] 

 TOTAL NOUN PHRASES --> 4 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['nlp', 'applic', 'use', 'comput', 'translat', 'languag', ',', 'convert', 'voic', 'text', 'text', 'voic', ',', 'creat', 'human-lik', 'convers', 'agent', 'help', 'custom', ',', 'employe', 'other', 'deal', 'issu', ',', 'question', 'concern', '.']

 TOTAL PORTER STEM WORDS ==> 28



 ---- SNOWBALL STEMMING ----

['nlp', 'applic', 'use', 'comput', 'translat', 'languag', ',', 'convert', 'voic', 'text', 'text', 'voic', ',', 'creat', 'human-lik', 'convers', 'agent', 'help', 'custom', ',', 'employe', 'other', 'deal', 'issu', ',', 'question', 'concern', '.']

 TOTAL SNOWBALL STEM WORDS ==> 28



 ---- LEMMATIZATION ----

['NLP', 'application', 'use', 'computer', 'translate', 'language', ',', 'convert', 'voice', 'text', 'text', 'voice', ',', 'create', 'human-like', 'conversational', 'agent', 'help', 'customer', ',', 'employee', 'others', 'deal', 'issue', ',', 'question', 'concern', '.']

 TOTAL LEMMATIZE WORDS ==> 28

************************************************************************************************************************

136 --> In recent years, the field of NLP has been transformed by the shift from statistical machine  learning methods to the use of neural networks and deep learning. 


 ---- TOKENS ----

 ['In', 'recent', 'years', ',', 'the', 'field', 'of', 'NLP', 'has', 'been', 'transformed', 'by', 'the', 'shift', 'from', 'statistical', 'machine', 'learning', 'methods', 'to', 'the', 'use', 'of', 'neural', 'networks', 'and', 'deep', 'learning', '.'] 

 TOTAL TOKENS ==> 29

 ---- POST ----

 [('In', 'IN'), ('recent', 'JJ'), ('years', 'NNS'), (',', ','), ('the', 'DT'), ('field', 'NN'), ('of', 'IN'), ('NLP', 'NNP'), ('has', 'VBZ'), ('been', 'VBN'), ('transformed', 'VBN'), ('by', 'IN'), ('the', 'DT'), ('shift', 'NN'), ('from', 'IN'), ('statistical', 'JJ'), ('machine', 'NN'), ('learning', 'VBG'), ('methods', 'NNS'), ('to', 'TO'), ('the', 'DT'), ('use', 'NN'), ('of', 'IN'), ('neural', 'JJ'), ('networks', 'NNS'), ('and', 'CC'), ('deep', 'JJ'), ('learning', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['recent', 'years', ',', 'field', 'NLP', 'transformed', 'shift', 'statistical', 'machine', 'learning', 'methods', 'use', 'neural', 'networks', 'deep', 'learning', '.']

 TOTAL FILTERED TOKENS ==>  17

 ---- POST FOR FILTERED TOKENS ----

 [('recent', 'JJ'), ('years', 'NNS'), (',', ','), ('field', 'NN'), ('NLP', 'NNP'), ('transformed', 'VBD'), ('shift', 'JJ'), ('statistical', 'JJ'), ('machine', 'NN'), ('learning', 'VBG'), ('methods', 'NNS'), ('use', 'RB'), ('neural', 'JJ'), ('networks', 'NNS'), ('deep', 'RB'), ('learning', 'VBG'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['recent years', 'years ,', ', field', 'field NLP', 'NLP transformed', 'transformed shift', 'shift statistical', 'statistical machine', 'machine learning', 'learning methods', 'methods use', 'use neural', 'neural networks', 'networks deep', 'deep learning', 'learning .'] 

 TOTAL BIGRAMS --> 16 



 ---- TRI-GRAMS ---- 

 ['recent years ,', 'years , field', ', field NLP', 'field NLP transformed', 'NLP transformed shift', 'transformed shift statistical', 'shift statistical machine', 'statistical machine learning', 'machine learning methods', 'learning methods use', 'methods use neural', 'use neural networks', 'neural networks deep', 'networks deep learning', 'deep learning .'] 

 TOTAL TRIGRAMS --> 15 



 ---- NOUN PHRASES ---- 

 ['field', 'shift statistical machine'] 

 TOTAL NOUN PHRASES --> 2 



 ---- NER ----

 
 ORGANIZATION ---> ['NLP']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['recent', 'year', ',', 'field', 'nlp', 'transform', 'shift', 'statist', 'machin', 'learn', 'method', 'use', 'neural', 'network', 'deep', 'learn', '.']

 TOTAL PORTER STEM WORDS ==> 17



 ---- SNOWBALL STEMMING ----

['recent', 'year', ',', 'field', 'nlp', 'transform', 'shift', 'statist', 'machin', 'learn', 'method', 'use', 'neural', 'network', 'deep', 'learn', '.']

 TOTAL SNOWBALL STEM WORDS ==> 17



 ---- LEMMATIZATION ----

['recent', 'year', ',', 'field', 'NLP', 'transformed', 'shift', 'statistical', 'machine', 'learning', 'method', 'use', 'neural', 'network', 'deep', 'learning', '.']

 TOTAL LEMMATIZE WORDS ==> 17

************************************************************************************************************************

137 --> With these new  approaches, it is now possible to build automated systems that can interact with people  more naturally than ever before. 


 ---- TOKENS ----

 ['With', 'these', 'new', 'approaches', ',', 'it', 'is', 'now', 'possible', 'to', 'build', 'automated', 'systems', 'that', 'can', 'interact', 'with', 'people', 'more', 'naturally', 'than', 'ever', 'before', '.'] 

 TOTAL TOKENS ==> 24

 ---- POST ----

 [('With', 'IN'), ('these', 'DT'), ('new', 'JJ'), ('approaches', 'NNS'), (',', ','), ('it', 'PRP'), ('is', 'VBZ'), ('now', 'RB'), ('possible', 'JJ'), ('to', 'TO'), ('build', 'VB'), ('automated', 'JJ'), ('systems', 'NNS'), ('that', 'WDT'), ('can', 'MD'), ('interact', 'VB'), ('with', 'IN'), ('people', 'NNS'), ('more', 'RBR'), ('naturally', 'RB'), ('than', 'IN'), ('ever', 'RB'), ('before', 'IN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['new', 'approaches', ',', 'possible', 'build', 'automated', 'systems', 'interact', 'people', 'naturally', 'ever', '.']

 TOTAL FILTERED TOKENS ==>  12

 ---- POST FOR FILTERED TOKENS ----

 [('new', 'JJ'), ('approaches', 'NNS'), (',', ','), ('possible', 'JJ'), ('build', 'NN'), ('automated', 'VBD'), ('systems', 'NNS'), ('interact', 'JJ'), ('people', 'NNS'), ('naturally', 'RB'), ('ever', 'RB'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['new approaches', 'approaches ,', ', possible', 'possible build', 'build automated', 'automated systems', 'systems interact', 'interact people', 'people naturally', 'naturally ever', 'ever .'] 

 TOTAL BIGRAMS --> 11 



 ---- TRI-GRAMS ---- 

 ['new approaches ,', 'approaches , possible', ', possible build', 'possible build automated', 'build automated systems', 'automated systems interact', 'systems interact people', 'interact people naturally', 'people naturally ever', 'naturally ever .'] 

 TOTAL TRIGRAMS --> 10 



 ---- NOUN PHRASES ---- 

 ['possible build'] 

 TOTAL NOUN PHRASES --> 1 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['new', 'approach', ',', 'possibl', 'build', 'autom', 'system', 'interact', 'peopl', 'natur', 'ever', '.']

 TOTAL PORTER STEM WORDS ==> 12



 ---- SNOWBALL STEMMING ----

['new', 'approach', ',', 'possibl', 'build', 'autom', 'system', 'interact', 'peopl', 'natur', 'ever', '.']

 TOTAL SNOWBALL STEM WORDS ==> 12



 ---- LEMMATIZATION ----

['new', 'approach', ',', 'possible', 'build', 'automated', 'system', 'interact', 'people', 'naturally', 'ever', '.']

 TOTAL LEMMATIZE WORDS ==> 12

************************************************************************************************************************

138 --> And forward-looking businesses are seizing the day,  incorporating NLP into a wide range of their processes for both customer-facing activities  and internal operations. 


 ---- TOKENS ----

 ['And', 'forward-looking', 'businesses', 'are', 'seizing', 'the', 'day', ',', 'incorporating', 'NLP', 'into', 'a', 'wide', 'range', 'of', 'their', 'processes', 'for', 'both', 'customer-facing', 'activities', 'and', 'internal', 'operations', '.'] 

 TOTAL TOKENS ==> 25

 ---- POST ----

 [('And', 'CC'), ('forward-looking', 'JJ'), ('businesses', 'NNS'), ('are', 'VBP'), ('seizing', 'VBG'), ('the', 'DT'), ('day', 'NN'), (',', ','), ('incorporating', 'VBG'), ('NLP', 'NNP'), ('into', 'IN'), ('a', 'DT'), ('wide', 'JJ'), ('range', 'NN'), ('of', 'IN'), ('their', 'PRP$'), ('processes', 'NNS'), ('for', 'IN'), ('both', 'DT'), ('customer-facing', 'JJ'), ('activities', 'NNS'), ('and', 'CC'), ('internal', 'JJ'), ('operations', 'NNS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['forward-looking', 'businesses', 'seizing', 'day', ',', 'incorporating', 'NLP', 'wide', 'range', 'processes', 'customer-facing', 'activities', 'internal', 'operations', '.']

 TOTAL FILTERED TOKENS ==>  15

 ---- POST FOR FILTERED TOKENS ----

 [('forward-looking', 'JJ'), ('businesses', 'NNS'), ('seizing', 'VBG'), ('day', 'NN'), (',', ','), ('incorporating', 'VBG'), ('NLP', 'NNP'), ('wide', 'JJ'), ('range', 'NN'), ('processes', 'VBZ'), ('customer-facing', 'JJ'), ('activities', 'NNS'), ('internal', 'JJ'), ('operations', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['forward-looking businesses', 'businesses seizing', 'seizing day', 'day ,', ', incorporating', 'incorporating NLP', 'NLP wide', 'wide range', 'range processes', 'processes customer-facing', 'customer-facing activities', 'activities internal', 'internal operations', 'operations .'] 

 TOTAL BIGRAMS --> 14 



 ---- TRI-GRAMS ---- 

 ['forward-looking businesses seizing', 'businesses seizing day', 'seizing day ,', 'day , incorporating', ', incorporating NLP', 'incorporating NLP wide', 'NLP wide range', 'wide range processes', 'range processes customer-facing', 'processes customer-facing activities', 'customer-facing activities internal', 'activities internal operations', 'internal operations .'] 

 TOTAL TRIGRAMS --> 13 



 ---- NOUN PHRASES ---- 

 ['day', 'wide range'] 

 TOTAL NOUN PHRASES --> 2 



 ---- NER ----

 
 ORGANIZATION ---> ['NLP']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['forward-look', 'busi', 'seiz', 'day', ',', 'incorpor', 'nlp', 'wide', 'rang', 'process', 'customer-fac', 'activ', 'intern', 'oper', '.']

 TOTAL PORTER STEM WORDS ==> 15



 ---- SNOWBALL STEMMING ----

['forward-look', 'busi', 'seiz', 'day', ',', 'incorpor', 'nlp', 'wide', 'rang', 'process', 'customer-fac', 'activ', 'intern', 'oper', '.']

 TOTAL SNOWBALL STEM WORDS ==> 15



 ---- LEMMATIZATION ----

['forward-looking', 'business', 'seizing', 'day', ',', 'incorporating', 'NLP', 'wide', 'range', 'process', 'customer-facing', 'activity', 'internal', 'operation', '.']

 TOTAL LEMMATIZE WORDS ==> 15

************************************************************************************************************************

139 --> To help organizations capitalize on this trend, Dell Technologies has an active research  program focused on advancing the technologies and methodologies for the development  of NLP applications. 


 ---- TOKENS ----

 ['To', 'help', 'organizations', 'capitalize', 'on', 'this', 'trend', ',', 'Dell', 'Technologies', 'has', 'an', 'active', 'research', 'program', 'focused', 'on', 'advancing', 'the', 'technologies', 'and', 'methodologies', 'for', 'the', 'development', 'of', 'NLP', 'applications', '.'] 

 TOTAL TOKENS ==> 29

 ---- POST ----

 [('To', 'TO'), ('help', 'VB'), ('organizations', 'NNS'), ('capitalize', 'VB'), ('on', 'IN'), ('this', 'DT'), ('trend', 'NN'), (',', ','), ('Dell', 'NNP'), ('Technologies', 'NNP'), ('has', 'VBZ'), ('an', 'DT'), ('active', 'JJ'), ('research', 'NN'), ('program', 'NN'), ('focused', 'VBD'), ('on', 'IN'), ('advancing', 'VBG'), ('the', 'DT'), ('technologies', 'NNS'), ('and', 'CC'), ('methodologies', 'NNS'), ('for', 'IN'), ('the', 'DT'), ('development', 'NN'), ('of', 'IN'), ('NLP', 'NNP'), ('applications', 'NNS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['help', 'organizations', 'capitalize', 'trend', ',', 'Dell', 'Technologies', 'active', 'research', 'program', 'focused', 'advancing', 'technologies', 'methodologies', 'development', 'NLP', 'applications', '.']

 TOTAL FILTERED TOKENS ==>  18

 ---- POST FOR FILTERED TOKENS ----

 [('help', 'NN'), ('organizations', 'NNS'), ('capitalize', 'VB'), ('trend', 'NN'), (',', ','), ('Dell', 'NNP'), ('Technologies', 'NNPS'), ('active', 'JJ'), ('research', 'NN'), ('program', 'NN'), ('focused', 'VBD'), ('advancing', 'VBG'), ('technologies', 'NNS'), ('methodologies', 'NNS'), ('development', 'NN'), ('NLP', 'NNP'), ('applications', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['help organizations', 'organizations capitalize', 'capitalize trend', 'trend ,', ', Dell', 'Dell Technologies', 'Technologies active', 'active research', 'research program', 'program focused', 'focused advancing', 'advancing technologies', 'technologies methodologies', 'methodologies development', 'development NLP', 'NLP applications', 'applications .'] 

 TOTAL BIGRAMS --> 17 



 ---- TRI-GRAMS ---- 

 ['help organizations capitalize', 'organizations capitalize trend', 'capitalize trend ,', 'trend , Dell', ', Dell Technologies', 'Dell Technologies active', 'Technologies active research', 'active research program', 'research program focused', 'program focused advancing', 'focused advancing technologies', 'advancing technologies methodologies', 'technologies methodologies development', 'methodologies development NLP', 'development NLP applications', 'NLP applications .'] 

 TOTAL TRIGRAMS --> 16 



 ---- NOUN PHRASES ---- 

 ['help', 'trend', 'active research', 'program', 'development'] 

 TOTAL NOUN PHRASES --> 5 



 ---- NER ----

 
 ORGANIZATION ---> ['Dell Technologies', 'NLP']
 TOTAL ORGANIZATION ENTITY --> 2 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['help', 'organ', 'capit', 'trend', ',', 'dell', 'technolog', 'activ', 'research', 'program', 'focus', 'advanc', 'technolog', 'methodolog', 'develop', 'nlp', 'applic', '.']

 TOTAL PORTER STEM WORDS ==> 18



 ---- SNOWBALL STEMMING ----

['help', 'organ', 'capit', 'trend', ',', 'dell', 'technolog', 'activ', 'research', 'program', 'focus', 'advanc', 'technolog', 'methodolog', 'develop', 'nlp', 'applic', '.']

 TOTAL SNOWBALL STEM WORDS ==> 18



 ---- LEMMATIZATION ----

['help', 'organization', 'capitalize', 'trend', ',', 'Dell', 'Technologies', 'active', 'research', 'program', 'focused', 'advancing', 'technology', 'methodology', 'development', 'NLP', 'application', '.']

 TOTAL LEMMATIZE WORDS ==> 18

************************************************************************************************************************

140 --> This research is carried out by a data sciences team in the Dell  Technologies HPC & AI Innovation Lab in Austin, Texas. 


 ---- TOKENS ----

 ['This', 'research', 'is', 'carried', 'out', 'by', 'a', 'data', 'sciences', 'team', 'in', 'the', 'Dell', 'Technologies', 'HPC', '&', 'AI', 'Innovation', 'Lab', 'in', 'Austin', ',', 'Texas', '.'] 

 TOTAL TOKENS ==> 24

 ---- POST ----

 [('This', 'DT'), ('research', 'NN'), ('is', 'VBZ'), ('carried', 'VBN'), ('out', 'RP'), ('by', 'IN'), ('a', 'DT'), ('data', 'NN'), ('sciences', 'NNS'), ('team', 'NN'), ('in', 'IN'), ('the', 'DT'), ('Dell', 'NNP'), ('Technologies', 'NNPS'), ('HPC', 'NNP'), ('&', 'CC'), ('AI', 'NNP'), ('Innovation', 'NNP'), ('Lab', 'NNP'), ('in', 'IN'), ('Austin', 'NNP'), (',', ','), ('Texas', 'NNP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['research', 'carried', 'data', 'sciences', 'team', 'Dell', 'Technologies', 'HPC', '&', 'AI', 'Innovation', 'Lab', 'Austin', ',', 'Texas', '.']

 TOTAL FILTERED TOKENS ==>  16

 ---- POST FOR FILTERED TOKENS ----

 [('research', 'NN'), ('carried', 'VBD'), ('data', 'NNS'), ('sciences', 'NNS'), ('team', 'NN'), ('Dell', 'NNP'), ('Technologies', 'NNP'), ('HPC', 'NNP'), ('&', 'CC'), ('AI', 'NNP'), ('Innovation', 'NNP'), ('Lab', 'NNP'), ('Austin', 'NNP'), (',', ','), ('Texas', 'NNP'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['research carried', 'carried data', 'data sciences', 'sciences team', 'team Dell', 'Dell Technologies', 'Technologies HPC', 'HPC &', '& AI', 'AI Innovation', 'Innovation Lab', 'Lab Austin', 'Austin ,', ', Texas', 'Texas .'] 

 TOTAL BIGRAMS --> 15 



 ---- TRI-GRAMS ---- 

 ['research carried data', 'carried data sciences', 'data sciences team', 'sciences team Dell', 'team Dell Technologies', 'Dell Technologies HPC', 'Technologies HPC &', 'HPC & AI', '& AI Innovation', 'AI Innovation Lab', 'Innovation Lab Austin', 'Lab Austin ,', 'Austin , Texas', ', Texas .'] 

 TOTAL TRIGRAMS --> 14 



 ---- NOUN PHRASES ---- 

 ['research', 'team'] 

 TOTAL NOUN PHRASES --> 2 



 ---- NER ----

 
 ORGANIZATION ---> ['Dell Technologies', 'AI Innovation Lab Austin']
 TOTAL ORGANIZATION ENTITY --> 2 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> ['Texas']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['research', 'carri', 'data', 'scienc', 'team', 'dell', 'technolog', 'hpc', '&', 'ai', 'innov', 'lab', 'austin', ',', 'texa', '.']

 TOTAL PORTER STEM WORDS ==> 16



 ---- SNOWBALL STEMMING ----

['research', 'carri', 'data', 'scienc', 'team', 'dell', 'technolog', 'hpc', '&', 'ai', 'innov', 'lab', 'austin', ',', 'texa', '.']

 TOTAL SNOWBALL STEM WORDS ==> 16



 ---- LEMMATIZATION ----

['research', 'carried', 'data', 'science', 'team', 'Dell', 'Technologies', 'HPC', '&', 'AI', 'Innovation', 'Lab', 'Austin', ',', 'Texas', '.']

 TOTAL LEMMATIZE WORDS ==> 16

************************************************************************************************************************

141 --> The lab currently has two key  projects under way in this realm. 


 ---- TOKENS ----

 ['The', 'lab', 'currently', 'has', 'two', 'key', 'projects', 'under', 'way', 'in', 'this', 'realm', '.'] 

 TOTAL TOKENS ==> 13

 ---- POST ----

 [('The', 'DT'), ('lab', 'NN'), ('currently', 'RB'), ('has', 'VBZ'), ('two', 'CD'), ('key', 'JJ'), ('projects', 'NNS'), ('under', 'IN'), ('way', 'NN'), ('in', 'IN'), ('this', 'DT'), ('realm', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['lab', 'currently', 'two', 'key', 'projects', 'way', 'realm', '.']

 TOTAL FILTERED TOKENS ==>  8

 ---- POST FOR FILTERED TOKENS ----

 [('lab', 'RB'), ('currently', 'RB'), ('two', 'CD'), ('key', 'JJ'), ('projects', 'NNS'), ('way', 'NN'), ('realm', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['lab currently', 'currently two', 'two key', 'key projects', 'projects way', 'way realm', 'realm .'] 

 TOTAL BIGRAMS --> 7 



 ---- TRI-GRAMS ---- 

 ['lab currently two', 'currently two key', 'two key projects', 'key projects way', 'projects way realm', 'way realm .'] 

 TOTAL TRIGRAMS --> 6 



 ---- NOUN PHRASES ---- 

 ['way', 'realm'] 

 TOTAL NOUN PHRASES --> 2 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['lab', 'current', 'two', 'key', 'project', 'way', 'realm', '.']

 TOTAL PORTER STEM WORDS ==> 8



 ---- SNOWBALL STEMMING ----

['lab', 'current', 'two', 'key', 'project', 'way', 'realm', '.']

 TOTAL SNOWBALL STEM WORDS ==> 8



 ---- LEMMATIZATION ----

['lab', 'currently', 'two', 'key', 'project', 'way', 'realm', '.']

 TOTAL LEMMATIZE WORDS ==> 8

************************************************************************************************************************

142 --> One focuses on language-to-language translation and  the other focuses on text-to-voice translation. 


 ---- TOKENS ----

 ['One', 'focuses', 'on', 'language-to-language', 'translation', 'and', 'the', 'other', 'focuses', 'on', 'text-to-voice', 'translation', '.'] 

 TOTAL TOKENS ==> 13

 ---- POST ----

 [('One', 'CD'), ('focuses', 'VBZ'), ('on', 'IN'), ('language-to-language', 'JJ'), ('translation', 'NN'), ('and', 'CC'), ('the', 'DT'), ('other', 'JJ'), ('focuses', 'NNS'), ('on', 'IN'), ('text-to-voice', 'JJ'), ('translation', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['One', 'focuses', 'language-to-language', 'translation', 'focuses', 'text-to-voice', 'translation', '.']

 TOTAL FILTERED TOKENS ==>  8

 ---- POST FOR FILTERED TOKENS ----

 [('One', 'CD'), ('focuses', 'VBZ'), ('language-to-language', 'NN'), ('translation', 'NN'), ('focuses', 'VBZ'), ('text-to-voice', 'JJ'), ('translation', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['One focuses', 'focuses language-to-language', 'language-to-language translation', 'translation focuses', 'focuses text-to-voice', 'text-to-voice translation', 'translation .'] 

 TOTAL BIGRAMS --> 7 



 ---- TRI-GRAMS ---- 

 ['One focuses language-to-language', 'focuses language-to-language translation', 'language-to-language translation focuses', 'translation focuses text-to-voice', 'focuses text-to-voice translation', 'text-to-voice translation .'] 

 TOTAL TRIGRAMS --> 6 



 ---- NOUN PHRASES ---- 

 ['language-to-language', 'translation', 'text-to-voice translation'] 

 TOTAL NOUN PHRASES --> 3 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['one', 'focus', 'language-to-languag', 'translat', 'focus', 'text-to-voic', 'translat', '.']

 TOTAL PORTER STEM WORDS ==> 8



 ---- SNOWBALL STEMMING ----

['one', 'focus', 'language-to-languag', 'translat', 'focus', 'text-to-voic', 'translat', '.']

 TOTAL SNOWBALL STEM WORDS ==> 8



 ---- LEMMATIZATION ----

['One', 'focus', 'language-to-language', 'translation', 'focus', 'text-to-voice', 'translation', '.']

 TOTAL LEMMATIZE WORDS ==> 8

************************************************************************************************************************

143 --> LANGUAGE-TO-LANGUAGE TRANSLATION In the lab’s research project focused on language-to-language translation, our data  scientists are working to solve key problems associated with translating from one human  language to another using a neural network. 


 ---- TOKENS ----

 ['LANGUAGE-TO-LANGUAGE', 'TRANSLATION', 'In', 'the', 'lab', '’', 's', 'research', 'project', 'focused', 'on', 'language-to-language', 'translation', ',', 'our', 'data', 'scientists', 'are', 'working', 'to', 'solve', 'key', 'problems', 'associated', 'with', 'translating', 'from', 'one', 'human', 'language', 'to', 'another', 'using', 'a', 'neural', 'network', '.'] 

 TOTAL TOKENS ==> 37

 ---- POST ----

 [('LANGUAGE-TO-LANGUAGE', 'JJ'), ('TRANSLATION', 'NNP'), ('In', 'IN'), ('the', 'DT'), ('lab', 'NN'), ('’', 'NNP'), ('s', 'VBD'), ('research', 'NN'), ('project', 'NN'), ('focused', 'VBD'), ('on', 'IN'), ('language-to-language', 'JJ'), ('translation', 'NN'), (',', ','), ('our', 'PRP$'), ('data', 'NNS'), ('scientists', 'NNS'), ('are', 'VBP'), ('working', 'VBG'), ('to', 'TO'), ('solve', 'VB'), ('key', 'JJ'), ('problems', 'NNS'), ('associated', 'VBN'), ('with', 'IN'), ('translating', 'VBG'), ('from', 'IN'), ('one', 'CD'), ('human', 'JJ'), ('language', 'NN'), ('to', 'TO'), ('another', 'DT'), ('using', 'VBG'), ('a', 'DT'), ('neural', 'JJ'), ('network', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['LANGUAGE-TO-LANGUAGE', 'TRANSLATION', 'lab', '’', 'research', 'project', 'focused', 'language-to-language', 'translation', ',', 'data', 'scientists', 'working', 'solve', 'key', 'problems', 'associated', 'translating', 'one', 'human', 'language', 'another', 'using', 'neural', 'network', '.']

 TOTAL FILTERED TOKENS ==>  26

 ---- POST FOR FILTERED TOKENS ----

 [('LANGUAGE-TO-LANGUAGE', 'JJ'), ('TRANSLATION', 'NNP'), ('lab', 'NN'), ('’', 'NNP'), ('research', 'NN'), ('project', 'NN'), ('focused', 'VBD'), ('language-to-language', 'JJ'), ('translation', 'NN'), (',', ','), ('data', 'NNS'), ('scientists', 'NNS'), ('working', 'VBG'), ('solve', 'NN'), ('key', 'JJ'), ('problems', 'NNS'), ('associated', 'VBD'), ('translating', 'VBG'), ('one', 'CD'), ('human', 'JJ'), ('language', 'NN'), ('another', 'DT'), ('using', 'VBG'), ('neural', 'JJ'), ('network', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['LANGUAGE-TO-LANGUAGE TRANSLATION', 'TRANSLATION lab', 'lab ’', '’ research', 'research project', 'project focused', 'focused language-to-language', 'language-to-language translation', 'translation ,', ', data', 'data scientists', 'scientists working', 'working solve', 'solve key', 'key problems', 'problems associated', 'associated translating', 'translating one', 'one human', 'human language', 'language another', 'another using', 'using neural', 'neural network', 'network .'] 

 TOTAL BIGRAMS --> 25 



 ---- TRI-GRAMS ---- 

 ['LANGUAGE-TO-LANGUAGE TRANSLATION lab', 'TRANSLATION lab ’', 'lab ’ research', '’ research project', 'research project focused', 'project focused language-to-language', 'focused language-to-language translation', 'language-to-language translation ,', 'translation , data', ', data scientists', 'data scientists working', 'scientists working solve', 'working solve key', 'solve key problems', 'key problems associated', 'problems associated translating', 'associated translating one', 'translating one human', 'one human language', 'human language another', 'language another using', 'another using neural', 'using neural network', 'neural network .'] 

 TOTAL TRIGRAMS --> 24 



 ---- NOUN PHRASES ---- 

 ['lab', 'research', 'project', 'language-to-language translation', 'solve', 'human language', 'neural network'] 

 TOTAL NOUN PHRASES --> 7 



 ---- NER ----

 
 ORGANIZATION ---> ['TRANSLATION']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['language-to-languag', 'translat', 'lab', '’', 'research', 'project', 'focus', 'language-to-languag', 'translat', ',', 'data', 'scientist', 'work', 'solv', 'key', 'problem', 'associ', 'translat', 'one', 'human', 'languag', 'anoth', 'use', 'neural', 'network', '.']

 TOTAL PORTER STEM WORDS ==> 26



 ---- SNOWBALL STEMMING ----

['language-to-languag', 'translat', 'lab', '’', 'research', 'project', 'focus', 'language-to-languag', 'translat', ',', 'data', 'scientist', 'work', 'solv', 'key', 'problem', 'associ', 'translat', 'one', 'human', 'languag', 'anoth', 'use', 'neural', 'network', '.']

 TOTAL SNOWBALL STEM WORDS ==> 26



 ---- LEMMATIZATION ----

['LANGUAGE-TO-LANGUAGE', 'TRANSLATION', 'lab', '’', 'research', 'project', 'focused', 'language-to-language', 'translation', ',', 'data', 'scientist', 'working', 'solve', 'key', 'problem', 'associated', 'translating', 'one', 'human', 'language', 'another', 'using', 'neural', 'network', '.']

 TOTAL LEMMATIZE WORDS ==> 26

************************************************************************************************************************

144 --> This is a process that involves taking inputs  from a source language and converting it to a target language. 


 ---- TOKENS ----

 ['This', 'is', 'a', 'process', 'that', 'involves', 'taking', 'inputs', 'from', 'a', 'source', 'language', 'and', 'converting', 'it', 'to', 'a', 'target', 'language', '.'] 

 TOTAL TOKENS ==> 20

 ---- POST ----

 [('This', 'DT'), ('is', 'VBZ'), ('a', 'DT'), ('process', 'NN'), ('that', 'WDT'), ('involves', 'VBZ'), ('taking', 'VBG'), ('inputs', 'NNS'), ('from', 'IN'), ('a', 'DT'), ('source', 'NN'), ('language', 'NN'), ('and', 'CC'), ('converting', 'VBG'), ('it', 'PRP'), ('to', 'TO'), ('a', 'DT'), ('target', 'NN'), ('language', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['process', 'involves', 'taking', 'inputs', 'source', 'language', 'converting', 'target', 'language', '.']

 TOTAL FILTERED TOKENS ==>  10

 ---- POST FOR FILTERED TOKENS ----

 [('process', 'NN'), ('involves', 'VBZ'), ('taking', 'VBG'), ('inputs', 'NNS'), ('source', 'NN'), ('language', 'NN'), ('converting', 'VBG'), ('target', 'NN'), ('language', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['process involves', 'involves taking', 'taking inputs', 'inputs source', 'source language', 'language converting', 'converting target', 'target language', 'language .'] 

 TOTAL BIGRAMS --> 9 



 ---- TRI-GRAMS ---- 

 ['process involves taking', 'involves taking inputs', 'taking inputs source', 'inputs source language', 'source language converting', 'language converting target', 'converting target language', 'target language .'] 

 TOTAL TRIGRAMS --> 8 



 ---- NOUN PHRASES ---- 

 ['process', 'source', 'language', 'target', 'language'] 

 TOTAL NOUN PHRASES --> 5 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['process', 'involv', 'take', 'input', 'sourc', 'languag', 'convert', 'target', 'languag', '.']

 TOTAL PORTER STEM WORDS ==> 10



 ---- SNOWBALL STEMMING ----

['process', 'involv', 'take', 'input', 'sourc', 'languag', 'convert', 'target', 'languag', '.']

 TOTAL SNOWBALL STEM WORDS ==> 10



 ---- LEMMATIZATION ----

['process', 'involves', 'taking', 'input', 'source', 'language', 'converting', 'target', 'language', '.']

 TOTAL LEMMATIZE WORDS ==> 10

************************************************************************************************************************

145 --> In this process, the translation model first reads a sentence in a source language and then  passes it to an encoder, which builds an intermediate representation. 


 ---- TOKENS ----

 ['In', 'this', 'process', ',', 'the', 'translation', 'model', 'first', 'reads', 'a', 'sentence', 'in', 'a', 'source', 'language', 'and', 'then', 'passes', 'it', 'to', 'an', 'encoder', ',', 'which', 'builds', 'an', 'intermediate', 'representation', '.'] 

 TOTAL TOKENS ==> 29

 ---- POST ----

 [('In', 'IN'), ('this', 'DT'), ('process', 'NN'), (',', ','), ('the', 'DT'), ('translation', 'NN'), ('model', 'NN'), ('first', 'RB'), ('reads', 'VBZ'), ('a', 'DT'), ('sentence', 'NN'), ('in', 'IN'), ('a', 'DT'), ('source', 'NN'), ('language', 'NN'), ('and', 'CC'), ('then', 'RB'), ('passes', 'VBZ'), ('it', 'PRP'), ('to', 'TO'), ('an', 'DT'), ('encoder', 'NN'), (',', ','), ('which', 'WDT'), ('builds', 'VBZ'), ('an', 'DT'), ('intermediate', 'JJ'), ('representation', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['process', ',', 'translation', 'model', 'first', 'reads', 'sentence', 'source', 'language', 'passes', 'encoder', ',', 'builds', 'intermediate', 'representation', '.']

 TOTAL FILTERED TOKENS ==>  16

 ---- POST FOR FILTERED TOKENS ----

 [('process', 'NN'), (',', ','), ('translation', 'NN'), ('model', 'NN'), ('first', 'RB'), ('reads', 'VBZ'), ('sentence', 'NN'), ('source', 'NN'), ('language', 'NN'), ('passes', 'VBZ'), ('encoder', 'JJR'), (',', ','), ('builds', 'NNS'), ('intermediate', 'JJ'), ('representation', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['process ,', ', translation', 'translation model', 'model first', 'first reads', 'reads sentence', 'sentence source', 'source language', 'language passes', 'passes encoder', 'encoder ,', ', builds', 'builds intermediate', 'intermediate representation', 'representation .'] 

 TOTAL BIGRAMS --> 15 



 ---- TRI-GRAMS ---- 

 ['process , translation', ', translation model', 'translation model first', 'model first reads', 'first reads sentence', 'reads sentence source', 'sentence source language', 'source language passes', 'language passes encoder', 'passes encoder ,', 'encoder , builds', ', builds intermediate', 'builds intermediate representation', 'intermediate representation .'] 

 TOTAL TRIGRAMS --> 14 



 ---- NOUN PHRASES ---- 

 ['process', 'translation', 'model', 'sentence', 'source', 'language', 'intermediate representation'] 

 TOTAL NOUN PHRASES --> 7 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['process', ',', 'translat', 'model', 'first', 'read', 'sentenc', 'sourc', 'languag', 'pass', 'encod', ',', 'build', 'intermedi', 'represent', '.']

 TOTAL PORTER STEM WORDS ==> 16



 ---- SNOWBALL STEMMING ----

['process', ',', 'translat', 'model', 'first', 'read', 'sentenc', 'sourc', 'languag', 'pass', 'encod', ',', 'build', 'intermedi', 'represent', '.']

 TOTAL SNOWBALL STEM WORDS ==> 16



 ---- LEMMATIZATION ----

['process', ',', 'translation', 'model', 'first', 'read', 'sentence', 'source', 'language', 'pass', 'encoder', ',', 'build', 'intermediate', 'representation', '.']

 TOTAL LEMMATIZE WORDS ==> 16

************************************************************************************************************************

146 --> This intermediate  representation is then passed to a decoder, which processes the intermediate  representation to produce the translated sentence in the target language. 


 ---- TOKENS ----

 ['This', 'intermediate', 'representation', 'is', 'then', 'passed', 'to', 'a', 'decoder', ',', 'which', 'processes', 'the', 'intermediate', 'representation', 'to', 'produce', 'the', 'translated', 'sentence', 'in', 'the', 'target', 'language', '.'] 

 TOTAL TOKENS ==> 25

 ---- POST ----

 [('This', 'DT'), ('intermediate', 'JJ'), ('representation', 'NN'), ('is', 'VBZ'), ('then', 'RB'), ('passed', 'VBN'), ('to', 'TO'), ('a', 'DT'), ('decoder', 'NN'), (',', ','), ('which', 'WDT'), ('processes', 'VBZ'), ('the', 'DT'), ('intermediate', 'JJ'), ('representation', 'NN'), ('to', 'TO'), ('produce', 'VB'), ('the', 'DT'), ('translated', 'JJ'), ('sentence', 'NN'), ('in', 'IN'), ('the', 'DT'), ('target', 'NN'), ('language', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['intermediate', 'representation', 'passed', 'decoder', ',', 'processes', 'intermediate', 'representation', 'produce', 'translated', 'sentence', 'target', 'language', '.']

 TOTAL FILTERED TOKENS ==>  14

 ---- POST FOR FILTERED TOKENS ----

 [('intermediate', 'JJ'), ('representation', 'NN'), ('passed', 'VBD'), ('decoder', 'NN'), (',', ','), ('processes', 'VBZ'), ('intermediate', 'JJ'), ('representation', 'NN'), ('produce', 'NN'), ('translated', 'VBD'), ('sentence', 'NN'), ('target', 'NN'), ('language', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['intermediate representation', 'representation passed', 'passed decoder', 'decoder ,', ', processes', 'processes intermediate', 'intermediate representation', 'representation produce', 'produce translated', 'translated sentence', 'sentence target', 'target language', 'language .'] 

 TOTAL BIGRAMS --> 13 



 ---- TRI-GRAMS ---- 

 ['intermediate representation passed', 'representation passed decoder', 'passed decoder ,', 'decoder , processes', ', processes intermediate', 'processes intermediate representation', 'intermediate representation produce', 'representation produce translated', 'produce translated sentence', 'translated sentence target', 'sentence target language', 'target language .'] 

 TOTAL TRIGRAMS --> 12 



 ---- NOUN PHRASES ---- 

 ['intermediate representation', 'decoder', 'intermediate representation', 'produce', 'sentence', 'target', 'language'] 

 TOTAL NOUN PHRASES --> 7 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['intermedi', 'represent', 'pass', 'decod', ',', 'process', 'intermedi', 'represent', 'produc', 'translat', 'sentenc', 'target', 'languag', '.']

 TOTAL PORTER STEM WORDS ==> 14



 ---- SNOWBALL STEMMING ----

['intermedi', 'represent', 'pass', 'decod', ',', 'process', 'intermedi', 'represent', 'produc', 'translat', 'sentenc', 'target', 'languag', '.']

 TOTAL SNOWBALL STEM WORDS ==> 14



 ---- LEMMATIZATION ----

['intermediate', 'representation', 'passed', 'decoder', ',', 'process', 'intermediate', 'representation', 'produce', 'translated', 'sentence', 'target', 'language', '.']

 TOTAL LEMMATIZE WORDS ==> 14

************************************************************************************************************************

147 --> For our language-to-language translation project, we started with a stock topology created  by Google, and then improved some of the underlying mathematics, so we could parallelize  the workflows more efficiently. 


 ---- TOKENS ----

 ['For', 'our', 'language-to-language', 'translation', 'project', ',', 'we', 'started', 'with', 'a', 'stock', 'topology', 'created', 'by', 'Google', ',', 'and', 'then', 'improved', 'some', 'of', 'the', 'underlying', 'mathematics', ',', 'so', 'we', 'could', 'parallelize', 'the', 'workflows', 'more', 'efficiently', '.'] 

 TOTAL TOKENS ==> 34

 ---- POST ----

 [('For', 'IN'), ('our', 'PRP$'), ('language-to-language', 'JJ'), ('translation', 'NN'), ('project', 'NN'), (',', ','), ('we', 'PRP'), ('started', 'VBD'), ('with', 'IN'), ('a', 'DT'), ('stock', 'NN'), ('topology', 'NN'), ('created', 'VBN'), ('by', 'IN'), ('Google', 'NNP'), (',', ','), ('and', 'CC'), ('then', 'RB'), ('improved', 'VBD'), ('some', 'DT'), ('of', 'IN'), ('the', 'DT'), ('underlying', 'VBG'), ('mathematics', 'NNS'), (',', ','), ('so', 'IN'), ('we', 'PRP'), ('could', 'MD'), ('parallelize', 'VB'), ('the', 'DT'), ('workflows', 'NNS'), ('more', 'RBR'), ('efficiently', 'RB'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['language-to-language', 'translation', 'project', ',', 'started', 'stock', 'topology', 'created', 'Google', ',', 'improved', 'underlying', 'mathematics', ',', 'could', 'parallelize', 'workflows', 'efficiently', '.']

 TOTAL FILTERED TOKENS ==>  19

 ---- POST FOR FILTERED TOKENS ----

 [('language-to-language', 'JJ'), ('translation', 'NN'), ('project', 'NN'), (',', ','), ('started', 'VBD'), ('stock', 'NN'), ('topology', 'NN'), ('created', 'VBD'), ('Google', 'NNP'), (',', ','), ('improved', 'VBD'), ('underlying', 'JJ'), ('mathematics', 'NNS'), (',', ','), ('could', 'MD'), ('parallelize', 'VB'), ('workflows', 'NNS'), ('efficiently', 'RB'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['language-to-language translation', 'translation project', 'project ,', ', started', 'started stock', 'stock topology', 'topology created', 'created Google', 'Google ,', ', improved', 'improved underlying', 'underlying mathematics', 'mathematics ,', ', could', 'could parallelize', 'parallelize workflows', 'workflows efficiently', 'efficiently .'] 

 TOTAL BIGRAMS --> 18 



 ---- TRI-GRAMS ---- 

 ['language-to-language translation project', 'translation project ,', 'project , started', ', started stock', 'started stock topology', 'stock topology created', 'topology created Google', 'created Google ,', 'Google , improved', ', improved underlying', 'improved underlying mathematics', 'underlying mathematics ,', 'mathematics , could', ', could parallelize', 'could parallelize workflows', 'parallelize workflows efficiently', 'workflows efficiently .'] 

 TOTAL TRIGRAMS --> 17 



 ---- NOUN PHRASES ---- 

 ['language-to-language translation', 'project', 'stock', 'topology'] 

 TOTAL NOUN PHRASES --> 4 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> ['Google']
 TOTAL PERSON ENTITY --> 1 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['language-to-languag', 'translat', 'project', ',', 'start', 'stock', 'topolog', 'creat', 'googl', ',', 'improv', 'underli', 'mathemat', ',', 'could', 'parallel', 'workflow', 'effici', '.']

 TOTAL PORTER STEM WORDS ==> 19



 ---- SNOWBALL STEMMING ----

['language-to-languag', 'translat', 'project', ',', 'start', 'stock', 'topolog', 'creat', 'googl', ',', 'improv', 'under', 'mathemat', ',', 'could', 'parallel', 'workflow', 'effici', '.']

 TOTAL SNOWBALL STEM WORDS ==> 19



 ---- LEMMATIZATION ----

['language-to-language', 'translation', 'project', ',', 'started', 'stock', 'topology', 'created', 'Google', ',', 'improved', 'underlying', 'mathematics', ',', 'could', 'parallelize', 'workflow', 'efficiently', '.']

 TOTAL LEMMATIZE WORDS ==> 19

************************************************************************************************************************

148 --> Our goal was to run our model on hundreds of compute  nodes at the same time to get to a solution more quickly. 


 ---- TOKENS ----

 ['Our', 'goal', 'was', 'to', 'run', 'our', 'model', 'on', 'hundreds', 'of', 'compute', 'nodes', 'at', 'the', 'same', 'time', 'to', 'get', 'to', 'a', 'solution', 'more', 'quickly', '.'] 

 TOTAL TOKENS ==> 24

 ---- POST ----

 [('Our', 'PRP$'), ('goal', 'NN'), ('was', 'VBD'), ('to', 'TO'), ('run', 'VB'), ('our', 'PRP$'), ('model', 'NN'), ('on', 'IN'), ('hundreds', 'NNS'), ('of', 'IN'), ('compute', 'NN'), ('nodes', 'NNS'), ('at', 'IN'), ('the', 'DT'), ('same', 'JJ'), ('time', 'NN'), ('to', 'TO'), ('get', 'VB'), ('to', 'TO'), ('a', 'DT'), ('solution', 'NN'), ('more', 'RBR'), ('quickly', 'RB'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['goal', 'run', 'model', 'hundreds', 'compute', 'nodes', 'time', 'get', 'solution', 'quickly', '.']

 TOTAL FILTERED TOKENS ==>  11

 ---- POST FOR FILTERED TOKENS ----

 [('goal', 'NN'), ('run', 'VB'), ('model', 'NN'), ('hundreds', 'NNS'), ('compute', 'VBP'), ('nodes', 'NNS'), ('time', 'NN'), ('get', 'VB'), ('solution', 'JJ'), ('quickly', 'RB'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['goal run', 'run model', 'model hundreds', 'hundreds compute', 'compute nodes', 'nodes time', 'time get', 'get solution', 'solution quickly', 'quickly .'] 

 TOTAL BIGRAMS --> 10 



 ---- TRI-GRAMS ---- 

 ['goal run model', 'run model hundreds', 'model hundreds compute', 'hundreds compute nodes', 'compute nodes time', 'nodes time get', 'time get solution', 'get solution quickly', 'solution quickly .'] 

 TOTAL TRIGRAMS --> 9 



 ---- NOUN PHRASES ---- 

 ['goal', 'model', 'time'] 

 TOTAL NOUN PHRASES --> 3 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['goal', 'run', 'model', 'hundr', 'comput', 'node', 'time', 'get', 'solut', 'quickli', '.']

 TOTAL PORTER STEM WORDS ==> 11



 ---- SNOWBALL STEMMING ----

['goal', 'run', 'model', 'hundr', 'comput', 'node', 'time', 'get', 'solut', 'quick', '.']

 TOTAL SNOWBALL STEM WORDS ==> 11



 ---- LEMMATIZATION ----

['goal', 'run', 'model', 'hundred', 'compute', 'node', 'time', 'get', 'solution', 'quickly', '.']

 TOTAL LEMMATIZE WORDS ==> 11

************************************************************************************************************************

149 --> In this optimization process, which spanned several months, our team looked at how  the system was using memory, how the system was performing computation and how  we could improve certain parts of the system. 


 ---- TOKENS ----

 ['In', 'this', 'optimization', 'process', ',', 'which', 'spanned', 'several', 'months', ',', 'our', 'team', 'looked', 'at', 'how', 'the', 'system', 'was', 'using', 'memory', ',', 'how', 'the', 'system', 'was', 'performing', 'computation', 'and', 'how', 'we', 'could', 'improve', 'certain', 'parts', 'of', 'the', 'system', '.'] 

 TOTAL TOKENS ==> 38

 ---- POST ----

 [('In', 'IN'), ('this', 'DT'), ('optimization', 'NN'), ('process', 'NN'), (',', ','), ('which', 'WDT'), ('spanned', 'VBD'), ('several', 'JJ'), ('months', 'NNS'), (',', ','), ('our', 'PRP$'), ('team', 'NN'), ('looked', 'VBD'), ('at', 'IN'), ('how', 'WRB'), ('the', 'DT'), ('system', 'NN'), ('was', 'VBD'), ('using', 'VBG'), ('memory', 'NN'), (',', ','), ('how', 'WRB'), ('the', 'DT'), ('system', 'NN'), ('was', 'VBD'), ('performing', 'VBG'), ('computation', 'NN'), ('and', 'CC'), ('how', 'WRB'), ('we', 'PRP'), ('could', 'MD'), ('improve', 'VB'), ('certain', 'JJ'), ('parts', 'NNS'), ('of', 'IN'), ('the', 'DT'), ('system', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['optimization', 'process', ',', 'spanned', 'several', 'months', ',', 'team', 'looked', 'system', 'using', 'memory', ',', 'system', 'performing', 'computation', 'could', 'improve', 'certain', 'parts', 'system', '.']

 TOTAL FILTERED TOKENS ==>  22

 ---- POST FOR FILTERED TOKENS ----

 [('optimization', 'NN'), ('process', 'NN'), (',', ','), ('spanned', 'VBD'), ('several', 'JJ'), ('months', 'NNS'), (',', ','), ('team', 'NN'), ('looked', 'VBD'), ('system', 'NN'), ('using', 'VBG'), ('memory', 'NN'), (',', ','), ('system', 'NN'), ('performing', 'VBG'), ('computation', 'NN'), ('could', 'MD'), ('improve', 'VB'), ('certain', 'JJ'), ('parts', 'NNS'), ('system', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['optimization process', 'process ,', ', spanned', 'spanned several', 'several months', 'months ,', ', team', 'team looked', 'looked system', 'system using', 'using memory', 'memory ,', ', system', 'system performing', 'performing computation', 'computation could', 'could improve', 'improve certain', 'certain parts', 'parts system', 'system .'] 

 TOTAL BIGRAMS --> 21 



 ---- TRI-GRAMS ---- 

 ['optimization process ,', 'process , spanned', ', spanned several', 'spanned several months', 'several months ,', 'months , team', ', team looked', 'team looked system', 'looked system using', 'system using memory', 'using memory ,', 'memory , system', ', system performing', 'system performing computation', 'performing computation could', 'computation could improve', 'could improve certain', 'improve certain parts', 'certain parts system', 'parts system .'] 

 TOTAL TRIGRAMS --> 20 



 ---- NOUN PHRASES ---- 

 ['optimization', 'process', 'team', 'system', 'memory', 'system', 'computation', 'system'] 

 TOTAL NOUN PHRASES --> 8 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['optim', 'process', ',', 'span', 'sever', 'month', ',', 'team', 'look', 'system', 'use', 'memori', ',', 'system', 'perform', 'comput', 'could', 'improv', 'certain', 'part', 'system', '.']

 TOTAL PORTER STEM WORDS ==> 22



 ---- SNOWBALL STEMMING ----

['optim', 'process', ',', 'span', 'sever', 'month', ',', 'team', 'look', 'system', 'use', 'memori', ',', 'system', 'perform', 'comput', 'could', 'improv', 'certain', 'part', 'system', '.']

 TOTAL SNOWBALL STEM WORDS ==> 22



 ---- LEMMATIZATION ----

['optimization', 'process', ',', 'spanned', 'several', 'month', ',', 'team', 'looked', 'system', 'using', 'memory', ',', 'system', 'performing', 'computation', 'could', 'improve', 'certain', 'part', 'system', '.']

 TOTAL LEMMATIZE WORDS ==> 22

************************************************************************************************************************

150 --> We then ran tests to make sure the model  we got in the end was just as accurate as what Google could achieve. 


 ---- TOKENS ----

 ['We', 'then', 'ran', 'tests', 'to', 'make', 'sure', 'the', 'model', 'we', 'got', 'in', 'the', 'end', 'was', 'just', 'as', 'accurate', 'as', 'what', 'Google', 'could', 'achieve', '.'] 

 TOTAL TOKENS ==> 24

 ---- POST ----

 [('We', 'PRP'), ('then', 'RB'), ('ran', 'VBD'), ('tests', 'NNS'), ('to', 'TO'), ('make', 'VB'), ('sure', 'JJ'), ('the', 'DT'), ('model', 'NN'), ('we', 'PRP'), ('got', 'VBD'), ('in', 'IN'), ('the', 'DT'), ('end', 'NN'), ('was', 'VBD'), ('just', 'RB'), ('as', 'RB'), ('accurate', 'JJ'), ('as', 'IN'), ('what', 'WP'), ('Google', 'NNP'), ('could', 'MD'), ('achieve', 'VB'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['ran', 'tests', 'make', 'sure', 'model', 'got', 'end', 'accurate', 'Google', 'could', 'achieve', '.']

 TOTAL FILTERED TOKENS ==>  12

 ---- POST FOR FILTERED TOKENS ----

 [('ran', 'NN'), ('tests', 'NNS'), ('make', 'VBP'), ('sure', 'JJ'), ('model', 'NN'), ('got', 'VBD'), ('end', 'JJ'), ('accurate', 'NN'), ('Google', 'NNP'), ('could', 'MD'), ('achieve', 'VB'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['ran tests', 'tests make', 'make sure', 'sure model', 'model got', 'got end', 'end accurate', 'accurate Google', 'Google could', 'could achieve', 'achieve .'] 

 TOTAL BIGRAMS --> 11 



 ---- TRI-GRAMS ---- 

 ['ran tests make', 'tests make sure', 'make sure model', 'sure model got', 'model got end', 'got end accurate', 'end accurate Google', 'accurate Google could', 'Google could achieve', 'could achieve .'] 

 TOTAL TRIGRAMS --> 10 



 ---- NOUN PHRASES ---- 

 ['ran', 'sure model', 'end accurate'] 

 TOTAL NOUN PHRASES --> 3 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> ['Google']
 TOTAL PERSON ENTITY --> 1 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['ran', 'test', 'make', 'sure', 'model', 'got', 'end', 'accur', 'googl', 'could', 'achiev', '.']

 TOTAL PORTER STEM WORDS ==> 12



 ---- SNOWBALL STEMMING ----

['ran', 'test', 'make', 'sure', 'model', 'got', 'end', 'accur', 'googl', 'could', 'achiev', '.']

 TOTAL SNOWBALL STEM WORDS ==> 12



 ---- LEMMATIZATION ----

['ran', 'test', 'make', 'sure', 'model', 'got', 'end', 'accurate', 'Google', 'could', 'achieve', '.']

 TOTAL LEMMATIZE WORDS ==> 12

************************************************************************************************************************

151 --> This validation  of the accuracy of the model gave us the assurance that in our efforts to speed up the  computation, we didn’t end up with lower-quality answers. 


 ---- TOKENS ----

 ['This', 'validation', 'of', 'the', 'accuracy', 'of', 'the', 'model', 'gave', 'us', 'the', 'assurance', 'that', 'in', 'our', 'efforts', 'to', 'speed', 'up', 'the', 'computation', ',', 'we', 'didn', '’', 't', 'end', 'up', 'with', 'lower-quality', 'answers', '.'] 

 TOTAL TOKENS ==> 32

 ---- POST ----

 [('This', 'DT'), ('validation', 'NN'), ('of', 'IN'), ('the', 'DT'), ('accuracy', 'NN'), ('of', 'IN'), ('the', 'DT'), ('model', 'NN'), ('gave', 'VBD'), ('us', 'PRP'), ('the', 'DT'), ('assurance', 'NN'), ('that', 'IN'), ('in', 'IN'), ('our', 'PRP$'), ('efforts', 'NNS'), ('to', 'TO'), ('speed', 'VB'), ('up', 'RP'), ('the', 'DT'), ('computation', 'NN'), (',', ','), ('we', 'PRP'), ('didn', 'VBP'), ('’', 'JJ'), ('t', 'NNS'), ('end', 'VBP'), ('up', 'RP'), ('with', 'IN'), ('lower-quality', 'JJ'), ('answers', 'NNS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['validation', 'accuracy', 'model', 'gave', 'us', 'assurance', 'efforts', 'speed', 'computation', ',', '’', 'end', 'lower-quality', 'answers', '.']

 TOTAL FILTERED TOKENS ==>  15

 ---- POST FOR FILTERED TOKENS ----

 [('validation', 'NN'), ('accuracy', 'NN'), ('model', 'NN'), ('gave', 'VBD'), ('us', 'PRP'), ('assurance', 'NN'), ('efforts', 'NNS'), ('speed', 'NN'), ('computation', 'NN'), (',', ','), ('’', 'JJ'), ('end', 'NN'), ('lower-quality', 'NN'), ('answers', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['validation accuracy', 'accuracy model', 'model gave', 'gave us', 'us assurance', 'assurance efforts', 'efforts speed', 'speed computation', 'computation ,', ', ’', '’ end', 'end lower-quality', 'lower-quality answers', 'answers .'] 

 TOTAL BIGRAMS --> 14 



 ---- TRI-GRAMS ---- 

 ['validation accuracy model', 'accuracy model gave', 'model gave us', 'gave us assurance', 'us assurance efforts', 'assurance efforts speed', 'efforts speed computation', 'speed computation ,', 'computation , ’', ', ’ end', '’ end lower-quality', 'end lower-quality answers', 'lower-quality answers .'] 

 TOTAL TRIGRAMS --> 13 



 ---- NOUN PHRASES ---- 

 ['validation', 'accuracy', 'model', 'assurance', 'speed', 'computation', '’ end', 'lower-quality'] 

 TOTAL NOUN PHRASES --> 8 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['valid', 'accuraci', 'model', 'gave', 'us', 'assur', 'effort', 'speed', 'comput', ',', '’', 'end', 'lower-qu', 'answer', '.']

 TOTAL PORTER STEM WORDS ==> 15



 ---- SNOWBALL STEMMING ----

['valid', 'accuraci', 'model', 'gave', 'us', 'assur', 'effort', 'speed', 'comput', ',', '’', 'end', 'lower-qu', 'answer', '.']

 TOTAL SNOWBALL STEM WORDS ==> 15



 ---- LEMMATIZATION ----

['validation', 'accuracy', 'model', 'gave', 'u', 'assurance', 'effort', 'speed', 'computation', ',', '’', 'end', 'lower-quality', 'answer', '.']

 TOTAL LEMMATIZE WORDS ==> 15

************************************************************************************************************************

152 --> 2 DELL TECHNOLOGIES WHITE PAPER COMPUTING RESOURCES For this project, our research team used Dell EMC systems with 2nd-generation Intel®  Xeon® Scalable processors in the Zenith supercomputer, one of three HPC clusters in  our HPC & AI Innovation Lab. 


 ---- TOKENS ----

 ['2', 'DELL', 'TECHNOLOGIES', 'WHITE', 'PAPER', 'COMPUTING', 'RESOURCES', 'For', 'this', 'project', ',', 'our', 'research', 'team', 'used', 'Dell', 'EMC', 'systems', 'with', '2nd-generation', 'Intel®', 'Xeon®', 'Scalable', 'processors', 'in', 'the', 'Zenith', 'supercomputer', ',', 'one', 'of', 'three', 'HPC', 'clusters', 'in', 'our', 'HPC', '&', 'AI', 'Innovation', 'Lab', '.'] 

 TOTAL TOKENS ==> 42

 ---- POST ----

 [('2', 'CD'), ('DELL', 'NNP'), ('TECHNOLOGIES', 'NNP'), ('WHITE', 'NNP'), ('PAPER', 'NNP'), ('COMPUTING', 'NNP'), ('RESOURCES', 'NNP'), ('For', 'IN'), ('this', 'DT'), ('project', 'NN'), (',', ','), ('our', 'PRP$'), ('research', 'NN'), ('team', 'NN'), ('used', 'VBN'), ('Dell', 'NNP'), ('EMC', 'NNP'), ('systems', 'NNS'), ('with', 'IN'), ('2nd-generation', 'JJ'), ('Intel®', 'NNP'), ('Xeon®', 'NNP'), ('Scalable', 'NNP'), ('processors', 'NNS'), ('in', 'IN'), ('the', 'DT'), ('Zenith', 'NNP'), ('supercomputer', 'NN'), (',', ','), ('one', 'CD'), ('of', 'IN'), ('three', 'CD'), ('HPC', 'NNP'), ('clusters', 'NNS'), ('in', 'IN'), ('our', 'PRP$'), ('HPC', 'NNP'), ('&', 'CC'), ('AI', 'NNP'), ('Innovation', 'NNP'), ('Lab', 'NNP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['2', 'DELL', 'TECHNOLOGIES', 'WHITE', 'PAPER', 'COMPUTING', 'RESOURCES', 'project', ',', 'research', 'team', 'used', 'Dell', 'EMC', 'systems', '2nd-generation', 'Intel®', 'Xeon®', 'Scalable', 'processors', 'Zenith', 'supercomputer', ',', 'one', 'three', 'HPC', 'clusters', 'HPC', '&', 'AI', 'Innovation', 'Lab', '.']

 TOTAL FILTERED TOKENS ==>  33

 ---- POST FOR FILTERED TOKENS ----

 [('2', 'CD'), ('DELL', 'NNP'), ('TECHNOLOGIES', 'NNP'), ('WHITE', 'NNP'), ('PAPER', 'NNP'), ('COMPUTING', 'NNP'), ('RESOURCES', 'NNP'), ('project', 'NN'), (',', ','), ('research', 'NN'), ('team', 'NN'), ('used', 'VBN'), ('Dell', 'NNP'), ('EMC', 'NNP'), ('systems', 'NNS'), ('2nd-generation', 'JJ'), ('Intel®', 'NNP'), ('Xeon®', 'NNP'), ('Scalable', 'NNP'), ('processors', 'NNS'), ('Zenith', 'NNP'), ('supercomputer', 'NN'), (',', ','), ('one', 'CD'), ('three', 'CD'), ('HPC', 'NNP'), ('clusters', 'NNS'), ('HPC', 'NNP'), ('&', 'CC'), ('AI', 'NNP'), ('Innovation', 'NNP'), ('Lab', 'NNP'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['2 DELL', 'DELL TECHNOLOGIES', 'TECHNOLOGIES WHITE', 'WHITE PAPER', 'PAPER COMPUTING', 'COMPUTING RESOURCES', 'RESOURCES project', 'project ,', ', research', 'research team', 'team used', 'used Dell', 'Dell EMC', 'EMC systems', 'systems 2nd-generation', '2nd-generation Intel®', 'Intel® Xeon®', 'Xeon® Scalable', 'Scalable processors', 'processors Zenith', 'Zenith supercomputer', 'supercomputer ,', ', one', 'one three', 'three HPC', 'HPC clusters', 'clusters HPC', 'HPC &', '& AI', 'AI Innovation', 'Innovation Lab', 'Lab .'] 

 TOTAL BIGRAMS --> 32 



 ---- TRI-GRAMS ---- 

 ['2 DELL TECHNOLOGIES', 'DELL TECHNOLOGIES WHITE', 'TECHNOLOGIES WHITE PAPER', 'WHITE PAPER COMPUTING', 'PAPER COMPUTING RESOURCES', 'COMPUTING RESOURCES project', 'RESOURCES project ,', 'project , research', ', research team', 'research team used', 'team used Dell', 'used Dell EMC', 'Dell EMC systems', 'EMC systems 2nd-generation', 'systems 2nd-generation Intel®', '2nd-generation Intel® Xeon®', 'Intel® Xeon® Scalable', 'Xeon® Scalable processors', 'Scalable processors Zenith', 'processors Zenith supercomputer', 'Zenith supercomputer ,', 'supercomputer , one', ', one three', 'one three HPC', 'three HPC clusters', 'HPC clusters HPC', 'clusters HPC &', 'HPC & AI', '& AI Innovation', 'AI Innovation Lab', 'Innovation Lab .'] 

 TOTAL TRIGRAMS --> 31 



 ---- NOUN PHRASES ---- 

 ['project', 'research', 'team', 'supercomputer'] 

 TOTAL NOUN PHRASES --> 4 



 ---- NER ----

 
 ORGANIZATION ---> ['DELL', 'WHITE', 'Dell', 'HPC', 'HPC', 'AI Innovation Lab']
 TOTAL ORGANIZATION ENTITY --> 6 


 PERSON ---> ['Zenith']
 TOTAL PERSON ENTITY --> 1 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['2', 'dell', 'technolog', 'white', 'paper', 'comput', 'resourc', 'project', ',', 'research', 'team', 'use', 'dell', 'emc', 'system', '2nd-gener', 'intel®', 'xeon®', 'scalabl', 'processor', 'zenith', 'supercomput', ',', 'one', 'three', 'hpc', 'cluster', 'hpc', '&', 'ai', 'innov', 'lab', '.']

 TOTAL PORTER STEM WORDS ==> 33



 ---- SNOWBALL STEMMING ----

['2', 'dell', 'technolog', 'white', 'paper', 'comput', 'resourc', 'project', ',', 'research', 'team', 'use', 'dell', 'emc', 'system', '2nd-gener', 'intel®', 'xeon®', 'scalabl', 'processor', 'zenith', 'supercomput', ',', 'one', 'three', 'hpc', 'cluster', 'hpc', '&', 'ai', 'innov', 'lab', '.']

 TOTAL SNOWBALL STEM WORDS ==> 33



 ---- LEMMATIZATION ----

['2', 'DELL', 'TECHNOLOGIES', 'WHITE', 'PAPER', 'COMPUTING', 'RESOURCES', 'project', ',', 'research', 'team', 'used', 'Dell', 'EMC', 'system', '2nd-generation', 'Intel®', 'Xeon®', 'Scalable', 'processor', 'Zenith', 'supercomputer', ',', 'one', 'three', 'HPC', 'cluster', 'HPC', '&', 'AI', 'Innovation', 'Lab', '.']

 TOTAL LEMMATIZE WORDS ==> 33

************************************************************************************************************************

153 --> This TOP500 system, which resulted from a partnership  between Dell Technologies and Intel, serves as a benchmarking system for internal teams,  as well as a showcase resource for evaluations. 


 ---- TOKENS ----

 ['This', 'TOP500', 'system', ',', 'which', 'resulted', 'from', 'a', 'partnership', 'between', 'Dell', 'Technologies', 'and', 'Intel', ',', 'serves', 'as', 'a', 'benchmarking', 'system', 'for', 'internal', 'teams', ',', 'as', 'well', 'as', 'a', 'showcase', 'resource', 'for', 'evaluations', '.'] 

 TOTAL TOKENS ==> 33

 ---- POST ----

 [('This', 'DT'), ('TOP500', 'NNP'), ('system', 'NN'), (',', ','), ('which', 'WDT'), ('resulted', 'VBD'), ('from', 'IN'), ('a', 'DT'), ('partnership', 'NN'), ('between', 'IN'), ('Dell', 'NNP'), ('Technologies', 'NNPS'), ('and', 'CC'), ('Intel', 'NNP'), (',', ','), ('serves', 'VBZ'), ('as', 'IN'), ('a', 'DT'), ('benchmarking', 'NN'), ('system', 'NN'), ('for', 'IN'), ('internal', 'JJ'), ('teams', 'NNS'), (',', ','), ('as', 'RB'), ('well', 'RB'), ('as', 'IN'), ('a', 'DT'), ('showcase', 'NN'), ('resource', 'NN'), ('for', 'IN'), ('evaluations', 'NNS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['TOP500', 'system', ',', 'resulted', 'partnership', 'Dell', 'Technologies', 'Intel', ',', 'serves', 'benchmarking', 'system', 'internal', 'teams', ',', 'well', 'showcase', 'resource', 'evaluations', '.']

 TOTAL FILTERED TOKENS ==>  20

 ---- POST FOR FILTERED TOKENS ----

 [('TOP500', 'NNP'), ('system', 'NN'), (',', ','), ('resulted', 'VBD'), ('partnership', 'NN'), ('Dell', 'NNP'), ('Technologies', 'NNP'), ('Intel', 'NNP'), (',', ','), ('serves', 'VBZ'), ('benchmarking', 'VBG'), ('system', 'NN'), ('internal', 'JJ'), ('teams', 'NNS'), (',', ','), ('well', 'RB'), ('showcase', 'VB'), ('resource', 'NN'), ('evaluations', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['TOP500 system', 'system ,', ', resulted', 'resulted partnership', 'partnership Dell', 'Dell Technologies', 'Technologies Intel', 'Intel ,', ', serves', 'serves benchmarking', 'benchmarking system', 'system internal', 'internal teams', 'teams ,', ', well', 'well showcase', 'showcase resource', 'resource evaluations', 'evaluations .'] 

 TOTAL BIGRAMS --> 19 



 ---- TRI-GRAMS ---- 

 ['TOP500 system ,', 'system , resulted', ', resulted partnership', 'resulted partnership Dell', 'partnership Dell Technologies', 'Dell Technologies Intel', 'Technologies Intel ,', 'Intel , serves', ', serves benchmarking', 'serves benchmarking system', 'benchmarking system internal', 'system internal teams', 'internal teams ,', 'teams , well', ', well showcase', 'well showcase resource', 'showcase resource evaluations', 'resource evaluations .'] 

 TOTAL TRIGRAMS --> 18 



 ---- NOUN PHRASES ---- 

 ['system', 'partnership', 'system', 'resource'] 

 TOTAL NOUN PHRASES --> 4 



 ---- NER ----

 
 ORGANIZATION ---> ['Dell Technologies Intel']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> ['TOP500']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['top500', 'system', ',', 'result', 'partnership', 'dell', 'technolog', 'intel', ',', 'serv', 'benchmark', 'system', 'intern', 'team', ',', 'well', 'showcas', 'resourc', 'evalu', '.']

 TOTAL PORTER STEM WORDS ==> 20



 ---- SNOWBALL STEMMING ----

['top500', 'system', ',', 'result', 'partnership', 'dell', 'technolog', 'intel', ',', 'serv', 'benchmark', 'system', 'intern', 'team', ',', 'well', 'showcas', 'resourc', 'evalu', '.']

 TOTAL SNOWBALL STEM WORDS ==> 20



 ---- LEMMATIZATION ----

['TOP500', 'system', ',', 'resulted', 'partnership', 'Dell', 'Technologies', 'Intel', ',', 'serf', 'benchmarking', 'system', 'internal', 'team', ',', 'well', 'showcase', 'resource', 'evaluation', '.']

 TOTAL LEMMATIZE WORDS ==> 20

************************************************************************************************************************

154 --> In addition, the team leveraged the processing power of the Stampede2 supercomputer at  the Texas Advanced Computing Center (TACC) at The University of Texas at Austin. 


 ---- TOKENS ----

 ['In', 'addition', ',', 'the', 'team', 'leveraged', 'the', 'processing', 'power', 'of', 'the', 'Stampede2', 'supercomputer', 'at', 'the', 'Texas', 'Advanced', 'Computing', 'Center', '(', 'TACC', ')', 'at', 'The', 'University', 'of', 'Texas', 'at', 'Austin', '.'] 

 TOTAL TOKENS ==> 30

 ---- POST ----

 [('In', 'IN'), ('addition', 'NN'), (',', ','), ('the', 'DT'), ('team', 'NN'), ('leveraged', 'VBD'), ('the', 'DT'), ('processing', 'VBG'), ('power', 'NN'), ('of', 'IN'), ('the', 'DT'), ('Stampede2', 'NNP'), ('supercomputer', 'NN'), ('at', 'IN'), ('the', 'DT'), ('Texas', 'NNP'), ('Advanced', 'NNP'), ('Computing', 'NNP'), ('Center', 'NNP'), ('(', '('), ('TACC', 'NNP'), (')', ')'), ('at', 'IN'), ('The', 'DT'), ('University', 'NNP'), ('of', 'IN'), ('Texas', 'NNP'), ('at', 'IN'), ('Austin', 'NNP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['addition', ',', 'team', 'leveraged', 'processing', 'power', 'Stampede2', 'supercomputer', 'Texas', 'Advanced', 'Computing', 'Center', '(', 'TACC', ')', 'University', 'Texas', 'Austin', '.']

 TOTAL FILTERED TOKENS ==>  19

 ---- POST FOR FILTERED TOKENS ----

 [('addition', 'NN'), (',', ','), ('team', 'NN'), ('leveraged', 'VBD'), ('processing', 'VBG'), ('power', 'NN'), ('Stampede2', 'NNP'), ('supercomputer', 'NN'), ('Texas', 'NNP'), ('Advanced', 'NNP'), ('Computing', 'NNP'), ('Center', 'NNP'), ('(', '('), ('TACC', 'NNP'), (')', ')'), ('University', 'NNP'), ('Texas', 'NNP'), ('Austin', 'NNP'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['addition ,', ', team', 'team leveraged', 'leveraged processing', 'processing power', 'power Stampede2', 'Stampede2 supercomputer', 'supercomputer Texas', 'Texas Advanced', 'Advanced Computing', 'Computing Center', 'Center (', '( TACC', 'TACC )', ') University', 'University Texas', 'Texas Austin', 'Austin .'] 

 TOTAL BIGRAMS --> 18 



 ---- TRI-GRAMS ---- 

 ['addition , team', ', team leveraged', 'team leveraged processing', 'leveraged processing power', 'processing power Stampede2', 'power Stampede2 supercomputer', 'Stampede2 supercomputer Texas', 'supercomputer Texas Advanced', 'Texas Advanced Computing', 'Advanced Computing Center', 'Computing Center (', 'Center ( TACC', '( TACC )', 'TACC ) University', ') University Texas', 'University Texas Austin', 'Texas Austin .'] 

 TOTAL TRIGRAMS --> 17 



 ---- NOUN PHRASES ---- 

 ['addition', 'team', 'power', 'supercomputer'] 

 TOTAL NOUN PHRASES --> 4 



 ---- NER ----

 
 ORGANIZATION ---> ['University Texas Austin']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> ['Texas Advanced']
 TOTAL PERSON ENTITY --> 1 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['addit', ',', 'team', 'leverag', 'process', 'power', 'stampede2', 'supercomput', 'texa', 'advanc', 'comput', 'center', '(', 'tacc', ')', 'univers', 'texa', 'austin', '.']

 TOTAL PORTER STEM WORDS ==> 19



 ---- SNOWBALL STEMMING ----

['addit', ',', 'team', 'leverag', 'process', 'power', 'stampede2', 'supercomput', 'texa', 'advanc', 'comput', 'center', '(', 'tacc', ')', 'univers', 'texa', 'austin', '.']

 TOTAL SNOWBALL STEM WORDS ==> 19



 ---- LEMMATIZATION ----

['addition', ',', 'team', 'leveraged', 'processing', 'power', 'Stampede2', 'supercomputer', 'Texas', 'Advanced', 'Computing', 'Center', '(', 'TACC', ')', 'University', 'Texas', 'Austin', '.']

 TOTAL LEMMATIZE WORDS ==> 19

************************************************************************************************************************

155 --> This  Intel-based system, ranked in the TOP500 list, serves as a strategic national resource that  provides HPC capabilities to thousands of researchers across the United States. 


 ---- TOKENS ----

 ['This', 'Intel-based', 'system', ',', 'ranked', 'in', 'the', 'TOP500', 'list', ',', 'serves', 'as', 'a', 'strategic', 'national', 'resource', 'that', 'provides', 'HPC', 'capabilities', 'to', 'thousands', 'of', 'researchers', 'across', 'the', 'United', 'States', '.'] 

 TOTAL TOKENS ==> 29

 ---- POST ----

 [('This', 'DT'), ('Intel-based', 'JJ'), ('system', 'NN'), (',', ','), ('ranked', 'VBN'), ('in', 'IN'), ('the', 'DT'), ('TOP500', 'NNP'), ('list', 'NN'), (',', ','), ('serves', 'VBZ'), ('as', 'IN'), ('a', 'DT'), ('strategic', 'JJ'), ('national', 'JJ'), ('resource', 'NN'), ('that', 'WDT'), ('provides', 'VBZ'), ('HPC', 'NNP'), ('capabilities', 'NNS'), ('to', 'TO'), ('thousands', 'NNS'), ('of', 'IN'), ('researchers', 'NNS'), ('across', 'IN'), ('the', 'DT'), ('United', 'NNP'), ('States', 'NNPS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Intel-based', 'system', ',', 'ranked', 'TOP500', 'list', ',', 'serves', 'strategic', 'national', 'resource', 'provides', 'HPC', 'capabilities', 'thousands', 'researchers', 'across', 'United', 'States', '.']

 TOTAL FILTERED TOKENS ==>  20

 ---- POST FOR FILTERED TOKENS ----

 [('Intel-based', 'JJ'), ('system', 'NN'), (',', ','), ('ranked', 'VBD'), ('TOP500', 'NNP'), ('list', 'NN'), (',', ','), ('serves', 'VBZ'), ('strategic', 'JJ'), ('national', 'JJ'), ('resource', 'NN'), ('provides', 'VBZ'), ('HPC', 'NNP'), ('capabilities', 'NNS'), ('thousands', 'NNS'), ('researchers', 'NNS'), ('across', 'IN'), ('United', 'NNP'), ('States', 'NNPS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Intel-based system', 'system ,', ', ranked', 'ranked TOP500', 'TOP500 list', 'list ,', ', serves', 'serves strategic', 'strategic national', 'national resource', 'resource provides', 'provides HPC', 'HPC capabilities', 'capabilities thousands', 'thousands researchers', 'researchers across', 'across United', 'United States', 'States .'] 

 TOTAL BIGRAMS --> 19 



 ---- TRI-GRAMS ---- 

 ['Intel-based system ,', 'system , ranked', ', ranked TOP500', 'ranked TOP500 list', 'TOP500 list ,', 'list , serves', ', serves strategic', 'serves strategic national', 'strategic national resource', 'national resource provides', 'resource provides HPC', 'provides HPC capabilities', 'HPC capabilities thousands', 'capabilities thousands researchers', 'thousands researchers across', 'researchers across United', 'across United States', 'United States .'] 

 TOTAL TRIGRAMS --> 18 



 ---- NOUN PHRASES ---- 

 ['Intel-based system', 'list', 'strategic national resource'] 

 TOTAL NOUN PHRASES --> 3 



 ---- NER ----

 
 ORGANIZATION ---> ['TOP500', 'HPC']
 TOTAL ORGANIZATION ENTITY --> 2 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> ['United States']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['intel-bas', 'system', ',', 'rank', 'top500', 'list', ',', 'serv', 'strateg', 'nation', 'resourc', 'provid', 'hpc', 'capabl', 'thousand', 'research', 'across', 'unit', 'state', '.']

 TOTAL PORTER STEM WORDS ==> 20



 ---- SNOWBALL STEMMING ----

['intel-bas', 'system', ',', 'rank', 'top500', 'list', ',', 'serv', 'strateg', 'nation', 'resourc', 'provid', 'hpc', 'capabl', 'thousand', 'research', 'across', 'unit', 'state', '.']

 TOTAL SNOWBALL STEM WORDS ==> 20



 ---- LEMMATIZATION ----

['Intel-based', 'system', ',', 'ranked', 'TOP500', 'list', ',', 'serf', 'strategic', 'national', 'resource', 'provides', 'HPC', 'capability', 'thousand', 'researcher', 'across', 'United', 'States', '.']

 TOTAL LEMMATIZE WORDS ==> 20

************************************************************************************************************************

156 --> RESULTS In this project, we demonstrated that the process of training models for language-to- language translation could be scaled to an extreme level — up to 512 nodes — without  impacts on the quality of the results. 


 ---- TOKENS ----

 ['RESULTS', 'In', 'this', 'project', ',', 'we', 'demonstrated', 'that', 'the', 'process', 'of', 'training', 'models', 'for', 'language-to-', 'language', 'translation', 'could', 'be', 'scaled', 'to', 'an', 'extreme', 'level', '—', 'up', 'to', '512', 'nodes', '—', 'without', 'impacts', 'on', 'the', 'quality', 'of', 'the', 'results', '.'] 

 TOTAL TOKENS ==> 39

 ---- POST ----

 [('RESULTS', 'NN'), ('In', 'IN'), ('this', 'DT'), ('project', 'NN'), (',', ','), ('we', 'PRP'), ('demonstrated', 'VBD'), ('that', 'IN'), ('the', 'DT'), ('process', 'NN'), ('of', 'IN'), ('training', 'NN'), ('models', 'NNS'), ('for', 'IN'), ('language-to-', 'JJ'), ('language', 'NN'), ('translation', 'NN'), ('could', 'MD'), ('be', 'VB'), ('scaled', 'VBN'), ('to', 'TO'), ('an', 'DT'), ('extreme', 'JJ'), ('level', 'NN'), ('—', 'VBD'), ('up', 'RB'), ('to', 'TO'), ('512', 'CD'), ('nodes', 'NNS'), ('—', 'VBP'), ('without', 'IN'), ('impacts', 'NNS'), ('on', 'IN'), ('the', 'DT'), ('quality', 'NN'), ('of', 'IN'), ('the', 'DT'), ('results', 'NNS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['RESULTS', 'project', ',', 'demonstrated', 'process', 'training', 'models', 'language-to-', 'language', 'translation', 'could', 'scaled', 'extreme', 'level', '—', '512', 'nodes', '—', 'without', 'impacts', 'quality', 'results', '.']

 TOTAL FILTERED TOKENS ==>  23

 ---- POST FOR FILTERED TOKENS ----

 [('RESULTS', 'NNP'), ('project', 'NN'), (',', ','), ('demonstrated', 'VBD'), ('process', 'NN'), ('training', 'NN'), ('models', 'NNS'), ('language-to-', 'JJ'), ('language', 'NN'), ('translation', 'NN'), ('could', 'MD'), ('scaled', 'VB'), ('extreme', 'JJ'), ('level', 'NN'), ('—', 'VBD'), ('512', 'CD'), ('nodes', 'NNS'), ('—', 'VBP'), ('without', 'IN'), ('impacts', 'NNS'), ('quality', 'JJ'), ('results', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['RESULTS project', 'project ,', ', demonstrated', 'demonstrated process', 'process training', 'training models', 'models language-to-', 'language-to- language', 'language translation', 'translation could', 'could scaled', 'scaled extreme', 'extreme level', 'level —', '— 512', '512 nodes', 'nodes —', '— without', 'without impacts', 'impacts quality', 'quality results', 'results .'] 

 TOTAL BIGRAMS --> 22 



 ---- TRI-GRAMS ---- 

 ['RESULTS project ,', 'project , demonstrated', ', demonstrated process', 'demonstrated process training', 'process training models', 'training models language-to-', 'models language-to- language', 'language-to- language translation', 'language translation could', 'translation could scaled', 'could scaled extreme', 'scaled extreme level', 'extreme level —', 'level — 512', '— 512 nodes', '512 nodes —', 'nodes — without', '— without impacts', 'without impacts quality', 'impacts quality results', 'quality results .'] 

 TOTAL TRIGRAMS --> 21 



 ---- NOUN PHRASES ---- 

 ['project', 'process', 'training', 'language-to- language', 'translation', 'extreme level'] 

 TOTAL NOUN PHRASES --> 6 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> ['RESULTS']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['result', 'project', ',', 'demonstr', 'process', 'train', 'model', 'language-to-', 'languag', 'translat', 'could', 'scale', 'extrem', 'level', '—', '512', 'node', '—', 'without', 'impact', 'qualiti', 'result', '.']

 TOTAL PORTER STEM WORDS ==> 23



 ---- SNOWBALL STEMMING ----

['result', 'project', ',', 'demonstr', 'process', 'train', 'model', 'language-to-', 'languag', 'translat', 'could', 'scale', 'extrem', 'level', '—', '512', 'node', '—', 'without', 'impact', 'qualiti', 'result', '.']

 TOTAL SNOWBALL STEM WORDS ==> 23



 ---- LEMMATIZATION ----

['RESULTS', 'project', ',', 'demonstrated', 'process', 'training', 'model', 'language-to-', 'language', 'translation', 'could', 'scaled', 'extreme', 'level', '—', '512', 'node', '—', 'without', 'impact', 'quality', 'result', '.']

 TOTAL LEMMATIZE WORDS ==> 23

************************************************************************************************************************

157 --> This result suggests that these models can now be  trained at a much faster pace and at a much large scale without breaking the current  state of the art. 


 ---- TOKENS ----

 ['This', 'result', 'suggests', 'that', 'these', 'models', 'can', 'now', 'be', 'trained', 'at', 'a', 'much', 'faster', 'pace', 'and', 'at', 'a', 'much', 'large', 'scale', 'without', 'breaking', 'the', 'current', 'state', 'of', 'the', 'art', '.'] 

 TOTAL TOKENS ==> 30

 ---- POST ----

 [('This', 'DT'), ('result', 'NN'), ('suggests', 'VBZ'), ('that', 'IN'), ('these', 'DT'), ('models', 'NNS'), ('can', 'MD'), ('now', 'RB'), ('be', 'VB'), ('trained', 'VBN'), ('at', 'IN'), ('a', 'DT'), ('much', 'RB'), ('faster', 'RBR'), ('pace', 'NN'), ('and', 'CC'), ('at', 'IN'), ('a', 'DT'), ('much', 'RB'), ('large', 'JJ'), ('scale', 'NN'), ('without', 'IN'), ('breaking', 'VBG'), ('the', 'DT'), ('current', 'JJ'), ('state', 'NN'), ('of', 'IN'), ('the', 'DT'), ('art', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['result', 'suggests', 'models', 'trained', 'much', 'faster', 'pace', 'much', 'large', 'scale', 'without', 'breaking', 'current', 'state', 'art', '.']

 TOTAL FILTERED TOKENS ==>  16

 ---- POST FOR FILTERED TOKENS ----

 [('result', 'NN'), ('suggests', 'VBZ'), ('models', 'NNS'), ('trained', 'VBN'), ('much', 'RB'), ('faster', 'RBR'), ('pace', 'NN'), ('much', 'JJ'), ('large', 'JJ'), ('scale', 'NN'), ('without', 'IN'), ('breaking', 'VBG'), ('current', 'JJ'), ('state', 'NN'), ('art', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['result suggests', 'suggests models', 'models trained', 'trained much', 'much faster', 'faster pace', 'pace much', 'much large', 'large scale', 'scale without', 'without breaking', 'breaking current', 'current state', 'state art', 'art .'] 

 TOTAL BIGRAMS --> 15 



 ---- TRI-GRAMS ---- 

 ['result suggests models', 'suggests models trained', 'models trained much', 'trained much faster', 'much faster pace', 'faster pace much', 'pace much large', 'much large scale', 'large scale without', 'scale without breaking', 'without breaking current', 'breaking current state', 'current state art', 'state art .'] 

 TOTAL TRIGRAMS --> 14 



 ---- NOUN PHRASES ---- 

 ['result', 'pace', 'much large scale', 'current state', 'art'] 

 TOTAL NOUN PHRASES --> 5 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['result', 'suggest', 'model', 'train', 'much', 'faster', 'pace', 'much', 'larg', 'scale', 'without', 'break', 'current', 'state', 'art', '.']

 TOTAL PORTER STEM WORDS ==> 16



 ---- SNOWBALL STEMMING ----

['result', 'suggest', 'model', 'train', 'much', 'faster', 'pace', 'much', 'larg', 'scale', 'without', 'break', 'current', 'state', 'art', '.']

 TOTAL SNOWBALL STEM WORDS ==> 16



 ---- LEMMATIZATION ----

['result', 'suggests', 'model', 'trained', 'much', 'faster', 'pace', 'much', 'large', 'scale', 'without', 'breaking', 'current', 'state', 'art', '.']

 TOTAL LEMMATIZE WORDS ==> 16

************************************************************************************************************************

158 --> TEXT-TO-VOICE TRANSLATION Text-to-voice translation takes written words and converts them to audio. 


 ---- TOKENS ----

 ['TEXT-TO-VOICE', 'TRANSLATION', 'Text-to-voice', 'translation', 'takes', 'written', 'words', 'and', 'converts', 'them', 'to', 'audio', '.'] 

 TOTAL TOKENS ==> 13

 ---- POST ----

 [('TEXT-TO-VOICE', 'JJ'), ('TRANSLATION', 'NNP'), ('Text-to-voice', 'NNP'), ('translation', 'NN'), ('takes', 'VBZ'), ('written', 'VBN'), ('words', 'NNS'), ('and', 'CC'), ('converts', 'NNS'), ('them', 'PRP'), ('to', 'TO'), ('audio', 'VB'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['TEXT-TO-VOICE', 'TRANSLATION', 'Text-to-voice', 'translation', 'takes', 'written', 'words', 'converts', 'audio', '.']

 TOTAL FILTERED TOKENS ==>  10

 ---- POST FOR FILTERED TOKENS ----

 [('TEXT-TO-VOICE', 'JJ'), ('TRANSLATION', 'NNP'), ('Text-to-voice', 'NNP'), ('translation', 'NN'), ('takes', 'VBZ'), ('written', 'VBN'), ('words', 'NNS'), ('converts', 'NNS'), ('audio', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['TEXT-TO-VOICE TRANSLATION', 'TRANSLATION Text-to-voice', 'Text-to-voice translation', 'translation takes', 'takes written', 'written words', 'words converts', 'converts audio', 'audio .'] 

 TOTAL BIGRAMS --> 9 



 ---- TRI-GRAMS ---- 

 ['TEXT-TO-VOICE TRANSLATION Text-to-voice', 'TRANSLATION Text-to-voice translation', 'Text-to-voice translation takes', 'translation takes written', 'takes written words', 'written words converts', 'words converts audio', 'converts audio .'] 

 TOTAL TRIGRAMS --> 8 



 ---- NOUN PHRASES ---- 

 ['translation', 'audio'] 

 TOTAL NOUN PHRASES --> 2 



 ---- NER ----

 
 ORGANIZATION ---> ['TRANSLATION']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['text-to-voic', 'translat', 'text-to-voic', 'translat', 'take', 'written', 'word', 'convert', 'audio', '.']

 TOTAL PORTER STEM WORDS ==> 10



 ---- SNOWBALL STEMMING ----

['text-to-voic', 'translat', 'text-to-voic', 'translat', 'take', 'written', 'word', 'convert', 'audio', '.']

 TOTAL SNOWBALL STEM WORDS ==> 10



 ---- LEMMATIZATION ----

['TEXT-TO-VOICE', 'TRANSLATION', 'Text-to-voice', 'translation', 'take', 'written', 'word', 'convert', 'audio', '.']

 TOTAL LEMMATIZE WORDS ==> 10

************************************************************************************************************************

159 --> The objective is to  generate a complete audio wave form synthetically — while not using the mechanized, clip  recordings that we have been used to hearing on telephone systems for the last 20 years. 


 ---- TOKENS ----

 ['The', 'objective', 'is', 'to', 'generate', 'a', 'complete', 'audio', 'wave', 'form', 'synthetically', '—', 'while', 'not', 'using', 'the', 'mechanized', ',', 'clip', 'recordings', 'that', 'we', 'have', 'been', 'used', 'to', 'hearing', 'on', 'telephone', 'systems', 'for', 'the', 'last', '20', 'years', '.'] 

 TOTAL TOKENS ==> 36

 ---- POST ----

 [('The', 'DT'), ('objective', 'NN'), ('is', 'VBZ'), ('to', 'TO'), ('generate', 'VB'), ('a', 'DT'), ('complete', 'JJ'), ('audio', 'NN'), ('wave', 'NN'), ('form', 'NN'), ('synthetically', 'RB'), ('—', 'NNP'), ('while', 'IN'), ('not', 'RB'), ('using', 'VBG'), ('the', 'DT'), ('mechanized', 'VBN'), (',', ','), ('clip', 'NN'), ('recordings', 'NNS'), ('that', 'IN'), ('we', 'PRP'), ('have', 'VBP'), ('been', 'VBN'), ('used', 'VBN'), ('to', 'TO'), ('hearing', 'VBG'), ('on', 'IN'), ('telephone', 'NN'), ('systems', 'NNS'), ('for', 'IN'), ('the', 'DT'), ('last', 'JJ'), ('20', 'CD'), ('years', 'NNS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['objective', 'generate', 'complete', 'audio', 'wave', 'form', 'synthetically', '—', 'using', 'mechanized', ',', 'clip', 'recordings', 'used', 'hearing', 'telephone', 'systems', 'last', '20', 'years', '.']

 TOTAL FILTERED TOKENS ==>  21

 ---- POST FOR FILTERED TOKENS ----

 [('objective', 'JJ'), ('generate', 'NN'), ('complete', 'JJ'), ('audio', 'NN'), ('wave', 'NN'), ('form', 'NN'), ('synthetically', 'RB'), ('—', 'JJ'), ('using', 'VBG'), ('mechanized', 'VBN'), (',', ','), ('clip', 'NN'), ('recordings', 'NNS'), ('used', 'VBD'), ('hearing', 'VBG'), ('telephone', 'NN'), ('systems', 'NNS'), ('last', 'JJ'), ('20', 'CD'), ('years', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['objective generate', 'generate complete', 'complete audio', 'audio wave', 'wave form', 'form synthetically', 'synthetically —', '— using', 'using mechanized', 'mechanized ,', ', clip', 'clip recordings', 'recordings used', 'used hearing', 'hearing telephone', 'telephone systems', 'systems last', 'last 20', '20 years', 'years .'] 

 TOTAL BIGRAMS --> 20 



 ---- TRI-GRAMS ---- 

 ['objective generate complete', 'generate complete audio', 'complete audio wave', 'audio wave form', 'wave form synthetically', 'form synthetically —', 'synthetically — using', '— using mechanized', 'using mechanized ,', 'mechanized , clip', ', clip recordings', 'clip recordings used', 'recordings used hearing', 'used hearing telephone', 'hearing telephone systems', 'telephone systems last', 'systems last 20', 'last 20 years', '20 years .'] 

 TOTAL TRIGRAMS --> 19 



 ---- NOUN PHRASES ---- 

 ['objective generate', 'complete audio', 'wave', 'form', 'clip', 'telephone'] 

 TOTAL NOUN PHRASES --> 6 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['object', 'gener', 'complet', 'audio', 'wave', 'form', 'synthet', '—', 'use', 'mechan', ',', 'clip', 'record', 'use', 'hear', 'telephon', 'system', 'last', '20', 'year', '.']

 TOTAL PORTER STEM WORDS ==> 21



 ---- SNOWBALL STEMMING ----

['object', 'generat', 'complet', 'audio', 'wave', 'form', 'synthet', '—', 'use', 'mechan', ',', 'clip', 'record', 'use', 'hear', 'telephon', 'system', 'last', '20', 'year', '.']

 TOTAL SNOWBALL STEM WORDS ==> 21



 ---- LEMMATIZATION ----

['objective', 'generate', 'complete', 'audio', 'wave', 'form', 'synthetically', '—', 'using', 'mechanized', ',', 'clip', 'recording', 'used', 'hearing', 'telephone', 'system', 'last', '20', 'year', '.']

 TOTAL LEMMATIZE WORDS ==> 21

************************************************************************************************************************

160 --> With these more advanced approaches, developers use training data that consists of a  transcript and clips of a voice actor reading that transcript. 


 ---- TOKENS ----

 ['With', 'these', 'more', 'advanced', 'approaches', ',', 'developers', 'use', 'training', 'data', 'that', 'consists', 'of', 'a', 'transcript', 'and', 'clips', 'of', 'a', 'voice', 'actor', 'reading', 'that', 'transcript', '.'] 

 TOTAL TOKENS ==> 25

 ---- POST ----

 [('With', 'IN'), ('these', 'DT'), ('more', 'RBR'), ('advanced', 'JJ'), ('approaches', 'NNS'), (',', ','), ('developers', 'NNS'), ('use', 'VBP'), ('training', 'VBG'), ('data', 'NNS'), ('that', 'WDT'), ('consists', 'VBZ'), ('of', 'IN'), ('a', 'DT'), ('transcript', 'NN'), ('and', 'CC'), ('clips', 'NNS'), ('of', 'IN'), ('a', 'DT'), ('voice', 'NN'), ('actor', 'NN'), ('reading', 'NN'), ('that', 'IN'), ('transcript', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['advanced', 'approaches', ',', 'developers', 'use', 'training', 'data', 'consists', 'transcript', 'clips', 'voice', 'actor', 'reading', 'transcript', '.']

 TOTAL FILTERED TOKENS ==>  15

 ---- POST FOR FILTERED TOKENS ----

 [('advanced', 'JJ'), ('approaches', 'NNS'), (',', ','), ('developers', 'NNS'), ('use', 'VBP'), ('training', 'VBG'), ('data', 'NNS'), ('consists', 'VBZ'), ('transcript', 'JJ'), ('clips', 'NNS'), ('voice', 'NN'), ('actor', 'NN'), ('reading', 'VBG'), ('transcript', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['advanced approaches', 'approaches ,', ', developers', 'developers use', 'use training', 'training data', 'data consists', 'consists transcript', 'transcript clips', 'clips voice', 'voice actor', 'actor reading', 'reading transcript', 'transcript .'] 

 TOTAL BIGRAMS --> 14 



 ---- TRI-GRAMS ---- 

 ['advanced approaches ,', 'approaches , developers', ', developers use', 'developers use training', 'use training data', 'training data consists', 'data consists transcript', 'consists transcript clips', 'transcript clips voice', 'clips voice actor', 'voice actor reading', 'actor reading transcript', 'reading transcript .'] 

 TOTAL TRIGRAMS --> 13 



 ---- NOUN PHRASES ---- 

 ['voice', 'actor', 'transcript'] 

 TOTAL NOUN PHRASES --> 3 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['advanc', 'approach', ',', 'develop', 'use', 'train', 'data', 'consist', 'transcript', 'clip', 'voic', 'actor', 'read', 'transcript', '.']

 TOTAL PORTER STEM WORDS ==> 15



 ---- SNOWBALL STEMMING ----

['advanc', 'approach', ',', 'develop', 'use', 'train', 'data', 'consist', 'transcript', 'clip', 'voic', 'actor', 'read', 'transcript', '.']

 TOTAL SNOWBALL STEM WORDS ==> 15



 ---- LEMMATIZATION ----

['advanced', 'approach', ',', 'developer', 'use', 'training', 'data', 'consists', 'transcript', 'clip', 'voice', 'actor', 'reading', 'transcript', '.']

 TOTAL LEMMATIZE WORDS ==> 15

************************************************************************************************************************

161 --> These resources serve as the  training foundation for the creation of a voice that a computer will mimic. 


 ---- TOKENS ----

 ['These', 'resources', 'serve', 'as', 'the', 'training', 'foundation', 'for', 'the', 'creation', 'of', 'a', 'voice', 'that', 'a', 'computer', 'will', 'mimic', '.'] 

 TOTAL TOKENS ==> 19

 ---- POST ----

 [('These', 'DT'), ('resources', 'NNS'), ('serve', 'VBP'), ('as', 'IN'), ('the', 'DT'), ('training', 'NN'), ('foundation', 'NN'), ('for', 'IN'), ('the', 'DT'), ('creation', 'NN'), ('of', 'IN'), ('a', 'DT'), ('voice', 'NN'), ('that', 'IN'), ('a', 'DT'), ('computer', 'NN'), ('will', 'MD'), ('mimic', 'VB'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['resources', 'serve', 'training', 'foundation', 'creation', 'voice', 'computer', 'mimic', '.']

 TOTAL FILTERED TOKENS ==>  9

 ---- POST FOR FILTERED TOKENS ----

 [('resources', 'NNS'), ('serve', 'VBP'), ('training', 'VBG'), ('foundation', 'NN'), ('creation', 'NN'), ('voice', 'NN'), ('computer', 'NN'), ('mimic', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['resources serve', 'serve training', 'training foundation', 'foundation creation', 'creation voice', 'voice computer', 'computer mimic', 'mimic .'] 

 TOTAL BIGRAMS --> 8 



 ---- TRI-GRAMS ---- 

 ['resources serve training', 'serve training foundation', 'training foundation creation', 'foundation creation voice', 'creation voice computer', 'voice computer mimic', 'computer mimic .'] 

 TOTAL TRIGRAMS --> 7 



 ---- NOUN PHRASES ---- 

 ['foundation', 'creation', 'voice', 'computer', 'mimic'] 

 TOTAL NOUN PHRASES --> 5 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['resourc', 'serv', 'train', 'foundat', 'creation', 'voic', 'comput', 'mimic', '.']

 TOTAL PORTER STEM WORDS ==> 9



 ---- SNOWBALL STEMMING ----

['resourc', 'serv', 'train', 'foundat', 'creation', 'voic', 'comput', 'mimic', '.']

 TOTAL SNOWBALL STEM WORDS ==> 9



 ---- LEMMATIZATION ----

['resource', 'serve', 'training', 'foundation', 'creation', 'voice', 'computer', 'mimic', '.']

 TOTAL LEMMATIZE WORDS ==> 9

************************************************************************************************************************

162 --> The developers  then train the neural network to produce a voice that sounds extremely similar to the  actor’s voice, although it’s not that person speaking. 


 ---- TOKENS ----

 ['The', 'developers', 'then', 'train', 'the', 'neural', 'network', 'to', 'produce', 'a', 'voice', 'that', 'sounds', 'extremely', 'similar', 'to', 'the', 'actor', '’', 's', 'voice', ',', 'although', 'it', '’', 's', 'not', 'that', 'person', 'speaking', '.'] 

 TOTAL TOKENS ==> 31

 ---- POST ----

 [('The', 'DT'), ('developers', 'NNS'), ('then', 'RB'), ('train', 'VBP'), ('the', 'DT'), ('neural', 'JJ'), ('network', 'NN'), ('to', 'TO'), ('produce', 'VB'), ('a', 'DT'), ('voice', 'NN'), ('that', 'WDT'), ('sounds', 'VBZ'), ('extremely', 'RB'), ('similar', 'JJ'), ('to', 'TO'), ('the', 'DT'), ('actor', 'NN'), ('’', 'NNP'), ('s', 'NN'), ('voice', 'NN'), (',', ','), ('although', 'IN'), ('it', 'PRP'), ('’', 'VBZ'), ('s', 'PRP'), ('not', 'RB'), ('that', 'IN'), ('person', 'NN'), ('speaking', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['developers', 'train', 'neural', 'network', 'produce', 'voice', 'sounds', 'extremely', 'similar', 'actor', '’', 'voice', ',', 'although', '’', 'person', 'speaking', '.']

 TOTAL FILTERED TOKENS ==>  18

 ---- POST FOR FILTERED TOKENS ----

 [('developers', 'NNS'), ('train', 'VBP'), ('neural', 'JJ'), ('network', 'NN'), ('produce', 'VBP'), ('voice', 'NN'), ('sounds', 'NNS'), ('extremely', 'RB'), ('similar', 'JJ'), ('actor', 'NN'), ('’', 'JJ'), ('voice', 'NN'), (',', ','), ('although', 'IN'), ('’', 'NNP'), ('person', 'NN'), ('speaking', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['developers train', 'train neural', 'neural network', 'network produce', 'produce voice', 'voice sounds', 'sounds extremely', 'extremely similar', 'similar actor', 'actor ’', '’ voice', 'voice ,', ', although', 'although ’', '’ person', 'person speaking', 'speaking .'] 

 TOTAL BIGRAMS --> 17 



 ---- TRI-GRAMS ---- 

 ['developers train neural', 'train neural network', 'neural network produce', 'network produce voice', 'produce voice sounds', 'voice sounds extremely', 'sounds extremely similar', 'extremely similar actor', 'similar actor ’', 'actor ’ voice', '’ voice ,', 'voice , although', ', although ’', 'although ’ person', '’ person speaking', 'person speaking .'] 

 TOTAL TRIGRAMS --> 16 



 ---- NOUN PHRASES ---- 

 ['neural network', 'voice', 'similar actor', '’ voice', 'person', 'speaking'] 

 TOTAL NOUN PHRASES --> 6 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['develop', 'train', 'neural', 'network', 'produc', 'voic', 'sound', 'extrem', 'similar', 'actor', '’', 'voic', ',', 'although', '’', 'person', 'speak', '.']

 TOTAL PORTER STEM WORDS ==> 18



 ---- SNOWBALL STEMMING ----

['develop', 'train', 'neural', 'network', 'produc', 'voic', 'sound', 'extrem', 'similar', 'actor', '’', 'voic', ',', 'although', '’', 'person', 'speak', '.']

 TOTAL SNOWBALL STEM WORDS ==> 18



 ---- LEMMATIZATION ----

['developer', 'train', 'neural', 'network', 'produce', 'voice', 'sound', 'extremely', 'similar', 'actor', '’', 'voice', ',', 'although', '’', 'person', 'speaking', '.']

 TOTAL LEMMATIZE WORDS ==> 18

************************************************************************************************************************

163 --> It’s a neural network creating that  voice completely from scratch. 


 ---- TOKENS ----

 ['It', '’', 's', 'a', 'neural', 'network', 'creating', 'that', 'voice', 'completely', 'from', 'scratch', '.'] 

 TOTAL TOKENS ==> 13

 ---- POST ----

 [('It', 'PRP'), ('’', 'VBZ'), ('s', 'VBZ'), ('a', 'DT'), ('neural', 'JJ'), ('network', 'NN'), ('creating', 'VBG'), ('that', 'IN'), ('voice', 'NN'), ('completely', 'RB'), ('from', 'IN'), ('scratch', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['’', 'neural', 'network', 'creating', 'voice', 'completely', 'scratch', '.']

 TOTAL FILTERED TOKENS ==>  8

 ---- POST FOR FILTERED TOKENS ----

 [('’', 'JJ'), ('neural', 'JJ'), ('network', 'NN'), ('creating', 'VBG'), ('voice', 'NN'), ('completely', 'RB'), ('scratch', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['’ neural', 'neural network', 'network creating', 'creating voice', 'voice completely', 'completely scratch', 'scratch .'] 

 TOTAL BIGRAMS --> 7 



 ---- TRI-GRAMS ---- 

 ['’ neural network', 'neural network creating', 'network creating voice', 'creating voice completely', 'voice completely scratch', 'completely scratch .'] 

 TOTAL TRIGRAMS --> 6 



 ---- NOUN PHRASES ---- 

 ['’ neural network', 'voice', 'scratch'] 

 TOTAL NOUN PHRASES --> 3 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['’', 'neural', 'network', 'creat', 'voic', 'complet', 'scratch', '.']

 TOTAL PORTER STEM WORDS ==> 8



 ---- SNOWBALL STEMMING ----

['’', 'neural', 'network', 'creat', 'voic', 'complet', 'scratch', '.']

 TOTAL SNOWBALL STEM WORDS ==> 8



 ---- LEMMATIZATION ----

['’', 'neural', 'network', 'creating', 'voice', 'completely', 'scratch', '.']

 TOTAL LEMMATIZE WORDS ==> 8

************************************************************************************************************************

164 --> About the lab The Dell Technologies HPC & AI Innovation Lab  encompasses a 13,000 square foot data center in Austin,  Texas, devoted to high-performance computing and artificial  intelligence (AI). 


 ---- TOKENS ----

 ['About', 'the', 'lab', 'The', 'Dell', 'Technologies', 'HPC', '&', 'AI', 'Innovation', 'Lab', 'encompasses', 'a', '13,000', 'square', 'foot', 'data', 'center', 'in', 'Austin', ',', 'Texas', ',', 'devoted', 'to', 'high-performance', 'computing', 'and', 'artificial', 'intelligence', '(', 'AI', ')', '.'] 

 TOTAL TOKENS ==> 34

 ---- POST ----

 [('About', 'IN'), ('the', 'DT'), ('lab', 'NN'), ('The', 'DT'), ('Dell', 'NNP'), ('Technologies', 'NNPS'), ('HPC', 'NNP'), ('&', 'CC'), ('AI', 'NNP'), ('Innovation', 'NNP'), ('Lab', 'NNP'), ('encompasses', 'VBZ'), ('a', 'DT'), ('13,000', 'CD'), ('square', 'JJ'), ('foot', 'NN'), ('data', 'NNS'), ('center', 'NN'), ('in', 'IN'), ('Austin', 'NNP'), (',', ','), ('Texas', 'NNP'), (',', ','), ('devoted', 'VBD'), ('to', 'TO'), ('high-performance', 'NN'), ('computing', 'NN'), ('and', 'CC'), ('artificial', 'JJ'), ('intelligence', 'NN'), ('(', '('), ('AI', 'NNP'), (')', ')'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['lab', 'Dell', 'Technologies', 'HPC', '&', 'AI', 'Innovation', 'Lab', 'encompasses', '13,000', 'square', 'foot', 'data', 'center', 'Austin', ',', 'Texas', ',', 'devoted', 'high-performance', 'computing', 'artificial', 'intelligence', '(', 'AI', ')', '.']

 TOTAL FILTERED TOKENS ==>  27

 ---- POST FOR FILTERED TOKENS ----

 [('lab', 'NN'), ('Dell', 'NNP'), ('Technologies', 'NNP'), ('HPC', 'NNP'), ('&', 'CC'), ('AI', 'NNP'), ('Innovation', 'NNP'), ('Lab', 'NNP'), ('encompasses', 'VBZ'), ('13,000', 'CD'), ('square', 'JJ'), ('foot', 'NN'), ('data', 'NNS'), ('center', 'NN'), ('Austin', 'NNP'), (',', ','), ('Texas', 'NNP'), (',', ','), ('devoted', 'VBD'), ('high-performance', 'NN'), ('computing', 'VBG'), ('artificial', 'JJ'), ('intelligence', 'NN'), ('(', '('), ('AI', 'NNP'), (')', ')'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['lab Dell', 'Dell Technologies', 'Technologies HPC', 'HPC &', '& AI', 'AI Innovation', 'Innovation Lab', 'Lab encompasses', 'encompasses 13,000', '13,000 square', 'square foot', 'foot data', 'data center', 'center Austin', 'Austin ,', ', Texas', 'Texas ,', ', devoted', 'devoted high-performance', 'high-performance computing', 'computing artificial', 'artificial intelligence', 'intelligence (', '( AI', 'AI )', ') .'] 

 TOTAL BIGRAMS --> 26 



 ---- TRI-GRAMS ---- 

 ['lab Dell Technologies', 'Dell Technologies HPC', 'Technologies HPC &', 'HPC & AI', '& AI Innovation', 'AI Innovation Lab', 'Innovation Lab encompasses', 'Lab encompasses 13,000', 'encompasses 13,000 square', '13,000 square foot', 'square foot data', 'foot data center', 'data center Austin', 'center Austin ,', 'Austin , Texas', ', Texas ,', 'Texas , devoted', ', devoted high-performance', 'devoted high-performance computing', 'high-performance computing artificial', 'computing artificial intelligence', 'artificial intelligence (', 'intelligence ( AI', '( AI )', 'AI ) .'] 

 TOTAL TRIGRAMS --> 25 



 ---- NOUN PHRASES ---- 

 ['lab', 'square foot', 'center', 'high-performance', 'artificial intelligence'] 

 TOTAL NOUN PHRASES --> 5 



 ---- NER ----

 
 ORGANIZATION ---> ['Dell Technologies', 'AI Innovation Lab']
 TOTAL ORGANIZATION ENTITY --> 2 


 PERSON ---> ['Austin']
 TOTAL PERSON ENTITY --> 1 


 GPE ---> ['Texas']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['lab', 'dell', 'technolog', 'hpc', '&', 'ai', 'innov', 'lab', 'encompass', '13,000', 'squar', 'foot', 'data', 'center', 'austin', ',', 'texa', ',', 'devot', 'high-perform', 'comput', 'artifici', 'intellig', '(', 'ai', ')', '.']

 TOTAL PORTER STEM WORDS ==> 27



 ---- SNOWBALL STEMMING ----

['lab', 'dell', 'technolog', 'hpc', '&', 'ai', 'innov', 'lab', 'encompass', '13,000', 'squar', 'foot', 'data', 'center', 'austin', ',', 'texa', ',', 'devot', 'high-perform', 'comput', 'artifici', 'intellig', '(', 'ai', ')', '.']

 TOTAL SNOWBALL STEM WORDS ==> 27



 ---- LEMMATIZATION ----

['lab', 'Dell', 'Technologies', 'HPC', '&', 'AI', 'Innovation', 'Lab', 'encompasses', '13,000', 'square', 'foot', 'data', 'center', 'Austin', ',', 'Texas', ',', 'devoted', 'high-performance', 'computing', 'artificial', 'intelligence', '(', 'AI', ')', '.']

 TOTAL LEMMATIZE WORDS ==> 27

************************************************************************************************************************

165 --> It houses thousands of servers, a TOP500  cluster, and a wide range of storage and network systems. 


 ---- TOKENS ----

 ['It', 'houses', 'thousands', 'of', 'servers', ',', 'a', 'TOP500', 'cluster', ',', 'and', 'a', 'wide', 'range', 'of', 'storage', 'and', 'network', 'systems', '.'] 

 TOTAL TOKENS ==> 20

 ---- POST ----

 [('It', 'PRP'), ('houses', 'NNS'), ('thousands', 'NNS'), ('of', 'IN'), ('servers', 'NNS'), (',', ','), ('a', 'DT'), ('TOP500', 'NNP'), ('cluster', 'NN'), (',', ','), ('and', 'CC'), ('a', 'DT'), ('wide', 'JJ'), ('range', 'NN'), ('of', 'IN'), ('storage', 'NN'), ('and', 'CC'), ('network', 'NN'), ('systems', 'NNS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['houses', 'thousands', 'servers', ',', 'TOP500', 'cluster', ',', 'wide', 'range', 'storage', 'network', 'systems', '.']

 TOTAL FILTERED TOKENS ==>  13

 ---- POST FOR FILTERED TOKENS ----

 [('houses', 'NNS'), ('thousands', 'NNS'), ('servers', 'NNS'), (',', ','), ('TOP500', 'NNP'), ('cluster', 'NN'), (',', ','), ('wide', 'JJ'), ('range', 'NN'), ('storage', 'NN'), ('network', 'NN'), ('systems', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['houses thousands', 'thousands servers', 'servers ,', ', TOP500', 'TOP500 cluster', 'cluster ,', ', wide', 'wide range', 'range storage', 'storage network', 'network systems', 'systems .'] 

 TOTAL BIGRAMS --> 12 



 ---- TRI-GRAMS ---- 

 ['houses thousands servers', 'thousands servers ,', 'servers , TOP500', ', TOP500 cluster', 'TOP500 cluster ,', 'cluster , wide', ', wide range', 'wide range storage', 'range storage network', 'storage network systems', 'network systems .'] 

 TOTAL TRIGRAMS --> 11 



 ---- NOUN PHRASES ---- 

 ['cluster', 'wide range', 'storage', 'network'] 

 TOTAL NOUN PHRASES --> 4 



 ---- NER ----

 
 ORGANIZATION ---> ['TOP500']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['hous', 'thousand', 'server', ',', 'top500', 'cluster', ',', 'wide', 'rang', 'storag', 'network', 'system', '.']

 TOTAL PORTER STEM WORDS ==> 13



 ---- SNOWBALL STEMMING ----

['hous', 'thousand', 'server', ',', 'top500', 'cluster', ',', 'wide', 'rang', 'storag', 'network', 'system', '.']

 TOTAL SNOWBALL STEM WORDS ==> 13



 ---- LEMMATIZATION ----

['house', 'thousand', 'server', ',', 'TOP500', 'cluster', ',', 'wide', 'range', 'storage', 'network', 'system', '.']

 TOTAL LEMMATIZE WORDS ==> 13

************************************************************************************************************************

166 --> In addition to providing access to world class infrastructure,  the lab brings together HPC operational excellence and  expertise. 


 ---- TOKENS ----

 ['In', 'addition', 'to', 'providing', 'access', 'to', 'world', 'class', 'infrastructure', ',', 'the', 'lab', 'brings', 'together', 'HPC', 'operational', 'excellence', 'and', 'expertise', '.'] 

 TOTAL TOKENS ==> 20

 ---- POST ----

 [('In', 'IN'), ('addition', 'NN'), ('to', 'TO'), ('providing', 'VBG'), ('access', 'NN'), ('to', 'TO'), ('world', 'NN'), ('class', 'NN'), ('infrastructure', 'NN'), (',', ','), ('the', 'DT'), ('lab', 'NN'), ('brings', 'VBZ'), ('together', 'RB'), ('HPC', 'NNP'), ('operational', 'JJ'), ('excellence', 'NN'), ('and', 'CC'), ('expertise', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['addition', 'providing', 'access', 'world', 'class', 'infrastructure', ',', 'lab', 'brings', 'together', 'HPC', 'operational', 'excellence', 'expertise', '.']

 TOTAL FILTERED TOKENS ==>  15

 ---- POST FOR FILTERED TOKENS ----

 [('addition', 'NN'), ('providing', 'VBG'), ('access', 'NN'), ('world', 'NN'), ('class', 'NN'), ('infrastructure', 'NN'), (',', ','), ('lab', 'JJ'), ('brings', 'VBZ'), ('together', 'RB'), ('HPC', 'NNP'), ('operational', 'JJ'), ('excellence', 'NN'), ('expertise', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['addition providing', 'providing access', 'access world', 'world class', 'class infrastructure', 'infrastructure ,', ', lab', 'lab brings', 'brings together', 'together HPC', 'HPC operational', 'operational excellence', 'excellence expertise', 'expertise .'] 

 TOTAL BIGRAMS --> 14 



 ---- TRI-GRAMS ---- 

 ['addition providing access', 'providing access world', 'access world class', 'world class infrastructure', 'class infrastructure ,', 'infrastructure , lab', ', lab brings', 'lab brings together', 'brings together HPC', 'together HPC operational', 'HPC operational excellence', 'operational excellence expertise', 'excellence expertise .'] 

 TOTAL TRIGRAMS --> 13 



 ---- NOUN PHRASES ---- 

 ['addition', 'access', 'world', 'class', 'infrastructure', 'operational excellence', 'expertise'] 

 TOTAL NOUN PHRASES --> 7 



 ---- NER ----

 
 ORGANIZATION ---> ['HPC']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['addit', 'provid', 'access', 'world', 'class', 'infrastructur', ',', 'lab', 'bring', 'togeth', 'hpc', 'oper', 'excel', 'expertis', '.']

 TOTAL PORTER STEM WORDS ==> 15



 ---- SNOWBALL STEMMING ----

['addit', 'provid', 'access', 'world', 'class', 'infrastructur', ',', 'lab', 'bring', 'togeth', 'hpc', 'oper', 'excel', 'expertis', '.']

 TOTAL SNOWBALL STEM WORDS ==> 15



 ---- LEMMATIZATION ----

['addition', 'providing', 'access', 'world', 'class', 'infrastructure', ',', 'lab', 'brings', 'together', 'HPC', 'operational', 'excellence', 'expertise', '.']

 TOTAL LEMMATIZE WORDS ==> 15

************************************************************************************************************************

167 --> It is staffed by a dedicated group of computer  scientists, engineers and subject matter experts who actively  partner and collaborate with customers and other members  of the HPC community. 


 ---- TOKENS ----

 ['It', 'is', 'staffed', 'by', 'a', 'dedicated', 'group', 'of', 'computer', 'scientists', ',', 'engineers', 'and', 'subject', 'matter', 'experts', 'who', 'actively', 'partner', 'and', 'collaborate', 'with', 'customers', 'and', 'other', 'members', 'of', 'the', 'HPC', 'community', '.'] 

 TOTAL TOKENS ==> 31

 ---- POST ----

 [('It', 'PRP'), ('is', 'VBZ'), ('staffed', 'VBN'), ('by', 'IN'), ('a', 'DT'), ('dedicated', 'JJ'), ('group', 'NN'), ('of', 'IN'), ('computer', 'NN'), ('scientists', 'NNS'), (',', ','), ('engineers', 'NNS'), ('and', 'CC'), ('subject', 'JJ'), ('matter', 'NN'), ('experts', 'NNS'), ('who', 'WP'), ('actively', 'RB'), ('partner', 'NN'), ('and', 'CC'), ('collaborate', 'NN'), ('with', 'IN'), ('customers', 'NNS'), ('and', 'CC'), ('other', 'JJ'), ('members', 'NNS'), ('of', 'IN'), ('the', 'DT'), ('HPC', 'NNP'), ('community', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['staffed', 'dedicated', 'group', 'computer', 'scientists', ',', 'engineers', 'subject', 'matter', 'experts', 'actively', 'partner', 'collaborate', 'customers', 'members', 'HPC', 'community', '.']

 TOTAL FILTERED TOKENS ==>  18

 ---- POST FOR FILTERED TOKENS ----

 [('staffed', 'RB'), ('dedicated', 'VBN'), ('group', 'NN'), ('computer', 'NN'), ('scientists', 'NNS'), (',', ','), ('engineers', 'NNS'), ('subject', 'VBP'), ('matter', 'NN'), ('experts', 'NNS'), ('actively', 'RB'), ('partner', 'NN'), ('collaborate', 'NN'), ('customers', 'NNS'), ('members', 'NNS'), ('HPC', 'NNP'), ('community', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['staffed dedicated', 'dedicated group', 'group computer', 'computer scientists', 'scientists ,', ', engineers', 'engineers subject', 'subject matter', 'matter experts', 'experts actively', 'actively partner', 'partner collaborate', 'collaborate customers', 'customers members', 'members HPC', 'HPC community', 'community .'] 

 TOTAL BIGRAMS --> 17 



 ---- TRI-GRAMS ---- 

 ['staffed dedicated group', 'dedicated group computer', 'group computer scientists', 'computer scientists ,', 'scientists , engineers', ', engineers subject', 'engineers subject matter', 'subject matter experts', 'matter experts actively', 'experts actively partner', 'actively partner collaborate', 'partner collaborate customers', 'collaborate customers members', 'customers members HPC', 'members HPC community', 'HPC community .'] 

 TOTAL TRIGRAMS --> 16 



 ---- NOUN PHRASES ---- 

 ['group', 'computer', 'matter', 'partner', 'collaborate', 'community'] 

 TOTAL NOUN PHRASES --> 6 



 ---- NER ----

 
 ORGANIZATION ---> ['HPC']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['staf', 'dedic', 'group', 'comput', 'scientist', ',', 'engin', 'subject', 'matter', 'expert', 'activ', 'partner', 'collabor', 'custom', 'member', 'hpc', 'commun', '.']

 TOTAL PORTER STEM WORDS ==> 18



 ---- SNOWBALL STEMMING ----

['staf', 'dedic', 'group', 'comput', 'scientist', ',', 'engin', 'subject', 'matter', 'expert', 'activ', 'partner', 'collabor', 'custom', 'member', 'hpc', 'communiti', '.']

 TOTAL SNOWBALL STEM WORDS ==> 18



 ---- LEMMATIZATION ----

['staffed', 'dedicated', 'group', 'computer', 'scientist', ',', 'engineer', 'subject', 'matter', 'expert', 'actively', 'partner', 'collaborate', 'customer', 'member', 'HPC', 'community', '.']

 TOTAL LEMMATIZE WORDS ==> 18

************************************************************************************************************************

168 --> Among other activities, the team  gets early access to new technologies, integrates and tunes  clusters, benchmarks applications, develops best practices,  and publishes white papers. 


 ---- TOKENS ----

 ['Among', 'other', 'activities', ',', 'the', 'team', 'gets', 'early', 'access', 'to', 'new', 'technologies', ',', 'integrates', 'and', 'tunes', 'clusters', ',', 'benchmarks', 'applications', ',', 'develops', 'best', 'practices', ',', 'and', 'publishes', 'white', 'papers', '.'] 

 TOTAL TOKENS ==> 30

 ---- POST ----

 [('Among', 'IN'), ('other', 'JJ'), ('activities', 'NNS'), (',', ','), ('the', 'DT'), ('team', 'NN'), ('gets', 'VBZ'), ('early', 'JJ'), ('access', 'NN'), ('to', 'TO'), ('new', 'JJ'), ('technologies', 'NNS'), (',', ','), ('integrates', 'NNS'), ('and', 'CC'), ('tunes', 'NNS'), ('clusters', 'NNS'), (',', ','), ('benchmarks', 'NNS'), ('applications', 'NNS'), (',', ','), ('develops', 'VBZ'), ('best', 'JJS'), ('practices', 'NNS'), (',', ','), ('and', 'CC'), ('publishes', 'NNS'), ('white', 'JJ'), ('papers', 'NNS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Among', 'activities', ',', 'team', 'gets', 'early', 'access', 'new', 'technologies', ',', 'integrates', 'tunes', 'clusters', ',', 'benchmarks', 'applications', ',', 'develops', 'best', 'practices', ',', 'publishes', 'white', 'papers', '.']

 TOTAL FILTERED TOKENS ==>  25

 ---- POST FOR FILTERED TOKENS ----

 [('Among', 'IN'), ('activities', 'NNS'), (',', ','), ('team', 'NN'), ('gets', 'VBZ'), ('early', 'JJ'), ('access', 'NN'), ('new', 'JJ'), ('technologies', 'NNS'), (',', ','), ('integrates', 'VBZ'), ('tunes', 'NNS'), ('clusters', 'NNS'), (',', ','), ('benchmarks', 'NNS'), ('applications', 'NNS'), (',', ','), ('develops', 'VBZ'), ('best', 'JJS'), ('practices', 'NNS'), (',', ','), ('publishes', 'NNS'), ('white', 'JJ'), ('papers', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Among activities', 'activities ,', ', team', 'team gets', 'gets early', 'early access', 'access new', 'new technologies', 'technologies ,', ', integrates', 'integrates tunes', 'tunes clusters', 'clusters ,', ', benchmarks', 'benchmarks applications', 'applications ,', ', develops', 'develops best', 'best practices', 'practices ,', ', publishes', 'publishes white', 'white papers', 'papers .'] 

 TOTAL BIGRAMS --> 24 



 ---- TRI-GRAMS ---- 

 ['Among activities ,', 'activities , team', ', team gets', 'team gets early', 'gets early access', 'early access new', 'access new technologies', 'new technologies ,', 'technologies , integrates', ', integrates tunes', 'integrates tunes clusters', 'tunes clusters ,', 'clusters , benchmarks', ', benchmarks applications', 'benchmarks applications ,', 'applications , develops', ', develops best', 'develops best practices', 'best practices ,', 'practices , publishes', ', publishes white', 'publishes white papers', 'white papers .'] 

 TOTAL TRIGRAMS --> 23 



 ---- NOUN PHRASES ---- 

 ['team', 'early access'] 

 TOTAL NOUN PHRASES --> 2 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['among', 'activ', ',', 'team', 'get', 'earli', 'access', 'new', 'technolog', ',', 'integr', 'tune', 'cluster', ',', 'benchmark', 'applic', ',', 'develop', 'best', 'practic', ',', 'publish', 'white', 'paper', '.']

 TOTAL PORTER STEM WORDS ==> 25



 ---- SNOWBALL STEMMING ----

['among', 'activ', ',', 'team', 'get', 'earli', 'access', 'new', 'technolog', ',', 'integr', 'tune', 'cluster', ',', 'benchmark', 'applic', ',', 'develop', 'best', 'practic', ',', 'publish', 'white', 'paper', '.']

 TOTAL SNOWBALL STEM WORDS ==> 25



 ---- LEMMATIZATION ----

['Among', 'activity', ',', 'team', 'get', 'early', 'access', 'new', 'technology', ',', 'integrates', 'tune', 'cluster', ',', 'benchmark', 'application', ',', 'develops', 'best', 'practice', ',', 'publishes', 'white', 'paper', '.']

 TOTAL LEMMATIZE WORDS ==> 25

************************************************************************************************************************

169 --> https://www.dellemc.com/en-us/collaterals/unauth/sales-documents/products/storage/h16221-hpc-lab-brochure.pdf https://www.tacc.utexas.edu/systems/stampede2 https://www.top500.org/system/179045/  3 DELL TECHNOLOGIES WHITE PAPER For the text-to-voice translation project initiated in August 2019, we used a two-part  process, with two deep learning models: • We began by taking text and converting it to a spectrogram image, and that takes one  deep learning model. 


 ---- TOKENS ----

 ['https', ':', '//www.dellemc.com/en-us/collaterals/unauth/sales-documents/products/storage/h16221-hpc-lab-brochure.pdf', 'https', ':', '//www.tacc.utexas.edu/systems/stampede2', 'https', ':', '//www.top500.org/system/179045/', '3', 'DELL', 'TECHNOLOGIES', 'WHITE', 'PAPER', 'For', 'the', 'text-to-voice', 'translation', 'project', 'initiated', 'in', 'August', '2019', ',', 'we', 'used', 'a', 'two-part', 'process', ',', 'with', 'two', 'deep', 'learning', 'models', ':', '•', 'We', 'began', 'by', 'taking', 'text', 'and', 'converting', 'it', 'to', 'a', 'spectrogram', 'image', ',', 'and', 'that', 'takes', 'one', 'deep', 'learning', 'model', '.'] 

 TOTAL TOKENS ==> 58

 ---- POST ----

 [('https', 'NN'), (':', ':'), ('//www.dellemc.com/en-us/collaterals/unauth/sales-documents/products/storage/h16221-hpc-lab-brochure.pdf', 'JJ'), ('https', 'NN'), (':', ':'), ('//www.tacc.utexas.edu/systems/stampede2', 'JJ'), ('https', 'NN'), (':', ':'), ('//www.top500.org/system/179045/', 'JJ'), ('3', 'CD'), ('DELL', 'NNP'), ('TECHNOLOGIES', 'NNP'), ('WHITE', 'NNP'), ('PAPER', 'NNP'), ('For', 'IN'), ('the', 'DT'), ('text-to-voice', 'JJ'), ('translation', 'NN'), ('project', 'NN'), ('initiated', 'VBN'), ('in', 'IN'), ('August', 'NNP'), ('2019', 'CD'), (',', ','), ('we', 'PRP'), ('used', 'VBD'), ('a', 'DT'), ('two-part', 'JJ'), ('process', 'NN'), (',', ','), ('with', 'IN'), ('two', 'CD'), ('deep', 'JJ'), ('learning', 'NN'), ('models', 'NNS'), (':', ':'), ('•', 'NN'), ('We', 'PRP'), ('began', 'VBD'), ('by', 'IN'), ('taking', 'VBG'), ('text', 'NN'), ('and', 'CC'), ('converting', 'VBG'), ('it', 'PRP'), ('to', 'TO'), ('a', 'DT'), ('spectrogram', 'JJ'), ('image', 'NN'), (',', ','), ('and', 'CC'), ('that', 'IN'), ('takes', 'VBZ'), ('one', 'CD'), ('deep', 'NN'), ('learning', 'NN'), ('model', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['https', ':', '//www.dellemc.com/en-us/collaterals/unauth/sales-documents/products/storage/h16221-hpc-lab-brochure.pdf', 'https', ':', '//www.tacc.utexas.edu/systems/stampede2', 'https', ':', '//www.top500.org/system/179045/', '3', 'DELL', 'TECHNOLOGIES', 'WHITE', 'PAPER', 'text-to-voice', 'translation', 'project', 'initiated', 'August', '2019', ',', 'used', 'two-part', 'process', ',', 'two', 'deep', 'learning', 'models', ':', '•', 'began', 'taking', 'text', 'converting', 'spectrogram', 'image', ',', 'takes', 'one', 'deep', 'learning', 'model', '.']

 TOTAL FILTERED TOKENS ==>  44

 ---- POST FOR FILTERED TOKENS ----

 [('https', 'NN'), (':', ':'), ('//www.dellemc.com/en-us/collaterals/unauth/sales-documents/products/storage/h16221-hpc-lab-brochure.pdf', 'JJ'), ('https', 'NN'), (':', ':'), ('//www.tacc.utexas.edu/systems/stampede2', 'JJ'), ('https', 'NN'), (':', ':'), ('//www.top500.org/system/179045/', 'JJ'), ('3', 'CD'), ('DELL', 'NNP'), ('TECHNOLOGIES', 'NNP'), ('WHITE', 'NNP'), ('PAPER', 'NNP'), ('text-to-voice', 'NN'), ('translation', 'NN'), ('project', 'NN'), ('initiated', 'VBN'), ('August', 'NNP'), ('2019', 'CD'), (',', ','), ('used', 'VBD'), ('two-part', 'JJ'), ('process', 'NN'), (',', ','), ('two', 'CD'), ('deep', 'JJ'), ('learning', 'NN'), ('models', 'NNS'), (':', ':'), ('•', 'NN'), ('began', 'VBD'), ('taking', 'VBG'), ('text', 'NN'), ('converting', 'VBG'), ('spectrogram', 'JJ'), ('image', 'NN'), (',', ','), ('takes', 'VBZ'), ('one', 'CD'), ('deep', 'NN'), ('learning', 'NN'), ('model', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['https :', ': //www.dellemc.com/en-us/collaterals/unauth/sales-documents/products/storage/h16221-hpc-lab-brochure.pdf', '//www.dellemc.com/en-us/collaterals/unauth/sales-documents/products/storage/h16221-hpc-lab-brochure.pdf https', 'https :', ': //www.tacc.utexas.edu/systems/stampede2', '//www.tacc.utexas.edu/systems/stampede2 https', 'https :', ': //www.top500.org/system/179045/', '//www.top500.org/system/179045/ 3', '3 DELL', 'DELL TECHNOLOGIES', 'TECHNOLOGIES WHITE', 'WHITE PAPER', 'PAPER text-to-voice', 'text-to-voice translation', 'translation project', 'project initiated', 'initiated August', 'August 2019', '2019 ,', ', used', 'used two-part', 'two-part process', 'process ,', ', two', 'two deep', 'deep learning', 'learning models', 'models :', ': •', '• began', 'began taking', 'taking text', 'text converting', 'converting spectrogram', 'spectrogram image', 'image ,', ', takes', 'takes one', 'one deep', 'deep learning', 'learning model', 'model .'] 

 TOTAL BIGRAMS --> 43 



 ---- TRI-GRAMS ---- 

 ['https : //www.dellemc.com/en-us/collaterals/unauth/sales-documents/products/storage/h16221-hpc-lab-brochure.pdf', ': //www.dellemc.com/en-us/collaterals/unauth/sales-documents/products/storage/h16221-hpc-lab-brochure.pdf https', '//www.dellemc.com/en-us/collaterals/unauth/sales-documents/products/storage/h16221-hpc-lab-brochure.pdf https :', 'https : //www.tacc.utexas.edu/systems/stampede2', ': //www.tacc.utexas.edu/systems/stampede2 https', '//www.tacc.utexas.edu/systems/stampede2 https :', 'https : //www.top500.org/system/179045/', ': //www.top500.org/system/179045/ 3', '//www.top500.org/system/179045/ 3 DELL', '3 DELL TECHNOLOGIES', 'DELL TECHNOLOGIES WHITE', 'TECHNOLOGIES WHITE PAPER', 'WHITE PAPER text-to-voice', 'PAPER text-to-voice translation', 'text-to-voice translation project', 'translation project initiated', 'project initiated August', 'initiated August 2019', 'August 2019 ,', '2019 , used', ', used two-part', 'used two-part process', 'two-part process ,', 'process , two', ', two deep', 'two deep learning', 'deep learning models', 'learning models :', 'models : •', ': • began', '• began taking', 'began taking text', 'taking text converting', 'text converting spectrogram', 'converting spectrogram image', 'spectrogram image ,', 'image , takes', ', takes one', 'takes one deep', 'one deep learning', 'deep learning model', 'learning model .'] 

 TOTAL TRIGRAMS --> 42 



 ---- NOUN PHRASES ---- 

 ['https', ' https', ' https', 'text-to-voice', 'translation', 'project', 'two-part process', 'deep learning', '•', 'text', 'spectrogram image', 'deep', 'learning', 'model'] 

 TOTAL NOUN PHRASES --> 14 



 ---- NER ----

 
 ORGANIZATION ---> ['DELL', 'WHITE']
 TOTAL ORGANIZATION ENTITY --> 2 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['http', ':', '//www.dellemc.com/en-us/collaterals/unauth/sales-documents/products/storage/h16221-hpc-lab-brochure.pdf', 'http', ':', '//www.tacc.utexas.edu/systems/stampede2', 'http', ':', '//www.top500.org/system/179045/', '3', 'dell', 'technolog', 'white', 'paper', 'text-to-voic', 'translat', 'project', 'initi', 'august', '2019', ',', 'use', 'two-part', 'process', ',', 'two', 'deep', 'learn', 'model', ':', '•', 'began', 'take', 'text', 'convert', 'spectrogram', 'imag', ',', 'take', 'one', 'deep', 'learn', 'model', '.']

 TOTAL PORTER STEM WORDS ==> 44



 ---- SNOWBALL STEMMING ----

['https', ':', '//www.dellemc.com/en-us/collaterals/unauth/sales-documents/products/storage/h16221-hpc-lab-brochure.pdf', 'https', ':', '//www.tacc.utexas.edu/systems/stampede2', 'https', ':', '//www.top500.org/system/179045/', '3', 'dell', 'technolog', 'white', 'paper', 'text-to-voic', 'translat', 'project', 'initi', 'august', '2019', ',', 'use', 'two-part', 'process', ',', 'two', 'deep', 'learn', 'model', ':', '•', 'began', 'take', 'text', 'convert', 'spectrogram', 'imag', ',', 'take', 'one', 'deep', 'learn', 'model', '.']

 TOTAL SNOWBALL STEM WORDS ==> 44



 ---- LEMMATIZATION ----

['http', ':', '//www.dellemc.com/en-us/collaterals/unauth/sales-documents/products/storage/h16221-hpc-lab-brochure.pdf', 'http', ':', '//www.tacc.utexas.edu/systems/stampede2', 'http', ':', '//www.top500.org/system/179045/', '3', 'DELL', 'TECHNOLOGIES', 'WHITE', 'PAPER', 'text-to-voice', 'translation', 'project', 'initiated', 'August', '2019', ',', 'used', 'two-part', 'process', ',', 'two', 'deep', 'learning', 'model', ':', '•', 'began', 'taking', 'text', 'converting', 'spectrogram', 'image', ',', 'take', 'one', 'deep', 'learning', 'model', '.']

 TOTAL LEMMATIZE WORDS ==> 44

************************************************************************************************************************

170 --> This spectrogram image is a frequency distribution of the letters  and sounds that are expected to be produced in the resulting voice. 


 ---- TOKENS ----

 ['This', 'spectrogram', 'image', 'is', 'a', 'frequency', 'distribution', 'of', 'the', 'letters', 'and', 'sounds', 'that', 'are', 'expected', 'to', 'be', 'produced', 'in', 'the', 'resulting', 'voice', '.'] 

 TOTAL TOKENS ==> 23

 ---- POST ----

 [('This', 'DT'), ('spectrogram', 'JJ'), ('image', 'NN'), ('is', 'VBZ'), ('a', 'DT'), ('frequency', 'NN'), ('distribution', 'NN'), ('of', 'IN'), ('the', 'DT'), ('letters', 'NNS'), ('and', 'CC'), ('sounds', 'NNS'), ('that', 'WDT'), ('are', 'VBP'), ('expected', 'VBN'), ('to', 'TO'), ('be', 'VB'), ('produced', 'VBN'), ('in', 'IN'), ('the', 'DT'), ('resulting', 'JJ'), ('voice', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['spectrogram', 'image', 'frequency', 'distribution', 'letters', 'sounds', 'expected', 'produced', 'resulting', 'voice', '.']

 TOTAL FILTERED TOKENS ==>  11

 ---- POST FOR FILTERED TOKENS ----

 [('spectrogram', 'JJ'), ('image', 'NN'), ('frequency', 'NN'), ('distribution', 'NN'), ('letters', 'NNS'), ('sounds', 'VBZ'), ('expected', 'VBN'), ('produced', 'VBD'), ('resulting', 'JJ'), ('voice', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['spectrogram image', 'image frequency', 'frequency distribution', 'distribution letters', 'letters sounds', 'sounds expected', 'expected produced', 'produced resulting', 'resulting voice', 'voice .'] 

 TOTAL BIGRAMS --> 10 



 ---- TRI-GRAMS ---- 

 ['spectrogram image frequency', 'image frequency distribution', 'frequency distribution letters', 'distribution letters sounds', 'letters sounds expected', 'sounds expected produced', 'expected produced resulting', 'produced resulting voice', 'resulting voice .'] 

 TOTAL TRIGRAMS --> 9 



 ---- NOUN PHRASES ---- 

 ['spectrogram image', 'frequency', 'distribution', 'resulting voice'] 

 TOTAL NOUN PHRASES --> 4 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['spectrogram', 'imag', 'frequenc', 'distribut', 'letter', 'sound', 'expect', 'produc', 'result', 'voic', '.']

 TOTAL PORTER STEM WORDS ==> 11



 ---- SNOWBALL STEMMING ----

['spectrogram', 'imag', 'frequenc', 'distribut', 'letter', 'sound', 'expect', 'produc', 'result', 'voic', '.']

 TOTAL SNOWBALL STEM WORDS ==> 11



 ---- LEMMATIZATION ----

['spectrogram', 'image', 'frequency', 'distribution', 'letter', 'sound', 'expected', 'produced', 'resulting', 'voice', '.']

 TOTAL LEMMATIZE WORDS ==> 11

************************************************************************************************************************

171 --> • We then created a second model that takes the spectrogram and generates a complete  audio waveform that uses a completely synthetic voice that pretends to be the voice actor  that was used in the training process. 


 ---- TOKENS ----

 ['•', 'We', 'then', 'created', 'a', 'second', 'model', 'that', 'takes', 'the', 'spectrogram', 'and', 'generates', 'a', 'complete', 'audio', 'waveform', 'that', 'uses', 'a', 'completely', 'synthetic', 'voice', 'that', 'pretends', 'to', 'be', 'the', 'voice', 'actor', 'that', 'was', 'used', 'in', 'the', 'training', 'process', '.'] 

 TOTAL TOKENS ==> 38

 ---- POST ----

 [('•', 'NN'), ('We', 'PRP'), ('then', 'RB'), ('created', 'VBD'), ('a', 'DT'), ('second', 'JJ'), ('model', 'NN'), ('that', 'WDT'), ('takes', 'VBZ'), ('the', 'DT'), ('spectrogram', 'NN'), ('and', 'CC'), ('generates', 'VBZ'), ('a', 'DT'), ('complete', 'JJ'), ('audio', 'NN'), ('waveform', 'NN'), ('that', 'WDT'), ('uses', 'VBZ'), ('a', 'DT'), ('completely', 'RB'), ('synthetic', 'JJ'), ('voice', 'NN'), ('that', 'WDT'), ('pretends', 'VBZ'), ('to', 'TO'), ('be', 'VB'), ('the', 'DT'), ('voice', 'NN'), ('actor', 'NN'), ('that', 'WDT'), ('was', 'VBD'), ('used', 'VBN'), ('in', 'IN'), ('the', 'DT'), ('training', 'NN'), ('process', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['•', 'created', 'second', 'model', 'takes', 'spectrogram', 'generates', 'complete', 'audio', 'waveform', 'uses', 'completely', 'synthetic', 'voice', 'pretends', 'voice', 'actor', 'used', 'training', 'process', '.']

 TOTAL FILTERED TOKENS ==>  21

 ---- POST FOR FILTERED TOKENS ----

 [('•', 'RB'), ('created', 'VBN'), ('second', 'JJ'), ('model', 'NN'), ('takes', 'VBZ'), ('spectrogram', 'JJ'), ('generates', 'NNS'), ('complete', 'JJ'), ('audio', 'JJ'), ('waveform', 'NN'), ('uses', 'VBZ'), ('completely', 'RB'), ('synthetic', 'JJ'), ('voice', 'NN'), ('pretends', 'NNS'), ('voice', 'NN'), ('actor', 'NN'), ('used', 'VBD'), ('training', 'NN'), ('process', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['• created', 'created second', 'second model', 'model takes', 'takes spectrogram', 'spectrogram generates', 'generates complete', 'complete audio', 'audio waveform', 'waveform uses', 'uses completely', 'completely synthetic', 'synthetic voice', 'voice pretends', 'pretends voice', 'voice actor', 'actor used', 'used training', 'training process', 'process .'] 

 TOTAL BIGRAMS --> 20 



 ---- TRI-GRAMS ---- 

 ['• created second', 'created second model', 'second model takes', 'model takes spectrogram', 'takes spectrogram generates', 'spectrogram generates complete', 'generates complete audio', 'complete audio waveform', 'audio waveform uses', 'waveform uses completely', 'uses completely synthetic', 'completely synthetic voice', 'synthetic voice pretends', 'voice pretends voice', 'pretends voice actor', 'voice actor used', 'actor used training', 'used training process', 'training process .'] 

 TOTAL TRIGRAMS --> 19 



 ---- NOUN PHRASES ---- 

 ['second model', 'complete audio waveform', 'synthetic voice', 'voice', 'actor', 'training', 'process'] 

 TOTAL NOUN PHRASES --> 7 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['•', 'creat', 'second', 'model', 'take', 'spectrogram', 'gener', 'complet', 'audio', 'waveform', 'use', 'complet', 'synthet', 'voic', 'pretend', 'voic', 'actor', 'use', 'train', 'process', '.']

 TOTAL PORTER STEM WORDS ==> 21



 ---- SNOWBALL STEMMING ----

['•', 'creat', 'second', 'model', 'take', 'spectrogram', 'generat', 'complet', 'audio', 'waveform', 'use', 'complet', 'synthet', 'voic', 'pretend', 'voic', 'actor', 'use', 'train', 'process', '.']

 TOTAL SNOWBALL STEM WORDS ==> 21



 ---- LEMMATIZATION ----

['•', 'created', 'second', 'model', 'take', 'spectrogram', 'generates', 'complete', 'audio', 'waveform', 'us', 'completely', 'synthetic', 'voice', 'pretend', 'voice', 'actor', 'used', 'training', 'process', '.']

 TOTAL LEMMATIZE WORDS ==> 21

************************************************************************************************************************

172 --> Again, we’re not just stitching together voice clips of  an actor talking. 


 ---- TOKENS ----

 ['Again', ',', 'we', '’', 're', 'not', 'just', 'stitching', 'together', 'voice', 'clips', 'of', 'an', 'actor', 'talking', '.'] 

 TOTAL TOKENS ==> 16

 ---- POST ----

 [('Again', 'RB'), (',', ','), ('we', 'PRP'), ('’', 'VBP'), ('re', 'JJ'), ('not', 'RB'), ('just', 'RB'), ('stitching', 'VBG'), ('together', 'RB'), ('voice', 'JJ'), ('clips', 'NNS'), ('of', 'IN'), ('an', 'DT'), ('actor', 'NN'), ('talking', 'VBG'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 [',', '’', 'stitching', 'together', 'voice', 'clips', 'actor', 'talking', '.']

 TOTAL FILTERED TOKENS ==>  9

 ---- POST FOR FILTERED TOKENS ----

 [(',', ','), ('’', 'EX'), ('stitching', 'VBG'), ('together', 'RB'), ('voice', 'NN'), ('clips', 'NNS'), ('actor', 'NN'), ('talking', 'VBG'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 [', ’', '’ stitching', 'stitching together', 'together voice', 'voice clips', 'clips actor', 'actor talking', 'talking .'] 

 TOTAL BIGRAMS --> 8 



 ---- TRI-GRAMS ---- 

 [', ’ stitching', '’ stitching together', 'stitching together voice', 'together voice clips', 'voice clips actor', 'clips actor talking', 'actor talking .'] 

 TOTAL TRIGRAMS --> 7 



 ---- NOUN PHRASES ---- 

 ['voice', 'actor'] 

 TOTAL NOUN PHRASES --> 2 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

[',', '’', 'stitch', 'togeth', 'voic', 'clip', 'actor', 'talk', '.']

 TOTAL PORTER STEM WORDS ==> 9



 ---- SNOWBALL STEMMING ----

[',', '’', 'stitch', 'togeth', 'voic', 'clip', 'actor', 'talk', '.']

 TOTAL SNOWBALL STEM WORDS ==> 9



 ---- LEMMATIZATION ----

[',', '’', 'stitching', 'together', 'voice', 'clip', 'actor', 'talking', '.']

 TOTAL LEMMATIZE WORDS ==> 9

************************************************************************************************************************

173 --> We’re creating a synthetic voice that sounds a lot like the original. 


 ---- TOKENS ----

 ['We', '’', 're', 'creating', 'a', 'synthetic', 'voice', 'that', 'sounds', 'a', 'lot', 'like', 'the', 'original', '.'] 

 TOTAL TOKENS ==> 15

 ---- POST ----

 [('We', 'PRP'), ('’', 'VBP'), ('re', 'JJ'), ('creating', 'VBG'), ('a', 'DT'), ('synthetic', 'JJ'), ('voice', 'NN'), ('that', 'WDT'), ('sounds', 'VBZ'), ('a', 'DT'), ('lot', 'NN'), ('like', 'IN'), ('the', 'DT'), ('original', 'JJ'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['’', 'creating', 'synthetic', 'voice', 'sounds', 'lot', 'like', 'original', '.']

 TOTAL FILTERED TOKENS ==>  9

 ---- POST FOR FILTERED TOKENS ----

 [('’', 'NN'), ('creating', 'VBG'), ('synthetic', 'JJ'), ('voice', 'NN'), ('sounds', 'VBZ'), ('lot', 'NN'), ('like', 'IN'), ('original', 'JJ'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['’ creating', 'creating synthetic', 'synthetic voice', 'voice sounds', 'sounds lot', 'lot like', 'like original', 'original .'] 

 TOTAL BIGRAMS --> 8 



 ---- TRI-GRAMS ---- 

 ['’ creating synthetic', 'creating synthetic voice', 'synthetic voice sounds', 'voice sounds lot', 'sounds lot like', 'lot like original', 'like original .'] 

 TOTAL TRIGRAMS --> 7 



 ---- NOUN PHRASES ---- 

 ['’', 'synthetic voice', 'lot'] 

 TOTAL NOUN PHRASES --> 3 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['’', 'creat', 'synthet', 'voic', 'sound', 'lot', 'like', 'origin', '.']

 TOTAL PORTER STEM WORDS ==> 9



 ---- SNOWBALL STEMMING ----

['’', 'creat', 'synthet', 'voic', 'sound', 'lot', 'like', 'origin', '.']

 TOTAL SNOWBALL STEM WORDS ==> 9



 ---- LEMMATIZATION ----

['’', 'creating', 'synthetic', 'voice', 'sound', 'lot', 'like', 'original', '.']

 TOTAL LEMMATIZE WORDS ==> 9

************************************************************************************************************************

174 --> In this ongoing project, we are now working to accelerate the process of producing the  audio waveforms. 


 ---- TOKENS ----

 ['In', 'this', 'ongoing', 'project', ',', 'we', 'are', 'now', 'working', 'to', 'accelerate', 'the', 'process', 'of', 'producing', 'the', 'audio', 'waveforms', '.'] 

 TOTAL TOKENS ==> 19

 ---- POST ----

 [('In', 'IN'), ('this', 'DT'), ('ongoing', 'JJ'), ('project', 'NN'), (',', ','), ('we', 'PRP'), ('are', 'VBP'), ('now', 'RB'), ('working', 'VBG'), ('to', 'TO'), ('accelerate', 'VB'), ('the', 'DT'), ('process', 'NN'), ('of', 'IN'), ('producing', 'VBG'), ('the', 'DT'), ('audio', 'NN'), ('waveforms', 'NNS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['ongoing', 'project', ',', 'working', 'accelerate', 'process', 'producing', 'audio', 'waveforms', '.']

 TOTAL FILTERED TOKENS ==>  10

 ---- POST FOR FILTERED TOKENS ----

 [('ongoing', 'VBG'), ('project', 'NN'), (',', ','), ('working', 'VBG'), ('accelerate', 'JJ'), ('process', 'NN'), ('producing', 'VBG'), ('audio', 'JJ'), ('waveforms', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['ongoing project', 'project ,', ', working', 'working accelerate', 'accelerate process', 'process producing', 'producing audio', 'audio waveforms', 'waveforms .'] 

 TOTAL BIGRAMS --> 9 



 ---- TRI-GRAMS ---- 

 ['ongoing project ,', 'project , working', ', working accelerate', 'working accelerate process', 'accelerate process producing', 'process producing audio', 'producing audio waveforms', 'audio waveforms .'] 

 TOTAL TRIGRAMS --> 8 



 ---- NOUN PHRASES ---- 

 ['project', 'accelerate process'] 

 TOTAL NOUN PHRASES --> 2 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['ongo', 'project', ',', 'work', 'acceler', 'process', 'produc', 'audio', 'waveform', '.']

 TOTAL PORTER STEM WORDS ==> 10



 ---- SNOWBALL STEMMING ----

['ongo', 'project', ',', 'work', 'acceler', 'process', 'produc', 'audio', 'waveform', '.']

 TOTAL SNOWBALL STEM WORDS ==> 10



 ---- LEMMATIZATION ----

['ongoing', 'project', ',', 'working', 'accelerate', 'process', 'producing', 'audio', 'waveform', '.']

 TOTAL LEMMATIZE WORDS ==> 10

************************************************************************************************************************

175 --> COMPUTING RESOURCES For this work, we are parallelizing the work across NVIDIA V100 GPUs in the Rattler  supercomputer, which is housed in our HPC & AI Innovation Lab. 


 ---- TOKENS ----

 ['COMPUTING', 'RESOURCES', 'For', 'this', 'work', ',', 'we', 'are', 'parallelizing', 'the', 'work', 'across', 'NVIDIA', 'V100', 'GPUs', 'in', 'the', 'Rattler', 'supercomputer', ',', 'which', 'is', 'housed', 'in', 'our', 'HPC', '&', 'AI', 'Innovation', 'Lab', '.'] 

 TOTAL TOKENS ==> 31

 ---- POST ----

 [('COMPUTING', 'NNP'), ('RESOURCES', 'NNP'), ('For', 'IN'), ('this', 'DT'), ('work', 'NN'), (',', ','), ('we', 'PRP'), ('are', 'VBP'), ('parallelizing', 'VBG'), ('the', 'DT'), ('work', 'NN'), ('across', 'IN'), ('NVIDIA', 'NNP'), ('V100', 'NNP'), ('GPUs', 'NNP'), ('in', 'IN'), ('the', 'DT'), ('Rattler', 'NNP'), ('supercomputer', 'NN'), (',', ','), ('which', 'WDT'), ('is', 'VBZ'), ('housed', 'VBN'), ('in', 'IN'), ('our', 'PRP$'), ('HPC', 'NNP'), ('&', 'CC'), ('AI', 'NNP'), ('Innovation', 'NNP'), ('Lab', 'NNP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['COMPUTING', 'RESOURCES', 'work', ',', 'parallelizing', 'work', 'across', 'NVIDIA', 'V100', 'GPUs', 'Rattler', 'supercomputer', ',', 'housed', 'HPC', '&', 'AI', 'Innovation', 'Lab', '.']

 TOTAL FILTERED TOKENS ==>  20

 ---- POST FOR FILTERED TOKENS ----

 [('COMPUTING', 'NNP'), ('RESOURCES', 'NNP'), ('work', 'NN'), (',', ','), ('parallelizing', 'VBG'), ('work', 'NN'), ('across', 'IN'), ('NVIDIA', 'NNP'), ('V100', 'NNP'), ('GPUs', 'NNP'), ('Rattler', 'NNP'), ('supercomputer', 'NN'), (',', ','), ('housed', 'VBD'), ('HPC', 'NNP'), ('&', 'CC'), ('AI', 'NNP'), ('Innovation', 'NNP'), ('Lab', 'NNP'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['COMPUTING RESOURCES', 'RESOURCES work', 'work ,', ', parallelizing', 'parallelizing work', 'work across', 'across NVIDIA', 'NVIDIA V100', 'V100 GPUs', 'GPUs Rattler', 'Rattler supercomputer', 'supercomputer ,', ', housed', 'housed HPC', 'HPC &', '& AI', 'AI Innovation', 'Innovation Lab', 'Lab .'] 

 TOTAL BIGRAMS --> 19 



 ---- TRI-GRAMS ---- 

 ['COMPUTING RESOURCES work', 'RESOURCES work ,', 'work , parallelizing', ', parallelizing work', 'parallelizing work across', 'work across NVIDIA', 'across NVIDIA V100', 'NVIDIA V100 GPUs', 'V100 GPUs Rattler', 'GPUs Rattler supercomputer', 'Rattler supercomputer ,', 'supercomputer , housed', ', housed HPC', 'housed HPC &', 'HPC & AI', '& AI Innovation', 'AI Innovation Lab', 'Innovation Lab .'] 

 TOTAL TRIGRAMS --> 18 



 ---- NOUN PHRASES ---- 

 ['work', 'work', 'supercomputer'] 

 TOTAL NOUN PHRASES --> 3 



 ---- NER ----

 
 ORGANIZATION ---> ['COMPUTING', 'NVIDIA V100', 'HPC', 'AI Innovation Lab']
 TOTAL ORGANIZATION ENTITY --> 4 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['comput', 'resourc', 'work', ',', 'parallel', 'work', 'across', 'nvidia', 'v100', 'gpu', 'rattler', 'supercomput', ',', 'hous', 'hpc', '&', 'ai', 'innov', 'lab', '.']

 TOTAL PORTER STEM WORDS ==> 20



 ---- SNOWBALL STEMMING ----

['comput', 'resourc', 'work', ',', 'parallel', 'work', 'across', 'nvidia', 'v100', 'gpus', 'rattler', 'supercomput', ',', 'hous', 'hpc', '&', 'ai', 'innov', 'lab', '.']

 TOTAL SNOWBALL STEM WORDS ==> 20



 ---- LEMMATIZATION ----

['COMPUTING', 'RESOURCES', 'work', ',', 'parallelizing', 'work', 'across', 'NVIDIA', 'V100', 'GPUs', 'Rattler', 'supercomputer', ',', 'housed', 'HPC', '&', 'AI', 'Innovation', 'Lab', '.']

 TOTAL LEMMATIZE WORDS ==> 20

************************************************************************************************************************

176 --> The Rattler cluster is the  result of a partnership among Dell Technologies, Mellanox, Bright Computing and NVIDIA. 


 ---- TOKENS ----

 ['The', 'Rattler', 'cluster', 'is', 'the', 'result', 'of', 'a', 'partnership', 'among', 'Dell', 'Technologies', ',', 'Mellanox', ',', 'Bright', 'Computing', 'and', 'NVIDIA', '.'] 

 TOTAL TOKENS ==> 20

 ---- POST ----

 [('The', 'DT'), ('Rattler', 'NNP'), ('cluster', 'NN'), ('is', 'VBZ'), ('the', 'DT'), ('result', 'NN'), ('of', 'IN'), ('a', 'DT'), ('partnership', 'NN'), ('among', 'IN'), ('Dell', 'NNP'), ('Technologies', 'NNPS'), (',', ','), ('Mellanox', 'NNP'), (',', ','), ('Bright', 'NNP'), ('Computing', 'NNP'), ('and', 'CC'), ('NVIDIA', 'NNP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Rattler', 'cluster', 'result', 'partnership', 'among', 'Dell', 'Technologies', ',', 'Mellanox', ',', 'Bright', 'Computing', 'NVIDIA', '.']

 TOTAL FILTERED TOKENS ==>  14

 ---- POST FOR FILTERED TOKENS ----

 [('Rattler', 'NNP'), ('cluster', 'NN'), ('result', 'NN'), ('partnership', 'NN'), ('among', 'IN'), ('Dell', 'NNP'), ('Technologies', 'NNPS'), (',', ','), ('Mellanox', 'NNP'), (',', ','), ('Bright', 'NNP'), ('Computing', 'NNP'), ('NVIDIA', 'NNP'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Rattler cluster', 'cluster result', 'result partnership', 'partnership among', 'among Dell', 'Dell Technologies', 'Technologies ,', ', Mellanox', 'Mellanox ,', ', Bright', 'Bright Computing', 'Computing NVIDIA', 'NVIDIA .'] 

 TOTAL BIGRAMS --> 13 



 ---- TRI-GRAMS ---- 

 ['Rattler cluster result', 'cluster result partnership', 'result partnership among', 'partnership among Dell', 'among Dell Technologies', 'Dell Technologies ,', 'Technologies , Mellanox', ', Mellanox ,', 'Mellanox , Bright', ', Bright Computing', 'Bright Computing NVIDIA', 'Computing NVIDIA .'] 

 TOTAL TRIGRAMS --> 12 



 ---- NOUN PHRASES ---- 

 ['cluster', 'result', 'partnership'] 

 TOTAL NOUN PHRASES --> 3 



 ---- NER ----

 
 ORGANIZATION ---> ['Dell Technologies']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> ['Bright Computing']
 TOTAL PERSON ENTITY --> 1 


 GPE ---> ['Rattler', 'Mellanox']
 TOTAL GPE ENTITY --> 2 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['rattler', 'cluster', 'result', 'partnership', 'among', 'dell', 'technolog', ',', 'mellanox', ',', 'bright', 'comput', 'nvidia', '.']

 TOTAL PORTER STEM WORDS ==> 14



 ---- SNOWBALL STEMMING ----

['rattler', 'cluster', 'result', 'partnership', 'among', 'dell', 'technolog', ',', 'mellanox', ',', 'bright', 'comput', 'nvidia', '.']

 TOTAL SNOWBALL STEM WORDS ==> 14



 ---- LEMMATIZATION ----

['Rattler', 'cluster', 'result', 'partnership', 'among', 'Dell', 'Technologies', ',', 'Mellanox', ',', 'Bright', 'Computing', 'NVIDIA', '.']

 TOTAL LEMMATIZE WORDS ==> 14

************************************************************************************************************************

177 --> It is designed to showcase extreme scalability, as well as provide application-specific  benchmarking and characterizations. 


 ---- TOKENS ----

 ['It', 'is', 'designed', 'to', 'showcase', 'extreme', 'scalability', ',', 'as', 'well', 'as', 'provide', 'application-specific', 'benchmarking', 'and', 'characterizations', '.'] 

 TOTAL TOKENS ==> 17

 ---- POST ----

 [('It', 'PRP'), ('is', 'VBZ'), ('designed', 'VBN'), ('to', 'TO'), ('showcase', 'VB'), ('extreme', 'JJ'), ('scalability', 'NN'), (',', ','), ('as', 'RB'), ('well', 'RB'), ('as', 'IN'), ('provide', 'JJ'), ('application-specific', 'JJ'), ('benchmarking', 'NN'), ('and', 'CC'), ('characterizations', 'NNS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['designed', 'showcase', 'extreme', 'scalability', ',', 'well', 'provide', 'application-specific', 'benchmarking', 'characterizations', '.']

 TOTAL FILTERED TOKENS ==>  11

 ---- POST FOR FILTERED TOKENS ----

 [('designed', 'VBN'), ('showcase', 'JJ'), ('extreme', 'NN'), ('scalability', 'NN'), (',', ','), ('well', 'RB'), ('provide', 'IN'), ('application-specific', 'JJ'), ('benchmarking', 'NN'), ('characterizations', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['designed showcase', 'showcase extreme', 'extreme scalability', 'scalability ,', ', well', 'well provide', 'provide application-specific', 'application-specific benchmarking', 'benchmarking characterizations', 'characterizations .'] 

 TOTAL BIGRAMS --> 10 



 ---- TRI-GRAMS ---- 

 ['designed showcase extreme', 'showcase extreme scalability', 'extreme scalability ,', 'scalability , well', ', well provide', 'well provide application-specific', 'provide application-specific benchmarking', 'application-specific benchmarking characterizations', 'benchmarking characterizations .'] 

 TOTAL TRIGRAMS --> 9 



 ---- NOUN PHRASES ---- 

 ['showcase extreme', 'scalability', 'application-specific benchmarking'] 

 TOTAL NOUN PHRASES --> 3 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['design', 'showcas', 'extrem', 'scalabl', ',', 'well', 'provid', 'application-specif', 'benchmark', 'character', '.']

 TOTAL PORTER STEM WORDS ==> 11



 ---- SNOWBALL STEMMING ----

['design', 'showcas', 'extrem', 'scalabl', ',', 'well', 'provid', 'application-specif', 'benchmark', 'character', '.']

 TOTAL SNOWBALL STEM WORDS ==> 11



 ---- LEMMATIZATION ----

['designed', 'showcase', 'extreme', 'scalability', ',', 'well', 'provide', 'application-specific', 'benchmarking', 'characterization', '.']

 TOTAL LEMMATIZE WORDS ==> 11

************************************************************************************************************************

178 --> RESULTS In our ongoing lab research we have demonstrated that we can create a voice that  sounds like any voice we want to mimic, and that we can use parallelization to create the  model for this task in a relatively short period of time. 


 ---- TOKENS ----

 ['RESULTS', 'In', 'our', 'ongoing', 'lab', 'research', 'we', 'have', 'demonstrated', 'that', 'we', 'can', 'create', 'a', 'voice', 'that', 'sounds', 'like', 'any', 'voice', 'we', 'want', 'to', 'mimic', ',', 'and', 'that', 'we', 'can', 'use', 'parallelization', 'to', 'create', 'the', 'model', 'for', 'this', 'task', 'in', 'a', 'relatively', 'short', 'period', 'of', 'time', '.'] 

 TOTAL TOKENS ==> 46

 ---- POST ----

 [('RESULTS', 'NN'), ('In', 'IN'), ('our', 'PRP$'), ('ongoing', 'JJ'), ('lab', 'NN'), ('research', 'NN'), ('we', 'PRP'), ('have', 'VBP'), ('demonstrated', 'VBN'), ('that', 'IN'), ('we', 'PRP'), ('can', 'MD'), ('create', 'VB'), ('a', 'DT'), ('voice', 'NN'), ('that', 'WDT'), ('sounds', 'VBZ'), ('like', 'IN'), ('any', 'DT'), ('voice', 'NN'), ('we', 'PRP'), ('want', 'VBP'), ('to', 'TO'), ('mimic', 'VB'), (',', ','), ('and', 'CC'), ('that', 'IN'), ('we', 'PRP'), ('can', 'MD'), ('use', 'VB'), ('parallelization', 'NN'), ('to', 'TO'), ('create', 'VB'), ('the', 'DT'), ('model', 'NN'), ('for', 'IN'), ('this', 'DT'), ('task', 'NN'), ('in', 'IN'), ('a', 'DT'), ('relatively', 'RB'), ('short', 'JJ'), ('period', 'NN'), ('of', 'IN'), ('time', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['RESULTS', 'ongoing', 'lab', 'research', 'demonstrated', 'create', 'voice', 'sounds', 'like', 'voice', 'want', 'mimic', ',', 'use', 'parallelization', 'create', 'model', 'task', 'relatively', 'short', 'period', 'time', '.']

 TOTAL FILTERED TOKENS ==>  23

 ---- POST FOR FILTERED TOKENS ----

 [('RESULTS', 'NNP'), ('ongoing', 'VBG'), ('lab', 'JJ'), ('research', 'NN'), ('demonstrated', 'VBD'), ('create', 'JJ'), ('voice', 'NN'), ('sounds', 'NNS'), ('like', 'IN'), ('voice', 'NN'), ('want', 'VBP'), ('mimic', 'NN'), (',', ','), ('use', 'VBP'), ('parallelization', 'NN'), ('create', 'NN'), ('model', 'NN'), ('task', 'NN'), ('relatively', 'RB'), ('short', 'JJ'), ('period', 'NN'), ('time', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['RESULTS ongoing', 'ongoing lab', 'lab research', 'research demonstrated', 'demonstrated create', 'create voice', 'voice sounds', 'sounds like', 'like voice', 'voice want', 'want mimic', 'mimic ,', ', use', 'use parallelization', 'parallelization create', 'create model', 'model task', 'task relatively', 'relatively short', 'short period', 'period time', 'time .'] 

 TOTAL BIGRAMS --> 22 



 ---- TRI-GRAMS ---- 

 ['RESULTS ongoing lab', 'ongoing lab research', 'lab research demonstrated', 'research demonstrated create', 'demonstrated create voice', 'create voice sounds', 'voice sounds like', 'sounds like voice', 'like voice want', 'voice want mimic', 'want mimic ,', 'mimic , use', ', use parallelization', 'use parallelization create', 'parallelization create model', 'create model task', 'model task relatively', 'task relatively short', 'relatively short period', 'short period time', 'period time .'] 

 TOTAL TRIGRAMS --> 21 



 ---- NOUN PHRASES ---- 

 ['lab research', 'create voice', 'voice', 'mimic', 'parallelization', 'create', 'model', 'task', 'short period', 'time'] 

 TOTAL NOUN PHRASES --> 10 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['result', 'ongo', 'lab', 'research', 'demonstr', 'creat', 'voic', 'sound', 'like', 'voic', 'want', 'mimic', ',', 'use', 'parallel', 'creat', 'model', 'task', 'rel', 'short', 'period', 'time', '.']

 TOTAL PORTER STEM WORDS ==> 23



 ---- SNOWBALL STEMMING ----

['result', 'ongo', 'lab', 'research', 'demonstr', 'creat', 'voic', 'sound', 'like', 'voic', 'want', 'mimic', ',', 'use', 'parallel', 'creat', 'model', 'task', 'relat', 'short', 'period', 'time', '.']

 TOTAL SNOWBALL STEM WORDS ==> 23



 ---- LEMMATIZATION ----

['RESULTS', 'ongoing', 'lab', 'research', 'demonstrated', 'create', 'voice', 'sound', 'like', 'voice', 'want', 'mimic', ',', 'use', 'parallelization', 'create', 'model', 'task', 'relatively', 'short', 'period', 'time', '.']

 TOTAL LEMMATIZE WORDS ==> 23

************************************************************************************************************************

179 --> We have reduced the process of  producing a realistic voice model from more than a month to less than three days, just by  parallelizing the process on the Rattler supercomputer. 


 ---- TOKENS ----

 ['We', 'have', 'reduced', 'the', 'process', 'of', 'producing', 'a', 'realistic', 'voice', 'model', 'from', 'more', 'than', 'a', 'month', 'to', 'less', 'than', 'three', 'days', ',', 'just', 'by', 'parallelizing', 'the', 'process', 'on', 'the', 'Rattler', 'supercomputer', '.'] 

 TOTAL TOKENS ==> 32

 ---- POST ----

 [('We', 'PRP'), ('have', 'VBP'), ('reduced', 'VBN'), ('the', 'DT'), ('process', 'NN'), ('of', 'IN'), ('producing', 'VBG'), ('a', 'DT'), ('realistic', 'JJ'), ('voice', 'NN'), ('model', 'NN'), ('from', 'IN'), ('more', 'JJR'), ('than', 'IN'), ('a', 'DT'), ('month', 'NN'), ('to', 'TO'), ('less', 'JJR'), ('than', 'IN'), ('three', 'CD'), ('days', 'NNS'), (',', ','), ('just', 'RB'), ('by', 'IN'), ('parallelizing', 'VBG'), ('the', 'DT'), ('process', 'NN'), ('on', 'IN'), ('the', 'DT'), ('Rattler', 'NNP'), ('supercomputer', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['reduced', 'process', 'producing', 'realistic', 'voice', 'model', 'month', 'less', 'three', 'days', ',', 'parallelizing', 'process', 'Rattler', 'supercomputer', '.']

 TOTAL FILTERED TOKENS ==>  16

 ---- POST FOR FILTERED TOKENS ----

 [('reduced', 'JJ'), ('process', 'NN'), ('producing', 'VBG'), ('realistic', 'JJ'), ('voice', 'NN'), ('model', 'FW'), ('month', 'NN'), ('less', 'JJR'), ('three', 'CD'), ('days', 'NNS'), (',', ','), ('parallelizing', 'VBG'), ('process', 'NN'), ('Rattler', 'NNP'), ('supercomputer', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['reduced process', 'process producing', 'producing realistic', 'realistic voice', 'voice model', 'model month', 'month less', 'less three', 'three days', 'days ,', ', parallelizing', 'parallelizing process', 'process Rattler', 'Rattler supercomputer', 'supercomputer .'] 

 TOTAL BIGRAMS --> 15 



 ---- TRI-GRAMS ---- 

 ['reduced process producing', 'process producing realistic', 'producing realistic voice', 'realistic voice model', 'voice model month', 'model month less', 'month less three', 'less three days', 'three days ,', 'days , parallelizing', ', parallelizing process', 'parallelizing process Rattler', 'process Rattler supercomputer', 'Rattler supercomputer .'] 

 TOTAL TRIGRAMS --> 14 



 ---- NOUN PHRASES ---- 

 ['reduced process', 'realistic voice', 'month', 'process', 'supercomputer'] 

 TOTAL NOUN PHRASES --> 5 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> ['Rattler']
 TOTAL PERSON ENTITY --> 1 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['reduc', 'process', 'produc', 'realist', 'voic', 'model', 'month', 'less', 'three', 'day', ',', 'parallel', 'process', 'rattler', 'supercomput', '.']

 TOTAL PORTER STEM WORDS ==> 16



 ---- SNOWBALL STEMMING ----

['reduc', 'process', 'produc', 'realist', 'voic', 'model', 'month', 'less', 'three', 'day', ',', 'parallel', 'process', 'rattler', 'supercomput', '.']

 TOTAL SNOWBALL STEM WORDS ==> 16



 ---- LEMMATIZATION ----

['reduced', 'process', 'producing', 'realistic', 'voice', 'model', 'month', 'le', 'three', 'day', ',', 'parallelizing', 'process', 'Rattler', 'supercomputer', '.']

 TOTAL LEMMATIZE WORDS ==> 16

************************************************************************************************************************

180 --> TIPS FOR YOUR PROJECT At the Dell Technologies HPC & AI Innovation Lab, we work actively to share our  learnings, insights and best practices with organizations seeking to capitalize on the  technologies for high performance computing and artificial intelligence. 


 ---- TOKENS ----

 ['TIPS', 'FOR', 'YOUR', 'PROJECT', 'At', 'the', 'Dell', 'Technologies', 'HPC', '&', 'AI', 'Innovation', 'Lab', ',', 'we', 'work', 'actively', 'to', 'share', 'our', 'learnings', ',', 'insights', 'and', 'best', 'practices', 'with', 'organizations', 'seeking', 'to', 'capitalize', 'on', 'the', 'technologies', 'for', 'high', 'performance', 'computing', 'and', 'artificial', 'intelligence', '.'] 

 TOTAL TOKENS ==> 42

 ---- POST ----

 [('TIPS', 'NNP'), ('FOR', 'NNP'), ('YOUR', 'NNP'), ('PROJECT', 'NNP'), ('At', 'IN'), ('the', 'DT'), ('Dell', 'NNP'), ('Technologies', 'NNPS'), ('HPC', 'NNP'), ('&', 'CC'), ('AI', 'NNP'), ('Innovation', 'NNP'), ('Lab', 'NNP'), (',', ','), ('we', 'PRP'), ('work', 'VBP'), ('actively', 'RB'), ('to', 'TO'), ('share', 'NN'), ('our', 'PRP$'), ('learnings', 'NNS'), (',', ','), ('insights', 'NNS'), ('and', 'CC'), ('best', 'JJS'), ('practices', 'NNS'), ('with', 'IN'), ('organizations', 'NNS'), ('seeking', 'VBG'), ('to', 'TO'), ('capitalize', 'VB'), ('on', 'IN'), ('the', 'DT'), ('technologies', 'NNS'), ('for', 'IN'), ('high', 'JJ'), ('performance', 'NN'), ('computing', 'NN'), ('and', 'CC'), ('artificial', 'JJ'), ('intelligence', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['TIPS', 'PROJECT', 'Dell', 'Technologies', 'HPC', '&', 'AI', 'Innovation', 'Lab', ',', 'work', 'actively', 'share', 'learnings', ',', 'insights', 'best', 'practices', 'organizations', 'seeking', 'capitalize', 'technologies', 'high', 'performance', 'computing', 'artificial', 'intelligence', '.']

 TOTAL FILTERED TOKENS ==>  28

 ---- POST FOR FILTERED TOKENS ----

 [('TIPS', 'NNP'), ('PROJECT', 'NNP'), ('Dell', 'NNP'), ('Technologies', 'NNP'), ('HPC', 'NNP'), ('&', 'CC'), ('AI', 'NNP'), ('Innovation', 'NNP'), ('Lab', 'NNP'), (',', ','), ('work', 'NN'), ('actively', 'RB'), ('share', 'NN'), ('learnings', 'NNS'), (',', ','), ('insights', 'NNS'), ('best', 'JJS'), ('practices', 'NNS'), ('organizations', 'NNS'), ('seeking', 'VBG'), ('capitalize', 'NN'), ('technologies', 'NNS'), ('high', 'JJ'), ('performance', 'NN'), ('computing', 'VBG'), ('artificial', 'JJ'), ('intelligence', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['TIPS PROJECT', 'PROJECT Dell', 'Dell Technologies', 'Technologies HPC', 'HPC &', '& AI', 'AI Innovation', 'Innovation Lab', 'Lab ,', ', work', 'work actively', 'actively share', 'share learnings', 'learnings ,', ', insights', 'insights best', 'best practices', 'practices organizations', 'organizations seeking', 'seeking capitalize', 'capitalize technologies', 'technologies high', 'high performance', 'performance computing', 'computing artificial', 'artificial intelligence', 'intelligence .'] 

 TOTAL BIGRAMS --> 27 



 ---- TRI-GRAMS ---- 

 ['TIPS PROJECT Dell', 'PROJECT Dell Technologies', 'Dell Technologies HPC', 'Technologies HPC &', 'HPC & AI', '& AI Innovation', 'AI Innovation Lab', 'Innovation Lab ,', 'Lab , work', ', work actively', 'work actively share', 'actively share learnings', 'share learnings ,', 'learnings , insights', ', insights best', 'insights best practices', 'best practices organizations', 'practices organizations seeking', 'organizations seeking capitalize', 'seeking capitalize technologies', 'capitalize technologies high', 'technologies high performance', 'high performance computing', 'performance computing artificial', 'computing artificial intelligence', 'artificial intelligence .'] 

 TOTAL TRIGRAMS --> 26 



 ---- NOUN PHRASES ---- 

 ['work', 'share', 'capitalize', 'high performance', 'artificial intelligence'] 

 TOTAL NOUN PHRASES --> 5 



 ---- NER ----

 
 ORGANIZATION ---> ['TIPS', 'PROJECT Dell', 'AI Innovation Lab']
 TOTAL ORGANIZATION ENTITY --> 3 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['tip', 'project', 'dell', 'technolog', 'hpc', '&', 'ai', 'innov', 'lab', ',', 'work', 'activ', 'share', 'learn', ',', 'insight', 'best', 'practic', 'organ', 'seek', 'capit', 'technolog', 'high', 'perform', 'comput', 'artifici', 'intellig', '.']

 TOTAL PORTER STEM WORDS ==> 28



 ---- SNOWBALL STEMMING ----

['tip', 'project', 'dell', 'technolog', 'hpc', '&', 'ai', 'innov', 'lab', ',', 'work', 'activ', 'share', 'learn', ',', 'insight', 'best', 'practic', 'organ', 'seek', 'capit', 'technolog', 'high', 'perform', 'comput', 'artifici', 'intellig', '.']

 TOTAL SNOWBALL STEM WORDS ==> 28



 ---- LEMMATIZATION ----

['TIPS', 'PROJECT', 'Dell', 'Technologies', 'HPC', '&', 'AI', 'Innovation', 'Lab', ',', 'work', 'actively', 'share', 'learning', ',', 'insight', 'best', 'practice', 'organization', 'seeking', 'capitalize', 'technology', 'high', 'performance', 'computing', 'artificial', 'intelligence', '.']

 TOTAL LEMMATIZE WORDS ==> 28

************************************************************************************************************************

181 --> With that thought  in mind, here are some of our thoughts on how your organization can get on the path to  a successful NLP project. 


 ---- TOKENS ----

 ['With', 'that', 'thought', 'in', 'mind', ',', 'here', 'are', 'some', 'of', 'our', 'thoughts', 'on', 'how', 'your', 'organization', 'can', 'get', 'on', 'the', 'path', 'to', 'a', 'successful', 'NLP', 'project', '.'] 

 TOTAL TOKENS ==> 27

 ---- POST ----

 [('With', 'IN'), ('that', 'DT'), ('thought', 'NN'), ('in', 'IN'), ('mind', 'NN'), (',', ','), ('here', 'RB'), ('are', 'VBP'), ('some', 'DT'), ('of', 'IN'), ('our', 'PRP$'), ('thoughts', 'NNS'), ('on', 'IN'), ('how', 'WRB'), ('your', 'PRP$'), ('organization', 'NN'), ('can', 'MD'), ('get', 'VB'), ('on', 'IN'), ('the', 'DT'), ('path', 'NN'), ('to', 'TO'), ('a', 'DT'), ('successful', 'JJ'), ('NLP', 'NNP'), ('project', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['thought', 'mind', ',', 'thoughts', 'organization', 'get', 'path', 'successful', 'NLP', 'project', '.']

 TOTAL FILTERED TOKENS ==>  11

 ---- POST FOR FILTERED TOKENS ----

 [('thought', 'VBN'), ('mind', 'NN'), (',', ','), ('thoughts', 'NNS'), ('organization', 'NN'), ('get', 'VBP'), ('path', 'VBN'), ('successful', 'JJ'), ('NLP', 'NNP'), ('project', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['thought mind', 'mind ,', ', thoughts', 'thoughts organization', 'organization get', 'get path', 'path successful', 'successful NLP', 'NLP project', 'project .'] 

 TOTAL BIGRAMS --> 10 



 ---- TRI-GRAMS ---- 

 ['thought mind ,', 'mind , thoughts', ', thoughts organization', 'thoughts organization get', 'organization get path', 'get path successful', 'path successful NLP', 'successful NLP project', 'NLP project .'] 

 TOTAL TRIGRAMS --> 9 



 ---- NOUN PHRASES ---- 

 ['mind', 'organization', 'project'] 

 TOTAL NOUN PHRASES --> 3 



 ---- NER ----

 
 ORGANIZATION ---> ['NLP']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['thought', 'mind', ',', 'thought', 'organ', 'get', 'path', 'success', 'nlp', 'project', '.']

 TOTAL PORTER STEM WORDS ==> 11



 ---- SNOWBALL STEMMING ----

['thought', 'mind', ',', 'thought', 'organ', 'get', 'path', 'success', 'nlp', 'project', '.']

 TOTAL SNOWBALL STEM WORDS ==> 11



 ---- LEMMATIZATION ----

['thought', 'mind', ',', 'thought', 'organization', 'get', 'path', 'successful', 'NLP', 'project', '.']

 TOTAL LEMMATIZE WORDS ==> 11

************************************************************************************************************************

182 --> DON’T TRY TO REINVENT THE WHEEL. 


 ---- TOKENS ----

 ['DON', '’', 'T', 'TRY', 'TO', 'REINVENT', 'THE', 'WHEEL', '.'] 

 TOTAL TOKENS ==> 9

 ---- POST ----

 [('DON', 'NNP'), ('’', 'NNP'), ('T', 'NNP'), ('TRY', 'NNP'), ('TO', 'NNP'), ('REINVENT', 'NNP'), ('THE', 'NNP'), ('WHEEL', 'NNP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['’', 'TRY', 'REINVENT', 'WHEEL', '.']

 TOTAL FILTERED TOKENS ==>  5

 ---- POST FOR FILTERED TOKENS ----

 [('’', 'JJ'), ('TRY', 'NNP'), ('REINVENT', 'NNP'), ('WHEEL', 'NNP'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['’ TRY', 'TRY REINVENT', 'REINVENT WHEEL', 'WHEEL .'] 

 TOTAL BIGRAMS --> 4 



 ---- TRI-GRAMS ---- 

 ['’ TRY REINVENT', 'TRY REINVENT WHEEL', 'REINVENT WHEEL .'] 

 TOTAL TRIGRAMS --> 3 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> ['TRY']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['’', 'tri', 'reinvent', 'wheel', '.']

 TOTAL PORTER STEM WORDS ==> 5



 ---- SNOWBALL STEMMING ----

['’', 'tri', 'reinvent', 'wheel', '.']

 TOTAL SNOWBALL STEM WORDS ==> 5



 ---- LEMMATIZATION ----

['’', 'TRY', 'REINVENT', 'WHEEL', '.']

 TOTAL LEMMATIZE WORDS ==> 5

************************************************************************************************************************

183 --> Build on the work that others have done. 


 ---- TOKENS ----

 ['Build', 'on', 'the', 'work', 'that', 'others', 'have', 'done', '.'] 

 TOTAL TOKENS ==> 9

 ---- POST ----

 [('Build', 'VB'), ('on', 'IN'), ('the', 'DT'), ('work', 'NN'), ('that', 'WDT'), ('others', 'NNS'), ('have', 'VBP'), ('done', 'VBN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Build', 'work', 'others', 'done', '.']

 TOTAL FILTERED TOKENS ==>  5

 ---- POST FOR FILTERED TOKENS ----

 [('Build', 'NNP'), ('work', 'NN'), ('others', 'NNS'), ('done', 'VBN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Build work', 'work others', 'others done', 'done .'] 

 TOTAL BIGRAMS --> 4 



 ---- TRI-GRAMS ---- 

 ['Build work others', 'work others done', 'others done .'] 

 TOTAL TRIGRAMS --> 3 



 ---- NOUN PHRASES ---- 

 ['work'] 

 TOTAL NOUN PHRASES --> 1 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> ['Build']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['build', 'work', 'other', 'done', '.']

 TOTAL PORTER STEM WORDS ==> 5



 ---- SNOWBALL STEMMING ----

['build', 'work', 'other', 'done', '.']

 TOTAL SNOWBALL STEM WORDS ==> 5



 ---- LEMMATIZATION ----

['Build', 'work', 'others', 'done', '.']

 TOTAL LEMMATIZE WORDS ==> 5

************************************************************************************************************************

184 --> For example, in our research we work with open  source data that your organization can access should you want to try to replicate our  results in proofs of concept and other projects. 


 ---- TOKENS ----

 ['For', 'example', ',', 'in', 'our', 'research', 'we', 'work', 'with', 'open', 'source', 'data', 'that', 'your', 'organization', 'can', 'access', 'should', 'you', 'want', 'to', 'try', 'to', 'replicate', 'our', 'results', 'in', 'proofs', 'of', 'concept', 'and', 'other', 'projects', '.'] 

 TOTAL TOKENS ==> 34

 ---- POST ----

 [('For', 'IN'), ('example', 'NN'), (',', ','), ('in', 'IN'), ('our', 'PRP$'), ('research', 'NN'), ('we', 'PRP'), ('work', 'VBP'), ('with', 'IN'), ('open', 'JJ'), ('source', 'NN'), ('data', 'NNS'), ('that', 'IN'), ('your', 'PRP$'), ('organization', 'NN'), ('can', 'MD'), ('access', 'NN'), ('should', 'MD'), ('you', 'PRP'), ('want', 'VB'), ('to', 'TO'), ('try', 'VB'), ('to', 'TO'), ('replicate', 'VB'), ('our', 'PRP$'), ('results', 'NNS'), ('in', 'IN'), ('proofs', 'NN'), ('of', 'IN'), ('concept', 'NN'), ('and', 'CC'), ('other', 'JJ'), ('projects', 'NNS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['example', ',', 'research', 'work', 'open', 'source', 'data', 'organization', 'access', 'want', 'try', 'replicate', 'results', 'proofs', 'concept', 'projects', '.']

 TOTAL FILTERED TOKENS ==>  17

 ---- POST FOR FILTERED TOKENS ----

 [('example', 'NN'), (',', ','), ('research', 'NN'), ('work', 'NN'), ('open', 'JJ'), ('source', 'NN'), ('data', 'NNS'), ('organization', 'NN'), ('access', 'NN'), ('want', 'VBP'), ('try', 'NN'), ('replicate', 'NN'), ('results', 'NNS'), ('proofs', 'VBP'), ('concept', 'JJ'), ('projects', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['example ,', ', research', 'research work', 'work open', 'open source', 'source data', 'data organization', 'organization access', 'access want', 'want try', 'try replicate', 'replicate results', 'results proofs', 'proofs concept', 'concept projects', 'projects .'] 

 TOTAL BIGRAMS --> 16 



 ---- TRI-GRAMS ---- 

 ['example , research', ', research work', 'research work open', 'work open source', 'open source data', 'source data organization', 'data organization access', 'organization access want', 'access want try', 'want try replicate', 'try replicate results', 'replicate results proofs', 'results proofs concept', 'proofs concept projects', 'concept projects .'] 

 TOTAL TRIGRAMS --> 15 



 ---- NOUN PHRASES ---- 

 ['example', 'research', 'work', 'open source', 'organization', 'access', 'try', 'replicate'] 

 TOTAL NOUN PHRASES --> 8 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['exampl', ',', 'research', 'work', 'open', 'sourc', 'data', 'organ', 'access', 'want', 'tri', 'replic', 'result', 'proof', 'concept', 'project', '.']

 TOTAL PORTER STEM WORDS ==> 17



 ---- SNOWBALL STEMMING ----

['exampl', ',', 'research', 'work', 'open', 'sourc', 'data', 'organ', 'access', 'want', 'tri', 'replic', 'result', 'proof', 'concept', 'project', '.']

 TOTAL SNOWBALL STEM WORDS ==> 17



 ---- LEMMATIZATION ----

['example', ',', 'research', 'work', 'open', 'source', 'data', 'organization', 'access', 'want', 'try', 'replicate', 'result', 'proof', 'concept', 'project', '.']

 TOTAL LEMMATIZE WORDS ==> 17

************************************************************************************************************************

185 --> DON’T THINK YOU WILL GET THE RIGHT ANSWER THE FIRST TIME. 


 ---- TOKENS ----

 ['DON', '’', 'T', 'THINK', 'YOU', 'WILL', 'GET', 'THE', 'RIGHT', 'ANSWER', 'THE', 'FIRST', 'TIME', '.'] 

 TOTAL TOKENS ==> 14

 ---- POST ----

 [('DON', 'NNP'), ('’', 'VBD'), ('T', 'NNP'), ('THINK', 'NNP'), ('YOU', 'NNP'), ('WILL', 'MD'), ('GET', 'VB'), ('THE', 'NNP'), ('RIGHT', 'NNP'), ('ANSWER', 'NNP'), ('THE', 'NNP'), ('FIRST', 'NNP'), ('TIME', 'NNP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['’', 'THINK', 'GET', 'RIGHT', 'ANSWER', 'FIRST', 'TIME', '.']

 TOTAL FILTERED TOKENS ==>  8

 ---- POST FOR FILTERED TOKENS ----

 [('’', 'NN'), ('THINK', 'NNP'), ('GET', 'NNP'), ('RIGHT', 'NNP'), ('ANSWER', 'NNP'), ('FIRST', 'NNP'), ('TIME', 'NNP'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['’ THINK', 'THINK GET', 'GET RIGHT', 'RIGHT ANSWER', 'ANSWER FIRST', 'FIRST TIME', 'TIME .'] 

 TOTAL BIGRAMS --> 7 



 ---- TRI-GRAMS ---- 

 ['’ THINK GET', 'THINK GET RIGHT', 'GET RIGHT ANSWER', 'RIGHT ANSWER FIRST', 'ANSWER FIRST TIME', 'FIRST TIME .'] 

 TOTAL TRIGRAMS --> 6 



 ---- NOUN PHRASES ---- 

 ['’'] 

 TOTAL NOUN PHRASES --> 1 



 ---- NER ----

 
 ORGANIZATION ---> ['THINK']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['’', 'think', 'get', 'right', 'answer', 'first', 'time', '.']

 TOTAL PORTER STEM WORDS ==> 8



 ---- SNOWBALL STEMMING ----

['’', 'think', 'get', 'right', 'answer', 'first', 'time', '.']

 TOTAL SNOWBALL STEM WORDS ==> 8



 ---- LEMMATIZATION ----

['’', 'THINK', 'GET', 'RIGHT', 'ANSWER', 'FIRST', 'TIME', '.']

 TOTAL LEMMATIZE WORDS ==> 8

************************************************************************************************************************

186 --> The training process for a deep learning application is highly iterative. 


 ---- TOKENS ----

 ['The', 'training', 'process', 'for', 'a', 'deep', 'learning', 'application', 'is', 'highly', 'iterative', '.'] 

 TOTAL TOKENS ==> 12

 ---- POST ----

 [('The', 'DT'), ('training', 'NN'), ('process', 'NN'), ('for', 'IN'), ('a', 'DT'), ('deep', 'JJ'), ('learning', 'NN'), ('application', 'NN'), ('is', 'VBZ'), ('highly', 'RB'), ('iterative', 'JJ'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['training', 'process', 'deep', 'learning', 'application', 'highly', 'iterative', '.']

 TOTAL FILTERED TOKENS ==>  8

 ---- POST FOR FILTERED TOKENS ----

 [('training', 'VBG'), ('process', 'NN'), ('deep', 'JJ'), ('learning', 'VBG'), ('application', 'NN'), ('highly', 'RB'), ('iterative', 'JJ'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['training process', 'process deep', 'deep learning', 'learning application', 'application highly', 'highly iterative', 'iterative .'] 

 TOTAL BIGRAMS --> 7 



 ---- TRI-GRAMS ---- 

 ['training process deep', 'process deep learning', 'deep learning application', 'learning application highly', 'application highly iterative', 'highly iterative .'] 

 TOTAL TRIGRAMS --> 6 



 ---- NOUN PHRASES ---- 

 ['process', 'application'] 

 TOTAL NOUN PHRASES --> 2 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['train', 'process', 'deep', 'learn', 'applic', 'highli', 'iter', '.']

 TOTAL PORTER STEM WORDS ==> 8



 ---- SNOWBALL STEMMING ----

['train', 'process', 'deep', 'learn', 'applic', 'high', 'iter', '.']

 TOTAL SNOWBALL STEM WORDS ==> 8



 ---- LEMMATIZATION ----

['training', 'process', 'deep', 'learning', 'application', 'highly', 'iterative', '.']

 TOTAL LEMMATIZE WORDS ==> 8

************************************************************************************************************************

187 --> You go down  one path and see what sort of results you get. 


 ---- TOKENS ----

 ['You', 'go', 'down', 'one', 'path', 'and', 'see', 'what', 'sort', 'of', 'results', 'you', 'get', '.'] 

 TOTAL TOKENS ==> 14

 ---- POST ----

 [('You', 'PRP'), ('go', 'VBP'), ('down', 'RP'), ('one', 'CD'), ('path', 'NN'), ('and', 'CC'), ('see', 'VB'), ('what', 'WP'), ('sort', 'NN'), ('of', 'IN'), ('results', 'NNS'), ('you', 'PRP'), ('get', 'VBP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['go', 'one', 'path', 'see', 'sort', 'results', 'get', '.']

 TOTAL FILTERED TOKENS ==>  8

 ---- POST FOR FILTERED TOKENS ----

 [('go', 'VB'), ('one', 'CD'), ('path', 'NN'), ('see', 'NN'), ('sort', 'NN'), ('results', 'NNS'), ('get', 'VBP'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['go one', 'one path', 'path see', 'see sort', 'sort results', 'results get', 'get .'] 

 TOTAL BIGRAMS --> 7 



 ---- TRI-GRAMS ---- 

 ['go one path', 'one path see', 'path see sort', 'see sort results', 'sort results get', 'results get .'] 

 TOTAL TRIGRAMS --> 6 



 ---- NOUN PHRASES ---- 

 ['path', 'see', 'sort'] 

 TOTAL NOUN PHRASES --> 3 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['go', 'one', 'path', 'see', 'sort', 'result', 'get', '.']

 TOTAL PORTER STEM WORDS ==> 8



 ---- SNOWBALL STEMMING ----

['go', 'one', 'path', 'see', 'sort', 'result', 'get', '.']

 TOTAL SNOWBALL STEM WORDS ==> 8



 ---- LEMMATIZATION ----

['go', 'one', 'path', 'see', 'sort', 'result', 'get', '.']

 TOTAL LEMMATIZE WORDS ==> 8

************************************************************************************************************************

188 --> And then you go down another path, and  another path after that. 


 ---- TOKENS ----

 ['And', 'then', 'you', 'go', 'down', 'another', 'path', ',', 'and', 'another', 'path', 'after', 'that', '.'] 

 TOTAL TOKENS ==> 14

 ---- POST ----

 [('And', 'CC'), ('then', 'RB'), ('you', 'PRP'), ('go', 'VBP'), ('down', 'RP'), ('another', 'DT'), ('path', 'NN'), (',', ','), ('and', 'CC'), ('another', 'DT'), ('path', 'NN'), ('after', 'IN'), ('that', 'DT'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['go', 'another', 'path', ',', 'another', 'path', '.']

 TOTAL FILTERED TOKENS ==>  7

 ---- POST FOR FILTERED TOKENS ----

 [('go', 'VB'), ('another', 'DT'), ('path', 'NN'), (',', ','), ('another', 'DT'), ('path', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['go another', 'another path', 'path ,', ', another', 'another path', 'path .'] 

 TOTAL BIGRAMS --> 6 



 ---- TRI-GRAMS ---- 

 ['go another path', 'another path ,', 'path , another', ', another path', 'another path .'] 

 TOTAL TRIGRAMS --> 5 



 ---- NOUN PHRASES ---- 

 ['another path', 'another path'] 

 TOTAL NOUN PHRASES --> 2 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['go', 'anoth', 'path', ',', 'anoth', 'path', '.']

 TOTAL PORTER STEM WORDS ==> 7



 ---- SNOWBALL STEMMING ----

['go', 'anoth', 'path', ',', 'anoth', 'path', '.']

 TOTAL SNOWBALL STEM WORDS ==> 7



 ---- LEMMATIZATION ----

['go', 'another', 'path', ',', 'another', 'path', '.']

 TOTAL LEMMATIZE WORDS ==> 7

************************************************************************************************************************

189 --> In our research in the HPC & AI Innovation Lab, we spend months  training and tweaking our models. 


 ---- TOKENS ----

 ['In', 'our', 'research', 'in', 'the', 'HPC', '&', 'AI', 'Innovation', 'Lab', ',', 'we', 'spend', 'months', 'training', 'and', 'tweaking', 'our', 'models', '.'] 

 TOTAL TOKENS ==> 20

 ---- POST ----

 [('In', 'IN'), ('our', 'PRP$'), ('research', 'NN'), ('in', 'IN'), ('the', 'DT'), ('HPC', 'NNP'), ('&', 'CC'), ('AI', 'NNP'), ('Innovation', 'NNP'), ('Lab', 'NNP'), (',', ','), ('we', 'PRP'), ('spend', 'VBP'), ('months', 'NNS'), ('training', 'NN'), ('and', 'CC'), ('tweaking', 'VBG'), ('our', 'PRP$'), ('models', 'NNS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['research', 'HPC', '&', 'AI', 'Innovation', 'Lab', ',', 'spend', 'months', 'training', 'tweaking', 'models', '.']

 TOTAL FILTERED TOKENS ==>  13

 ---- POST FOR FILTERED TOKENS ----

 [('research', 'NN'), ('HPC', 'NNP'), ('&', 'CC'), ('AI', 'NNP'), ('Innovation', 'NNP'), ('Lab', 'NNP'), (',', ','), ('spend', 'JJ'), ('months', 'NNS'), ('training', 'VBG'), ('tweaking', 'NN'), ('models', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['research HPC', 'HPC &', '& AI', 'AI Innovation', 'Innovation Lab', 'Lab ,', ', spend', 'spend months', 'months training', 'training tweaking', 'tweaking models', 'models .'] 

 TOTAL BIGRAMS --> 12 



 ---- TRI-GRAMS ---- 

 ['research HPC &', 'HPC & AI', '& AI Innovation', 'AI Innovation Lab', 'Innovation Lab ,', 'Lab , spend', ', spend months', 'spend months training', 'months training tweaking', 'training tweaking models', 'tweaking models .'] 

 TOTAL TRIGRAMS --> 11 



 ---- NOUN PHRASES ---- 

 ['research', 'tweaking'] 

 TOTAL NOUN PHRASES --> 2 



 ---- NER ----

 
 ORGANIZATION ---> ['HPC', 'AI Innovation Lab']
 TOTAL ORGANIZATION ENTITY --> 2 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['research', 'hpc', '&', 'ai', 'innov', 'lab', ',', 'spend', 'month', 'train', 'tweak', 'model', '.']

 TOTAL PORTER STEM WORDS ==> 13



 ---- SNOWBALL STEMMING ----

['research', 'hpc', '&', 'ai', 'innov', 'lab', ',', 'spend', 'month', 'train', 'tweak', 'model', '.']

 TOTAL SNOWBALL STEM WORDS ==> 13



 ---- LEMMATIZATION ----

['research', 'HPC', '&', 'AI', 'Innovation', 'Lab', ',', 'spend', 'month', 'training', 'tweaking', 'model', '.']

 TOTAL LEMMATIZE WORDS ==> 13

************************************************************************************************************************

190 --> BE WILLING TO FAIL. 


 ---- TOKENS ----

 ['BE', 'WILLING', 'TO', 'FAIL', '.'] 

 TOTAL TOKENS ==> 5

 ---- POST ----

 [('BE', 'VB'), ('WILLING', 'VBN'), ('TO', 'NNP'), ('FAIL', 'NNP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['WILLING', 'FAIL', '.']

 TOTAL FILTERED TOKENS ==>  3

 ---- POST FOR FILTERED TOKENS ----

 [('WILLING', 'NN'), ('FAIL', 'NNP'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['WILLING FAIL', 'FAIL .'] 

 TOTAL BIGRAMS --> 2 



 ---- TRI-GRAMS ---- 

 ['WILLING FAIL .'] 

 TOTAL TRIGRAMS --> 1 



 ---- NOUN PHRASES ---- 

 ['WILLING'] 

 TOTAL NOUN PHRASES --> 1 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['will', 'fail', '.']

 TOTAL PORTER STEM WORDS ==> 3



 ---- SNOWBALL STEMMING ----

['will', 'fail', '.']

 TOTAL SNOWBALL STEM WORDS ==> 3



 ---- LEMMATIZATION ----

['WILLING', 'FAIL', '.']

 TOTAL LEMMATIZE WORDS ==> 3

************************************************************************************************************************

191 --> The development of an NLP application is not a one-and-done undertaking. 


 ---- TOKENS ----

 ['The', 'development', 'of', 'an', 'NLP', 'application', 'is', 'not', 'a', 'one-and-done', 'undertaking', '.'] 

 TOTAL TOKENS ==> 12

 ---- POST ----

 [('The', 'DT'), ('development', 'NN'), ('of', 'IN'), ('an', 'DT'), ('NLP', 'NNP'), ('application', 'NN'), ('is', 'VBZ'), ('not', 'RB'), ('a', 'DT'), ('one-and-done', 'JJ'), ('undertaking', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['development', 'NLP', 'application', 'one-and-done', 'undertaking', '.']

 TOTAL FILTERED TOKENS ==>  6

 ---- POST FOR FILTERED TOKENS ----

 [('development', 'NN'), ('NLP', 'NNP'), ('application', 'NN'), ('one-and-done', 'NN'), ('undertaking', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['development NLP', 'NLP application', 'application one-and-done', 'one-and-done undertaking', 'undertaking .'] 

 TOTAL BIGRAMS --> 5 



 ---- TRI-GRAMS ---- 

 ['development NLP application', 'NLP application one-and-done', 'application one-and-done undertaking', 'one-and-done undertaking .'] 

 TOTAL TRIGRAMS --> 4 



 ---- NOUN PHRASES ---- 

 ['development', 'application', 'one-and-done', 'undertaking'] 

 TOTAL NOUN PHRASES --> 4 



 ---- NER ----

 
 ORGANIZATION ---> ['NLP']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['develop', 'nlp', 'applic', 'one-and-don', 'undertak', '.']

 TOTAL PORTER STEM WORDS ==> 6



 ---- SNOWBALL STEMMING ----

['develop', 'nlp', 'applic', 'one-and-don', 'undertak', '.']

 TOTAL SNOWBALL STEM WORDS ==> 6



 ---- LEMMATIZATION ----

['development', 'NLP', 'application', 'one-and-done', 'undertaking', '.']

 TOTAL LEMMATIZE WORDS ==> 6

************************************************************************************************************************

192 --> You need to  be willing to fail in the short term in order to achieve success in the long run. 


 ---- TOKENS ----

 ['You', 'need', 'to', 'be', 'willing', 'to', 'fail', 'in', 'the', 'short', 'term', 'in', 'order', 'to', 'achieve', 'success', 'in', 'the', 'long', 'run', '.'] 

 TOTAL TOKENS ==> 21

 ---- POST ----

 [('You', 'PRP'), ('need', 'VBP'), ('to', 'TO'), ('be', 'VB'), ('willing', 'JJ'), ('to', 'TO'), ('fail', 'VB'), ('in', 'IN'), ('the', 'DT'), ('short', 'JJ'), ('term', 'NN'), ('in', 'IN'), ('order', 'NN'), ('to', 'TO'), ('achieve', 'VB'), ('success', 'NN'), ('in', 'IN'), ('the', 'DT'), ('long', 'JJ'), ('run', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['need', 'willing', 'fail', 'short', 'term', 'order', 'achieve', 'success', 'long', 'run', '.']

 TOTAL FILTERED TOKENS ==>  11

 ---- POST FOR FILTERED TOKENS ----

 [('need', 'NN'), ('willing', 'JJ'), ('fail', 'JJ'), ('short', 'JJ'), ('term', 'NN'), ('order', 'NN'), ('achieve', 'VBP'), ('success', 'NN'), ('long', 'JJ'), ('run', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['need willing', 'willing fail', 'fail short', 'short term', 'term order', 'order achieve', 'achieve success', 'success long', 'long run', 'run .'] 

 TOTAL BIGRAMS --> 10 



 ---- TRI-GRAMS ---- 

 ['need willing fail', 'willing fail short', 'fail short term', 'short term order', 'term order achieve', 'order achieve success', 'achieve success long', 'success long run', 'long run .'] 

 TOTAL TRIGRAMS --> 9 



 ---- NOUN PHRASES ---- 

 ['need', 'willing fail short term', 'order', 'success', 'long run'] 

 TOTAL NOUN PHRASES --> 5 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['need', 'will', 'fail', 'short', 'term', 'order', 'achiev', 'success', 'long', 'run', '.']

 TOTAL PORTER STEM WORDS ==> 11



 ---- SNOWBALL STEMMING ----

['need', 'will', 'fail', 'short', 'term', 'order', 'achiev', 'success', 'long', 'run', '.']

 TOTAL SNOWBALL STEM WORDS ==> 11



 ---- LEMMATIZATION ----

['need', 'willing', 'fail', 'short', 'term', 'order', 'achieve', 'success', 'long', 'run', '.']

 TOTAL LEMMATIZE WORDS ==> 11

************************************************************************************************************************

193 --> Be ready to  abandon unproductive approaches, to rethink things that you thought you knew for sure,  and to go back to the drawing board to map out a new path forward. 


 ---- TOKENS ----

 ['Be', 'ready', 'to', 'abandon', 'unproductive', 'approaches', ',', 'to', 'rethink', 'things', 'that', 'you', 'thought', 'you', 'knew', 'for', 'sure', ',', 'and', 'to', 'go', 'back', 'to', 'the', 'drawing', 'board', 'to', 'map', 'out', 'a', 'new', 'path', 'forward', '.'] 

 TOTAL TOKENS ==> 34

 ---- POST ----

 [('Be', 'NNP'), ('ready', 'JJ'), ('to', 'TO'), ('abandon', 'VB'), ('unproductive', 'JJ'), ('approaches', 'NNS'), (',', ','), ('to', 'TO'), ('rethink', 'VB'), ('things', 'NNS'), ('that', 'IN'), ('you', 'PRP'), ('thought', 'VBD'), ('you', 'PRP'), ('knew', 'VB'), ('for', 'IN'), ('sure', 'JJ'), (',', ','), ('and', 'CC'), ('to', 'TO'), ('go', 'VB'), ('back', 'RB'), ('to', 'TO'), ('the', 'DT'), ('drawing', 'VBG'), ('board', 'NN'), ('to', 'TO'), ('map', 'VB'), ('out', 'RP'), ('a', 'DT'), ('new', 'JJ'), ('path', 'NN'), ('forward', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['ready', 'abandon', 'unproductive', 'approaches', ',', 'rethink', 'things', 'thought', 'knew', 'sure', ',', 'go', 'back', 'drawing', 'board', 'map', 'new', 'path', 'forward', '.']

 TOTAL FILTERED TOKENS ==>  20

 ---- POST FOR FILTERED TOKENS ----

 [('ready', 'JJ'), ('abandon', 'RB'), ('unproductive', 'JJ'), ('approaches', 'NNS'), (',', ','), ('rethink', 'VBP'), ('things', 'NNS'), ('thought', 'VBD'), ('knew', 'RB'), ('sure', 'JJ'), (',', ','), ('go', 'VB'), ('back', 'RB'), ('drawing', 'VBG'), ('board', 'NN'), ('map', 'VBD'), ('new', 'JJ'), ('path', 'NN'), ('forward', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['ready abandon', 'abandon unproductive', 'unproductive approaches', 'approaches ,', ', rethink', 'rethink things', 'things thought', 'thought knew', 'knew sure', 'sure ,', ', go', 'go back', 'back drawing', 'drawing board', 'board map', 'map new', 'new path', 'path forward', 'forward .'] 

 TOTAL BIGRAMS --> 19 



 ---- TRI-GRAMS ---- 

 ['ready abandon unproductive', 'abandon unproductive approaches', 'unproductive approaches ,', 'approaches , rethink', ', rethink things', 'rethink things thought', 'things thought knew', 'thought knew sure', 'knew sure ,', 'sure , go', ', go back', 'go back drawing', 'back drawing board', 'drawing board map', 'board map new', 'map new path', 'new path forward', 'path forward .'] 

 TOTAL TRIGRAMS --> 18 



 ---- NOUN PHRASES ---- 

 ['board', 'new path', 'forward'] 

 TOTAL NOUN PHRASES --> 3 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['readi', 'abandon', 'unproduct', 'approach', ',', 'rethink', 'thing', 'thought', 'knew', 'sure', ',', 'go', 'back', 'draw', 'board', 'map', 'new', 'path', 'forward', '.']

 TOTAL PORTER STEM WORDS ==> 20



 ---- SNOWBALL STEMMING ----

['readi', 'abandon', 'unproduct', 'approach', ',', 'rethink', 'thing', 'thought', 'knew', 'sure', ',', 'go', 'back', 'draw', 'board', 'map', 'new', 'path', 'forward', '.']

 TOTAL SNOWBALL STEM WORDS ==> 20



 ---- LEMMATIZATION ----

['ready', 'abandon', 'unproductive', 'approach', ',', 'rethink', 'thing', 'thought', 'knew', 'sure', ',', 'go', 'back', 'drawing', 'board', 'map', 'new', 'path', 'forward', '.']

 TOTAL LEMMATIZE WORDS ==> 20

************************************************************************************************************************

194 --> https://www.dellemc.com/en-us/collaterals/unauth/sales-documents/products/storage/h16221-hpc-lab-brochure.pdf https://www.dellemc.com/en-us/collaterals/unauth/sales-documents/products/storage/h16221-hpc-lab-brochure.pdf  4 DELL TECHNOLOGIES WHITE PAPER START WITH THE EASY STUFF. 


 ---- TOKENS ----

 ['https', ':', '//www.dellemc.com/en-us/collaterals/unauth/sales-documents/products/storage/h16221-hpc-lab-brochure.pdf', 'https', ':', '//www.dellemc.com/en-us/collaterals/unauth/sales-documents/products/storage/h16221-hpc-lab-brochure.pdf', '4', 'DELL', 'TECHNOLOGIES', 'WHITE', 'PAPER', 'START', 'WITH', 'THE', 'EASY', 'STUFF', '.'] 

 TOTAL TOKENS ==> 17

 ---- POST ----

 [('https', 'NN'), (':', ':'), ('//www.dellemc.com/en-us/collaterals/unauth/sales-documents/products/storage/h16221-hpc-lab-brochure.pdf', 'JJ'), ('https', 'NN'), (':', ':'), ('//www.dellemc.com/en-us/collaterals/unauth/sales-documents/products/storage/h16221-hpc-lab-brochure.pdf', 'JJ'), ('4', 'CD'), ('DELL', 'NNP'), ('TECHNOLOGIES', 'NNP'), ('WHITE', 'NNP'), ('PAPER', 'NNP'), ('START', 'NNP'), ('WITH', 'NNP'), ('THE', 'NNP'), ('EASY', 'NNP'), ('STUFF', 'NNP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['https', ':', '//www.dellemc.com/en-us/collaterals/unauth/sales-documents/products/storage/h16221-hpc-lab-brochure.pdf', 'https', ':', '//www.dellemc.com/en-us/collaterals/unauth/sales-documents/products/storage/h16221-hpc-lab-brochure.pdf', '4', 'DELL', 'TECHNOLOGIES', 'WHITE', 'PAPER', 'START', 'EASY', 'STUFF', '.']

 TOTAL FILTERED TOKENS ==>  15

 ---- POST FOR FILTERED TOKENS ----

 [('https', 'NN'), (':', ':'), ('//www.dellemc.com/en-us/collaterals/unauth/sales-documents/products/storage/h16221-hpc-lab-brochure.pdf', 'JJ'), ('https', 'NN'), (':', ':'), ('//www.dellemc.com/en-us/collaterals/unauth/sales-documents/products/storage/h16221-hpc-lab-brochure.pdf', 'JJ'), ('4', 'CD'), ('DELL', 'NNP'), ('TECHNOLOGIES', 'NNP'), ('WHITE', 'NNP'), ('PAPER', 'NNP'), ('START', 'NNP'), ('EASY', 'NNP'), ('STUFF', 'NNP'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['https :', ': //www.dellemc.com/en-us/collaterals/unauth/sales-documents/products/storage/h16221-hpc-lab-brochure.pdf', '//www.dellemc.com/en-us/collaterals/unauth/sales-documents/products/storage/h16221-hpc-lab-brochure.pdf https', 'https :', ': //www.dellemc.com/en-us/collaterals/unauth/sales-documents/products/storage/h16221-hpc-lab-brochure.pdf', '//www.dellemc.com/en-us/collaterals/unauth/sales-documents/products/storage/h16221-hpc-lab-brochure.pdf 4', '4 DELL', 'DELL TECHNOLOGIES', 'TECHNOLOGIES WHITE', 'WHITE PAPER', 'PAPER START', 'START EASY', 'EASY STUFF', 'STUFF .'] 

 TOTAL BIGRAMS --> 14 



 ---- TRI-GRAMS ---- 

 ['https : //www.dellemc.com/en-us/collaterals/unauth/sales-documents/products/storage/h16221-hpc-lab-brochure.pdf', ': //www.dellemc.com/en-us/collaterals/unauth/sales-documents/products/storage/h16221-hpc-lab-brochure.pdf https', '//www.dellemc.com/en-us/collaterals/unauth/sales-documents/products/storage/h16221-hpc-lab-brochure.pdf https :', 'https : //www.dellemc.com/en-us/collaterals/unauth/sales-documents/products/storage/h16221-hpc-lab-brochure.pdf', ': //www.dellemc.com/en-us/collaterals/unauth/sales-documents/products/storage/h16221-hpc-lab-brochure.pdf 4', '//www.dellemc.com/en-us/collaterals/unauth/sales-documents/products/storage/h16221-hpc-lab-brochure.pdf 4 DELL', '4 DELL TECHNOLOGIES', 'DELL TECHNOLOGIES WHITE', 'TECHNOLOGIES WHITE PAPER', 'WHITE PAPER START', 'PAPER START EASY', 'START EASY STUFF', 'EASY STUFF .'] 

 TOTAL TRIGRAMS --> 13 



 ---- NOUN PHRASES ---- 

 ['https', ' https'] 

 TOTAL NOUN PHRASES --> 2 



 ---- NER ----

 
 ORGANIZATION ---> ['DELL', 'WHITE']
 TOTAL ORGANIZATION ENTITY --> 2 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['http', ':', '//www.dellemc.com/en-us/collaterals/unauth/sales-documents/products/storage/h16221-hpc-lab-brochure.pdf', 'http', ':', '//www.dellemc.com/en-us/collaterals/unauth/sales-documents/products/storage/h16221-hpc-lab-brochure.pdf', '4', 'dell', 'technolog', 'white', 'paper', 'start', 'easi', 'stuff', '.']

 TOTAL PORTER STEM WORDS ==> 15



 ---- SNOWBALL STEMMING ----

['https', ':', '//www.dellemc.com/en-us/collaterals/unauth/sales-documents/products/storage/h16221-hpc-lab-brochure.pdf', 'https', ':', '//www.dellemc.com/en-us/collaterals/unauth/sales-documents/products/storage/h16221-hpc-lab-brochure.pdf', '4', 'dell', 'technolog', 'white', 'paper', 'start', 'easi', 'stuff', '.']

 TOTAL SNOWBALL STEM WORDS ==> 15



 ---- LEMMATIZATION ----

['http', ':', '//www.dellemc.com/en-us/collaterals/unauth/sales-documents/products/storage/h16221-hpc-lab-brochure.pdf', 'http', ':', '//www.dellemc.com/en-us/collaterals/unauth/sales-documents/products/storage/h16221-hpc-lab-brochure.pdf', '4', 'DELL', 'TECHNOLOGIES', 'WHITE', 'PAPER', 'START', 'EASY', 'STUFF', '.']

 TOTAL LEMMATIZE WORDS ==> 15

************************************************************************************************************************

195 --> Don’t try to achieve your big vision right out of the gate. 


 ---- TOKENS ----

 ['Don', '’', 't', 'try', 'to', 'achieve', 'your', 'big', 'vision', 'right', 'out', 'of', 'the', 'gate', '.'] 

 TOTAL TOKENS ==> 15

 ---- POST ----

 [('Don', 'NNP'), ('’', 'NNP'), ('t', 'VBD'), ('try', 'VB'), ('to', 'TO'), ('achieve', 'VB'), ('your', 'PRP$'), ('big', 'JJ'), ('vision', 'NN'), ('right', 'VBD'), ('out', 'IN'), ('of', 'IN'), ('the', 'DT'), ('gate', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['’', 'try', 'achieve', 'big', 'vision', 'right', 'gate', '.']

 TOTAL FILTERED TOKENS ==>  8

 ---- POST FOR FILTERED TOKENS ----

 [('’', 'JJ'), ('try', 'NN'), ('achieve', 'VBP'), ('big', 'JJ'), ('vision', 'NN'), ('right', 'NN'), ('gate', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['’ try', 'try achieve', 'achieve big', 'big vision', 'vision right', 'right gate', 'gate .'] 

 TOTAL BIGRAMS --> 7 



 ---- TRI-GRAMS ---- 

 ['’ try achieve', 'try achieve big', 'achieve big vision', 'big vision right', 'vision right gate', 'right gate .'] 

 TOTAL TRIGRAMS --> 6 



 ---- NOUN PHRASES ---- 

 ['’ try', 'big vision', 'right', 'gate'] 

 TOTAL NOUN PHRASES --> 4 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['’', 'tri', 'achiev', 'big', 'vision', 'right', 'gate', '.']

 TOTAL PORTER STEM WORDS ==> 8



 ---- SNOWBALL STEMMING ----

['’', 'tri', 'achiev', 'big', 'vision', 'right', 'gate', '.']

 TOTAL SNOWBALL STEM WORDS ==> 8



 ---- LEMMATIZATION ----

['’', 'try', 'achieve', 'big', 'vision', 'right', 'gate', '.']

 TOTAL LEMMATIZE WORDS ==> 8

************************************************************************************************************************

196 --> Start small. 


 ---- TOKENS ----

 ['Start', 'small', '.'] 

 TOTAL TOKENS ==> 3

 ---- POST ----

 [('Start', 'NNP'), ('small', 'JJ'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Start', 'small', '.']

 TOTAL FILTERED TOKENS ==>  3

 ---- POST FOR FILTERED TOKENS ----

 [('Start', 'NNP'), ('small', 'JJ'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Start small', 'small .'] 

 TOTAL BIGRAMS --> 2 



 ---- TRI-GRAMS ---- 

 ['Start small .'] 

 TOTAL TRIGRAMS --> 1 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['start', 'small', '.']

 TOTAL PORTER STEM WORDS ==> 3



 ---- SNOWBALL STEMMING ----

['start', 'small', '.']

 TOTAL SNOWBALL STEM WORDS ==> 3



 ---- LEMMATIZATION ----

['Start', 'small', '.']

 TOTAL LEMMATIZE WORDS ==> 3

************************************************************************************************************************

197 --> Try to figure out some  easy things that you can tease out of your dataset to prove what’s possible with your  machine learning and deep learning tools. 


 ---- TOKENS ----

 ['Try', 'to', 'figure', 'out', 'some', 'easy', 'things', 'that', 'you', 'can', 'tease', 'out', 'of', 'your', 'dataset', 'to', 'prove', 'what', '’', 's', 'possible', 'with', 'your', 'machine', 'learning', 'and', 'deep', 'learning', 'tools', '.'] 

 TOTAL TOKENS ==> 30

 ---- POST ----

 [('Try', 'VB'), ('to', 'TO'), ('figure', 'VB'), ('out', 'RP'), ('some', 'DT'), ('easy', 'JJ'), ('things', 'NNS'), ('that', 'IN'), ('you', 'PRP'), ('can', 'MD'), ('tease', 'VB'), ('out', 'IN'), ('of', 'IN'), ('your', 'PRP$'), ('dataset', 'NN'), ('to', 'TO'), ('prove', 'VB'), ('what', 'WP'), ('’', 'NNP'), ('s', 'VBD'), ('possible', 'JJ'), ('with', 'IN'), ('your', 'PRP$'), ('machine', 'NN'), ('learning', 'NN'), ('and', 'CC'), ('deep', 'JJ'), ('learning', 'NN'), ('tools', 'NNS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Try', 'figure', 'easy', 'things', 'tease', 'dataset', 'prove', '’', 'possible', 'machine', 'learning', 'deep', 'learning', 'tools', '.']

 TOTAL FILTERED TOKENS ==>  15

 ---- POST FOR FILTERED TOKENS ----

 [('Try', 'VB'), ('figure', 'NN'), ('easy', 'JJ'), ('things', 'NNS'), ('tease', 'NN'), ('dataset', 'VBN'), ('prove', 'IN'), ('’', 'NNP'), ('possible', 'JJ'), ('machine', 'NN'), ('learning', 'VBG'), ('deep', 'JJ'), ('learning', 'NN'), ('tools', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Try figure', 'figure easy', 'easy things', 'things tease', 'tease dataset', 'dataset prove', 'prove ’', '’ possible', 'possible machine', 'machine learning', 'learning deep', 'deep learning', 'learning tools', 'tools .'] 

 TOTAL BIGRAMS --> 14 



 ---- TRI-GRAMS ---- 

 ['Try figure easy', 'figure easy things', 'easy things tease', 'things tease dataset', 'tease dataset prove', 'dataset prove ’', 'prove ’ possible', '’ possible machine', 'possible machine learning', 'machine learning deep', 'learning deep learning', 'deep learning tools', 'learning tools .'] 

 TOTAL TRIGRAMS --> 13 



 ---- NOUN PHRASES ---- 

 ['figure', 'tease', 'possible machine', 'deep learning'] 

 TOTAL NOUN PHRASES --> 4 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['tri', 'figur', 'easi', 'thing', 'teas', 'dataset', 'prove', '’', 'possibl', 'machin', 'learn', 'deep', 'learn', 'tool', '.']

 TOTAL PORTER STEM WORDS ==> 15



 ---- SNOWBALL STEMMING ----

['tri', 'figur', 'easi', 'thing', 'teas', 'dataset', 'prove', '’', 'possibl', 'machin', 'learn', 'deep', 'learn', 'tool', '.']

 TOTAL SNOWBALL STEM WORDS ==> 15



 ---- LEMMATIZATION ----

['Try', 'figure', 'easy', 'thing', 'tease', 'dataset', 'prove', '’', 'possible', 'machine', 'learning', 'deep', 'learning', 'tool', '.']

 TOTAL LEMMATIZE WORDS ==> 15

************************************************************************************************************************

198 --> Get some initial wins, and then build on them. 


 ---- TOKENS ----

 ['Get', 'some', 'initial', 'wins', ',', 'and', 'then', 'build', 'on', 'them', '.'] 

 TOTAL TOKENS ==> 11

 ---- POST ----

 [('Get', 'VB'), ('some', 'DT'), ('initial', 'JJ'), ('wins', 'NNS'), (',', ','), ('and', 'CC'), ('then', 'RB'), ('build', 'VB'), ('on', 'IN'), ('them', 'PRP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Get', 'initial', 'wins', ',', 'build', '.']

 TOTAL FILTERED TOKENS ==>  6

 ---- POST FOR FILTERED TOKENS ----

 [('Get', 'NNP'), ('initial', 'JJ'), ('wins', 'NNS'), (',', ','), ('build', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Get initial', 'initial wins', 'wins ,', ', build', 'build .'] 

 TOTAL BIGRAMS --> 5 



 ---- TRI-GRAMS ---- 

 ['Get initial wins', 'initial wins ,', 'wins , build', ', build .'] 

 TOTAL TRIGRAMS --> 4 



 ---- NOUN PHRASES ---- 

 ['build'] 

 TOTAL NOUN PHRASES --> 1 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['get', 'initi', 'win', ',', 'build', '.']

 TOTAL PORTER STEM WORDS ==> 6



 ---- SNOWBALL STEMMING ----

['get', 'initi', 'win', ',', 'build', '.']

 TOTAL SNOWBALL STEM WORDS ==> 6



 ---- LEMMATIZATION ----

['Get', 'initial', 'win', ',', 'build', '.']

 TOTAL LEMMATIZE WORDS ==> 6

************************************************************************************************************************

199 --> KEEP YOUR EYES ON THE PRIZE. 


 ---- TOKENS ----

 ['KEEP', 'YOUR', 'EYES', 'ON', 'THE', 'PRIZE', '.'] 

 TOTAL TOKENS ==> 7

 ---- POST ----

 [('KEEP', 'VB'), ('YOUR', 'NNP'), ('EYES', 'NNP'), ('ON', 'NNP'), ('THE', 'NNP'), ('PRIZE', 'NNP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['KEEP', 'EYES', 'PRIZE', '.']

 TOTAL FILTERED TOKENS ==>  4

 ---- POST FOR FILTERED TOKENS ----

 [('KEEP', 'NNP'), ('EYES', 'NNP'), ('PRIZE', 'NNP'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['KEEP EYES', 'EYES PRIZE', 'PRIZE .'] 

 TOTAL BIGRAMS --> 3 



 ---- TRI-GRAMS ---- 

 ['KEEP EYES PRIZE', 'EYES PRIZE .'] 

 TOTAL TRIGRAMS --> 2 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> ['KEEP', 'EYES']
 TOTAL ORGANIZATION ENTITY --> 2 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['keep', 'eye', 'prize', '.']

 TOTAL PORTER STEM WORDS ==> 4



 ---- SNOWBALL STEMMING ----

['keep', 'eye', 'prize', '.']

 TOTAL SNOWBALL STEM WORDS ==> 4



 ---- LEMMATIZATION ----

['KEEP', 'EYES', 'PRIZE', '.']

 TOTAL LEMMATIZE WORDS ==> 4

************************************************************************************************************************

200 --> The development on an NLP application is a complex undertaking from beginning to end. 


 ---- TOKENS ----

 ['The', 'development', 'on', 'an', 'NLP', 'application', 'is', 'a', 'complex', 'undertaking', 'from', 'beginning', 'to', 'end', '.'] 

 TOTAL TOKENS ==> 15

 ---- POST ----

 [('The', 'DT'), ('development', 'NN'), ('on', 'IN'), ('an', 'DT'), ('NLP', 'NNP'), ('application', 'NN'), ('is', 'VBZ'), ('a', 'DT'), ('complex', 'JJ'), ('undertaking', 'NN'), ('from', 'IN'), ('beginning', 'VBG'), ('to', 'TO'), ('end', 'VB'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['development', 'NLP', 'application', 'complex', 'undertaking', 'beginning', 'end', '.']

 TOTAL FILTERED TOKENS ==>  8

 ---- POST FOR FILTERED TOKENS ----

 [('development', 'NN'), ('NLP', 'NNP'), ('application', 'NN'), ('complex', 'JJ'), ('undertaking', 'NN'), ('beginning', 'VBG'), ('end', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['development NLP', 'NLP application', 'application complex', 'complex undertaking', 'undertaking beginning', 'beginning end', 'end .'] 

 TOTAL BIGRAMS --> 7 



 ---- TRI-GRAMS ---- 

 ['development NLP application', 'NLP application complex', 'application complex undertaking', 'complex undertaking beginning', 'undertaking beginning end', 'beginning end .'] 

 TOTAL TRIGRAMS --> 6 



 ---- NOUN PHRASES ---- 

 ['development', 'application', 'complex undertaking', 'end'] 

 TOTAL NOUN PHRASES --> 4 



 ---- NER ----

 
 ORGANIZATION ---> ['NLP']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['develop', 'nlp', 'applic', 'complex', 'undertak', 'begin', 'end', '.']

 TOTAL PORTER STEM WORDS ==> 8



 ---- SNOWBALL STEMMING ----

['develop', 'nlp', 'applic', 'complex', 'undertak', 'begin', 'end', '.']

 TOTAL SNOWBALL STEM WORDS ==> 8



 ---- LEMMATIZATION ----

['development', 'NLP', 'application', 'complex', 'undertaking', 'beginning', 'end', '.']

 TOTAL LEMMATIZE WORDS ==> 8

************************************************************************************************************************

201 --> You’re trying to create a mathematical model that mimics the human brain. 


 ---- TOKENS ----

 ['You', '’', 're', 'trying', 'to', 'create', 'a', 'mathematical', 'model', 'that', 'mimics', 'the', 'human', 'brain', '.'] 

 TOTAL TOKENS ==> 15

 ---- POST ----

 [('You', 'PRP'), ('’', 'VBP'), ('re', 'VB'), ('trying', 'VBG'), ('to', 'TO'), ('create', 'VB'), ('a', 'DT'), ('mathematical', 'JJ'), ('model', 'NN'), ('that', 'WDT'), ('mimics', 'VBZ'), ('the', 'DT'), ('human', 'JJ'), ('brain', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['’', 'trying', 'create', 'mathematical', 'model', 'mimics', 'human', 'brain', '.']

 TOTAL FILTERED TOKENS ==>  9

 ---- POST FOR FILTERED TOKENS ----

 [('’', 'RB'), ('trying', 'VBG'), ('create', 'VB'), ('mathematical', 'JJ'), ('model', 'NN'), ('mimics', 'NNS'), ('human', 'JJ'), ('brain', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['’ trying', 'trying create', 'create mathematical', 'mathematical model', 'model mimics', 'mimics human', 'human brain', 'brain .'] 

 TOTAL BIGRAMS --> 8 



 ---- TRI-GRAMS ---- 

 ['’ trying create', 'trying create mathematical', 'create mathematical model', 'mathematical model mimics', 'model mimics human', 'mimics human brain', 'human brain .'] 

 TOTAL TRIGRAMS --> 7 



 ---- NOUN PHRASES ---- 

 ['mathematical model', 'human brain'] 

 TOTAL NOUN PHRASES --> 2 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['’', 'tri', 'creat', 'mathemat', 'model', 'mimic', 'human', 'brain', '.']

 TOTAL PORTER STEM WORDS ==> 9



 ---- SNOWBALL STEMMING ----

['’', 'tri', 'creat', 'mathemat', 'model', 'mimic', 'human', 'brain', '.']

 TOTAL SNOWBALL STEM WORDS ==> 9



 ---- LEMMATIZATION ----

['’', 'trying', 'create', 'mathematical', 'model', 'mimic', 'human', 'brain', '.']

 TOTAL LEMMATIZE WORDS ==> 9

************************************************************************************************************************

202 --> This isn’t going  to happen overnight. 


 ---- TOKENS ----

 ['This', 'isn', '’', 't', 'going', 'to', 'happen', 'overnight', '.'] 

 TOTAL TOKENS ==> 9

 ---- POST ----

 [('This', 'DT'), ('isn', 'NN'), ('’', 'VBZ'), ('t', 'RB'), ('going', 'VBG'), ('to', 'TO'), ('happen', 'VB'), ('overnight', 'RB'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['’', 'going', 'happen', 'overnight', '.']

 TOTAL FILTERED TOKENS ==>  5

 ---- POST FOR FILTERED TOKENS ----

 [('’', 'NN'), ('going', 'VBG'), ('happen', 'JJ'), ('overnight', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['’ going', 'going happen', 'happen overnight', 'overnight .'] 

 TOTAL BIGRAMS --> 4 



 ---- TRI-GRAMS ---- 

 ['’ going happen', 'going happen overnight', 'happen overnight .'] 

 TOTAL TRIGRAMS --> 3 



 ---- NOUN PHRASES ---- 

 ['’', 'happen overnight'] 

 TOTAL NOUN PHRASES --> 2 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['’', 'go', 'happen', 'overnight', '.']

 TOTAL PORTER STEM WORDS ==> 5



 ---- SNOWBALL STEMMING ----

['’', 'go', 'happen', 'overnight', '.']

 TOTAL SNOWBALL STEM WORDS ==> 5



 ---- LEMMATIZATION ----

['’', 'going', 'happen', 'overnight', '.']

 TOTAL LEMMATIZE WORDS ==> 5

************************************************************************************************************************

203 --> The key is to recognize what’s possible, and always work toward the  big goal — an application you can put to work to drive your business forward. 


 ---- TOKENS ----

 ['The', 'key', 'is', 'to', 'recognize', 'what', '’', 's', 'possible', ',', 'and', 'always', 'work', 'toward', 'the', 'big', 'goal', '—', 'an', 'application', 'you', 'can', 'put', 'to', 'work', 'to', 'drive', 'your', 'business', 'forward', '.'] 

 TOTAL TOKENS ==> 31

 ---- POST ----

 [('The', 'DT'), ('key', 'NN'), ('is', 'VBZ'), ('to', 'TO'), ('recognize', 'VB'), ('what', 'WP'), ('’', 'NNP'), ('s', 'VBD'), ('possible', 'JJ'), (',', ','), ('and', 'CC'), ('always', 'RB'), ('work', 'VB'), ('toward', 'IN'), ('the', 'DT'), ('big', 'JJ'), ('goal', 'NN'), ('—', 'VBD'), ('an', 'DT'), ('application', 'NN'), ('you', 'PRP'), ('can', 'MD'), ('put', 'VB'), ('to', 'TO'), ('work', 'VB'), ('to', 'TO'), ('drive', 'VB'), ('your', 'PRP$'), ('business', 'NN'), ('forward', 'RB'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['key', 'recognize', '’', 'possible', ',', 'always', 'work', 'toward', 'big', 'goal', '—', 'application', 'put', 'work', 'drive', 'business', 'forward', '.']

 TOTAL FILTERED TOKENS ==>  18

 ---- POST FOR FILTERED TOKENS ----

 [('key', 'JJ'), ('recognize', 'VB'), ('’', 'NNP'), ('possible', 'JJ'), (',', ','), ('always', 'RB'), ('work', 'NN'), ('toward', 'IN'), ('big', 'JJ'), ('goal', 'NN'), ('—', 'JJ'), ('application', 'NN'), ('put', 'VBD'), ('work', 'NN'), ('drive', 'NN'), ('business', 'NN'), ('forward', 'RB'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['key recognize', 'recognize ’', '’ possible', 'possible ,', ', always', 'always work', 'work toward', 'toward big', 'big goal', 'goal —', '— application', 'application put', 'put work', 'work drive', 'drive business', 'business forward', 'forward .'] 

 TOTAL BIGRAMS --> 17 



 ---- TRI-GRAMS ---- 

 ['key recognize ’', 'recognize ’ possible', '’ possible ,', 'possible , always', ', always work', 'always work toward', 'work toward big', 'toward big goal', 'big goal —', 'goal — application', '— application put', 'application put work', 'put work drive', 'work drive business', 'drive business forward', 'business forward .'] 

 TOTAL TRIGRAMS --> 16 



 ---- NOUN PHRASES ---- 

 ['work', 'big goal', '— application', 'work', 'drive', 'business'] 

 TOTAL NOUN PHRASES --> 6 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['key', 'recogn', '’', 'possibl', ',', 'alway', 'work', 'toward', 'big', 'goal', '—', 'applic', 'put', 'work', 'drive', 'busi', 'forward', '.']

 TOTAL PORTER STEM WORDS ==> 18



 ---- SNOWBALL STEMMING ----

['key', 'recogn', '’', 'possibl', ',', 'alway', 'work', 'toward', 'big', 'goal', '—', 'applic', 'put', 'work', 'drive', 'busi', 'forward', '.']

 TOTAL SNOWBALL STEM WORDS ==> 18



 ---- LEMMATIZATION ----

['key', 'recognize', '’', 'possible', ',', 'always', 'work', 'toward', 'big', 'goal', '—', 'application', 'put', 'work', 'drive', 'business', 'forward', '.']

 TOTAL LEMMATIZE WORDS ==> 18

************************************************************************************************************************

204 --> KEY TAKEAWAYS Natural language processing is a potentially powerful tool for enterprises and other  organizations that want to streamline their interactions with customers, employees,  partners and others. 


 ---- TOKENS ----

 ['KEY', 'TAKEAWAYS', 'Natural', 'language', 'processing', 'is', 'a', 'potentially', 'powerful', 'tool', 'for', 'enterprises', 'and', 'other', 'organizations', 'that', 'want', 'to', 'streamline', 'their', 'interactions', 'with', 'customers', ',', 'employees', ',', 'partners', 'and', 'others', '.'] 

 TOTAL TOKENS ==> 30

 ---- POST ----

 [('KEY', 'NNP'), ('TAKEAWAYS', 'NNP'), ('Natural', 'NNP'), ('language', 'NN'), ('processing', 'NN'), ('is', 'VBZ'), ('a', 'DT'), ('potentially', 'RB'), ('powerful', 'JJ'), ('tool', 'NN'), ('for', 'IN'), ('enterprises', 'NNS'), ('and', 'CC'), ('other', 'JJ'), ('organizations', 'NNS'), ('that', 'WDT'), ('want', 'VBP'), ('to', 'TO'), ('streamline', 'VB'), ('their', 'PRP$'), ('interactions', 'NNS'), ('with', 'IN'), ('customers', 'NNS'), (',', ','), ('employees', 'NNS'), (',', ','), ('partners', 'NNS'), ('and', 'CC'), ('others', 'NNS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['KEY', 'TAKEAWAYS', 'Natural', 'language', 'processing', 'potentially', 'powerful', 'tool', 'enterprises', 'organizations', 'want', 'streamline', 'interactions', 'customers', ',', 'employees', ',', 'partners', 'others', '.']

 TOTAL FILTERED TOKENS ==>  20

 ---- POST FOR FILTERED TOKENS ----

 [('KEY', 'NNP'), ('TAKEAWAYS', 'NNP'), ('Natural', 'NNP'), ('language', 'NN'), ('processing', 'NN'), ('potentially', 'RB'), ('powerful', 'JJ'), ('tool', 'NN'), ('enterprises', 'NNS'), ('organizations', 'NNS'), ('want', 'VBP'), ('streamline', 'JJ'), ('interactions', 'NNS'), ('customers', 'NNS'), (',', ','), ('employees', 'NNS'), (',', ','), ('partners', 'NNS'), ('others', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['KEY TAKEAWAYS', 'TAKEAWAYS Natural', 'Natural language', 'language processing', 'processing potentially', 'potentially powerful', 'powerful tool', 'tool enterprises', 'enterprises organizations', 'organizations want', 'want streamline', 'streamline interactions', 'interactions customers', 'customers ,', ', employees', 'employees ,', ', partners', 'partners others', 'others .'] 

 TOTAL BIGRAMS --> 19 



 ---- TRI-GRAMS ---- 

 ['KEY TAKEAWAYS Natural', 'TAKEAWAYS Natural language', 'Natural language processing', 'language processing potentially', 'processing potentially powerful', 'potentially powerful tool', 'powerful tool enterprises', 'tool enterprises organizations', 'enterprises organizations want', 'organizations want streamline', 'want streamline interactions', 'streamline interactions customers', 'interactions customers ,', 'customers , employees', ', employees ,', 'employees , partners', ', partners others', 'partners others .'] 

 TOTAL TRIGRAMS --> 18 



 ---- NOUN PHRASES ---- 

 ['language', 'processing', 'powerful tool'] 

 TOTAL NOUN PHRASES --> 3 



 ---- NER ----

 
 ORGANIZATION ---> ['KEY', 'TAKEAWAYS Natural']
 TOTAL ORGANIZATION ENTITY --> 2 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['key', 'takeaway', 'natur', 'languag', 'process', 'potenti', 'power', 'tool', 'enterpris', 'organ', 'want', 'streamlin', 'interact', 'custom', ',', 'employe', ',', 'partner', 'other', '.']

 TOTAL PORTER STEM WORDS ==> 20



 ---- SNOWBALL STEMMING ----

['key', 'takeaway', 'natur', 'languag', 'process', 'potenti', 'power', 'tool', 'enterpris', 'organ', 'want', 'streamlin', 'interact', 'custom', ',', 'employe', ',', 'partner', 'other', '.']

 TOTAL SNOWBALL STEM WORDS ==> 20



 ---- LEMMATIZATION ----

['KEY', 'TAKEAWAYS', 'Natural', 'language', 'processing', 'potentially', 'powerful', 'tool', 'enterprise', 'organization', 'want', 'streamline', 'interaction', 'customer', ',', 'employee', ',', 'partner', 'others', '.']

 TOTAL LEMMATIZE WORDS ==> 20

************************************************************************************************************************

205 --> To help organizations capitalize on this opportunity, researchers  in the Dell Technologies HPC & AI Innovation Lab are working actively to advance the  technologies and methodologies for the development of language-to-language translation  and text-to-voice translation applications. 


 ---- TOKENS ----

 ['To', 'help', 'organizations', 'capitalize', 'on', 'this', 'opportunity', ',', 'researchers', 'in', 'the', 'Dell', 'Technologies', 'HPC', '&', 'AI', 'Innovation', 'Lab', 'are', 'working', 'actively', 'to', 'advance', 'the', 'technologies', 'and', 'methodologies', 'for', 'the', 'development', 'of', 'language-to-language', 'translation', 'and', 'text-to-voice', 'translation', 'applications', '.'] 

 TOTAL TOKENS ==> 38

 ---- POST ----

 [('To', 'TO'), ('help', 'VB'), ('organizations', 'NNS'), ('capitalize', 'VB'), ('on', 'IN'), ('this', 'DT'), ('opportunity', 'NN'), (',', ','), ('researchers', 'NNS'), ('in', 'IN'), ('the', 'DT'), ('Dell', 'NNP'), ('Technologies', 'NNPS'), ('HPC', 'NNP'), ('&', 'CC'), ('AI', 'NNP'), ('Innovation', 'NNP'), ('Lab', 'NNP'), ('are', 'VBP'), ('working', 'VBG'), ('actively', 'RB'), ('to', 'TO'), ('advance', 'VB'), ('the', 'DT'), ('technologies', 'NNS'), ('and', 'CC'), ('methodologies', 'NNS'), ('for', 'IN'), ('the', 'DT'), ('development', 'NN'), ('of', 'IN'), ('language-to-language', 'JJ'), ('translation', 'NN'), ('and', 'CC'), ('text-to-voice', 'JJ'), ('translation', 'NN'), ('applications', 'NNS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['help', 'organizations', 'capitalize', 'opportunity', ',', 'researchers', 'Dell', 'Technologies', 'HPC', '&', 'AI', 'Innovation', 'Lab', 'working', 'actively', 'advance', 'technologies', 'methodologies', 'development', 'language-to-language', 'translation', 'text-to-voice', 'translation', 'applications', '.']

 TOTAL FILTERED TOKENS ==>  25

 ---- POST FOR FILTERED TOKENS ----

 [('help', 'NN'), ('organizations', 'NNS'), ('capitalize', 'VBP'), ('opportunity', 'NN'), (',', ','), ('researchers', 'NNS'), ('Dell', 'NNP'), ('Technologies', 'NNPS'), ('HPC', 'NNP'), ('&', 'CC'), ('AI', 'NNP'), ('Innovation', 'NNP'), ('Lab', 'NNP'), ('working', 'VBG'), ('actively', 'RB'), ('advance', 'JJ'), ('technologies', 'NNS'), ('methodologies', 'NNS'), ('development', 'NN'), ('language-to-language', 'JJ'), ('translation', 'NN'), ('text-to-voice', 'JJ'), ('translation', 'NN'), ('applications', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['help organizations', 'organizations capitalize', 'capitalize opportunity', 'opportunity ,', ', researchers', 'researchers Dell', 'Dell Technologies', 'Technologies HPC', 'HPC &', '& AI', 'AI Innovation', 'Innovation Lab', 'Lab working', 'working actively', 'actively advance', 'advance technologies', 'technologies methodologies', 'methodologies development', 'development language-to-language', 'language-to-language translation', 'translation text-to-voice', 'text-to-voice translation', 'translation applications', 'applications .'] 

 TOTAL BIGRAMS --> 24 



 ---- TRI-GRAMS ---- 

 ['help organizations capitalize', 'organizations capitalize opportunity', 'capitalize opportunity ,', 'opportunity , researchers', ', researchers Dell', 'researchers Dell Technologies', 'Dell Technologies HPC', 'Technologies HPC &', 'HPC & AI', '& AI Innovation', 'AI Innovation Lab', 'Innovation Lab working', 'Lab working actively', 'working actively advance', 'actively advance technologies', 'advance technologies methodologies', 'technologies methodologies development', 'methodologies development language-to-language', 'development language-to-language translation', 'language-to-language translation text-to-voice', 'translation text-to-voice translation', 'text-to-voice translation applications', 'translation applications .'] 

 TOTAL TRIGRAMS --> 23 



 ---- NOUN PHRASES ---- 

 ['help', 'opportunity', 'development', 'language-to-language translation', 'text-to-voice translation'] 

 TOTAL NOUN PHRASES --> 5 



 ---- NER ----

 
 ORGANIZATION ---> ['Dell Technologies', 'HPC', 'AI Innovation']
 TOTAL ORGANIZATION ENTITY --> 3 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['help', 'organ', 'capit', 'opportun', ',', 'research', 'dell', 'technolog', 'hpc', '&', 'ai', 'innov', 'lab', 'work', 'activ', 'advanc', 'technolog', 'methodolog', 'develop', 'language-to-languag', 'translat', 'text-to-voic', 'translat', 'applic', '.']

 TOTAL PORTER STEM WORDS ==> 25



 ---- SNOWBALL STEMMING ----

['help', 'organ', 'capit', 'opportun', ',', 'research', 'dell', 'technolog', 'hpc', '&', 'ai', 'innov', 'lab', 'work', 'activ', 'advanc', 'technolog', 'methodolog', 'develop', 'language-to-languag', 'translat', 'text-to-voic', 'translat', 'applic', '.']

 TOTAL SNOWBALL STEM WORDS ==> 25



 ---- LEMMATIZATION ----

['help', 'organization', 'capitalize', 'opportunity', ',', 'researcher', 'Dell', 'Technologies', 'HPC', '&', 'AI', 'Innovation', 'Lab', 'working', 'actively', 'advance', 'technology', 'methodology', 'development', 'language-to-language', 'translation', 'text-to-voice', 'translation', 'application', '.']

 TOTAL LEMMATIZE WORDS ==> 25

************************************************************************************************************************

206 --> TO LEARN MORE • To learn more about the resources available through the Dell Technologies HPC & AI  Innovation Lab, visit delltechnologies.com/innovationlab. 


 ---- TOKENS ----

 ['TO', 'LEARN', 'MORE', '•', 'To', 'learn', 'more', 'about', 'the', 'resources', 'available', 'through', 'the', 'Dell', 'Technologies', 'HPC', '&', 'AI', 'Innovation', 'Lab', ',', 'visit', 'delltechnologies.com/innovationlab', '.'] 

 TOTAL TOKENS ==> 24

 ---- POST ----

 [('TO', 'NNP'), ('LEARN', 'NNP'), ('MORE', 'NNP'), ('•', 'NNP'), ('To', 'TO'), ('learn', 'VB'), ('more', 'JJR'), ('about', 'IN'), ('the', 'DT'), ('resources', 'NNS'), ('available', 'JJ'), ('through', 'IN'), ('the', 'DT'), ('Dell', 'NNP'), ('Technologies', 'NNPS'), ('HPC', 'NNP'), ('&', 'CC'), ('AI', 'NNP'), ('Innovation', 'NNP'), ('Lab', 'NNP'), (',', ','), ('visit', 'NN'), ('delltechnologies.com/innovationlab', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['LEARN', '•', 'learn', 'resources', 'available', 'Dell', 'Technologies', 'HPC', '&', 'AI', 'Innovation', 'Lab', ',', 'visit', 'delltechnologies.com/innovationlab', '.']

 TOTAL FILTERED TOKENS ==>  16

 ---- POST FOR FILTERED TOKENS ----

 [('LEARN', 'NNP'), ('•', 'NNP'), ('learn', 'JJ'), ('resources', 'NNS'), ('available', 'JJ'), ('Dell', 'NNP'), ('Technologies', 'NNP'), ('HPC', 'NNP'), ('&', 'CC'), ('AI', 'NNP'), ('Innovation', 'NNP'), ('Lab', 'NNP'), (',', ','), ('visit', 'NN'), ('delltechnologies.com/innovationlab', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['LEARN •', '• learn', 'learn resources', 'resources available', 'available Dell', 'Dell Technologies', 'Technologies HPC', 'HPC &', '& AI', 'AI Innovation', 'Innovation Lab', 'Lab ,', ', visit', 'visit delltechnologies.com/innovationlab', 'delltechnologies.com/innovationlab .'] 

 TOTAL BIGRAMS --> 15 



 ---- TRI-GRAMS ---- 

 ['LEARN • learn', '• learn resources', 'learn resources available', 'resources available Dell', 'available Dell Technologies', 'Dell Technologies HPC', 'Technologies HPC &', 'HPC & AI', '& AI Innovation', 'AI Innovation Lab', 'Innovation Lab ,', 'Lab , visit', ', visit delltechnologies.com/innovationlab', 'visit delltechnologies.com/innovationlab .'] 

 TOTAL TRIGRAMS --> 14 



 ---- NOUN PHRASES ---- 

 ['visit', 'delltechnologies.com'] 

 TOTAL NOUN PHRASES --> 2 



 ---- NER ----

 
 ORGANIZATION ---> ['LEARN', 'Dell Technologies', 'AI Innovation Lab']
 TOTAL ORGANIZATION ENTITY --> 3 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['learn', '•', 'learn', 'resourc', 'avail', 'dell', 'technolog', 'hpc', '&', 'ai', 'innov', 'lab', ',', 'visit', 'delltechnologies.com/innovationlab', '.']

 TOTAL PORTER STEM WORDS ==> 16



 ---- SNOWBALL STEMMING ----

['learn', '•', 'learn', 'resourc', 'avail', 'dell', 'technolog', 'hpc', '&', 'ai', 'innov', 'lab', ',', 'visit', 'delltechnologies.com/innovationlab', '.']

 TOTAL SNOWBALL STEM WORDS ==> 16



 ---- LEMMATIZATION ----

['LEARN', '•', 'learn', 'resource', 'available', 'Dell', 'Technologies', 'HPC', '&', 'AI', 'Innovation', 'Lab', ',', 'visit', 'delltechnologies.com/innovationlab', '.']

 TOTAL LEMMATIZE WORDS ==> 16

************************************************************************************************************************

207 --> • For a broader look at NLP systems, see the article “Natural Language Processing Could  Be Key to Your Company’s Digital Transformation” by Dell Technologies data scientist  Lucas Wilson, Ph.D. • For an inside look at a recent neural machine translation project the Dell Technologies  HPC & AI Innovation Lab was involved with, read the white paper “Densifying Assumed- sparse Tensors: Improving Memory Efficiency and MPI Collective Performance during  Tensor Accumulation for Parallelized Training of Neural Machine Translation Models.” • To explore new HPC solutions for powering AI-driven applications,   visit delltechnologies.com/ai. 


 ---- TOKENS ----

 ['•', 'For', 'a', 'broader', 'look', 'at', 'NLP', 'systems', ',', 'see', 'the', 'article', '“', 'Natural', 'Language', 'Processing', 'Could', 'Be', 'Key', 'to', 'Your', 'Company', '’', 's', 'Digital', 'Transformation', '”', 'by', 'Dell', 'Technologies', 'data', 'scientist', 'Lucas', 'Wilson', ',', 'Ph.D.', '•', 'For', 'an', 'inside', 'look', 'at', 'a', 'recent', 'neural', 'machine', 'translation', 'project', 'the', 'Dell', 'Technologies', 'HPC', '&', 'AI', 'Innovation', 'Lab', 'was', 'involved', 'with', ',', 'read', 'the', 'white', 'paper', '“', 'Densifying', 'Assumed-', 'sparse', 'Tensors', ':', 'Improving', 'Memory', 'Efficiency', 'and', 'MPI', 'Collective', 'Performance', 'during', 'Tensor', 'Accumulation', 'for', 'Parallelized', 'Training', 'of', 'Neural', 'Machine', 'Translation', 'Models.', '”', '•', 'To', 'explore', 'new', 'HPC', 'solutions', 'for', 'powering', 'AI-driven', 'applications', ',', 'visit', 'delltechnologies.com/ai', '.'] 

 TOTAL TOKENS ==> 103

 ---- POST ----

 [('•', 'NN'), ('For', 'IN'), ('a', 'DT'), ('broader', 'JJR'), ('look', 'NN'), ('at', 'IN'), ('NLP', 'NNP'), ('systems', 'NNS'), (',', ','), ('see', 'VBP'), ('the', 'DT'), ('article', 'NN'), ('“', 'NNP'), ('Natural', 'NNP'), ('Language', 'NNP'), ('Processing', 'NNP'), ('Could', 'NNP'), ('Be', 'NNP'), ('Key', 'NNP'), ('to', 'TO'), ('Your', 'VB'), ('Company', 'NNP'), ('’', 'NNP'), ('s', 'VBD'), ('Digital', 'NNP'), ('Transformation', 'NNP'), ('”', 'NNP'), ('by', 'IN'), ('Dell', 'NNP'), ('Technologies', 'NNPS'), ('data', 'NNS'), ('scientist', 'NN'), ('Lucas', 'NNP'), ('Wilson', 'NNP'), (',', ','), ('Ph.D.', 'NNP'), ('•', 'NNP'), ('For', 'IN'), ('an', 'DT'), ('inside', 'JJ'), ('look', 'NN'), ('at', 'IN'), ('a', 'DT'), ('recent', 'JJ'), ('neural', 'JJ'), ('machine', 'NN'), ('translation', 'NN'), ('project', 'NN'), ('the', 'DT'), ('Dell', 'NNP'), ('Technologies', 'NNPS'), ('HPC', 'NNP'), ('&', 'CC'), ('AI', 'NNP'), ('Innovation', 'NNP'), ('Lab', 'NNP'), ('was', 'VBD'), ('involved', 'VBN'), ('with', 'IN'), (',', ','), ('read', 'VB'), ('the', 'DT'), ('white', 'JJ'), ('paper', 'NN'), ('“', 'NN'), ('Densifying', 'VBG'), ('Assumed-', 'NNP'), ('sparse', 'NN'), ('Tensors', 'NNS'), (':', ':'), ('Improving', 'VBG'), ('Memory', 'NNP'), ('Efficiency', 'NNP'), ('and', 'CC'), ('MPI', 'NNP'), ('Collective', 'NNP'), ('Performance', 'NNP'), ('during', 'IN'), ('Tensor', 'NNP'), ('Accumulation', 'NNP'), ('for', 'IN'), ('Parallelized', 'NNP'), ('Training', 'NNP'), ('of', 'IN'), ('Neural', 'NNP'), ('Machine', 'NNP'), ('Translation', 'NNP'), ('Models.', 'NNP'), ('”', 'NNP'), ('•', 'NNP'), ('To', 'TO'), ('explore', 'VB'), ('new', 'JJ'), ('HPC', 'NNP'), ('solutions', 'NNS'), ('for', 'IN'), ('powering', 'VBG'), ('AI-driven', 'NNP'), ('applications', 'NNS'), (',', ','), ('visit', 'NN'), ('delltechnologies.com/ai', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['•', 'broader', 'look', 'NLP', 'systems', ',', 'see', 'article', '“', 'Natural', 'Language', 'Processing', 'Could', 'Key', 'Company', '’', 'Digital', 'Transformation', '”', 'Dell', 'Technologies', 'data', 'scientist', 'Lucas', 'Wilson', ',', 'Ph.D.', '•', 'inside', 'look', 'recent', 'neural', 'machine', 'translation', 'project', 'Dell', 'Technologies', 'HPC', '&', 'AI', 'Innovation', 'Lab', 'involved', ',', 'read', 'white', 'paper', '“', 'Densifying', 'Assumed-', 'sparse', 'Tensors', ':', 'Improving', 'Memory', 'Efficiency', 'MPI', 'Collective', 'Performance', 'Tensor', 'Accumulation', 'Parallelized', 'Training', 'Neural', 'Machine', 'Translation', 'Models.', '”', '•', 'explore', 'new', 'HPC', 'solutions', 'powering', 'AI-driven', 'applications', ',', 'visit', 'delltechnologies.com/ai', '.']

 TOTAL FILTERED TOKENS ==>  80

 ---- POST FOR FILTERED TOKENS ----

 [('•', 'JJ'), ('broader', 'JJR'), ('look', 'NN'), ('NLP', 'NNP'), ('systems', 'NNS'), (',', ','), ('see', 'VBP'), ('article', 'NN'), ('“', 'NNP'), ('Natural', 'NNP'), ('Language', 'NNP'), ('Processing', 'NNP'), ('Could', 'NNP'), ('Key', 'NNP'), ('Company', 'NNP'), ('’', 'NNP'), ('Digital', 'NNP'), ('Transformation', 'NNP'), ('”', 'NNP'), ('Dell', 'NNP'), ('Technologies', 'NNPS'), ('data', 'NNS'), ('scientist', 'NN'), ('Lucas', 'NNP'), ('Wilson', 'NNP'), (',', ','), ('Ph.D.', 'NNP'), ('•', 'NNP'), ('inside', 'IN'), ('look', 'NN'), ('recent', 'JJ'), ('neural', 'JJ'), ('machine', 'NN'), ('translation', 'NN'), ('project', 'NN'), ('Dell', 'NNP'), ('Technologies', 'NNP'), ('HPC', 'NNP'), ('&', 'CC'), ('AI', 'NNP'), ('Innovation', 'NNP'), ('Lab', 'NNP'), ('involved', 'VBD'), (',', ','), ('read', 'JJ'), ('white', 'JJ'), ('paper', 'NN'), ('“', 'NN'), ('Densifying', 'VBG'), ('Assumed-', 'NNP'), ('sparse', 'NN'), ('Tensors', 'NNS'), (':', ':'), ('Improving', 'VBG'), ('Memory', 'NNP'), ('Efficiency', 'NNP'), ('MPI', 'NNP'), ('Collective', 'NNP'), ('Performance', 'NNP'), ('Tensor', 'NNP'), ('Accumulation', 'NNP'), ('Parallelized', 'NNP'), ('Training', 'NNP'), ('Neural', 'NNP'), ('Machine', 'NNP'), ('Translation', 'NNP'), ('Models.', 'NNP'), ('”', 'NNP'), ('•', 'NNP'), ('explore', 'VBD'), ('new', 'JJ'), ('HPC', 'NNP'), ('solutions', 'NNS'), ('powering', 'VBG'), ('AI-driven', 'JJ'), ('applications', 'NNS'), (',', ','), ('visit', 'NN'), ('delltechnologies.com/ai', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['• broader', 'broader look', 'look NLP', 'NLP systems', 'systems ,', ', see', 'see article', 'article “', '“ Natural', 'Natural Language', 'Language Processing', 'Processing Could', 'Could Key', 'Key Company', 'Company ’', '’ Digital', 'Digital Transformation', 'Transformation ”', '” Dell', 'Dell Technologies', 'Technologies data', 'data scientist', 'scientist Lucas', 'Lucas Wilson', 'Wilson ,', ', Ph.D.', 'Ph.D. •', '• inside', 'inside look', 'look recent', 'recent neural', 'neural machine', 'machine translation', 'translation project', 'project Dell', 'Dell Technologies', 'Technologies HPC', 'HPC &', '& AI', 'AI Innovation', 'Innovation Lab', 'Lab involved', 'involved ,', ', read', 'read white', 'white paper', 'paper “', '“ Densifying', 'Densifying Assumed-', 'Assumed- sparse', 'sparse Tensors', 'Tensors :', ': Improving', 'Improving Memory', 'Memory Efficiency', 'Efficiency MPI', 'MPI Collective', 'Collective Performance', 'Performance Tensor', 'Tensor Accumulation', 'Accumulation Parallelized', 'Parallelized Training', 'Training Neural', 'Neural Machine', 'Machine Translation', 'Translation Models.', 'Models. ”', '” •', '• explore', 'explore new', 'new HPC', 'HPC solutions', 'solutions powering', 'powering AI-driven', 'AI-driven applications', 'applications ,', ', visit', 'visit delltechnologies.com/ai', 'delltechnologies.com/ai .'] 

 TOTAL BIGRAMS --> 79 



 ---- TRI-GRAMS ---- 

 ['• broader look', 'broader look NLP', 'look NLP systems', 'NLP systems ,', 'systems , see', ', see article', 'see article “', 'article “ Natural', '“ Natural Language', 'Natural Language Processing', 'Language Processing Could', 'Processing Could Key', 'Could Key Company', 'Key Company ’', 'Company ’ Digital', '’ Digital Transformation', 'Digital Transformation ”', 'Transformation ” Dell', '” Dell Technologies', 'Dell Technologies data', 'Technologies data scientist', 'data scientist Lucas', 'scientist Lucas Wilson', 'Lucas Wilson ,', 'Wilson , Ph.D.', ', Ph.D. •', 'Ph.D. • inside', '• inside look', 'inside look recent', 'look recent neural', 'recent neural machine', 'neural machine translation', 'machine translation project', 'translation project Dell', 'project Dell Technologies', 'Dell Technologies HPC', 'Technologies HPC &', 'HPC & AI', '& AI Innovation', 'AI Innovation Lab', 'Innovation Lab involved', 'Lab involved ,', 'involved , read', ', read white', 'read white paper', 'white paper “', 'paper “ Densifying', '“ Densifying Assumed-', 'Densifying Assumed- sparse', 'Assumed- sparse Tensors', 'sparse Tensors :', 'Tensors : Improving', ': Improving Memory', 'Improving Memory Efficiency', 'Memory Efficiency MPI', 'Efficiency MPI Collective', 'MPI Collective Performance', 'Collective Performance Tensor', 'Performance Tensor Accumulation', 'Tensor Accumulation Parallelized', 'Accumulation Parallelized Training', 'Parallelized Training Neural', 'Training Neural Machine', 'Neural Machine Translation', 'Machine Translation Models.', 'Translation Models. ”', 'Models. ” •', '” • explore', '• explore new', 'explore new HPC', 'new HPC solutions', 'HPC solutions powering', 'solutions powering AI-driven', 'powering AI-driven applications', 'AI-driven applications ,', 'applications , visit', ', visit delltechnologies.com/ai', 'visit delltechnologies.com/ai .'] 

 TOTAL TRIGRAMS --> 78 



 ---- NOUN PHRASES ---- 

 ['look', 'article', 'scientist', 'look', 'recent neural machine', 'translation', 'project', 'read white paper', '“', 'sparse', 'visit', 'delltechnologies.com'] 

 TOTAL NOUN PHRASES --> 12 



 ---- NER ----

 
 ORGANIZATION ---> ['NLP', 'Digital Transformation', 'Dell Technologies', 'Dell Technologies', 'AI Innovation Lab', 'HPC']
 TOTAL ORGANIZATION ENTITY --> 6 


 PERSON ---> ['Lucas Wilson', 'Memory Efficiency', 'Machine Translation']
 TOTAL PERSON ENTITY --> 3 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['•', 'broader', 'look', 'nlp', 'system', ',', 'see', 'articl', '“', 'natur', 'languag', 'process', 'could', 'key', 'compani', '’', 'digit', 'transform', '”', 'dell', 'technolog', 'data', 'scientist', 'luca', 'wilson', ',', 'ph.d.', '•', 'insid', 'look', 'recent', 'neural', 'machin', 'translat', 'project', 'dell', 'technolog', 'hpc', '&', 'ai', 'innov', 'lab', 'involv', ',', 'read', 'white', 'paper', '“', 'densifi', 'assumed-', 'spars', 'tensor', ':', 'improv', 'memori', 'effici', 'mpi', 'collect', 'perform', 'tensor', 'accumul', 'parallel', 'train', 'neural', 'machin', 'translat', 'models.', '”', '•', 'explor', 'new', 'hpc', 'solut', 'power', 'ai-driven', 'applic', ',', 'visit', 'delltechnologies.com/ai', '.']

 TOTAL PORTER STEM WORDS ==> 80



 ---- SNOWBALL STEMMING ----

['•', 'broader', 'look', 'nlp', 'system', ',', 'see', 'articl', '“', 'natur', 'languag', 'process', 'could', 'key', 'compani', '’', 'digit', 'transform', '”', 'dell', 'technolog', 'data', 'scientist', 'luca', 'wilson', ',', 'ph.d.', '•', 'insid', 'look', 'recent', 'neural', 'machin', 'translat', 'project', 'dell', 'technolog', 'hpc', '&', 'ai', 'innov', 'lab', 'involv', ',', 'read', 'white', 'paper', '“', 'densifi', 'assumed-', 'spars', 'tensor', ':', 'improv', 'memori', 'effici', 'mpi', 'collect', 'perform', 'tensor', 'accumul', 'parallel', 'train', 'neural', 'machin', 'translat', 'models.', '”', '•', 'explor', 'new', 'hpc', 'solut', 'power', 'ai-driven', 'applic', ',', 'visit', 'delltechnologies.com/ai', '.']

 TOTAL SNOWBALL STEM WORDS ==> 80



 ---- LEMMATIZATION ----

['•', 'broader', 'look', 'NLP', 'system', ',', 'see', 'article', '“', 'Natural', 'Language', 'Processing', 'Could', 'Key', 'Company', '’', 'Digital', 'Transformation', '”', 'Dell', 'Technologies', 'data', 'scientist', 'Lucas', 'Wilson', ',', 'Ph.D.', '•', 'inside', 'look', 'recent', 'neural', 'machine', 'translation', 'project', 'Dell', 'Technologies', 'HPC', '&', 'AI', 'Innovation', 'Lab', 'involved', ',', 'read', 'white', 'paper', '“', 'Densifying', 'Assumed-', 'sparse', 'Tensors', ':', 'Improving', 'Memory', 'Efficiency', 'MPI', 'Collective', 'Performance', 'Tensor', 'Accumulation', 'Parallelized', 'Training', 'Neural', 'Machine', 'Translation', 'Models.', '”', '•', 'explore', 'new', 'HPC', 'solution', 'powering', 'AI-driven', 'application', ',', 'visit', 'delltechnologies.com/ai', '.']

 TOTAL LEMMATIZE WORDS ==> 80

************************************************************************************************************************

208 --> To learn more, visit hpcatdell.com. 


 ---- TOKENS ----

 ['To', 'learn', 'more', ',', 'visit', 'hpcatdell.com', '.'] 

 TOTAL TOKENS ==> 7

 ---- POST ----

 [('To', 'TO'), ('learn', 'VB'), ('more', 'JJR'), (',', ','), ('visit', 'NN'), ('hpcatdell.com', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['learn', ',', 'visit', 'hpcatdell.com', '.']

 TOTAL FILTERED TOKENS ==>  5

 ---- POST FOR FILTERED TOKENS ----

 [('learn', 'NN'), (',', ','), ('visit', 'NN'), ('hpcatdell.com', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['learn ,', ', visit', 'visit hpcatdell.com', 'hpcatdell.com .'] 

 TOTAL BIGRAMS --> 4 



 ---- TRI-GRAMS ---- 

 ['learn , visit', ', visit hpcatdell.com', 'visit hpcatdell.com .'] 

 TOTAL TRIGRAMS --> 3 



 ---- NOUN PHRASES ---- 

 ['learn', 'visit', 'hpcatdell.com'] 

 TOTAL NOUN PHRASES --> 3 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['learn', ',', 'visit', 'hpcatdell.com', '.']

 TOTAL PORTER STEM WORDS ==> 5



 ---- SNOWBALL STEMMING ----

['learn', ',', 'visit', 'hpcatdell.com', '.']

 TOTAL SNOWBALL STEM WORDS ==> 5



 ---- LEMMATIZATION ----

['learn', ',', 'visit', 'hpcatdell.com', '.']

 TOTAL LEMMATIZE WORDS ==> 5

************************************************************************************************************************

209 --> http://delltechnologies.com/innovationlab https://www.cio.com/article/3400699/natural-language-processing-could-be-key-to-your-company-s-digital-transformation.html https://www.cio.com/article/3400699/natural-language-processing-could-be-key-to-your-company-s-digital-transformation.html https://arxiv.org/abs/1905.04035 https://arxiv.org/abs/1905.04035 https://arxiv.org/abs/1905.04035 http://delltechnologies.com/ai http://www.hpcatdell.com 	Natural language processing 	Language-to-language translation 	Computing resources 	Results 	Text-to-voice translation 	Computing resources 	Results 	Tips for your project 	Key takeaways 	To learn more 


 ---- TOKENS ----

 ['http', ':', '//delltechnologies.com/innovationlab', 'https', ':', '//www.cio.com/article/3400699/natural-language-processing-could-be-key-to-your-company-s-digital-transformation.html', 'https', ':', '//www.cio.com/article/3400699/natural-language-processing-could-be-key-to-your-company-s-digital-transformation.html', 'https', ':', '//arxiv.org/abs/1905.04035', 'https', ':', '//arxiv.org/abs/1905.04035', 'https', ':', '//arxiv.org/abs/1905.04035', 'http', ':', '//delltechnologies.com/ai', 'http', ':', '//www.hpcatdell.com', 'Natural', 'language', 'processing', 'Language-to-language', 'translation', 'Computing', 'resources', 'Results', 'Text-to-voice', 'translation', 'Computing', 'resources', 'Results', 'Tips', 'for', 'your', 'project', 'Key', 'takeaways', 'To', 'learn', 'more'] 

 TOTAL TOKENS ==> 46

 ---- POST ----

 [('http', 'NN'), (':', ':'), ('//delltechnologies.com/innovationlab', 'JJ'), ('https', 'NN'), (':', ':'), ('//www.cio.com/article/3400699/natural-language-processing-could-be-key-to-your-company-s-digital-transformation.html', 'JJ'), ('https', 'NN'), (':', ':'), ('//www.cio.com/article/3400699/natural-language-processing-could-be-key-to-your-company-s-digital-transformation.html', 'JJ'), ('https', 'NN'), (':', ':'), ('//arxiv.org/abs/1905.04035', 'JJ'), ('https', 'NN'), (':', ':'), ('//arxiv.org/abs/1905.04035', 'JJ'), ('https', 'NN'), (':', ':'), ('//arxiv.org/abs/1905.04035', 'JJ'), ('http', 'NN'), (':', ':'), ('//delltechnologies.com/ai', 'JJ'), ('http', 'NN'), (':', ':'), ('//www.hpcatdell.com', 'JJ'), ('Natural', 'NNP'), ('language', 'NN'), ('processing', 'VBG'), ('Language-to-language', 'JJ'), ('translation', 'NN'), ('Computing', 'NNP'), ('resources', 'NNS'), ('Results', 'NNP'), ('Text-to-voice', 'NNP'), ('translation', 'NN'), ('Computing', 'NNP'), ('resources', 'NNS'), ('Results', 'NNP'), ('Tips', 'NNP'), ('for', 'IN'), ('your', 'PRP$'), ('project', 'NN'), ('Key', 'NNP'), ('takeaways', 'VBZ'), ('To', 'TO'), ('learn', 'VB'), ('more', 'JJR')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['http', ':', '//delltechnologies.com/innovationlab', 'https', ':', '//www.cio.com/article/3400699/natural-language-processing-could-be-key-to-your-company-s-digital-transformation.html', 'https', ':', '//www.cio.com/article/3400699/natural-language-processing-could-be-key-to-your-company-s-digital-transformation.html', 'https', ':', '//arxiv.org/abs/1905.04035', 'https', ':', '//arxiv.org/abs/1905.04035', 'https', ':', '//arxiv.org/abs/1905.04035', 'http', ':', '//delltechnologies.com/ai', 'http', ':', '//www.hpcatdell.com', 'Natural', 'language', 'processing', 'Language-to-language', 'translation', 'Computing', 'resources', 'Results', 'Text-to-voice', 'translation', 'Computing', 'resources', 'Results', 'Tips', 'project', 'Key', 'takeaways', 'learn']

 TOTAL FILTERED TOKENS ==>  42

 ---- POST FOR FILTERED TOKENS ----

 [('http', 'NN'), (':', ':'), ('//delltechnologies.com/innovationlab', 'JJ'), ('https', 'NN'), (':', ':'), ('//www.cio.com/article/3400699/natural-language-processing-could-be-key-to-your-company-s-digital-transformation.html', 'JJ'), ('https', 'NN'), (':', ':'), ('//www.cio.com/article/3400699/natural-language-processing-could-be-key-to-your-company-s-digital-transformation.html', 'JJ'), ('https', 'NN'), (':', ':'), ('//arxiv.org/abs/1905.04035', 'JJ'), ('https', 'NN'), (':', ':'), ('//arxiv.org/abs/1905.04035', 'JJ'), ('https', 'NN'), (':', ':'), ('//arxiv.org/abs/1905.04035', 'JJ'), ('http', 'NN'), (':', ':'), ('//delltechnologies.com/ai', 'JJ'), ('http', 'NN'), (':', ':'), ('//www.hpcatdell.com', 'JJ'), ('Natural', 'NNP'), ('language', 'NN'), ('processing', 'VBG'), ('Language-to-language', 'JJ'), ('translation', 'NN'), ('Computing', 'NNP'), ('resources', 'NNS'), ('Results', 'NNP'), ('Text-to-voice', 'NNP'), ('translation', 'NN'), ('Computing', 'NNP'), ('resources', 'NNS'), ('Results', 'NNP'), ('Tips', 'NNP'), ('project', 'NN'), ('Key', 'NNP'), ('takeaways', 'VBZ'), ('learn', 'VB')] 



 ---- BI-GRAMS ---- 

 ['http :', ': //delltechnologies.com/innovationlab', '//delltechnologies.com/innovationlab https', 'https :', ': //www.cio.com/article/3400699/natural-language-processing-could-be-key-to-your-company-s-digital-transformation.html', '//www.cio.com/article/3400699/natural-language-processing-could-be-key-to-your-company-s-digital-transformation.html https', 'https :', ': //www.cio.com/article/3400699/natural-language-processing-could-be-key-to-your-company-s-digital-transformation.html', '//www.cio.com/article/3400699/natural-language-processing-could-be-key-to-your-company-s-digital-transformation.html https', 'https :', ': //arxiv.org/abs/1905.04035', '//arxiv.org/abs/1905.04035 https', 'https :', ': //arxiv.org/abs/1905.04035', '//arxiv.org/abs/1905.04035 https', 'https :', ': //arxiv.org/abs/1905.04035', '//arxiv.org/abs/1905.04035 http', 'http :', ': //delltechnologies.com/ai', '//delltechnologies.com/ai http', 'http :', ': //www.hpcatdell.com', '//www.hpcatdell.com Natural', 'Natural language', 'language processing', 'processing Language-to-language', 'Language-to-language translation', 'translation Computing', 'Computing resources', 'resources Results', 'Results Text-to-voice', 'Text-to-voice translation', 'translation Computing', 'Computing resources', 'resources Results', 'Results Tips', 'Tips project', 'project Key', 'Key takeaways', 'takeaways learn'] 

 TOTAL BIGRAMS --> 41 



 ---- TRI-GRAMS ---- 

 ['http : //delltechnologies.com/innovationlab', ': //delltechnologies.com/innovationlab https', '//delltechnologies.com/innovationlab https :', 'https : //www.cio.com/article/3400699/natural-language-processing-could-be-key-to-your-company-s-digital-transformation.html', ': //www.cio.com/article/3400699/natural-language-processing-could-be-key-to-your-company-s-digital-transformation.html https', '//www.cio.com/article/3400699/natural-language-processing-could-be-key-to-your-company-s-digital-transformation.html https :', 'https : //www.cio.com/article/3400699/natural-language-processing-could-be-key-to-your-company-s-digital-transformation.html', ': //www.cio.com/article/3400699/natural-language-processing-could-be-key-to-your-company-s-digital-transformation.html https', '//www.cio.com/article/3400699/natural-language-processing-could-be-key-to-your-company-s-digital-transformation.html https :', 'https : //arxiv.org/abs/1905.04035', ': //arxiv.org/abs/1905.04035 https', '//arxiv.org/abs/1905.04035 https :', 'https : //arxiv.org/abs/1905.04035', ': //arxiv.org/abs/1905.04035 https', '//arxiv.org/abs/1905.04035 https :', 'https : //arxiv.org/abs/1905.04035', ': //arxiv.org/abs/1905.04035 http', '//arxiv.org/abs/1905.04035 http :', 'http : //delltechnologies.com/ai', ': //delltechnologies.com/ai http', '//delltechnologies.com/ai http :', 'http : //www.hpcatdell.com', ': //www.hpcatdell.com Natural', '//www.hpcatdell.com Natural language', 'Natural language processing', 'language processing Language-to-language', 'processing Language-to-language translation', 'Language-to-language translation Computing', 'translation Computing resources', 'Computing resources Results', 'resources Results Text-to-voice', 'Results Text-to-voice translation', 'Text-to-voice translation Computing', 'translation Computing resources', 'Computing resources Results', 'resources Results Tips', 'Results Tips project', 'Tips project Key', 'project Key takeaways', 'Key takeaways learn'] 

 TOTAL TRIGRAMS --> 40 



 ---- NOUN PHRASES ---- 

 ['http', ' https', ' https', ' https', ' https', ' https', ' http', ' http', 'language', 'Language-to-language translation', 'translation', 'project'] 

 TOTAL NOUN PHRASES --> 12 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> ['Results', 'Results Tips', 'Key']
 TOTAL PERSON ENTITY --> 3 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['http', ':', '//delltechnologies.com/innovationlab', 'http', ':', '//www.cio.com/article/3400699/natural-language-processing-could-be-key-to-your-company-s-digital-transformation.html', 'http', ':', '//www.cio.com/article/3400699/natural-language-processing-could-be-key-to-your-company-s-digital-transformation.html', 'http', ':', '//arxiv.org/abs/1905.04035', 'http', ':', '//arxiv.org/abs/1905.04035', 'http', ':', '//arxiv.org/abs/1905.04035', 'http', ':', '//delltechnologies.com/ai', 'http', ':', '//www.hpcatdell.com', 'natur', 'languag', 'process', 'language-to-languag', 'translat', 'comput', 'resourc', 'result', 'text-to-voic', 'translat', 'comput', 'resourc', 'result', 'tip', 'project', 'key', 'takeaway', 'learn']

 TOTAL PORTER STEM WORDS ==> 42



 ---- SNOWBALL STEMMING ----

['http', ':', '//delltechnologies.com/innovationlab', 'https', ':', '//www.cio.com/article/3400699/natural-language-processing-could-be-key-to-your-company-s-digital-transformation.html', 'https', ':', '//www.cio.com/article/3400699/natural-language-processing-could-be-key-to-your-company-s-digital-transformation.html', 'https', ':', '//arxiv.org/abs/1905.04035', 'https', ':', '//arxiv.org/abs/1905.04035', 'https', ':', '//arxiv.org/abs/1905.04035', 'http', ':', '//delltechnologies.com/ai', 'http', ':', '//www.hpcatdell.com', 'natur', 'languag', 'process', 'language-to-languag', 'translat', 'comput', 'resourc', 'result', 'text-to-voic', 'translat', 'comput', 'resourc', 'result', 'tip', 'project', 'key', 'takeaway', 'learn']

 TOTAL SNOWBALL STEM WORDS ==> 42



 ---- LEMMATIZATION ----

['http', ':', '//delltechnologies.com/innovationlab', 'http', ':', '//www.cio.com/article/3400699/natural-language-processing-could-be-key-to-your-company-s-digital-transformation.html', 'http', ':', '//www.cio.com/article/3400699/natural-language-processing-could-be-key-to-your-company-s-digital-transformation.html', 'http', ':', '//arxiv.org/abs/1905.04035', 'http', ':', '//arxiv.org/abs/1905.04035', 'http', ':', '//arxiv.org/abs/1905.04035', 'http', ':', '//delltechnologies.com/ai', 'http', ':', '//www.hpcatdell.com', 'Natural', 'language', 'processing', 'Language-to-language', 'translation', 'Computing', 'resource', 'Results', 'Text-to-voice', 'translation', 'Computing', 'resource', 'Results', 'Tips', 'project', 'Key', 'takeaway', 'learn']

 TOTAL LEMMATIZE WORDS ==> 42

************************************************************************************************************************

