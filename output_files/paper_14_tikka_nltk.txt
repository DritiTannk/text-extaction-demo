1 --> White Paper – Sentiments Analysis  By Rajaram Jagannathan         White Paper – Sentiments Analysis  Page 2 of 5    SENTIMENTS ANALYSIS – A MIRROR TO BRAND REPUTATION OF ECOMMERCE COMPANIES: A REVIEW  Abstract—This paper provides an analysis of how Sentiment analysis algorithm helps the ecommerce companies  to understand their brand perception in the market and help them to redesign their strategies to increase their  brand reputation and revenue. 


 ---- TOKENS ----

 ['White', 'Paper', '–', 'Sentiments', 'Analysis', 'By', 'Rajaram', 'Jagannathan', 'White', 'Paper', '–', 'Sentiments', 'Analysis', 'Page', '2', 'of', '5', 'SENTIMENTS', 'ANALYSIS', '–', 'A', 'MIRROR', 'TO', 'BRAND', 'REPUTATION', 'OF', 'ECOMMERCE', 'COMPANIES', ':', 'A', 'REVIEW', 'Abstract—This', 'paper', 'provides', 'an', 'analysis', 'of', 'how', 'Sentiment', 'analysis', 'algorithm', 'helps', 'the', 'ecommerce', 'companies', 'to', 'understand', 'their', 'brand', 'perception', 'in', 'the', 'market', 'and', 'help', 'them', 'to', 'redesign', 'their', 'strategies', 'to', 'increase', 'their', 'brand', 'reputation', 'and', 'revenue', '.'] 

 TOTAL TOKENS ==> 68

 ---- POST ----

 [('White', 'NNP'), ('Paper', 'NNP'), ('–', 'NN'), ('Sentiments', 'NNS'), ('Analysis', 'NN'), ('By', 'IN'), ('Rajaram', 'NNP'), ('Jagannathan', 'NNP'), ('White', 'NNP'), ('Paper', 'NNP'), ('–', 'NN'), ('Sentiments', 'NNP'), ('Analysis', 'NNP'), ('Page', 'NNP'), ('2', 'CD'), ('of', 'IN'), ('5', 'CD'), ('SENTIMENTS', 'NNP'), ('ANALYSIS', 'NNP'), ('–', 'VBD'), ('A', 'DT'), ('MIRROR', 'NNP'), ('TO', 'NNP'), ('BRAND', 'NNP'), ('REPUTATION', 'NNP'), ('OF', 'NNP'), ('ECOMMERCE', 'NNP'), ('COMPANIES', 'NNP'), (':', ':'), ('A', 'DT'), ('REVIEW', 'NNP'), ('Abstract—This', 'NNP'), ('paper', 'NN'), ('provides', 'VBZ'), ('an', 'DT'), ('analysis', 'NN'), ('of', 'IN'), ('how', 'WRB'), ('Sentiment', 'JJ'), ('analysis', 'NN'), ('algorithm', 'NN'), ('helps', 'VBZ'), ('the', 'DT'), ('ecommerce', 'NN'), ('companies', 'NNS'), ('to', 'TO'), ('understand', 'VB'), ('their', 'PRP$'), ('brand', 'NN'), ('perception', 'NN'), ('in', 'IN'), ('the', 'DT'), ('market', 'NN'), ('and', 'CC'), ('help', 'VB'), ('them', 'PRP'), ('to', 'TO'), ('redesign', 'VB'), ('their', 'PRP$'), ('strategies', 'NNS'), ('to', 'TO'), ('increase', 'VB'), ('their', 'PRP$'), ('brand', 'NN'), ('reputation', 'NN'), ('and', 'CC'), ('revenue', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['White', 'Paper', '–', 'Sentiments', 'Analysis', 'Rajaram', 'Jagannathan', 'White', 'Paper', '–', 'Sentiments', 'Analysis', 'Page', '2', '5', 'SENTIMENTS', 'ANALYSIS', '–', 'MIRROR', 'BRAND', 'REPUTATION', 'ECOMMERCE', 'COMPANIES', ':', 'REVIEW', 'Abstract—This', 'paper', 'provides', 'analysis', 'Sentiment', 'analysis', 'algorithm', 'helps', 'ecommerce', 'companies', 'understand', 'brand', 'perception', 'market', 'help', 'redesign', 'strategies', 'increase', 'brand', 'reputation', 'revenue', '.']

 TOTAL FILTERED TOKENS ==>  47

 ---- POST FOR FILTERED TOKENS ----

 [('White', 'NNP'), ('Paper', 'NNP'), ('–', 'NN'), ('Sentiments', 'NNP'), ('Analysis', 'NNP'), ('Rajaram', 'NNP'), ('Jagannathan', 'NNP'), ('White', 'NNP'), ('Paper', 'NNP'), ('–', 'NN'), ('Sentiments', 'NNP'), ('Analysis', 'NNP'), ('Page', 'NNP'), ('2', 'CD'), ('5', 'CD'), ('SENTIMENTS', 'NNP'), ('ANALYSIS', 'NNP'), ('–', 'NNP'), ('MIRROR', 'NNP'), ('BRAND', 'NNP'), ('REPUTATION', 'NNP'), ('ECOMMERCE', 'NNP'), ('COMPANIES', 'NNP'), (':', ':'), ('REVIEW', 'NNP'), ('Abstract—This', 'NNP'), ('paper', 'NN'), ('provides', 'VBZ'), ('analysis', 'NN'), ('Sentiment', 'NNP'), ('analysis', 'NN'), ('algorithm', 'NN'), ('helps', 'VBZ'), ('ecommerce', 'VB'), ('companies', 'NNS'), ('understand', 'VBP'), ('brand', 'NN'), ('perception', 'NN'), ('market', 'NN'), ('help', 'VBP'), ('redesign', 'VB'), ('strategies', 'NNS'), ('increase', 'VB'), ('brand', 'NN'), ('reputation', 'NN'), ('revenue', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['White Paper', 'Paper –', '– Sentiments', 'Sentiments Analysis', 'Analysis Rajaram', 'Rajaram Jagannathan', 'Jagannathan White', 'White Paper', 'Paper –', '– Sentiments', 'Sentiments Analysis', 'Analysis Page', 'Page 2', '2 5', '5 SENTIMENTS', 'SENTIMENTS ANALYSIS', 'ANALYSIS –', '– MIRROR', 'MIRROR BRAND', 'BRAND REPUTATION', 'REPUTATION ECOMMERCE', 'ECOMMERCE COMPANIES', 'COMPANIES :', ': REVIEW', 'REVIEW Abstract—This', 'Abstract—This paper', 'paper provides', 'provides analysis', 'analysis Sentiment', 'Sentiment analysis', 'analysis algorithm', 'algorithm helps', 'helps ecommerce', 'ecommerce companies', 'companies understand', 'understand brand', 'brand perception', 'perception market', 'market help', 'help redesign', 'redesign strategies', 'strategies increase', 'increase brand', 'brand reputation', 'reputation revenue', 'revenue .'] 

 TOTAL BIGRAMS --> 46 



 ---- TRI-GRAMS ---- 

 ['White Paper –', 'Paper – Sentiments', '– Sentiments Analysis', 'Sentiments Analysis Rajaram', 'Analysis Rajaram Jagannathan', 'Rajaram Jagannathan White', 'Jagannathan White Paper', 'White Paper –', 'Paper – Sentiments', '– Sentiments Analysis', 'Sentiments Analysis Page', 'Analysis Page 2', 'Page 2 5', '2 5 SENTIMENTS', '5 SENTIMENTS ANALYSIS', 'SENTIMENTS ANALYSIS –', 'ANALYSIS – MIRROR', '– MIRROR BRAND', 'MIRROR BRAND REPUTATION', 'BRAND REPUTATION ECOMMERCE', 'REPUTATION ECOMMERCE COMPANIES', 'ECOMMERCE COMPANIES :', 'COMPANIES : REVIEW', ': REVIEW Abstract—This', 'REVIEW Abstract—This paper', 'Abstract—This paper provides', 'paper provides analysis', 'provides analysis Sentiment', 'analysis Sentiment analysis', 'Sentiment analysis algorithm', 'analysis algorithm helps', 'algorithm helps ecommerce', 'helps ecommerce companies', 'ecommerce companies understand', 'companies understand brand', 'understand brand perception', 'brand perception market', 'perception market help', 'market help redesign', 'help redesign strategies', 'redesign strategies increase', 'strategies increase brand', 'increase brand reputation', 'brand reputation revenue', 'reputation revenue .'] 

 TOTAL TRIGRAMS --> 45 



 ---- NOUN PHRASES ---- 

 ['–', '–', 'paper', 'analysis', 'analysis', 'algorithm', 'brand', 'perception', 'market', 'brand', 'reputation', 'revenue'] 

 TOTAL NOUN PHRASES --> 12 



 ---- NER ----

 
 ORGANIZATION ---> ['Paper', 'Sentiments Analysis Rajaram Jagannathan', 'Sentiments Analysis', 'SENTIMENTS', 'BRAND', 'REVIEW']
 TOTAL ORGANIZATION ENTITY --> 6 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['white', 'paper', '–', 'sentiment', 'analysi', 'rajaram', 'jagannathan', 'white', 'paper', '–', 'sentiment', 'analysi', 'page', '2', '5', 'sentiment', 'analysi', '–', 'mirror', 'brand', 'reput', 'ecommerc', 'compani', ':', 'review', 'abstract—thi', 'paper', 'provid', 'analysi', 'sentiment', 'analysi', 'algorithm', 'help', 'ecommerc', 'compani', 'understand', 'brand', 'percept', 'market', 'help', 'redesign', 'strategi', 'increas', 'brand', 'reput', 'revenu', '.']

 TOTAL PORTER STEM WORDS ==> 47



 ---- SNOWBALL STEMMING ----

['white', 'paper', '–', 'sentiment', 'analysi', 'rajaram', 'jagannathan', 'white', 'paper', '–', 'sentiment', 'analysi', 'page', '2', '5', 'sentiment', 'analysi', '–', 'mirror', 'brand', 'reput', 'ecommerc', 'compani', ':', 'review', 'abstract—thi', 'paper', 'provid', 'analysi', 'sentiment', 'analysi', 'algorithm', 'help', 'ecommerc', 'compani', 'understand', 'brand', 'percept', 'market', 'help', 'redesign', 'strategi', 'increas', 'brand', 'reput', 'revenu', '.']

 TOTAL SNOWBALL STEM WORDS ==> 47



 ---- LEMMATIZATION ----

['White', 'Paper', '–', 'Sentiments', 'Analysis', 'Rajaram', 'Jagannathan', 'White', 'Paper', '–', 'Sentiments', 'Analysis', 'Page', '2', '5', 'SENTIMENTS', 'ANALYSIS', '–', 'MIRROR', 'BRAND', 'REPUTATION', 'ECOMMERCE', 'COMPANIES', ':', 'REVIEW', 'Abstract—This', 'paper', 'provides', 'analysis', 'Sentiment', 'analysis', 'algorithm', 'help', 'ecommerce', 'company', 'understand', 'brand', 'perception', 'market', 'help', 'redesign', 'strategy', 'increase', 'brand', 'reputation', 'revenue', '.']

 TOTAL LEMMATIZE WORDS ==> 47

************************************************************************************************************************

2 --> Index Terms—  Sentiment analysis, Machine Learning, Artificial Analysis, Natural Language Processing, NLP. 


 ---- TOKENS ----

 ['Index', 'Terms—', 'Sentiment', 'analysis', ',', 'Machine', 'Learning', ',', 'Artificial', 'Analysis', ',', 'Natural', 'Language', 'Processing', ',', 'NLP', '.'] 

 TOTAL TOKENS ==> 17

 ---- POST ----

 [('Index', 'NN'), ('Terms—', 'NNP'), ('Sentiment', 'NNP'), ('analysis', 'NN'), (',', ','), ('Machine', 'NNP'), ('Learning', 'NNP'), (',', ','), ('Artificial', 'NNP'), ('Analysis', 'NNP'), (',', ','), ('Natural', 'NNP'), ('Language', 'NNP'), ('Processing', 'NNP'), (',', ','), ('NLP', 'NNP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Index', 'Terms—', 'Sentiment', 'analysis', ',', 'Machine', 'Learning', ',', 'Artificial', 'Analysis', ',', 'Natural', 'Language', 'Processing', ',', 'NLP', '.']

 TOTAL FILTERED TOKENS ==>  17

 ---- POST FOR FILTERED TOKENS ----

 [('Index', 'NN'), ('Terms—', 'NNP'), ('Sentiment', 'NNP'), ('analysis', 'NN'), (',', ','), ('Machine', 'NNP'), ('Learning', 'NNP'), (',', ','), ('Artificial', 'NNP'), ('Analysis', 'NNP'), (',', ','), ('Natural', 'NNP'), ('Language', 'NNP'), ('Processing', 'NNP'), (',', ','), ('NLP', 'NNP'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Index Terms—', 'Terms— Sentiment', 'Sentiment analysis', 'analysis ,', ', Machine', 'Machine Learning', 'Learning ,', ', Artificial', 'Artificial Analysis', 'Analysis ,', ', Natural', 'Natural Language', 'Language Processing', 'Processing ,', ', NLP', 'NLP .'] 

 TOTAL BIGRAMS --> 16 



 ---- TRI-GRAMS ---- 

 ['Index Terms— Sentiment', 'Terms— Sentiment analysis', 'Sentiment analysis ,', 'analysis , Machine', ', Machine Learning', 'Machine Learning ,', 'Learning , Artificial', ', Artificial Analysis', 'Artificial Analysis ,', 'Analysis , Natural', ', Natural Language', 'Natural Language Processing', 'Language Processing ,', 'Processing , NLP', ', NLP .'] 

 TOTAL TRIGRAMS --> 15 



 ---- NOUN PHRASES ---- 

 ['Index', 'analysis'] 

 TOTAL NOUN PHRASES --> 2 



 ---- NER ----

 
 ORGANIZATION ---> ['Natural Language Processing', 'NLP']
 TOTAL ORGANIZATION ENTITY --> 2 


 PERSON ---> ['Machine Learning', 'Artificial Analysis']
 TOTAL PERSON ENTITY --> 2 


 GPE ---> ['Index']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['index', 'terms—', 'sentiment', 'analysi', ',', 'machin', 'learn', ',', 'artifici', 'analysi', ',', 'natur', 'languag', 'process', ',', 'nlp', '.']

 TOTAL PORTER STEM WORDS ==> 17



 ---- SNOWBALL STEMMING ----

['index', 'terms—', 'sentiment', 'analysi', ',', 'machin', 'learn', ',', 'artifici', 'analysi', ',', 'natur', 'languag', 'process', ',', 'nlp', '.']

 TOTAL SNOWBALL STEM WORDS ==> 17



 ---- LEMMATIZATION ----

['Index', 'Terms—', 'Sentiment', 'analysis', ',', 'Machine', 'Learning', ',', 'Artificial', 'Analysis', ',', 'Natural', 'Language', 'Processing', ',', 'NLP', '.']

 TOTAL LEMMATIZE WORDS ==> 17

************************************************************************************************************************

3 --> I. 


 ---- TOKENS ----

 ['I', '.'] 

 TOTAL TOKENS ==> 2

 ---- POST ----

 [('I', 'PRP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['.']

 TOTAL FILTERED TOKENS ==>  1

 ---- POST FOR FILTERED TOKENS ----

 [('.', '.')] 



 ---- BI-GRAMS ---- 

 [] 

 TOTAL BIGRAMS --> 0 



 ---- TRI-GRAMS ---- 

 [] 

 TOTAL TRIGRAMS --> 0 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['.']

 TOTAL PORTER STEM WORDS ==> 1



 ---- SNOWBALL STEMMING ----

['.']

 TOTAL SNOWBALL STEM WORDS ==> 1



 ---- LEMMATIZATION ----

['.']

 TOTAL LEMMATIZE WORDS ==> 1

************************************************************************************************************************

4 --> INTRODUCTION  The analysis and quick reaction to the customer’s opinion about the company’s brand / product / services is  inevitable for the success of the business. 


 ---- TOKENS ----

 ['INTRODUCTION', 'The', 'analysis', 'and', 'quick', 'reaction', 'to', 'the', 'customer', '’', 's', 'opinion', 'about', 'the', 'company', '’', 's', 'brand', '/', 'product', '/', 'services', 'is', 'inevitable', 'for', 'the', 'success', 'of', 'the', 'business', '.'] 

 TOTAL TOKENS ==> 31

 ---- POST ----

 [('INTRODUCTION', 'NNP'), ('The', 'DT'), ('analysis', 'NN'), ('and', 'CC'), ('quick', 'JJ'), ('reaction', 'NN'), ('to', 'TO'), ('the', 'DT'), ('customer', 'NN'), ('’', 'NNP'), ('s', 'JJ'), ('opinion', 'NN'), ('about', 'IN'), ('the', 'DT'), ('company', 'NN'), ('’', 'NNP'), ('s', 'NN'), ('brand', 'NN'), ('/', 'NNP'), ('product', 'NN'), ('/', 'NN'), ('services', 'NNS'), ('is', 'VBZ'), ('inevitable', 'JJ'), ('for', 'IN'), ('the', 'DT'), ('success', 'NN'), ('of', 'IN'), ('the', 'DT'), ('business', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['INTRODUCTION', 'analysis', 'quick', 'reaction', 'customer', '’', 'opinion', 'company', '’', 'brand', '/', 'product', '/', 'services', 'inevitable', 'success', 'business', '.']

 TOTAL FILTERED TOKENS ==>  18

 ---- POST FOR FILTERED TOKENS ----

 [('INTRODUCTION', 'NNP'), ('analysis', 'NN'), ('quick', 'JJ'), ('reaction', 'NN'), ('customer', 'NN'), ('’', 'NNP'), ('opinion', 'NN'), ('company', 'NN'), ('’', 'NNP'), ('brand', 'NN'), ('/', 'NNP'), ('product', 'NN'), ('/', 'NNP'), ('services', 'NNS'), ('inevitable', 'JJ'), ('success', 'NN'), ('business', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['INTRODUCTION analysis', 'analysis quick', 'quick reaction', 'reaction customer', 'customer ’', '’ opinion', 'opinion company', 'company ’', '’ brand', 'brand /', '/ product', 'product /', '/ services', 'services inevitable', 'inevitable success', 'success business', 'business .'] 

 TOTAL BIGRAMS --> 17 



 ---- TRI-GRAMS ---- 

 ['INTRODUCTION analysis quick', 'analysis quick reaction', 'quick reaction customer', 'reaction customer ’', 'customer ’ opinion', '’ opinion company', 'opinion company ’', 'company ’ brand', '’ brand /', 'brand / product', '/ product /', 'product / services', '/ services inevitable', 'services inevitable success', 'inevitable success business', 'success business .'] 

 TOTAL TRIGRAMS --> 16 



 ---- NOUN PHRASES ---- 

 ['analysis', 'quick reaction', 'customer', 'opinion', 'company', 'brand', 'product', 'inevitable success', 'business'] 

 TOTAL NOUN PHRASES --> 9 



 ---- NER ----

 
 ORGANIZATION ---> ['INTRODUCTION']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['introduct', 'analysi', 'quick', 'reaction', 'custom', '’', 'opinion', 'compani', '’', 'brand', '/', 'product', '/', 'servic', 'inevit', 'success', 'busi', '.']

 TOTAL PORTER STEM WORDS ==> 18



 ---- SNOWBALL STEMMING ----

['introduct', 'analysi', 'quick', 'reaction', 'custom', '’', 'opinion', 'compani', '’', 'brand', '/', 'product', '/', 'servic', 'inevit', 'success', 'busi', '.']

 TOTAL SNOWBALL STEM WORDS ==> 18



 ---- LEMMATIZATION ----

['INTRODUCTION', 'analysis', 'quick', 'reaction', 'customer', '’', 'opinion', 'company', '’', 'brand', '/', 'product', '/', 'service', 'inevitable', 'success', 'business', '.']

 TOTAL LEMMATIZE WORDS ==> 18

************************************************************************************************************************

5 --> There is a growing trend that the companies started to focus on the  tone of the customer along with the positive or negative comments about the products /services they availed  from the company as ecommerce companies loses significant portion of their sales as they could not capitalize  valuable business intelligence available in the social media posts. 


 ---- TOKENS ----

 ['There', 'is', 'a', 'growing', 'trend', 'that', 'the', 'companies', 'started', 'to', 'focus', 'on', 'the', 'tone', 'of', 'the', 'customer', 'along', 'with', 'the', 'positive', 'or', 'negative', 'comments', 'about', 'the', 'products', '/services', 'they', 'availed', 'from', 'the', 'company', 'as', 'ecommerce', 'companies', 'loses', 'significant', 'portion', 'of', 'their', 'sales', 'as', 'they', 'could', 'not', 'capitalize', 'valuable', 'business', 'intelligence', 'available', 'in', 'the', 'social', 'media', 'posts', '.'] 

 TOTAL TOKENS ==> 57

 ---- POST ----

 [('There', 'EX'), ('is', 'VBZ'), ('a', 'DT'), ('growing', 'VBG'), ('trend', 'NN'), ('that', 'IN'), ('the', 'DT'), ('companies', 'NNS'), ('started', 'VBD'), ('to', 'TO'), ('focus', 'VB'), ('on', 'IN'), ('the', 'DT'), ('tone', 'NN'), ('of', 'IN'), ('the', 'DT'), ('customer', 'NN'), ('along', 'IN'), ('with', 'IN'), ('the', 'DT'), ('positive', 'JJ'), ('or', 'CC'), ('negative', 'JJ'), ('comments', 'NNS'), ('about', 'IN'), ('the', 'DT'), ('products', 'NNS'), ('/services', 'VBZ'), ('they', 'PRP'), ('availed', 'VBD'), ('from', 'IN'), ('the', 'DT'), ('company', 'NN'), ('as', 'IN'), ('ecommerce', 'NN'), ('companies', 'NNS'), ('loses', 'VBZ'), ('significant', 'JJ'), ('portion', 'NN'), ('of', 'IN'), ('their', 'PRP$'), ('sales', 'NNS'), ('as', 'IN'), ('they', 'PRP'), ('could', 'MD'), ('not', 'RB'), ('capitalize', 'VB'), ('valuable', 'JJ'), ('business', 'NN'), ('intelligence', 'NN'), ('available', 'JJ'), ('in', 'IN'), ('the', 'DT'), ('social', 'JJ'), ('media', 'NNS'), ('posts', 'NNS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['growing', 'trend', 'companies', 'started', 'focus', 'tone', 'customer', 'along', 'positive', 'negative', 'comments', 'products', '/services', 'availed', 'company', 'ecommerce', 'companies', 'loses', 'significant', 'portion', 'sales', 'could', 'capitalize', 'valuable', 'business', 'intelligence', 'available', 'social', 'media', 'posts', '.']

 TOTAL FILTERED TOKENS ==>  31

 ---- POST FOR FILTERED TOKENS ----

 [('growing', 'VBG'), ('trend', 'NN'), ('companies', 'NNS'), ('started', 'VBD'), ('focus', 'RB'), ('tone', 'JJ'), ('customer', 'NN'), ('along', 'IN'), ('positive', 'JJ'), ('negative', 'JJ'), ('comments', 'NNS'), ('products', 'NNS'), ('/services', 'NNS'), ('availed', 'VBD'), ('company', 'NN'), ('ecommerce', 'NN'), ('companies', 'NNS'), ('loses', 'VBZ'), ('significant', 'JJ'), ('portion', 'NN'), ('sales', 'NNS'), ('could', 'MD'), ('capitalize', 'VB'), ('valuable', 'JJ'), ('business', 'NN'), ('intelligence', 'NN'), ('available', 'JJ'), ('social', 'JJ'), ('media', 'NNS'), ('posts', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['growing trend', 'trend companies', 'companies started', 'started focus', 'focus tone', 'tone customer', 'customer along', 'along positive', 'positive negative', 'negative comments', 'comments products', 'products /services', '/services availed', 'availed company', 'company ecommerce', 'ecommerce companies', 'companies loses', 'loses significant', 'significant portion', 'portion sales', 'sales could', 'could capitalize', 'capitalize valuable', 'valuable business', 'business intelligence', 'intelligence available', 'available social', 'social media', 'media posts', 'posts .'] 

 TOTAL BIGRAMS --> 30 



 ---- TRI-GRAMS ---- 

 ['growing trend companies', 'trend companies started', 'companies started focus', 'started focus tone', 'focus tone customer', 'tone customer along', 'customer along positive', 'along positive negative', 'positive negative comments', 'negative comments products', 'comments products /services', 'products /services availed', '/services availed company', 'availed company ecommerce', 'company ecommerce companies', 'ecommerce companies loses', 'companies loses significant', 'loses significant portion', 'significant portion sales', 'portion sales could', 'sales could capitalize', 'could capitalize valuable', 'capitalize valuable business', 'valuable business intelligence', 'business intelligence available', 'intelligence available social', 'available social media', 'social media posts', 'media posts .'] 

 TOTAL TRIGRAMS --> 29 



 ---- NOUN PHRASES ---- 

 ['trend', 'tone customer', 'company', 'ecommerce', 'significant portion', 'valuable business', 'intelligence'] 

 TOTAL NOUN PHRASES --> 7 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['grow', 'trend', 'compani', 'start', 'focu', 'tone', 'custom', 'along', 'posit', 'neg', 'comment', 'product', '/servic', 'avail', 'compani', 'ecommerc', 'compani', 'lose', 'signific', 'portion', 'sale', 'could', 'capit', 'valuabl', 'busi', 'intellig', 'avail', 'social', 'media', 'post', '.']

 TOTAL PORTER STEM WORDS ==> 31



 ---- SNOWBALL STEMMING ----

['grow', 'trend', 'compani', 'start', 'focus', 'tone', 'custom', 'along', 'posit', 'negat', 'comment', 'product', '/servic', 'avail', 'compani', 'ecommerc', 'compani', 'lose', 'signific', 'portion', 'sale', 'could', 'capit', 'valuabl', 'busi', 'intellig', 'avail', 'social', 'media', 'post', '.']

 TOTAL SNOWBALL STEM WORDS ==> 31



 ---- LEMMATIZATION ----

['growing', 'trend', 'company', 'started', 'focus', 'tone', 'customer', 'along', 'positive', 'negative', 'comment', 'product', '/services', 'availed', 'company', 'ecommerce', 'company', 'loses', 'significant', 'portion', 'sale', 'could', 'capitalize', 'valuable', 'business', 'intelligence', 'available', 'social', 'medium', 'post', '.']

 TOTAL LEMMATIZE WORDS ==> 31

************************************************************************************************************************

6 --> II. 


 ---- TOKENS ----

 ['II', '.'] 

 TOTAL TOKENS ==> 2

 ---- POST ----

 [('II', 'NNP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['II', '.']

 TOTAL FILTERED TOKENS ==>  2

 ---- POST FOR FILTERED TOKENS ----

 [('II', 'NNP'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['II .'] 

 TOTAL BIGRAMS --> 1 



 ---- TRI-GRAMS ---- 

 [] 

 TOTAL TRIGRAMS --> 0 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['ii', '.']

 TOTAL PORTER STEM WORDS ==> 2



 ---- SNOWBALL STEMMING ----

['ii', '.']

 TOTAL SNOWBALL STEM WORDS ==> 2



 ---- LEMMATIZATION ----

['II', '.']

 TOTAL LEMMATIZE WORDS ==> 2

************************************************************************************************************************

7 --> BUSINESS PROBLEMS  The ecommerce companies would like to understand about their brand reputation, the positive and negative  sentiments and problems faced by their users, product attributes which creates negative impact to the sales, the  user needs and understand the trending topics for the content marketing. 


 ---- TOKENS ----

 ['BUSINESS', 'PROBLEMS', 'The', 'ecommerce', 'companies', 'would', 'like', 'to', 'understand', 'about', 'their', 'brand', 'reputation', ',', 'the', 'positive', 'and', 'negative', 'sentiments', 'and', 'problems', 'faced', 'by', 'their', 'users', ',', 'product', 'attributes', 'which', 'creates', 'negative', 'impact', 'to', 'the', 'sales', ',', 'the', 'user', 'needs', 'and', 'understand', 'the', 'trending', 'topics', 'for', 'the', 'content', 'marketing', '.'] 

 TOTAL TOKENS ==> 49

 ---- POST ----

 [('BUSINESS', 'NN'), ('PROBLEMS', 'VBD'), ('The', 'DT'), ('ecommerce', 'NN'), ('companies', 'NNS'), ('would', 'MD'), ('like', 'VB'), ('to', 'TO'), ('understand', 'VB'), ('about', 'IN'), ('their', 'PRP$'), ('brand', 'NN'), ('reputation', 'NN'), (',', ','), ('the', 'DT'), ('positive', 'JJ'), ('and', 'CC'), ('negative', 'JJ'), ('sentiments', 'NNS'), ('and', 'CC'), ('problems', 'NNS'), ('faced', 'VBN'), ('by', 'IN'), ('their', 'PRP$'), ('users', 'NNS'), (',', ','), ('product', 'NN'), ('attributes', 'NNS'), ('which', 'WDT'), ('creates', 'VBZ'), ('negative', 'JJ'), ('impact', 'NN'), ('to', 'TO'), ('the', 'DT'), ('sales', 'NNS'), (',', ','), ('the', 'DT'), ('user', 'NN'), ('needs', 'NNS'), ('and', 'CC'), ('understand', 'VB'), ('the', 'DT'), ('trending', 'NN'), ('topics', 'NNS'), ('for', 'IN'), ('the', 'DT'), ('content', 'NN'), ('marketing', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['BUSINESS', 'PROBLEMS', 'ecommerce', 'companies', 'would', 'like', 'understand', 'brand', 'reputation', ',', 'positive', 'negative', 'sentiments', 'problems', 'faced', 'users', ',', 'product', 'attributes', 'creates', 'negative', 'impact', 'sales', ',', 'user', 'needs', 'understand', 'trending', 'topics', 'content', 'marketing', '.']

 TOTAL FILTERED TOKENS ==>  32

 ---- POST FOR FILTERED TOKENS ----

 [('BUSINESS', 'NN'), ('PROBLEMS', 'NNP'), ('ecommerce', 'NN'), ('companies', 'NNS'), ('would', 'MD'), ('like', 'VB'), ('understand', 'JJ'), ('brand', 'NN'), ('reputation', 'NN'), (',', ','), ('positive', 'JJ'), ('negative', 'JJ'), ('sentiments', 'NNS'), ('problems', 'NNS'), ('faced', 'VBD'), ('users', 'NNS'), (',', ','), ('product', 'NN'), ('attributes', 'VBZ'), ('creates', 'NNS'), ('negative', 'JJ'), ('impact', 'NN'), ('sales', 'NNS'), (',', ','), ('user', 'JJ'), ('needs', 'NNS'), ('understand', 'VBP'), ('trending', 'VBG'), ('topics', 'NNS'), ('content', 'JJ'), ('marketing', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['BUSINESS PROBLEMS', 'PROBLEMS ecommerce', 'ecommerce companies', 'companies would', 'would like', 'like understand', 'understand brand', 'brand reputation', 'reputation ,', ', positive', 'positive negative', 'negative sentiments', 'sentiments problems', 'problems faced', 'faced users', 'users ,', ', product', 'product attributes', 'attributes creates', 'creates negative', 'negative impact', 'impact sales', 'sales ,', ', user', 'user needs', 'needs understand', 'understand trending', 'trending topics', 'topics content', 'content marketing', 'marketing .'] 

 TOTAL BIGRAMS --> 31 



 ---- TRI-GRAMS ---- 

 ['BUSINESS PROBLEMS ecommerce', 'PROBLEMS ecommerce companies', 'ecommerce companies would', 'companies would like', 'would like understand', 'like understand brand', 'understand brand reputation', 'brand reputation ,', 'reputation , positive', ', positive negative', 'positive negative sentiments', 'negative sentiments problems', 'sentiments problems faced', 'problems faced users', 'faced users ,', 'users , product', ', product attributes', 'product attributes creates', 'attributes creates negative', 'creates negative impact', 'negative impact sales', 'impact sales ,', 'sales , user', ', user needs', 'user needs understand', 'needs understand trending', 'understand trending topics', 'trending topics content', 'topics content marketing', 'content marketing .'] 

 TOTAL TRIGRAMS --> 30 



 ---- NOUN PHRASES ---- 

 ['BUSINESS', 'ecommerce', 'understand brand', 'reputation', 'product', 'negative impact', 'content marketing'] 

 TOTAL NOUN PHRASES --> 7 



 ---- NER ----

 
 ORGANIZATION ---> ['BUSINESS', 'PROBLEMS']
 TOTAL ORGANIZATION ENTITY --> 2 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['busi', 'problem', 'ecommerc', 'compani', 'would', 'like', 'understand', 'brand', 'reput', ',', 'posit', 'neg', 'sentiment', 'problem', 'face', 'user', ',', 'product', 'attribut', 'creat', 'neg', 'impact', 'sale', ',', 'user', 'need', 'understand', 'trend', 'topic', 'content', 'market', '.']

 TOTAL PORTER STEM WORDS ==> 32



 ---- SNOWBALL STEMMING ----

['busi', 'problem', 'ecommerc', 'compani', 'would', 'like', 'understand', 'brand', 'reput', ',', 'posit', 'negat', 'sentiment', 'problem', 'face', 'user', ',', 'product', 'attribut', 'creat', 'negat', 'impact', 'sale', ',', 'user', 'need', 'understand', 'trend', 'topic', 'content', 'market', '.']

 TOTAL SNOWBALL STEM WORDS ==> 32



 ---- LEMMATIZATION ----

['BUSINESS', 'PROBLEMS', 'ecommerce', 'company', 'would', 'like', 'understand', 'brand', 'reputation', ',', 'positive', 'negative', 'sentiment', 'problem', 'faced', 'user', ',', 'product', 'attribute', 'creates', 'negative', 'impact', 'sale', ',', 'user', 'need', 'understand', 'trending', 'topic', 'content', 'marketing', '.']

 TOTAL LEMMATIZE WORDS ==> 32

************************************************************************************************************************

8 --> III. 


 ---- TOKENS ----

 ['III', '.'] 

 TOTAL TOKENS ==> 2

 ---- POST ----

 [('III', 'NNP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['III', '.']

 TOTAL FILTERED TOKENS ==>  2

 ---- POST FOR FILTERED TOKENS ----

 [('III', 'NNP'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['III .'] 

 TOTAL BIGRAMS --> 1 



 ---- TRI-GRAMS ---- 

 [] 

 TOTAL TRIGRAMS --> 0 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['iii', '.']

 TOTAL PORTER STEM WORDS ==> 2



 ---- SNOWBALL STEMMING ----

['iii', '.']

 TOTAL SNOWBALL STEM WORDS ==> 2



 ---- LEMMATIZATION ----

['III', '.']

 TOTAL LEMMATIZE WORDS ==> 2

************************************************************************************************************************

9 --> HOW THE SENTIMENT ANALYSIS HELPS THE BUSINESS? 


 ---- TOKENS ----

 ['HOW', 'THE', 'SENTIMENT', 'ANALYSIS', 'HELPS', 'THE', 'BUSINESS', '?'] 

 TOTAL TOKENS ==> 8

 ---- POST ----

 [('HOW', 'WRB'), ('THE', 'DT'), ('SENTIMENT', 'NNP'), ('ANALYSIS', 'NNP'), ('HELPS', 'NNP'), ('THE', 'NNP'), ('BUSINESS', 'NNP'), ('?', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['SENTIMENT', 'ANALYSIS', 'HELPS', 'BUSINESS', '?']

 TOTAL FILTERED TOKENS ==>  5

 ---- POST FOR FILTERED TOKENS ----

 [('SENTIMENT', 'NNP'), ('ANALYSIS', 'NNP'), ('HELPS', 'NNP'), ('BUSINESS', 'NNP'), ('?', '.')] 



 ---- BI-GRAMS ---- 

 ['SENTIMENT ANALYSIS', 'ANALYSIS HELPS', 'HELPS BUSINESS', 'BUSINESS ?'] 

 TOTAL BIGRAMS --> 4 



 ---- TRI-GRAMS ---- 

 ['SENTIMENT ANALYSIS HELPS', 'ANALYSIS HELPS BUSINESS', 'HELPS BUSINESS ?'] 

 TOTAL TRIGRAMS --> 3 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> ['SENTIMENT', 'ANALYSIS']
 TOTAL ORGANIZATION ENTITY --> 2 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['sentiment', 'analysi', 'help', 'busi', '?']

 TOTAL PORTER STEM WORDS ==> 5



 ---- SNOWBALL STEMMING ----

['sentiment', 'analysi', 'help', 'busi', '?']

 TOTAL SNOWBALL STEM WORDS ==> 5



 ---- LEMMATIZATION ----

['SENTIMENT', 'ANALYSIS', 'HELPS', 'BUSINESS', '?']

 TOTAL LEMMATIZE WORDS ==> 5

************************************************************************************************************************

10 --> A. 


 ---- TOKENS ----

 ['A', '.'] 

 TOTAL TOKENS ==> 2

 ---- POST ----

 [('A', 'DT'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['.']

 TOTAL FILTERED TOKENS ==>  1

 ---- POST FOR FILTERED TOKENS ----

 [('.', '.')] 



 ---- BI-GRAMS ---- 

 [] 

 TOTAL BIGRAMS --> 0 



 ---- TRI-GRAMS ---- 

 [] 

 TOTAL TRIGRAMS --> 0 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['.']

 TOTAL PORTER STEM WORDS ==> 1



 ---- SNOWBALL STEMMING ----

['.']

 TOTAL SNOWBALL STEM WORDS ==> 1



 ---- LEMMATIZATION ----

['.']

 TOTAL LEMMATIZE WORDS ==> 1

************************************************************************************************************************

11 --> Improve customer experience   Sentiment analysis helps to detect the customer’s sentiment on the products / services in their social media  postings and classify them as positive or negative. 


 ---- TOKENS ----

 ['Improve', 'customer', 'experience', 'Sentiment', 'analysis', 'helps', 'to', 'detect', 'the', 'customer', '’', 's', 'sentiment', 'on', 'the', 'products', '/', 'services', 'in', 'their', 'social', 'media', 'postings', 'and', 'classify', 'them', 'as', 'positive', 'or', 'negative', '.'] 

 TOTAL TOKENS ==> 31

 ---- POST ----

 [('Improve', 'VB'), ('customer', 'NN'), ('experience', 'NN'), ('Sentiment', 'NNP'), ('analysis', 'NN'), ('helps', 'VBZ'), ('to', 'TO'), ('detect', 'VB'), ('the', 'DT'), ('customer', 'NN'), ('’', 'NNP'), ('s', 'FW'), ('sentiment', 'NN'), ('on', 'IN'), ('the', 'DT'), ('products', 'NNS'), ('/', 'NNP'), ('services', 'NNS'), ('in', 'IN'), ('their', 'PRP$'), ('social', 'JJ'), ('media', 'NNS'), ('postings', 'NNS'), ('and', 'CC'), ('classify', 'VB'), ('them', 'PRP'), ('as', 'IN'), ('positive', 'JJ'), ('or', 'CC'), ('negative', 'JJ'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Improve', 'customer', 'experience', 'Sentiment', 'analysis', 'helps', 'detect', 'customer', '’', 'sentiment', 'products', '/', 'services', 'social', 'media', 'postings', 'classify', 'positive', 'negative', '.']

 TOTAL FILTERED TOKENS ==>  20

 ---- POST FOR FILTERED TOKENS ----

 [('Improve', 'VB'), ('customer', 'NN'), ('experience', 'NN'), ('Sentiment', 'NNP'), ('analysis', 'NN'), ('helps', 'VBZ'), ('detect', 'VB'), ('customer', 'NN'), ('’', 'NNP'), ('sentiment', 'NN'), ('products', 'NNS'), ('/', 'NNP'), ('services', 'NNS'), ('social', 'JJ'), ('media', 'NNS'), ('postings', 'NNS'), ('classify', 'VBP'), ('positive', 'JJ'), ('negative', 'JJ'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Improve customer', 'customer experience', 'experience Sentiment', 'Sentiment analysis', 'analysis helps', 'helps detect', 'detect customer', 'customer ’', '’ sentiment', 'sentiment products', 'products /', '/ services', 'services social', 'social media', 'media postings', 'postings classify', 'classify positive', 'positive negative', 'negative .'] 

 TOTAL BIGRAMS --> 19 



 ---- TRI-GRAMS ---- 

 ['Improve customer experience', 'customer experience Sentiment', 'experience Sentiment analysis', 'Sentiment analysis helps', 'analysis helps detect', 'helps detect customer', 'detect customer ’', 'customer ’ sentiment', '’ sentiment products', 'sentiment products /', 'products / services', '/ services social', 'services social media', 'social media postings', 'media postings classify', 'postings classify positive', 'classify positive negative', 'positive negative .'] 

 TOTAL TRIGRAMS --> 18 



 ---- NOUN PHRASES ---- 

 ['customer', 'experience', 'analysis', 'customer', 'sentiment'] 

 TOTAL NOUN PHRASES --> 5 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['improv', 'custom', 'experi', 'sentiment', 'analysi', 'help', 'detect', 'custom', '’', 'sentiment', 'product', '/', 'servic', 'social', 'media', 'post', 'classifi', 'posit', 'neg', '.']

 TOTAL PORTER STEM WORDS ==> 20



 ---- SNOWBALL STEMMING ----

['improv', 'custom', 'experi', 'sentiment', 'analysi', 'help', 'detect', 'custom', '’', 'sentiment', 'product', '/', 'servic', 'social', 'media', 'post', 'classifi', 'posit', 'negat', '.']

 TOTAL SNOWBALL STEM WORDS ==> 20



 ---- LEMMATIZATION ----

['Improve', 'customer', 'experience', 'Sentiment', 'analysis', 'help', 'detect', 'customer', '’', 'sentiment', 'product', '/', 'service', 'social', 'medium', 'posting', 'classify', 'positive', 'negative', '.']

 TOTAL LEMMATIZE WORDS ==> 20

************************************************************************************************************************

12 --> Ecommerce companies can make use these insights to improve  their customer experience and their brand image. 


 ---- TOKENS ----

 ['Ecommerce', 'companies', 'can', 'make', 'use', 'these', 'insights', 'to', 'improve', 'their', 'customer', 'experience', 'and', 'their', 'brand', 'image', '.'] 

 TOTAL TOKENS ==> 17

 ---- POST ----

 [('Ecommerce', 'NN'), ('companies', 'NNS'), ('can', 'MD'), ('make', 'VB'), ('use', 'VB'), ('these', 'DT'), ('insights', 'NNS'), ('to', 'TO'), ('improve', 'VB'), ('their', 'PRP$'), ('customer', 'NN'), ('experience', 'NN'), ('and', 'CC'), ('their', 'PRP$'), ('brand', 'NN'), ('image', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Ecommerce', 'companies', 'make', 'use', 'insights', 'improve', 'customer', 'experience', 'brand', 'image', '.']

 TOTAL FILTERED TOKENS ==>  11

 ---- POST FOR FILTERED TOKENS ----

 [('Ecommerce', 'NN'), ('companies', 'NNS'), ('make', 'VBP'), ('use', 'NN'), ('insights', 'NNS'), ('improve', 'VB'), ('customer', 'NN'), ('experience', 'NN'), ('brand', 'NN'), ('image', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Ecommerce companies', 'companies make', 'make use', 'use insights', 'insights improve', 'improve customer', 'customer experience', 'experience brand', 'brand image', 'image .'] 

 TOTAL BIGRAMS --> 10 



 ---- TRI-GRAMS ---- 

 ['Ecommerce companies make', 'companies make use', 'make use insights', 'use insights improve', 'insights improve customer', 'improve customer experience', 'customer experience brand', 'experience brand image', 'brand image .'] 

 TOTAL TRIGRAMS --> 9 



 ---- NOUN PHRASES ---- 

 ['Ecommerce', 'use', 'customer', 'experience', 'brand', 'image'] 

 TOTAL NOUN PHRASES --> 6 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> ['Ecommerce']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['ecommerc', 'compani', 'make', 'use', 'insight', 'improv', 'custom', 'experi', 'brand', 'imag', '.']

 TOTAL PORTER STEM WORDS ==> 11



 ---- SNOWBALL STEMMING ----

['ecommerc', 'compani', 'make', 'use', 'insight', 'improv', 'custom', 'experi', 'brand', 'imag', '.']

 TOTAL SNOWBALL STEM WORDS ==> 11



 ---- LEMMATIZATION ----

['Ecommerce', 'company', 'make', 'use', 'insight', 'improve', 'customer', 'experience', 'brand', 'image', '.']

 TOTAL LEMMATIZE WORDS ==> 11

************************************************************************************************************************

13 --> B. 


 ---- TOKENS ----

 ['B', '.'] 

 TOTAL TOKENS ==> 2

 ---- POST ----

 [('B', 'NNP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['B', '.']

 TOTAL FILTERED TOKENS ==>  2

 ---- POST FOR FILTERED TOKENS ----

 [('B', 'NNP'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['B .'] 

 TOTAL BIGRAMS --> 1 



 ---- TRI-GRAMS ---- 

 [] 

 TOTAL TRIGRAMS --> 0 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['b', '.']

 TOTAL PORTER STEM WORDS ==> 2



 ---- SNOWBALL STEMMING ----

['b', '.']

 TOTAL SNOWBALL STEM WORDS ==> 2



 ---- LEMMATIZATION ----

['B', '.']

 TOTAL LEMMATIZE WORDS ==> 2

************************************************************************************************************************

14 --> Achieve competitive e.g.-   The customers those are very much active on social media, expects that their concerns or feedback are heard and  replied within sixty minutes by the company. 


 ---- TOKENS ----

 ['Achieve', 'competitive', 'e.g.-', 'The', 'customers', 'those', 'are', 'very', 'much', 'active', 'on', 'social', 'media', ',', 'expects', 'that', 'their', 'concerns', 'or', 'feedback', 'are', 'heard', 'and', 'replied', 'within', 'sixty', 'minutes', 'by', 'the', 'company', '.'] 

 TOTAL TOKENS ==> 31

 ---- POST ----

 [('Achieve', 'NNP'), ('competitive', 'JJ'), ('e.g.-', 'NN'), ('The', 'DT'), ('customers', 'NNS'), ('those', 'DT'), ('are', 'VBP'), ('very', 'RB'), ('much', 'RB'), ('active', 'JJ'), ('on', 'IN'), ('social', 'JJ'), ('media', 'NNS'), (',', ','), ('expects', 'VBZ'), ('that', 'IN'), ('their', 'PRP$'), ('concerns', 'NNS'), ('or', 'CC'), ('feedback', 'NN'), ('are', 'VBP'), ('heard', 'VBN'), ('and', 'CC'), ('replied', 'VBN'), ('within', 'IN'), ('sixty', 'NN'), ('minutes', 'NNS'), ('by', 'IN'), ('the', 'DT'), ('company', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Achieve', 'competitive', 'e.g.-', 'customers', 'much', 'active', 'social', 'media', ',', 'expects', 'concerns', 'feedback', 'heard', 'replied', 'within', 'sixty', 'minutes', 'company', '.']

 TOTAL FILTERED TOKENS ==>  19

 ---- POST FOR FILTERED TOKENS ----

 [('Achieve', 'NNP'), ('competitive', 'JJ'), ('e.g.-', 'NN'), ('customers', 'NNS'), ('much', 'RB'), ('active', 'JJ'), ('social', 'JJ'), ('media', 'NNS'), (',', ','), ('expects', 'VBZ'), ('concerns', 'NNS'), ('feedback', 'RB'), ('heard', 'RB'), ('replied', 'VBD'), ('within', 'IN'), ('sixty', 'NN'), ('minutes', 'NNS'), ('company', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Achieve competitive', 'competitive e.g.-', 'e.g.- customers', 'customers much', 'much active', 'active social', 'social media', 'media ,', ', expects', 'expects concerns', 'concerns feedback', 'feedback heard', 'heard replied', 'replied within', 'within sixty', 'sixty minutes', 'minutes company', 'company .'] 

 TOTAL BIGRAMS --> 18 



 ---- TRI-GRAMS ---- 

 ['Achieve competitive e.g.-', 'competitive e.g.- customers', 'e.g.- customers much', 'customers much active', 'much active social', 'active social media', 'social media ,', 'media , expects', ', expects concerns', 'expects concerns feedback', 'concerns feedback heard', 'feedback heard replied', 'heard replied within', 'replied within sixty', 'within sixty minutes', 'sixty minutes company', 'minutes company .'] 

 TOTAL TRIGRAMS --> 17 



 ---- NOUN PHRASES ---- 

 ['competitive e.g.-', 'sixty', 'company'] 

 TOTAL NOUN PHRASES --> 3 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> ['Achieve']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['achiev', 'competit', 'e.g.-', 'custom', 'much', 'activ', 'social', 'media', ',', 'expect', 'concern', 'feedback', 'heard', 'repli', 'within', 'sixti', 'minut', 'compani', '.']

 TOTAL PORTER STEM WORDS ==> 19



 ---- SNOWBALL STEMMING ----

['achiev', 'competit', 'e.g.-', 'custom', 'much', 'activ', 'social', 'media', ',', 'expect', 'concern', 'feedback', 'heard', 'repli', 'within', 'sixti', 'minut', 'compani', '.']

 TOTAL SNOWBALL STEM WORDS ==> 19



 ---- LEMMATIZATION ----

['Achieve', 'competitive', 'e.g.-', 'customer', 'much', 'active', 'social', 'medium', ',', 'expects', 'concern', 'feedback', 'heard', 'replied', 'within', 'sixty', 'minute', 'company', '.']

 TOTAL LEMMATIZE WORDS ==> 19

************************************************************************************************************************

15 --> So, it is crucial for the customer support team to react to the  customer’s post in the social media. 


 ---- TOKENS ----

 ['So', ',', 'it', 'is', 'crucial', 'for', 'the', 'customer', 'support', 'team', 'to', 'react', 'to', 'the', 'customer', '’', 's', 'post', 'in', 'the', 'social', 'media', '.'] 

 TOTAL TOKENS ==> 23

 ---- POST ----

 [('So', 'RB'), (',', ','), ('it', 'PRP'), ('is', 'VBZ'), ('crucial', 'JJ'), ('for', 'IN'), ('the', 'DT'), ('customer', 'NN'), ('support', 'NN'), ('team', 'NN'), ('to', 'TO'), ('react', 'VB'), ('to', 'TO'), ('the', 'DT'), ('customer', 'NN'), ('’', 'NNP'), ('s', 'VBZ'), ('post', 'NN'), ('in', 'IN'), ('the', 'DT'), ('social', 'JJ'), ('media', 'NNS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 [',', 'crucial', 'customer', 'support', 'team', 'react', 'customer', '’', 'post', 'social', 'media', '.']

 TOTAL FILTERED TOKENS ==>  12

 ---- POST FOR FILTERED TOKENS ----

 [(',', ','), ('crucial', 'JJ'), ('customer', 'NN'), ('support', 'NN'), ('team', 'NN'), ('react', 'VB'), ('customer', 'NN'), ('’', 'NNP'), ('post', 'NN'), ('social', 'JJ'), ('media', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 [', crucial', 'crucial customer', 'customer support', 'support team', 'team react', 'react customer', 'customer ’', '’ post', 'post social', 'social media', 'media .'] 

 TOTAL BIGRAMS --> 11 



 ---- TRI-GRAMS ---- 

 [', crucial customer', 'crucial customer support', 'customer support team', 'support team react', 'team react customer', 'react customer ’', 'customer ’ post', '’ post social', 'post social media', 'social media .'] 

 TOTAL TRIGRAMS --> 10 



 ---- NOUN PHRASES ---- 

 ['crucial customer', 'support', 'team', 'customer', 'post'] 

 TOTAL NOUN PHRASES --> 5 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

[',', 'crucial', 'custom', 'support', 'team', 'react', 'custom', '’', 'post', 'social', 'media', '.']

 TOTAL PORTER STEM WORDS ==> 12



 ---- SNOWBALL STEMMING ----

[',', 'crucial', 'custom', 'support', 'team', 'react', 'custom', '’', 'post', 'social', 'media', '.']

 TOTAL SNOWBALL STEM WORDS ==> 12



 ---- LEMMATIZATION ----

[',', 'crucial', 'customer', 'support', 'team', 'react', 'customer', '’', 'post', 'social', 'medium', '.']

 TOTAL LEMMATIZE WORDS ==> 12

************************************************************************************************************************

16 --> Sentiment analysis helps to predict the customer trends and work on the  strategies to capitalize and achieve competitive advantage. 


 ---- TOKENS ----

 ['Sentiment', 'analysis', 'helps', 'to', 'predict', 'the', 'customer', 'trends', 'and', 'work', 'on', 'the', 'strategies', 'to', 'capitalize', 'and', 'achieve', 'competitive', 'advantage', '.'] 

 TOTAL TOKENS ==> 20

 ---- POST ----

 [('Sentiment', 'NN'), ('analysis', 'NN'), ('helps', 'VBZ'), ('to', 'TO'), ('predict', 'VB'), ('the', 'DT'), ('customer', 'NN'), ('trends', 'NNS'), ('and', 'CC'), ('work', 'NN'), ('on', 'IN'), ('the', 'DT'), ('strategies', 'NNS'), ('to', 'TO'), ('capitalize', 'VB'), ('and', 'CC'), ('achieve', 'VB'), ('competitive', 'JJ'), ('advantage', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Sentiment', 'analysis', 'helps', 'predict', 'customer', 'trends', 'work', 'strategies', 'capitalize', 'achieve', 'competitive', 'advantage', '.']

 TOTAL FILTERED TOKENS ==>  13

 ---- POST FOR FILTERED TOKENS ----

 [('Sentiment', 'NN'), ('analysis', 'NN'), ('helps', 'VBZ'), ('predict', 'VB'), ('customer', 'NN'), ('trends', 'NNS'), ('work', 'VBP'), ('strategies', 'NNS'), ('capitalize', 'VB'), ('achieve', 'RB'), ('competitive', 'JJ'), ('advantage', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Sentiment analysis', 'analysis helps', 'helps predict', 'predict customer', 'customer trends', 'trends work', 'work strategies', 'strategies capitalize', 'capitalize achieve', 'achieve competitive', 'competitive advantage', 'advantage .'] 

 TOTAL BIGRAMS --> 12 



 ---- TRI-GRAMS ---- 

 ['Sentiment analysis helps', 'analysis helps predict', 'helps predict customer', 'predict customer trends', 'customer trends work', 'trends work strategies', 'work strategies capitalize', 'strategies capitalize achieve', 'capitalize achieve competitive', 'achieve competitive advantage', 'competitive advantage .'] 

 TOTAL TRIGRAMS --> 11 



 ---- NOUN PHRASES ---- 

 ['Sentiment', 'analysis', 'customer', 'competitive advantage'] 

 TOTAL NOUN PHRASES --> 4 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['sentiment', 'analysi', 'help', 'predict', 'custom', 'trend', 'work', 'strategi', 'capit', 'achiev', 'competit', 'advantag', '.']

 TOTAL PORTER STEM WORDS ==> 13



 ---- SNOWBALL STEMMING ----

['sentiment', 'analysi', 'help', 'predict', 'custom', 'trend', 'work', 'strategi', 'capit', 'achiev', 'competit', 'advantag', '.']

 TOTAL SNOWBALL STEM WORDS ==> 13



 ---- LEMMATIZATION ----

['Sentiment', 'analysis', 'help', 'predict', 'customer', 'trend', 'work', 'strategy', 'capitalize', 'achieve', 'competitive', 'advantage', '.']

 TOTAL LEMMATIZE WORDS ==> 13

************************************************************************************************************************

17 --> C. Insightful Business Intelligence   Companies requires business intelligence to meet and improve their customer’s expectations in service delivery. 


 ---- TOKENS ----

 ['C.', 'Insightful', 'Business', 'Intelligence', 'Companies', 'requires', 'business', 'intelligence', 'to', 'meet', 'and', 'improve', 'their', 'customer', '’', 's', 'expectations', 'in', 'service', 'delivery', '.'] 

 TOTAL TOKENS ==> 21

 ---- POST ----

 [('C.', 'NNP'), ('Insightful', 'NNP'), ('Business', 'NNP'), ('Intelligence', 'NNP'), ('Companies', 'NNP'), ('requires', 'VBZ'), ('business', 'NN'), ('intelligence', 'NN'), ('to', 'TO'), ('meet', 'VB'), ('and', 'CC'), ('improve', 'VB'), ('their', 'PRP$'), ('customer', 'NN'), ('’', 'NN'), ('s', 'JJ'), ('expectations', 'NNS'), ('in', 'IN'), ('service', 'NN'), ('delivery', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['C.', 'Insightful', 'Business', 'Intelligence', 'Companies', 'requires', 'business', 'intelligence', 'meet', 'improve', 'customer', '’', 'expectations', 'service', 'delivery', '.']

 TOTAL FILTERED TOKENS ==>  16

 ---- POST FOR FILTERED TOKENS ----

 [('C.', 'NNP'), ('Insightful', 'NNP'), ('Business', 'NNP'), ('Intelligence', 'NNP'), ('Companies', 'NNP'), ('requires', 'VBZ'), ('business', 'NN'), ('intelligence', 'NN'), ('meet', 'NN'), ('improve', 'VB'), ('customer', 'NN'), ('’', 'NNP'), ('expectations', 'NNS'), ('service', 'NN'), ('delivery', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['C. Insightful', 'Insightful Business', 'Business Intelligence', 'Intelligence Companies', 'Companies requires', 'requires business', 'business intelligence', 'intelligence meet', 'meet improve', 'improve customer', 'customer ’', '’ expectations', 'expectations service', 'service delivery', 'delivery .'] 

 TOTAL BIGRAMS --> 15 



 ---- TRI-GRAMS ---- 

 ['C. Insightful Business', 'Insightful Business Intelligence', 'Business Intelligence Companies', 'Intelligence Companies requires', 'Companies requires business', 'requires business intelligence', 'business intelligence meet', 'intelligence meet improve', 'meet improve customer', 'improve customer ’', 'customer ’ expectations', '’ expectations service', 'expectations service delivery', 'service delivery .'] 

 TOTAL TRIGRAMS --> 14 



 ---- NOUN PHRASES ---- 

 ['business', 'intelligence', 'meet', 'customer', 'service', 'delivery'] 

 TOTAL NOUN PHRASES --> 6 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['c.', 'insight', 'busi', 'intellig', 'compani', 'requir', 'busi', 'intellig', 'meet', 'improv', 'custom', '’', 'expect', 'servic', 'deliveri', '.']

 TOTAL PORTER STEM WORDS ==> 16



 ---- SNOWBALL STEMMING ----

['c.', 'insight', 'busi', 'intellig', 'compani', 'requir', 'busi', 'intellig', 'meet', 'improv', 'custom', '’', 'expect', 'servic', 'deliveri', '.']

 TOTAL SNOWBALL STEM WORDS ==> 16



 ---- LEMMATIZATION ----

['C.', 'Insightful', 'Business', 'Intelligence', 'Companies', 'requires', 'business', 'intelligence', 'meet', 'improve', 'customer', '’', 'expectation', 'service', 'delivery', '.']

 TOTAL LEMMATIZE WORDS ==> 16

************************************************************************************************************************

18 --> Sentiment analysis provides the required insights about the customer’s expectation to define the strategies for  improvement of customer service. 


 ---- TOKENS ----

 ['Sentiment', 'analysis', 'provides', 'the', 'required', 'insights', 'about', 'the', 'customer', '’', 's', 'expectation', 'to', 'define', 'the', 'strategies', 'for', 'improvement', 'of', 'customer', 'service', '.'] 

 TOTAL TOKENS ==> 22

 ---- POST ----

 [('Sentiment', 'JJ'), ('analysis', 'NN'), ('provides', 'VBZ'), ('the', 'DT'), ('required', 'JJ'), ('insights', 'NNS'), ('about', 'IN'), ('the', 'DT'), ('customer', 'NN'), ('’', 'NNP'), ('s', 'VBZ'), ('expectation', 'NN'), ('to', 'TO'), ('define', 'VB'), ('the', 'DT'), ('strategies', 'NNS'), ('for', 'IN'), ('improvement', 'NN'), ('of', 'IN'), ('customer', 'NN'), ('service', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Sentiment', 'analysis', 'provides', 'required', 'insights', 'customer', '’', 'expectation', 'define', 'strategies', 'improvement', 'customer', 'service', '.']

 TOTAL FILTERED TOKENS ==>  14

 ---- POST FOR FILTERED TOKENS ----

 [('Sentiment', 'JJ'), ('analysis', 'NN'), ('provides', 'VBZ'), ('required', 'JJ'), ('insights', 'NNS'), ('customer', 'NN'), ('’', 'NNP'), ('expectation', 'NN'), ('define', 'NN'), ('strategies', 'NNS'), ('improvement', 'NN'), ('customer', 'NN'), ('service', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Sentiment analysis', 'analysis provides', 'provides required', 'required insights', 'insights customer', 'customer ’', '’ expectation', 'expectation define', 'define strategies', 'strategies improvement', 'improvement customer', 'customer service', 'service .'] 

 TOTAL BIGRAMS --> 13 



 ---- TRI-GRAMS ---- 

 ['Sentiment analysis provides', 'analysis provides required', 'provides required insights', 'required insights customer', 'insights customer ’', 'customer ’ expectation', '’ expectation define', 'expectation define strategies', 'define strategies improvement', 'strategies improvement customer', 'improvement customer service', 'customer service .'] 

 TOTAL TRIGRAMS --> 12 



 ---- NOUN PHRASES ---- 

 ['Sentiment analysis', 'customer', 'expectation', 'define', 'improvement', 'customer', 'service'] 

 TOTAL NOUN PHRASES --> 7 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['sentiment', 'analysi', 'provid', 'requir', 'insight', 'custom', '’', 'expect', 'defin', 'strategi', 'improv', 'custom', 'servic', '.']

 TOTAL PORTER STEM WORDS ==> 14



 ---- SNOWBALL STEMMING ----

['sentiment', 'analysi', 'provid', 'requir', 'insight', 'custom', '’', 'expect', 'defin', 'strategi', 'improv', 'custom', 'servic', '.']

 TOTAL SNOWBALL STEM WORDS ==> 14



 ---- LEMMATIZATION ----

['Sentiment', 'analysis', 'provides', 'required', 'insight', 'customer', '’', 'expectation', 'define', 'strategy', 'improvement', 'customer', 'service', '.']

 TOTAL LEMMATIZE WORDS ==> 14

************************************************************************************************************************

19 --> D. Market analysis   Sentiment analysis help to discover the latest trends and new business opportunities. 


 ---- TOKENS ----

 ['D.', 'Market', 'analysis', 'Sentiment', 'analysis', 'help', 'to', 'discover', 'the', 'latest', 'trends', 'and', 'new', 'business', 'opportunities', '.'] 

 TOTAL TOKENS ==> 16

 ---- POST ----

 [('D.', 'NNP'), ('Market', 'NNP'), ('analysis', 'NN'), ('Sentiment', 'NNP'), ('analysis', 'NN'), ('help', 'NN'), ('to', 'TO'), ('discover', 'VB'), ('the', 'DT'), ('latest', 'JJS'), ('trends', 'NNS'), ('and', 'CC'), ('new', 'JJ'), ('business', 'NN'), ('opportunities', 'NNS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['D.', 'Market', 'analysis', 'Sentiment', 'analysis', 'help', 'discover', 'latest', 'trends', 'new', 'business', 'opportunities', '.']

 TOTAL FILTERED TOKENS ==>  13

 ---- POST FOR FILTERED TOKENS ----

 [('D.', 'NNP'), ('Market', 'NNP'), ('analysis', 'NN'), ('Sentiment', 'NNP'), ('analysis', 'NN'), ('help', 'NN'), ('discover', 'VB'), ('latest', 'JJS'), ('trends', 'NNS'), ('new', 'JJ'), ('business', 'NN'), ('opportunities', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['D. Market', 'Market analysis', 'analysis Sentiment', 'Sentiment analysis', 'analysis help', 'help discover', 'discover latest', 'latest trends', 'trends new', 'new business', 'business opportunities', 'opportunities .'] 

 TOTAL BIGRAMS --> 12 



 ---- TRI-GRAMS ---- 

 ['D. Market analysis', 'Market analysis Sentiment', 'analysis Sentiment analysis', 'Sentiment analysis help', 'analysis help discover', 'help discover latest', 'discover latest trends', 'latest trends new', 'trends new business', 'new business opportunities', 'business opportunities .'] 

 TOTAL TRIGRAMS --> 11 



 ---- NOUN PHRASES ---- 

 ['analysis', 'analysis', 'help', 'new business'] 

 TOTAL NOUN PHRASES --> 4 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['d.', 'market', 'analysi', 'sentiment', 'analysi', 'help', 'discov', 'latest', 'trend', 'new', 'busi', 'opportun', '.']

 TOTAL PORTER STEM WORDS ==> 13



 ---- SNOWBALL STEMMING ----

['d.', 'market', 'analysi', 'sentiment', 'analysi', 'help', 'discov', 'latest', 'trend', 'new', 'busi', 'opportun', '.']

 TOTAL SNOWBALL STEM WORDS ==> 13



 ---- LEMMATIZATION ----

['D.', 'Market', 'analysis', 'Sentiment', 'analysis', 'help', 'discover', 'latest', 'trend', 'new', 'business', 'opportunity', '.']

 TOTAL LEMMATIZE WORDS ==> 13

************************************************************************************************************************

20 --> It provides an answer to  many questions like whether the market is stable enough and the brand is received well, what the customers are  taking about our products / services, Is there any untapped opportunities and customers, etc. 


 ---- TOKENS ----

 ['It', 'provides', 'an', 'answer', 'to', 'many', 'questions', 'like', 'whether', 'the', 'market', 'is', 'stable', 'enough', 'and', 'the', 'brand', 'is', 'received', 'well', ',', 'what', 'the', 'customers', 'are', 'taking', 'about', 'our', 'products', '/', 'services', ',', 'Is', 'there', 'any', 'untapped', 'opportunities', 'and', 'customers', ',', 'etc', '.'] 

 TOTAL TOKENS ==> 42

 ---- POST ----

 [('It', 'PRP'), ('provides', 'VBZ'), ('an', 'DT'), ('answer', 'NN'), ('to', 'TO'), ('many', 'JJ'), ('questions', 'NNS'), ('like', 'IN'), ('whether', 'IN'), ('the', 'DT'), ('market', 'NN'), ('is', 'VBZ'), ('stable', 'JJ'), ('enough', 'RB'), ('and', 'CC'), ('the', 'DT'), ('brand', 'NN'), ('is', 'VBZ'), ('received', 'VBN'), ('well', 'RB'), (',', ','), ('what', 'WP'), ('the', 'DT'), ('customers', 'NNS'), ('are', 'VBP'), ('taking', 'VBG'), ('about', 'IN'), ('our', 'PRP$'), ('products', 'NNS'), ('/', 'NNP'), ('services', 'NNS'), (',', ','), ('Is', 'VBZ'), ('there', 'RB'), ('any', 'DT'), ('untapped', 'JJ'), ('opportunities', 'NNS'), ('and', 'CC'), ('customers', 'NNS'), (',', ','), ('etc', 'FW'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['provides', 'answer', 'many', 'questions', 'like', 'whether', 'market', 'stable', 'enough', 'brand', 'received', 'well', ',', 'customers', 'taking', 'products', '/', 'services', ',', 'untapped', 'opportunities', 'customers', ',', 'etc', '.']

 TOTAL FILTERED TOKENS ==>  25

 ---- POST FOR FILTERED TOKENS ----

 [('provides', 'VBZ'), ('answer', 'RB'), ('many', 'JJ'), ('questions', 'NNS'), ('like', 'IN'), ('whether', 'IN'), ('market', 'NN'), ('stable', 'JJ'), ('enough', 'RB'), ('brand', 'NN'), ('received', 'VBD'), ('well', 'RB'), (',', ','), ('customers', 'NNS'), ('taking', 'VBG'), ('products', 'NNS'), ('/', 'NNP'), ('services', 'NNS'), (',', ','), ('untapped', 'JJ'), ('opportunities', 'NNS'), ('customers', 'NNS'), (',', ','), ('etc', 'FW'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['provides answer', 'answer many', 'many questions', 'questions like', 'like whether', 'whether market', 'market stable', 'stable enough', 'enough brand', 'brand received', 'received well', 'well ,', ', customers', 'customers taking', 'taking products', 'products /', '/ services', 'services ,', ', untapped', 'untapped opportunities', 'opportunities customers', 'customers ,', ', etc', 'etc .'] 

 TOTAL BIGRAMS --> 24 



 ---- TRI-GRAMS ---- 

 ['provides answer many', 'answer many questions', 'many questions like', 'questions like whether', 'like whether market', 'whether market stable', 'market stable enough', 'stable enough brand', 'enough brand received', 'brand received well', 'received well ,', 'well , customers', ', customers taking', 'customers taking products', 'taking products /', 'products / services', '/ services ,', 'services , untapped', ', untapped opportunities', 'untapped opportunities customers', 'opportunities customers ,', 'customers , etc', ', etc .'] 

 TOTAL TRIGRAMS --> 23 



 ---- NOUN PHRASES ---- 

 ['market', 'brand'] 

 TOTAL NOUN PHRASES --> 2 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['provid', 'answer', 'mani', 'question', 'like', 'whether', 'market', 'stabl', 'enough', 'brand', 'receiv', 'well', ',', 'custom', 'take', 'product', '/', 'servic', ',', 'untap', 'opportun', 'custom', ',', 'etc', '.']

 TOTAL PORTER STEM WORDS ==> 25



 ---- SNOWBALL STEMMING ----

['provid', 'answer', 'mani', 'question', 'like', 'whether', 'market', 'stabl', 'enough', 'brand', 'receiv', 'well', ',', 'custom', 'take', 'product', '/', 'servic', ',', 'untap', 'opportun', 'custom', ',', 'etc', '.']

 TOTAL SNOWBALL STEM WORDS ==> 25



 ---- LEMMATIZATION ----

['provides', 'answer', 'many', 'question', 'like', 'whether', 'market', 'stable', 'enough', 'brand', 'received', 'well', ',', 'customer', 'taking', 'product', '/', 'service', ',', 'untapped', 'opportunity', 'customer', ',', 'etc', '.']

 TOTAL LEMMATIZE WORDS ==> 25

************************************************************************************************************************

21 --> E. Brand revitalization          White Paper – Sentiments Analysis  Page 3 of 5    Brand image is none other than the customer’s perception about the overall company. 


 ---- TOKENS ----

 ['E.', 'Brand', 'revitalization', 'White', 'Paper', '–', 'Sentiments', 'Analysis', 'Page', '3', 'of', '5', 'Brand', 'image', 'is', 'none', 'other', 'than', 'the', 'customer', '’', 's', 'perception', 'about', 'the', 'overall', 'company', '.'] 

 TOTAL TOKENS ==> 28

 ---- POST ----

 [('E.', 'NNP'), ('Brand', 'NNP'), ('revitalization', 'NN'), ('White', 'NNP'), ('Paper', 'NNP'), ('–', 'NN'), ('Sentiments', 'NNP'), ('Analysis', 'NNP'), ('Page', 'NNP'), ('3', 'CD'), ('of', 'IN'), ('5', 'CD'), ('Brand', 'NNP'), ('image', 'NN'), ('is', 'VBZ'), ('none', 'RB'), ('other', 'JJ'), ('than', 'IN'), ('the', 'DT'), ('customer', 'NN'), ('’', 'NNP'), ('s', 'VBZ'), ('perception', 'NN'), ('about', 'IN'), ('the', 'DT'), ('overall', 'JJ'), ('company', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['E.', 'Brand', 'revitalization', 'White', 'Paper', '–', 'Sentiments', 'Analysis', 'Page', '3', '5', 'Brand', 'image', 'none', 'customer', '’', 'perception', 'overall', 'company', '.']

 TOTAL FILTERED TOKENS ==>  20

 ---- POST FOR FILTERED TOKENS ----

 [('E.', 'NNP'), ('Brand', 'NNP'), ('revitalization', 'NN'), ('White', 'NNP'), ('Paper', 'NNP'), ('–', 'NN'), ('Sentiments', 'NNP'), ('Analysis', 'NNP'), ('Page', 'NNP'), ('3', 'CD'), ('5', 'CD'), ('Brand', 'NNP'), ('image', 'NN'), ('none', 'NN'), ('customer', 'NN'), ('’', 'NNP'), ('perception', 'NN'), ('overall', 'JJ'), ('company', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['E. Brand', 'Brand revitalization', 'revitalization White', 'White Paper', 'Paper –', '– Sentiments', 'Sentiments Analysis', 'Analysis Page', 'Page 3', '3 5', '5 Brand', 'Brand image', 'image none', 'none customer', 'customer ’', '’ perception', 'perception overall', 'overall company', 'company .'] 

 TOTAL BIGRAMS --> 19 



 ---- TRI-GRAMS ---- 

 ['E. Brand revitalization', 'Brand revitalization White', 'revitalization White Paper', 'White Paper –', 'Paper – Sentiments', '– Sentiments Analysis', 'Sentiments Analysis Page', 'Analysis Page 3', 'Page 3 5', '3 5 Brand', '5 Brand image', 'Brand image none', 'image none customer', 'none customer ’', 'customer ’ perception', '’ perception overall', 'perception overall company', 'overall company .'] 

 TOTAL TRIGRAMS --> 18 



 ---- NOUN PHRASES ---- 

 ['revitalization', '–', 'image', 'none', 'customer', 'perception', 'overall company'] 

 TOTAL NOUN PHRASES --> 7 



 ---- NER ----

 
 ORGANIZATION ---> ['Sentiments Analysis']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> ['Brand', 'Brand']
 TOTAL GPE ENTITY --> 2 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['e.', 'brand', 'revit', 'white', 'paper', '–', 'sentiment', 'analysi', 'page', '3', '5', 'brand', 'imag', 'none', 'custom', '’', 'percept', 'overal', 'compani', '.']

 TOTAL PORTER STEM WORDS ==> 20



 ---- SNOWBALL STEMMING ----

['e.', 'brand', 'revit', 'white', 'paper', '–', 'sentiment', 'analysi', 'page', '3', '5', 'brand', 'imag', 'none', 'custom', '’', 'percept', 'overal', 'compani', '.']

 TOTAL SNOWBALL STEM WORDS ==> 20



 ---- LEMMATIZATION ----

['E.', 'Brand', 'revitalization', 'White', 'Paper', '–', 'Sentiments', 'Analysis', 'Page', '3', '5', 'Brand', 'image', 'none', 'customer', '’', 'perception', 'overall', 'company', '.']

 TOTAL LEMMATIZE WORDS ==> 20

************************************************************************************************************************

22 --> Sentiment analysis helps  to quantify the customer perception and get the competitive e.g.- in the market. 


 ---- TOKENS ----

 ['Sentiment', 'analysis', 'helps', 'to', 'quantify', 'the', 'customer', 'perception', 'and', 'get', 'the', 'competitive', 'e.g.-', 'in', 'the', 'market', '.'] 

 TOTAL TOKENS ==> 17

 ---- POST ----

 [('Sentiment', 'NN'), ('analysis', 'NN'), ('helps', 'VBZ'), ('to', 'TO'), ('quantify', 'VB'), ('the', 'DT'), ('customer', 'NN'), ('perception', 'NN'), ('and', 'CC'), ('get', 'VB'), ('the', 'DT'), ('competitive', 'JJ'), ('e.g.-', 'NN'), ('in', 'IN'), ('the', 'DT'), ('market', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Sentiment', 'analysis', 'helps', 'quantify', 'customer', 'perception', 'get', 'competitive', 'e.g.-', 'market', '.']

 TOTAL FILTERED TOKENS ==>  11

 ---- POST FOR FILTERED TOKENS ----

 [('Sentiment', 'NN'), ('analysis', 'NN'), ('helps', 'VBZ'), ('quantify', 'VB'), ('customer', 'NN'), ('perception', 'NN'), ('get', 'VB'), ('competitive', 'JJ'), ('e.g.-', 'JJ'), ('market', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Sentiment analysis', 'analysis helps', 'helps quantify', 'quantify customer', 'customer perception', 'perception get', 'get competitive', 'competitive e.g.-', 'e.g.- market', 'market .'] 

 TOTAL BIGRAMS --> 10 



 ---- TRI-GRAMS ---- 

 ['Sentiment analysis helps', 'analysis helps quantify', 'helps quantify customer', 'quantify customer perception', 'customer perception get', 'perception get competitive', 'get competitive e.g.-', 'competitive e.g.- market', 'e.g.- market .'] 

 TOTAL TRIGRAMS --> 9 



 ---- NOUN PHRASES ---- 

 ['Sentiment', 'analysis', 'customer', 'perception', 'competitive e.g.- market'] 

 TOTAL NOUN PHRASES --> 5 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['sentiment', 'analysi', 'help', 'quantifi', 'custom', 'percept', 'get', 'competit', 'e.g.-', 'market', '.']

 TOTAL PORTER STEM WORDS ==> 11



 ---- SNOWBALL STEMMING ----

['sentiment', 'analysi', 'help', 'quantifi', 'custom', 'percept', 'get', 'competit', 'e.g.-', 'market', '.']

 TOTAL SNOWBALL STEM WORDS ==> 11



 ---- LEMMATIZATION ----

['Sentiment', 'analysis', 'help', 'quantify', 'customer', 'perception', 'get', 'competitive', 'e.g.-', 'market', '.']

 TOTAL LEMMATIZE WORDS ==> 11

************************************************************************************************************************

23 --> IV. 


 ---- TOKENS ----

 ['IV', '.'] 

 TOTAL TOKENS ==> 2

 ---- POST ----

 [('IV', 'NNP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['IV', '.']

 TOTAL FILTERED TOKENS ==>  2

 ---- POST FOR FILTERED TOKENS ----

 [('IV', 'NNP'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['IV .'] 

 TOTAL BIGRAMS --> 1 



 ---- TRI-GRAMS ---- 

 [] 

 TOTAL TRIGRAMS --> 0 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['iv', '.']

 TOTAL PORTER STEM WORDS ==> 2



 ---- SNOWBALL STEMMING ----

['iv', '.']

 TOTAL SNOWBALL STEM WORDS ==> 2



 ---- LEMMATIZATION ----

['IV', '.']

 TOTAL LEMMATIZE WORDS ==> 2

************************************************************************************************************************

24 --> PROCESS OF SENTIMENT ANALYSIS  The following process is widely followed in Sentiment Analysis;   • Extract the consumer generated content like tweets, comments, status updates, etc., by leveraging the  APIs of different social media networks. 


 ---- TOKENS ----

 ['PROCESS', 'OF', 'SENTIMENT', 'ANALYSIS', 'The', 'following', 'process', 'is', 'widely', 'followed', 'in', 'Sentiment', 'Analysis', ';', '•', 'Extract', 'the', 'consumer', 'generated', 'content', 'like', 'tweets', ',', 'comments', ',', 'status', 'updates', ',', 'etc.', ',', 'by', 'leveraging', 'the', 'APIs', 'of', 'different', 'social', 'media', 'networks', '.'] 

 TOTAL TOKENS ==> 40

 ---- POST ----

 [('PROCESS', 'NN'), ('OF', 'NNP'), ('SENTIMENT', 'NNP'), ('ANALYSIS', 'NNP'), ('The', 'DT'), ('following', 'JJ'), ('process', 'NN'), ('is', 'VBZ'), ('widely', 'RB'), ('followed', 'VBN'), ('in', 'IN'), ('Sentiment', 'NNP'), ('Analysis', 'NNP'), (';', ':'), ('•', 'NNP'), ('Extract', 'NNP'), ('the', 'DT'), ('consumer', 'NN'), ('generated', 'VBD'), ('content', 'NN'), ('like', 'IN'), ('tweets', 'NNS'), (',', ','), ('comments', 'NNS'), (',', ','), ('status', 'NN'), ('updates', 'NNS'), (',', ','), ('etc.', 'NN'), (',', ','), ('by', 'IN'), ('leveraging', 'VBG'), ('the', 'DT'), ('APIs', 'NNP'), ('of', 'IN'), ('different', 'JJ'), ('social', 'JJ'), ('media', 'NNS'), ('networks', 'NNS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['PROCESS', 'SENTIMENT', 'ANALYSIS', 'following', 'process', 'widely', 'followed', 'Sentiment', 'Analysis', ';', '•', 'Extract', 'consumer', 'generated', 'content', 'like', 'tweets', ',', 'comments', ',', 'status', 'updates', ',', 'etc.', ',', 'leveraging', 'APIs', 'different', 'social', 'media', 'networks', '.']

 TOTAL FILTERED TOKENS ==>  32

 ---- POST FOR FILTERED TOKENS ----

 [('PROCESS', 'NNP'), ('SENTIMENT', 'NNP'), ('ANALYSIS', 'NNP'), ('following', 'VBG'), ('process', 'NN'), ('widely', 'RB'), ('followed', 'VBD'), ('Sentiment', 'NNP'), ('Analysis', 'NNP'), (';', ':'), ('•', 'NNP'), ('Extract', 'NNP'), ('consumer', 'NN'), ('generated', 'VBD'), ('content', 'NN'), ('like', 'IN'), ('tweets', 'NNS'), (',', ','), ('comments', 'NNS'), (',', ','), ('status', 'NN'), ('updates', 'NNS'), (',', ','), ('etc.', 'FW'), (',', ','), ('leveraging', 'VBG'), ('APIs', 'NNP'), ('different', 'JJ'), ('social', 'JJ'), ('media', 'NNS'), ('networks', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['PROCESS SENTIMENT', 'SENTIMENT ANALYSIS', 'ANALYSIS following', 'following process', 'process widely', 'widely followed', 'followed Sentiment', 'Sentiment Analysis', 'Analysis ;', '; •', '• Extract', 'Extract consumer', 'consumer generated', 'generated content', 'content like', 'like tweets', 'tweets ,', ', comments', 'comments ,', ', status', 'status updates', 'updates ,', ', etc.', 'etc. ,', ', leveraging', 'leveraging APIs', 'APIs different', 'different social', 'social media', 'media networks', 'networks .'] 

 TOTAL BIGRAMS --> 31 



 ---- TRI-GRAMS ---- 

 ['PROCESS SENTIMENT ANALYSIS', 'SENTIMENT ANALYSIS following', 'ANALYSIS following process', 'following process widely', 'process widely followed', 'widely followed Sentiment', 'followed Sentiment Analysis', 'Sentiment Analysis ;', 'Analysis ; •', '; • Extract', '• Extract consumer', 'Extract consumer generated', 'consumer generated content', 'generated content like', 'content like tweets', 'like tweets ,', 'tweets , comments', ', comments ,', 'comments , status', ', status updates', 'status updates ,', 'updates , etc.', ', etc. ,', 'etc. , leveraging', ', leveraging APIs', 'leveraging APIs different', 'APIs different social', 'different social media', 'social media networks', 'media networks .'] 

 TOTAL TRIGRAMS --> 30 



 ---- NOUN PHRASES ---- 

 ['process', 'consumer', 'content', 'status'] 

 TOTAL NOUN PHRASES --> 4 



 ---- NER ----

 
 ORGANIZATION ---> ['PROCESS', 'SENTIMENT', 'Sentiment Analysis', 'APIs']
 TOTAL ORGANIZATION ENTITY --> 4 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['process', 'sentiment', 'analysi', 'follow', 'process', 'wide', 'follow', 'sentiment', 'analysi', ';', '•', 'extract', 'consum', 'gener', 'content', 'like', 'tweet', ',', 'comment', ',', 'statu', 'updat', ',', 'etc.', ',', 'leverag', 'api', 'differ', 'social', 'media', 'network', '.']

 TOTAL PORTER STEM WORDS ==> 32



 ---- SNOWBALL STEMMING ----

['process', 'sentiment', 'analysi', 'follow', 'process', 'wide', 'follow', 'sentiment', 'analysi', ';', '•', 'extract', 'consum', 'generat', 'content', 'like', 'tweet', ',', 'comment', ',', 'status', 'updat', ',', 'etc.', ',', 'leverag', 'api', 'differ', 'social', 'media', 'network', '.']

 TOTAL SNOWBALL STEM WORDS ==> 32



 ---- LEMMATIZATION ----

['PROCESS', 'SENTIMENT', 'ANALYSIS', 'following', 'process', 'widely', 'followed', 'Sentiment', 'Analysis', ';', '•', 'Extract', 'consumer', 'generated', 'content', 'like', 'tweet', ',', 'comment', ',', 'status', 'update', ',', 'etc.', ',', 'leveraging', 'APIs', 'different', 'social', 'medium', 'network', '.']

 TOTAL LEMMATIZE WORDS ==> 32

************************************************************************************************************************

25 --> • Analyse the tweets, comments, status updates, etc., and build the data dictionary. 


 ---- TOKENS ----

 ['•', 'Analyse', 'the', 'tweets', ',', 'comments', ',', 'status', 'updates', ',', 'etc.', ',', 'and', 'build', 'the', 'data', 'dictionary', '.'] 

 TOTAL TOKENS ==> 18

 ---- POST ----

 [('•', 'NN'), ('Analyse', 'NNP'), ('the', 'DT'), ('tweets', 'NNS'), (',', ','), ('comments', 'NNS'), (',', ','), ('status', 'NN'), ('updates', 'NNS'), (',', ','), ('etc.', 'NN'), (',', ','), ('and', 'CC'), ('build', 'VB'), ('the', 'DT'), ('data', 'NNS'), ('dictionary', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['•', 'Analyse', 'tweets', ',', 'comments', ',', 'status', 'updates', ',', 'etc.', ',', 'build', 'data', 'dictionary', '.']

 TOTAL FILTERED TOKENS ==>  15

 ---- POST FOR FILTERED TOKENS ----

 [('•', 'JJ'), ('Analyse', 'NNP'), ('tweets', 'NNS'), (',', ','), ('comments', 'NNS'), (',', ','), ('status', 'NN'), ('updates', 'NNS'), (',', ','), ('etc.', 'FW'), (',', ','), ('build', 'VB'), ('data', 'NNS'), ('dictionary', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['• Analyse', 'Analyse tweets', 'tweets ,', ', comments', 'comments ,', ', status', 'status updates', 'updates ,', ', etc.', 'etc. ,', ', build', 'build data', 'data dictionary', 'dictionary .'] 

 TOTAL BIGRAMS --> 14 



 ---- TRI-GRAMS ---- 

 ['• Analyse tweets', 'Analyse tweets ,', 'tweets , comments', ', comments ,', 'comments , status', ', status updates', 'status updates ,', 'updates , etc.', ', etc. ,', 'etc. , build', ', build data', 'build data dictionary', 'data dictionary .'] 

 TOTAL TRIGRAMS --> 13 



 ---- NOUN PHRASES ---- 

 ['status', 'dictionary'] 

 TOTAL NOUN PHRASES --> 2 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['•', 'analys', 'tweet', ',', 'comment', ',', 'statu', 'updat', ',', 'etc.', ',', 'build', 'data', 'dictionari', '.']

 TOTAL PORTER STEM WORDS ==> 15



 ---- SNOWBALL STEMMING ----

['•', 'analys', 'tweet', ',', 'comment', ',', 'status', 'updat', ',', 'etc.', ',', 'build', 'data', 'dictionari', '.']

 TOTAL SNOWBALL STEM WORDS ==> 15



 ---- LEMMATIZATION ----

['•', 'Analyse', 'tweet', ',', 'comment', ',', 'status', 'update', ',', 'etc.', ',', 'build', 'data', 'dictionary', '.']

 TOTAL LEMMATIZE WORDS ==> 15

************************************************************************************************************************

26 --> Classify them into  different types of sentiments / emotions   • Apply Natural Language Processing (NLP) techniques and derive the meaningful insights. 


 ---- TOKENS ----

 ['Classify', 'them', 'into', 'different', 'types', 'of', 'sentiments', '/', 'emotions', '•', 'Apply', 'Natural', 'Language', 'Processing', '(', 'NLP', ')', 'techniques', 'and', 'derive', 'the', 'meaningful', 'insights', '.'] 

 TOTAL TOKENS ==> 24

 ---- POST ----

 [('Classify', 'VB'), ('them', 'PRP'), ('into', 'IN'), ('different', 'JJ'), ('types', 'NNS'), ('of', 'IN'), ('sentiments', 'NNS'), ('/', 'JJ'), ('emotions', 'NNS'), ('•', 'VBP'), ('Apply', 'NNP'), ('Natural', 'NNP'), ('Language', 'NNP'), ('Processing', 'NNP'), ('(', '('), ('NLP', 'NNP'), (')', ')'), ('techniques', 'NNS'), ('and', 'CC'), ('derive', 'VB'), ('the', 'DT'), ('meaningful', 'JJ'), ('insights', 'NNS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Classify', 'different', 'types', 'sentiments', '/', 'emotions', '•', 'Apply', 'Natural', 'Language', 'Processing', '(', 'NLP', ')', 'techniques', 'derive', 'meaningful', 'insights', '.']

 TOTAL FILTERED TOKENS ==>  19

 ---- POST FOR FILTERED TOKENS ----

 [('Classify', 'NNP'), ('different', 'JJ'), ('types', 'NNS'), ('sentiments', 'NNS'), ('/', 'VBP'), ('emotions', 'NNS'), ('•', 'VBP'), ('Apply', 'NNP'), ('Natural', 'NNP'), ('Language', 'NNP'), ('Processing', 'NNP'), ('(', '('), ('NLP', 'NNP'), (')', ')'), ('techniques', 'VBZ'), ('derive', 'JJ'), ('meaningful', 'JJ'), ('insights', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Classify different', 'different types', 'types sentiments', 'sentiments /', '/ emotions', 'emotions •', '• Apply', 'Apply Natural', 'Natural Language', 'Language Processing', 'Processing (', '( NLP', 'NLP )', ') techniques', 'techniques derive', 'derive meaningful', 'meaningful insights', 'insights .'] 

 TOTAL BIGRAMS --> 18 



 ---- TRI-GRAMS ---- 

 ['Classify different types', 'different types sentiments', 'types sentiments /', 'sentiments / emotions', '/ emotions •', 'emotions • Apply', '• Apply Natural', 'Apply Natural Language', 'Natural Language Processing', 'Language Processing (', 'Processing ( NLP', '( NLP )', 'NLP ) techniques', ') techniques derive', 'techniques derive meaningful', 'derive meaningful insights', 'meaningful insights .'] 

 TOTAL TRIGRAMS --> 17 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> ['Apply Natural Language']
 TOTAL PERSON ENTITY --> 1 


 GPE ---> ['Classify']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['classifi', 'differ', 'type', 'sentiment', '/', 'emot', '•', 'appli', 'natur', 'languag', 'process', '(', 'nlp', ')', 'techniqu', 'deriv', 'meaning', 'insight', '.']

 TOTAL PORTER STEM WORDS ==> 19



 ---- SNOWBALL STEMMING ----

['classifi', 'differ', 'type', 'sentiment', '/', 'emot', '•', 'appli', 'natur', 'languag', 'process', '(', 'nlp', ')', 'techniqu', 'deriv', 'meaning', 'insight', '.']

 TOTAL SNOWBALL STEM WORDS ==> 19



 ---- LEMMATIZATION ----

['Classify', 'different', 'type', 'sentiment', '/', 'emotion', '•', 'Apply', 'Natural', 'Language', 'Processing', '(', 'NLP', ')', 'technique', 'derive', 'meaningful', 'insight', '.']

 TOTAL LEMMATIZE WORDS ==> 19

************************************************************************************************************************

27 --> • Cleanse the data to exclude sarcasms, idioms and metaphors, etc. 


 ---- TOKENS ----

 ['•', 'Cleanse', 'the', 'data', 'to', 'exclude', 'sarcasms', ',', 'idioms', 'and', 'metaphors', ',', 'etc', '.'] 

 TOTAL TOKENS ==> 14

 ---- POST ----

 [('•', 'NN'), ('Cleanse', 'NNP'), ('the', 'DT'), ('data', 'NN'), ('to', 'TO'), ('exclude', 'VB'), ('sarcasms', 'NNS'), (',', ','), ('idioms', 'NNS'), ('and', 'CC'), ('metaphors', 'NNS'), (',', ','), ('etc', 'FW'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['•', 'Cleanse', 'data', 'exclude', 'sarcasms', ',', 'idioms', 'metaphors', ',', 'etc', '.']

 TOTAL FILTERED TOKENS ==>  11

 ---- POST FOR FILTERED TOKENS ----

 [('•', 'JJ'), ('Cleanse', 'NNP'), ('data', 'NNS'), ('exclude', 'NN'), ('sarcasms', 'NNS'), (',', ','), ('idioms', 'NNS'), ('metaphors', 'NNS'), (',', ','), ('etc', 'FW'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['• Cleanse', 'Cleanse data', 'data exclude', 'exclude sarcasms', 'sarcasms ,', ', idioms', 'idioms metaphors', 'metaphors ,', ', etc', 'etc .'] 

 TOTAL BIGRAMS --> 10 



 ---- TRI-GRAMS ---- 

 ['• Cleanse data', 'Cleanse data exclude', 'data exclude sarcasms', 'exclude sarcasms ,', 'sarcasms , idioms', ', idioms metaphors', 'idioms metaphors ,', 'metaphors , etc', ', etc .'] 

 TOTAL TRIGRAMS --> 9 



 ---- NOUN PHRASES ---- 

 ['exclude'] 

 TOTAL NOUN PHRASES --> 1 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['•', 'cleans', 'data', 'exclud', 'sarcasm', ',', 'idiom', 'metaphor', ',', 'etc', '.']

 TOTAL PORTER STEM WORDS ==> 11



 ---- SNOWBALL STEMMING ----

['•', 'cleans', 'data', 'exclud', 'sarcasm', ',', 'idiom', 'metaphor', ',', 'etc', '.']

 TOTAL SNOWBALL STEM WORDS ==> 11



 ---- LEMMATIZATION ----

['•', 'Cleanse', 'data', 'exclude', 'sarcasm', ',', 'idiom', 'metaphor', ',', 'etc', '.']

 TOTAL LEMMATIZE WORDS ==> 11

************************************************************************************************************************

28 --> • Conduct the trend analysis across different social media accounts and find the commonality in the  sentiments. 


 ---- TOKENS ----

 ['•', 'Conduct', 'the', 'trend', 'analysis', 'across', 'different', 'social', 'media', 'accounts', 'and', 'find', 'the', 'commonality', 'in', 'the', 'sentiments', '.'] 

 TOTAL TOKENS ==> 18

 ---- POST ----

 [('•', 'JJ'), ('Conduct', 'NNP'), ('the', 'DT'), ('trend', 'NN'), ('analysis', 'NN'), ('across', 'IN'), ('different', 'JJ'), ('social', 'JJ'), ('media', 'NNS'), ('accounts', 'NNS'), ('and', 'CC'), ('find', 'VB'), ('the', 'DT'), ('commonality', 'NN'), ('in', 'IN'), ('the', 'DT'), ('sentiments', 'NNS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['•', 'Conduct', 'trend', 'analysis', 'across', 'different', 'social', 'media', 'accounts', 'find', 'commonality', 'sentiments', '.']

 TOTAL FILTERED TOKENS ==>  13

 ---- POST FOR FILTERED TOKENS ----

 [('•', 'JJ'), ('Conduct', 'NNP'), ('trend', 'NN'), ('analysis', 'NN'), ('across', 'IN'), ('different', 'JJ'), ('social', 'JJ'), ('media', 'NNS'), ('accounts', 'NNS'), ('find', 'VBP'), ('commonality', 'NN'), ('sentiments', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['• Conduct', 'Conduct trend', 'trend analysis', 'analysis across', 'across different', 'different social', 'social media', 'media accounts', 'accounts find', 'find commonality', 'commonality sentiments', 'sentiments .'] 

 TOTAL BIGRAMS --> 12 



 ---- TRI-GRAMS ---- 

 ['• Conduct trend', 'Conduct trend analysis', 'trend analysis across', 'analysis across different', 'across different social', 'different social media', 'social media accounts', 'media accounts find', 'accounts find commonality', 'find commonality sentiments', 'commonality sentiments .'] 

 TOTAL TRIGRAMS --> 11 



 ---- NOUN PHRASES ---- 

 ['trend', 'analysis', 'commonality'] 

 TOTAL NOUN PHRASES --> 3 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['•', 'conduct', 'trend', 'analysi', 'across', 'differ', 'social', 'media', 'account', 'find', 'common', 'sentiment', '.']

 TOTAL PORTER STEM WORDS ==> 13



 ---- SNOWBALL STEMMING ----

['•', 'conduct', 'trend', 'analysi', 'across', 'differ', 'social', 'media', 'account', 'find', 'common', 'sentiment', '.']

 TOTAL SNOWBALL STEM WORDS ==> 13



 ---- LEMMATIZATION ----

['•', 'Conduct', 'trend', 'analysis', 'across', 'different', 'social', 'medium', 'account', 'find', 'commonality', 'sentiment', '.']

 TOTAL LEMMATIZE WORDS ==> 13

************************************************************************************************************************

29 --> V. LIMITATION OF SENTIMENT ANALYSIS  Customer’s sentiment about the product or brand may be influenced by many factors. 


 ---- TOKENS ----

 ['V.', 'LIMITATION', 'OF', 'SENTIMENT', 'ANALYSIS', 'Customer', '’', 's', 'sentiment', 'about', 'the', 'product', 'or', 'brand', 'may', 'be', 'influenced', 'by', 'many', 'factors', '.'] 

 TOTAL TOKENS ==> 21

 ---- POST ----

 [('V.', 'NNP'), ('LIMITATION', 'NNP'), ('OF', 'NNP'), ('SENTIMENT', 'NNP'), ('ANALYSIS', 'NNP'), ('Customer', 'NNP'), ('’', 'NNP'), ('s', 'JJ'), ('sentiment', 'NN'), ('about', 'IN'), ('the', 'DT'), ('product', 'NN'), ('or', 'CC'), ('brand', 'NN'), ('may', 'MD'), ('be', 'VB'), ('influenced', 'VBN'), ('by', 'IN'), ('many', 'JJ'), ('factors', 'NNS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['V.', 'LIMITATION', 'SENTIMENT', 'ANALYSIS', 'Customer', '’', 'sentiment', 'product', 'brand', 'may', 'influenced', 'many', 'factors', '.']

 TOTAL FILTERED TOKENS ==>  14

 ---- POST FOR FILTERED TOKENS ----

 [('V.', 'NNP'), ('LIMITATION', 'NNP'), ('SENTIMENT', 'NNP'), ('ANALYSIS', 'NNP'), ('Customer', 'NNP'), ('’', 'NNP'), ('sentiment', 'NN'), ('product', 'NN'), ('brand', 'NN'), ('may', 'MD'), ('influenced', 'VB'), ('many', 'JJ'), ('factors', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['V. LIMITATION', 'LIMITATION SENTIMENT', 'SENTIMENT ANALYSIS', 'ANALYSIS Customer', 'Customer ’', '’ sentiment', 'sentiment product', 'product brand', 'brand may', 'may influenced', 'influenced many', 'many factors', 'factors .'] 

 TOTAL BIGRAMS --> 13 



 ---- TRI-GRAMS ---- 

 ['V. LIMITATION SENTIMENT', 'LIMITATION SENTIMENT ANALYSIS', 'SENTIMENT ANALYSIS Customer', 'ANALYSIS Customer ’', 'Customer ’ sentiment', '’ sentiment product', 'sentiment product brand', 'product brand may', 'brand may influenced', 'may influenced many', 'influenced many factors', 'many factors .'] 

 TOTAL TRIGRAMS --> 12 



 ---- NOUN PHRASES ---- 

 ['sentiment', 'product', 'brand'] 

 TOTAL NOUN PHRASES --> 3 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['v.', 'limit', 'sentiment', 'analysi', 'custom', '’', 'sentiment', 'product', 'brand', 'may', 'influenc', 'mani', 'factor', '.']

 TOTAL PORTER STEM WORDS ==> 14



 ---- SNOWBALL STEMMING ----

['v.', 'limit', 'sentiment', 'analysi', 'custom', '’', 'sentiment', 'product', 'brand', 'may', 'influenc', 'mani', 'factor', '.']

 TOTAL SNOWBALL STEM WORDS ==> 14



 ---- LEMMATIZATION ----

['V.', 'LIMITATION', 'SENTIMENT', 'ANALYSIS', 'Customer', '’', 'sentiment', 'product', 'brand', 'may', 'influenced', 'many', 'factor', '.']

 TOTAL LEMMATIZE WORDS ==> 14

************************************************************************************************************************

30 --> He might have a bad day  and it may directly influence his remark negatively. 


 ---- TOKENS ----

 ['He', 'might', 'have', 'a', 'bad', 'day', 'and', 'it', 'may', 'directly', 'influence', 'his', 'remark', 'negatively', '.'] 

 TOTAL TOKENS ==> 15

 ---- POST ----

 [('He', 'PRP'), ('might', 'MD'), ('have', 'VB'), ('a', 'DT'), ('bad', 'JJ'), ('day', 'NN'), ('and', 'CC'), ('it', 'PRP'), ('may', 'MD'), ('directly', 'RB'), ('influence', 'VB'), ('his', 'PRP$'), ('remark', 'NN'), ('negatively', 'RB'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['might', 'bad', 'day', 'may', 'directly', 'influence', 'remark', 'negatively', '.']

 TOTAL FILTERED TOKENS ==>  9

 ---- POST FOR FILTERED TOKENS ----

 [('might', 'MD'), ('bad', 'JJ'), ('day', 'NN'), ('may', 'MD'), ('directly', 'RB'), ('influence', 'VB'), ('remark', 'NN'), ('negatively', 'RB'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['might bad', 'bad day', 'day may', 'may directly', 'directly influence', 'influence remark', 'remark negatively', 'negatively .'] 

 TOTAL BIGRAMS --> 8 



 ---- TRI-GRAMS ---- 

 ['might bad day', 'bad day may', 'day may directly', 'may directly influence', 'directly influence remark', 'influence remark negatively', 'remark negatively .'] 

 TOTAL TRIGRAMS --> 7 



 ---- NOUN PHRASES ---- 

 ['bad day', 'remark'] 

 TOTAL NOUN PHRASES --> 2 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['might', 'bad', 'day', 'may', 'directli', 'influenc', 'remark', 'neg', '.']

 TOTAL PORTER STEM WORDS ==> 9



 ---- SNOWBALL STEMMING ----

['might', 'bad', 'day', 'may', 'direct', 'influenc', 'remark', 'negat', '.']

 TOTAL SNOWBALL STEM WORDS ==> 9



 ---- LEMMATIZATION ----

['might', 'bad', 'day', 'may', 'directly', 'influence', 'remark', 'negatively', '.']

 TOTAL LEMMATIZE WORDS ==> 9

************************************************************************************************************************

31 --> Also, sentiment can change over a period based on his mood. 


 ---- TOKENS ----

 ['Also', ',', 'sentiment', 'can', 'change', 'over', 'a', 'period', 'based', 'on', 'his', 'mood', '.'] 

 TOTAL TOKENS ==> 13

 ---- POST ----

 [('Also', 'RB'), (',', ','), ('sentiment', 'NN'), ('can', 'MD'), ('change', 'VB'), ('over', 'IN'), ('a', 'DT'), ('period', 'NN'), ('based', 'VBN'), ('on', 'IN'), ('his', 'PRP$'), ('mood', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Also', ',', 'sentiment', 'change', 'period', 'based', 'mood', '.']

 TOTAL FILTERED TOKENS ==>  8

 ---- POST FOR FILTERED TOKENS ----

 [('Also', 'RB'), (',', ','), ('sentiment', 'JJ'), ('change', 'NN'), ('period', 'NN'), ('based', 'VBN'), ('mood', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Also ,', ', sentiment', 'sentiment change', 'change period', 'period based', 'based mood', 'mood .'] 

 TOTAL BIGRAMS --> 7 



 ---- TRI-GRAMS ---- 

 ['Also , sentiment', ', sentiment change', 'sentiment change period', 'change period based', 'period based mood', 'based mood .'] 

 TOTAL TRIGRAMS --> 6 



 ---- NOUN PHRASES ---- 

 ['sentiment change', 'period', 'mood'] 

 TOTAL NOUN PHRASES --> 3 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['also', ',', 'sentiment', 'chang', 'period', 'base', 'mood', '.']

 TOTAL PORTER STEM WORDS ==> 8



 ---- SNOWBALL STEMMING ----

['also', ',', 'sentiment', 'chang', 'period', 'base', 'mood', '.']

 TOTAL SNOWBALL STEM WORDS ==> 8



 ---- LEMMATIZATION ----

['Also', ',', 'sentiment', 'change', 'period', 'based', 'mood', '.']

 TOTAL LEMMATIZE WORDS ==> 8

************************************************************************************************************************

32 --> So, it is advisable to go with large sample of data. 


 ---- TOKENS ----

 ['So', ',', 'it', 'is', 'advisable', 'to', 'go', 'with', 'large', 'sample', 'of', 'data', '.'] 

 TOTAL TOKENS ==> 13

 ---- POST ----

 [('So', 'RB'), (',', ','), ('it', 'PRP'), ('is', 'VBZ'), ('advisable', 'JJ'), ('to', 'TO'), ('go', 'VB'), ('with', 'IN'), ('large', 'JJ'), ('sample', 'NN'), ('of', 'IN'), ('data', 'NNS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 [',', 'advisable', 'go', 'large', 'sample', 'data', '.']

 TOTAL FILTERED TOKENS ==>  7

 ---- POST FOR FILTERED TOKENS ----

 [(',', ','), ('advisable', 'JJ'), ('go', 'VBP'), ('large', 'JJ'), ('sample', 'NN'), ('data', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 [', advisable', 'advisable go', 'go large', 'large sample', 'sample data', 'data .'] 

 TOTAL BIGRAMS --> 6 



 ---- TRI-GRAMS ---- 

 [', advisable go', 'advisable go large', 'go large sample', 'large sample data', 'sample data .'] 

 TOTAL TRIGRAMS --> 5 



 ---- NOUN PHRASES ---- 

 ['large sample'] 

 TOTAL NOUN PHRASES --> 1 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

[',', 'advis', 'go', 'larg', 'sampl', 'data', '.']

 TOTAL PORTER STEM WORDS ==> 7



 ---- SNOWBALL STEMMING ----

[',', 'advis', 'go', 'larg', 'sampl', 'data', '.']

 TOTAL SNOWBALL STEM WORDS ==> 7



 ---- LEMMATIZATION ----

[',', 'advisable', 'go', 'large', 'sample', 'data', '.']

 TOTAL LEMMATIZE WORDS ==> 7

************************************************************************************************************************

33 --> It will be difficult for an algorithm to understand the sarcasm  and ironic language while interpret the sentiment in isolation. 


 ---- TOKENS ----

 ['It', 'will', 'be', 'difficult', 'for', 'an', 'algorithm', 'to', 'understand', 'the', 'sarcasm', 'and', 'ironic', 'language', 'while', 'interpret', 'the', 'sentiment', 'in', 'isolation', '.'] 

 TOTAL TOKENS ==> 21

 ---- POST ----

 [('It', 'PRP'), ('will', 'MD'), ('be', 'VB'), ('difficult', 'JJ'), ('for', 'IN'), ('an', 'DT'), ('algorithm', 'NN'), ('to', 'TO'), ('understand', 'VB'), ('the', 'DT'), ('sarcasm', 'NN'), ('and', 'CC'), ('ironic', 'JJ'), ('language', 'NN'), ('while', 'IN'), ('interpret', 'VBG'), ('the', 'DT'), ('sentiment', 'NN'), ('in', 'IN'), ('isolation', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['difficult', 'algorithm', 'understand', 'sarcasm', 'ironic', 'language', 'interpret', 'sentiment', 'isolation', '.']

 TOTAL FILTERED TOKENS ==>  10

 ---- POST FOR FILTERED TOKENS ----

 [('difficult', 'JJ'), ('algorithm', 'JJ'), ('understand', 'NN'), ('sarcasm', 'NN'), ('ironic', 'JJ'), ('language', 'NN'), ('interpret', 'JJ'), ('sentiment', 'NN'), ('isolation', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['difficult algorithm', 'algorithm understand', 'understand sarcasm', 'sarcasm ironic', 'ironic language', 'language interpret', 'interpret sentiment', 'sentiment isolation', 'isolation .'] 

 TOTAL BIGRAMS --> 9 



 ---- TRI-GRAMS ---- 

 ['difficult algorithm understand', 'algorithm understand sarcasm', 'understand sarcasm ironic', 'sarcasm ironic language', 'ironic language interpret', 'language interpret sentiment', 'interpret sentiment isolation', 'sentiment isolation .'] 

 TOTAL TRIGRAMS --> 8 



 ---- NOUN PHRASES ---- 

 ['difficult algorithm understand', 'sarcasm', 'ironic language', 'interpret sentiment', 'isolation'] 

 TOTAL NOUN PHRASES --> 5 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['difficult', 'algorithm', 'understand', 'sarcasm', 'iron', 'languag', 'interpret', 'sentiment', 'isol', '.']

 TOTAL PORTER STEM WORDS ==> 10



 ---- SNOWBALL STEMMING ----

['difficult', 'algorithm', 'understand', 'sarcasm', 'iron', 'languag', 'interpret', 'sentiment', 'isol', '.']

 TOTAL SNOWBALL STEM WORDS ==> 10



 ---- LEMMATIZATION ----

['difficult', 'algorithm', 'understand', 'sarcasm', 'ironic', 'language', 'interpret', 'sentiment', 'isolation', '.']

 TOTAL LEMMATIZE WORDS ==> 10

************************************************************************************************************************

34 --> So, there is a need to train the model rigorously. 


 ---- TOKENS ----

 ['So', ',', 'there', 'is', 'a', 'need', 'to', 'train', 'the', 'model', 'rigorously', '.'] 

 TOTAL TOKENS ==> 12

 ---- POST ----

 [('So', 'RB'), (',', ','), ('there', 'EX'), ('is', 'VBZ'), ('a', 'DT'), ('need', 'NN'), ('to', 'TO'), ('train', 'VB'), ('the', 'DT'), ('model', 'NN'), ('rigorously', 'RB'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 [',', 'need', 'train', 'model', 'rigorously', '.']

 TOTAL FILTERED TOKENS ==>  6

 ---- POST FOR FILTERED TOKENS ----

 [(',', ','), ('need', 'VBP'), ('train', 'VBP'), ('model', 'NN'), ('rigorously', 'RB'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 [', need', 'need train', 'train model', 'model rigorously', 'rigorously .'] 

 TOTAL BIGRAMS --> 5 



 ---- TRI-GRAMS ---- 

 [', need train', 'need train model', 'train model rigorously', 'model rigorously .'] 

 TOTAL TRIGRAMS --> 4 



 ---- NOUN PHRASES ---- 

 ['model'] 

 TOTAL NOUN PHRASES --> 1 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

[',', 'need', 'train', 'model', 'rigor', '.']

 TOTAL PORTER STEM WORDS ==> 6



 ---- SNOWBALL STEMMING ----

[',', 'need', 'train', 'model', 'rigor', '.']

 TOTAL SNOWBALL STEM WORDS ==> 6



 ---- LEMMATIZATION ----

[',', 'need', 'train', 'model', 'rigorously', '.']

 TOTAL LEMMATIZE WORDS ==> 6

************************************************************************************************************************

35 --> VI. 


 ---- TOKENS ----

 ['VI', '.'] 

 TOTAL TOKENS ==> 2

 ---- POST ----

 [('VI', 'NNP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['VI', '.']

 TOTAL FILTERED TOKENS ==>  2

 ---- POST FOR FILTERED TOKENS ----

 [('VI', 'NNP'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['VI .'] 

 TOTAL BIGRAMS --> 1 



 ---- TRI-GRAMS ---- 

 [] 

 TOTAL TRIGRAMS --> 0 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['vi', '.']

 TOTAL PORTER STEM WORDS ==> 2



 ---- SNOWBALL STEMMING ----

['vi', '.']

 TOTAL SNOWBALL STEM WORDS ==> 2



 ---- LEMMATIZATION ----

['VI', '.']

 TOTAL LEMMATIZE WORDS ==> 2

************************************************************************************************************************

36 --> CONCLUSION  There are many studies available on the implementation of the sentiment analysis which provides helping hand  to ecommerce companies to get to know about their brand reputation in the market but comprehensive details  on this topic is required. 


 ---- TOKENS ----

 ['CONCLUSION', 'There', 'are', 'many', 'studies', 'available', 'on', 'the', 'implementation', 'of', 'the', 'sentiment', 'analysis', 'which', 'provides', 'helping', 'hand', 'to', 'ecommerce', 'companies', 'to', 'get', 'to', 'know', 'about', 'their', 'brand', 'reputation', 'in', 'the', 'market', 'but', 'comprehensive', 'details', 'on', 'this', 'topic', 'is', 'required', '.'] 

 TOTAL TOKENS ==> 40

 ---- POST ----

 [('CONCLUSION', 'NN'), ('There', 'EX'), ('are', 'VBP'), ('many', 'JJ'), ('studies', 'NNS'), ('available', 'JJ'), ('on', 'IN'), ('the', 'DT'), ('implementation', 'NN'), ('of', 'IN'), ('the', 'DT'), ('sentiment', 'NN'), ('analysis', 'NN'), ('which', 'WDT'), ('provides', 'VBZ'), ('helping', 'VBG'), ('hand', 'NN'), ('to', 'TO'), ('ecommerce', 'VB'), ('companies', 'NNS'), ('to', 'TO'), ('get', 'VB'), ('to', 'TO'), ('know', 'VB'), ('about', 'IN'), ('their', 'PRP$'), ('brand', 'NN'), ('reputation', 'NN'), ('in', 'IN'), ('the', 'DT'), ('market', 'NN'), ('but', 'CC'), ('comprehensive', 'JJ'), ('details', 'NNS'), ('on', 'IN'), ('this', 'DT'), ('topic', 'NN'), ('is', 'VBZ'), ('required', 'VBN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['CONCLUSION', 'many', 'studies', 'available', 'implementation', 'sentiment', 'analysis', 'provides', 'helping', 'hand', 'ecommerce', 'companies', 'get', 'know', 'brand', 'reputation', 'market', 'comprehensive', 'details', 'topic', 'required', '.']

 TOTAL FILTERED TOKENS ==>  22

 ---- POST FOR FILTERED TOKENS ----

 [('CONCLUSION', 'NNP'), ('many', 'JJ'), ('studies', 'NNS'), ('available', 'JJ'), ('implementation', 'NN'), ('sentiment', 'NN'), ('analysis', 'NN'), ('provides', 'VBZ'), ('helping', 'VBG'), ('hand', 'NN'), ('ecommerce', 'NN'), ('companies', 'NNS'), ('get', 'VBP'), ('know', 'JJ'), ('brand', 'NN'), ('reputation', 'NN'), ('market', 'NN'), ('comprehensive', 'NN'), ('details', 'NNS'), ('topic', 'NN'), ('required', 'VBN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['CONCLUSION many', 'many studies', 'studies available', 'available implementation', 'implementation sentiment', 'sentiment analysis', 'analysis provides', 'provides helping', 'helping hand', 'hand ecommerce', 'ecommerce companies', 'companies get', 'get know', 'know brand', 'brand reputation', 'reputation market', 'market comprehensive', 'comprehensive details', 'details topic', 'topic required', 'required .'] 

 TOTAL BIGRAMS --> 21 



 ---- TRI-GRAMS ---- 

 ['CONCLUSION many studies', 'many studies available', 'studies available implementation', 'available implementation sentiment', 'implementation sentiment analysis', 'sentiment analysis provides', 'analysis provides helping', 'provides helping hand', 'helping hand ecommerce', 'hand ecommerce companies', 'ecommerce companies get', 'companies get know', 'get know brand', 'know brand reputation', 'brand reputation market', 'reputation market comprehensive', 'market comprehensive details', 'comprehensive details topic', 'details topic required', 'topic required .'] 

 TOTAL TRIGRAMS --> 20 



 ---- NOUN PHRASES ---- 

 ['available implementation', 'sentiment', 'analysis', 'hand', 'ecommerce', 'know brand', 'reputation', 'market', 'comprehensive', 'topic'] 

 TOTAL NOUN PHRASES --> 10 



 ---- NER ----

 
 ORGANIZATION ---> ['CONCLUSION']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['conclus', 'mani', 'studi', 'avail', 'implement', 'sentiment', 'analysi', 'provid', 'help', 'hand', 'ecommerc', 'compani', 'get', 'know', 'brand', 'reput', 'market', 'comprehens', 'detail', 'topic', 'requir', '.']

 TOTAL PORTER STEM WORDS ==> 22



 ---- SNOWBALL STEMMING ----

['conclus', 'mani', 'studi', 'avail', 'implement', 'sentiment', 'analysi', 'provid', 'help', 'hand', 'ecommerc', 'compani', 'get', 'know', 'brand', 'reput', 'market', 'comprehens', 'detail', 'topic', 'requir', '.']

 TOTAL SNOWBALL STEM WORDS ==> 22



 ---- LEMMATIZATION ----

['CONCLUSION', 'many', 'study', 'available', 'implementation', 'sentiment', 'analysis', 'provides', 'helping', 'hand', 'ecommerce', 'company', 'get', 'know', 'brand', 'reputation', 'market', 'comprehensive', 'detail', 'topic', 'required', '.']

 TOTAL LEMMATIZE WORDS ==> 22

************************************************************************************************************************

37 --> In this study, we have discussed the processes to be followed, best practices and  limitations of Sentiment analysis. 


 ---- TOKENS ----

 ['In', 'this', 'study', ',', 'we', 'have', 'discussed', 'the', 'processes', 'to', 'be', 'followed', ',', 'best', 'practices', 'and', 'limitations', 'of', 'Sentiment', 'analysis', '.'] 

 TOTAL TOKENS ==> 21

 ---- POST ----

 [('In', 'IN'), ('this', 'DT'), ('study', 'NN'), (',', ','), ('we', 'PRP'), ('have', 'VBP'), ('discussed', 'VBN'), ('the', 'DT'), ('processes', 'NNS'), ('to', 'TO'), ('be', 'VB'), ('followed', 'VBN'), (',', ','), ('best', 'JJS'), ('practices', 'NNS'), ('and', 'CC'), ('limitations', 'NNS'), ('of', 'IN'), ('Sentiment', 'NN'), ('analysis', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['study', ',', 'discussed', 'processes', 'followed', ',', 'best', 'practices', 'limitations', 'Sentiment', 'analysis', '.']

 TOTAL FILTERED TOKENS ==>  12

 ---- POST FOR FILTERED TOKENS ----

 [('study', 'NN'), (',', ','), ('discussed', 'VBN'), ('processes', 'NNS'), ('followed', 'VBD'), (',', ','), ('best', 'JJS'), ('practices', 'NNS'), ('limitations', 'NNS'), ('Sentiment', 'NNP'), ('analysis', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['study ,', ', discussed', 'discussed processes', 'processes followed', 'followed ,', ', best', 'best practices', 'practices limitations', 'limitations Sentiment', 'Sentiment analysis', 'analysis .'] 

 TOTAL BIGRAMS --> 11 



 ---- TRI-GRAMS ---- 

 ['study , discussed', ', discussed processes', 'discussed processes followed', 'processes followed ,', 'followed , best', ', best practices', 'best practices limitations', 'practices limitations Sentiment', 'limitations Sentiment analysis', 'Sentiment analysis .'] 

 TOTAL TRIGRAMS --> 10 



 ---- NOUN PHRASES ---- 

 ['study', 'analysis'] 

 TOTAL NOUN PHRASES --> 2 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['studi', ',', 'discuss', 'process', 'follow', ',', 'best', 'practic', 'limit', 'sentiment', 'analysi', '.']

 TOTAL PORTER STEM WORDS ==> 12



 ---- SNOWBALL STEMMING ----

['studi', ',', 'discuss', 'process', 'follow', ',', 'best', 'practic', 'limit', 'sentiment', 'analysi', '.']

 TOTAL SNOWBALL STEM WORDS ==> 12



 ---- LEMMATIZATION ----

['study', ',', 'discussed', 'process', 'followed', ',', 'best', 'practice', 'limitation', 'Sentiment', 'analysis', '.']

 TOTAL LEMMATIZE WORDS ==> 12

************************************************************************************************************************

38 --> This study will provide Artificial Intelligence Practitioners a better view on how  to approach Sentiment Analysis. 


 ---- TOKENS ----

 ['This', 'study', 'will', 'provide', 'Artificial', 'Intelligence', 'Practitioners', 'a', 'better', 'view', 'on', 'how', 'to', 'approach', 'Sentiment', 'Analysis', '.'] 

 TOTAL TOKENS ==> 17

 ---- POST ----

 [('This', 'DT'), ('study', 'NN'), ('will', 'MD'), ('provide', 'VB'), ('Artificial', 'NNP'), ('Intelligence', 'NNP'), ('Practitioners', 'NNP'), ('a', 'DT'), ('better', 'JJR'), ('view', 'NN'), ('on', 'IN'), ('how', 'WRB'), ('to', 'TO'), ('approach', 'VB'), ('Sentiment', 'JJ'), ('Analysis', 'NNP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['study', 'provide', 'Artificial', 'Intelligence', 'Practitioners', 'better', 'view', 'approach', 'Sentiment', 'Analysis', '.']

 TOTAL FILTERED TOKENS ==>  11

 ---- POST FOR FILTERED TOKENS ----

 [('study', 'NN'), ('provide', 'VBP'), ('Artificial', 'NNP'), ('Intelligence', 'NNP'), ('Practitioners', 'NNP'), ('better', 'RBR'), ('view', 'NN'), ('approach', 'VBP'), ('Sentiment', 'NN'), ('Analysis', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['study provide', 'provide Artificial', 'Artificial Intelligence', 'Intelligence Practitioners', 'Practitioners better', 'better view', 'view approach', 'approach Sentiment', 'Sentiment Analysis', 'Analysis .'] 

 TOTAL BIGRAMS --> 10 



 ---- TRI-GRAMS ---- 

 ['study provide Artificial', 'provide Artificial Intelligence', 'Artificial Intelligence Practitioners', 'Intelligence Practitioners better', 'Practitioners better view', 'better view approach', 'view approach Sentiment', 'approach Sentiment Analysis', 'Sentiment Analysis .'] 

 TOTAL TRIGRAMS --> 9 



 ---- NOUN PHRASES ---- 

 ['study', 'view', 'Sentiment', 'Analysis'] 

 TOTAL NOUN PHRASES --> 4 



 ---- NER ----

 
 ORGANIZATION ---> ['Artificial Intelligence', 'Sentiment Analysis']
 TOTAL ORGANIZATION ENTITY --> 2 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['studi', 'provid', 'artifici', 'intellig', 'practition', 'better', 'view', 'approach', 'sentiment', 'analysi', '.']

 TOTAL PORTER STEM WORDS ==> 11



 ---- SNOWBALL STEMMING ----

['studi', 'provid', 'artifici', 'intellig', 'practition', 'better', 'view', 'approach', 'sentiment', 'analysi', '.']

 TOTAL SNOWBALL STEM WORDS ==> 11



 ---- LEMMATIZATION ----

['study', 'provide', 'Artificial', 'Intelligence', 'Practitioners', 'better', 'view', 'approach', 'Sentiment', 'Analysis', '.']

 TOTAL LEMMATIZE WORDS ==> 11

************************************************************************************************************************

39 --> REFERENCES  1. 


 ---- TOKENS ----

 ['REFERENCES', '1', '.'] 

 TOTAL TOKENS ==> 3

 ---- POST ----

 [('REFERENCES', 'RB'), ('1', 'CD'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['REFERENCES', '1', '.']

 TOTAL FILTERED TOKENS ==>  3

 ---- POST FOR FILTERED TOKENS ----

 [('REFERENCES', 'RB'), ('1', 'CD'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['REFERENCES 1', '1 .'] 

 TOTAL BIGRAMS --> 2 



 ---- TRI-GRAMS ---- 

 ['REFERENCES 1 .'] 

 TOTAL TRIGRAMS --> 1 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['refer', '1', '.']

 TOTAL PORTER STEM WORDS ==> 3



 ---- SNOWBALL STEMMING ----

['refer', '1', '.']

 TOTAL SNOWBALL STEM WORDS ==> 3



 ---- LEMMATIZATION ----

['REFERENCES', '1', '.']

 TOTAL LEMMATIZE WORDS ==> 3

************************************************************************************************************************

40 --> Liu, B. 


 ---- TOKENS ----

 ['Liu', ',', 'B', '.'] 

 TOTAL TOKENS ==> 4

 ---- POST ----

 [('Liu', 'NNP'), (',', ','), ('B', 'NNP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Liu', ',', 'B', '.']

 TOTAL FILTERED TOKENS ==>  4

 ---- POST FOR FILTERED TOKENS ----

 [('Liu', 'NNP'), (',', ','), ('B', 'NNP'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Liu ,', ', B', 'B .'] 

 TOTAL BIGRAMS --> 3 



 ---- TRI-GRAMS ---- 

 ['Liu , B', ', B .'] 

 TOTAL TRIGRAMS --> 2 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> ['Liu']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['liu', ',', 'b', '.']

 TOTAL PORTER STEM WORDS ==> 4



 ---- SNOWBALL STEMMING ----

['liu', ',', 'b', '.']

 TOTAL SNOWBALL STEM WORDS ==> 4



 ---- LEMMATIZATION ----

['Liu', ',', 'B', '.']

 TOTAL LEMMATIZE WORDS ==> 4

************************************************************************************************************************

41 --> 2006. 


 ---- TOKENS ----

 ['2006', '.'] 

 TOTAL TOKENS ==> 2

 ---- POST ----

 [('2006', 'CD'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['2006', '.']

 TOTAL FILTERED TOKENS ==>  2

 ---- POST FOR FILTERED TOKENS ----

 [('2006', 'CD'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['2006 .'] 

 TOTAL BIGRAMS --> 1 



 ---- TRI-GRAMS ---- 

 [] 

 TOTAL TRIGRAMS --> 0 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['2006', '.']

 TOTAL PORTER STEM WORDS ==> 2



 ---- SNOWBALL STEMMING ----

['2006', '.']

 TOTAL SNOWBALL STEM WORDS ==> 2



 ---- LEMMATIZATION ----

['2006', '.']

 TOTAL LEMMATIZE WORDS ==> 2

************************************************************************************************************************

42 --> Web Data Mining:  Exploring Hyperlinks, Contents, and Usage Data, Springer. 


 ---- TOKENS ----

 ['Web', 'Data', 'Mining', ':', 'Exploring', 'Hyperlinks', ',', 'Contents', ',', 'and', 'Usage', 'Data', ',', 'Springer', '.'] 

 TOTAL TOKENS ==> 15

 ---- POST ----

 [('Web', 'NNP'), ('Data', 'NNP'), ('Mining', 'NN'), (':', ':'), ('Exploring', 'JJ'), ('Hyperlinks', 'NNP'), (',', ','), ('Contents', 'NNP'), (',', ','), ('and', 'CC'), ('Usage', 'NNP'), ('Data', 'NNP'), (',', ','), ('Springer', 'NNP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Web', 'Data', 'Mining', ':', 'Exploring', 'Hyperlinks', ',', 'Contents', ',', 'Usage', 'Data', ',', 'Springer', '.']

 TOTAL FILTERED TOKENS ==>  14

 ---- POST FOR FILTERED TOKENS ----

 [('Web', 'NNP'), ('Data', 'NNP'), ('Mining', 'NN'), (':', ':'), ('Exploring', 'JJ'), ('Hyperlinks', 'NNP'), (',', ','), ('Contents', 'NNP'), (',', ','), ('Usage', 'NNP'), ('Data', 'NNP'), (',', ','), ('Springer', 'NNP'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Web Data', 'Data Mining', 'Mining :', ': Exploring', 'Exploring Hyperlinks', 'Hyperlinks ,', ', Contents', 'Contents ,', ', Usage', 'Usage Data', 'Data ,', ', Springer', 'Springer .'] 

 TOTAL BIGRAMS --> 13 



 ---- TRI-GRAMS ---- 

 ['Web Data Mining', 'Data Mining :', 'Mining : Exploring', ': Exploring Hyperlinks', 'Exploring Hyperlinks ,', 'Hyperlinks , Contents', ', Contents ,', 'Contents , Usage', ', Usage Data', 'Usage Data ,', 'Data , Springer', ', Springer .'] 

 TOTAL TRIGRAMS --> 12 



 ---- NOUN PHRASES ---- 

 ['Mining'] 

 TOTAL NOUN PHRASES --> 1 



 ---- NER ----

 
 ORGANIZATION ---> ['Contents']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> ['Usage Data', 'Springer']
 TOTAL PERSON ENTITY --> 2 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['web', 'data', 'mine', ':', 'explor', 'hyperlink', ',', 'content', ',', 'usag', 'data', ',', 'springer', '.']

 TOTAL PORTER STEM WORDS ==> 14



 ---- SNOWBALL STEMMING ----

['web', 'data', 'mine', ':', 'explor', 'hyperlink', ',', 'content', ',', 'usag', 'data', ',', 'springer', '.']

 TOTAL SNOWBALL STEM WORDS ==> 14



 ---- LEMMATIZATION ----

['Web', 'Data', 'Mining', ':', 'Exploring', 'Hyperlinks', ',', 'Contents', ',', 'Usage', 'Data', ',', 'Springer', '.']

 TOTAL LEMMATIZE WORDS ==> 14

************************************************************************************************************************

43 --> 2. 


 ---- TOKENS ----

 ['2', '.'] 

 TOTAL TOKENS ==> 2

 ---- POST ----

 [('2', 'CD'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['2', '.']

 TOTAL FILTERED TOKENS ==>  2

 ---- POST FOR FILTERED TOKENS ----

 [('2', 'CD'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['2 .'] 

 TOTAL BIGRAMS --> 1 



 ---- TRI-GRAMS ---- 

 [] 

 TOTAL TRIGRAMS --> 0 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['2', '.']

 TOTAL PORTER STEM WORDS ==> 2



 ---- SNOWBALL STEMMING ----

['2', '.']

 TOTAL SNOWBALL STEM WORDS ==> 2



 ---- LEMMATIZATION ----

['2', '.']

 TOTAL LEMMATIZE WORDS ==> 2

************************************************************************************************************************

44 --> Wiebe, J. 


 ---- TOKENS ----

 ['Wiebe', ',', 'J', '.'] 

 TOTAL TOKENS ==> 4

 ---- POST ----

 [('Wiebe', 'NNP'), (',', ','), ('J', 'NNP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Wiebe', ',', 'J', '.']

 TOTAL FILTERED TOKENS ==>  4

 ---- POST FOR FILTERED TOKENS ----

 [('Wiebe', 'NNP'), (',', ','), ('J', 'NNP'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Wiebe ,', ', J', 'J .'] 

 TOTAL BIGRAMS --> 3 



 ---- TRI-GRAMS ---- 

 ['Wiebe , J', ', J .'] 

 TOTAL TRIGRAMS --> 2 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> ['Wiebe']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['wieb', ',', 'j', '.']

 TOTAL PORTER STEM WORDS ==> 4



 ---- SNOWBALL STEMMING ----

['wieb', ',', 'j', '.']

 TOTAL SNOWBALL STEM WORDS ==> 4



 ---- LEMMATIZATION ----

['Wiebe', ',', 'J', '.']

 TOTAL LEMMATIZE WORDS ==> 4

************************************************************************************************************************

45 --> & Riloff, E.  2005. 


 ---- TOKENS ----

 ['&', 'Riloff', ',', 'E.', '2005', '.'] 

 TOTAL TOKENS ==> 6

 ---- POST ----

 [('&', 'CC'), ('Riloff', 'NNP'), (',', ','), ('E.', 'NNP'), ('2005', 'CD'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['&', 'Riloff', ',', 'E.', '2005', '.']

 TOTAL FILTERED TOKENS ==>  6

 ---- POST FOR FILTERED TOKENS ----

 [('&', 'CC'), ('Riloff', 'NNP'), (',', ','), ('E.', 'NNP'), ('2005', 'CD'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['& Riloff', 'Riloff ,', ', E.', 'E. 2005', '2005 .'] 

 TOTAL BIGRAMS --> 5 



 ---- TRI-GRAMS ---- 

 ['& Riloff ,', 'Riloff , E.', ', E. 2005', 'E. 2005 .'] 

 TOTAL TRIGRAMS --> 4 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> ['Riloff']
 TOTAL PERSON ENTITY --> 1 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['&', 'riloff', ',', 'e.', '2005', '.']

 TOTAL PORTER STEM WORDS ==> 6



 ---- SNOWBALL STEMMING ----

['&', 'riloff', ',', 'e.', '2005', '.']

 TOTAL SNOWBALL STEM WORDS ==> 6



 ---- LEMMATIZATION ----

['&', 'Riloff', ',', 'E.', '2005', '.']

 TOTAL LEMMATIZE WORDS ==> 6

************************************************************************************************************************

46 --> Creating subjective and objective sentence classifiers from unannotated texts. 


 ---- TOKENS ----

 ['Creating', 'subjective', 'and', 'objective', 'sentence', 'classifiers', 'from', 'unannotated', 'texts', '.'] 

 TOTAL TOKENS ==> 10

 ---- POST ----

 [('Creating', 'VBG'), ('subjective', 'JJ'), ('and', 'CC'), ('objective', 'JJ'), ('sentence', 'NN'), ('classifiers', 'NNS'), ('from', 'IN'), ('unannotated', 'JJ'), ('texts', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Creating', 'subjective', 'objective', 'sentence', 'classifiers', 'unannotated', 'texts', '.']

 TOTAL FILTERED TOKENS ==>  8

 ---- POST FOR FILTERED TOKENS ----

 [('Creating', 'VBG'), ('subjective', 'JJ'), ('objective', 'JJ'), ('sentence', 'NN'), ('classifiers', 'NNS'), ('unannotated', 'VBD'), ('texts', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Creating subjective', 'subjective objective', 'objective sentence', 'sentence classifiers', 'classifiers unannotated', 'unannotated texts', 'texts .'] 

 TOTAL BIGRAMS --> 7 



 ---- TRI-GRAMS ---- 

 ['Creating subjective objective', 'subjective objective sentence', 'objective sentence classifiers', 'sentence classifiers unannotated', 'classifiers unannotated texts', 'unannotated texts .'] 

 TOTAL TRIGRAMS --> 6 



 ---- NOUN PHRASES ---- 

 ['subjective objective sentence', 'texts'] 

 TOTAL NOUN PHRASES --> 2 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['creat', 'subject', 'object', 'sentenc', 'classifi', 'unannot', 'text', '.']

 TOTAL PORTER STEM WORDS ==> 8



 ---- SNOWBALL STEMMING ----

['creat', 'subject', 'object', 'sentenc', 'classifi', 'unannot', 'text', '.']

 TOTAL SNOWBALL STEM WORDS ==> 8



 ---- LEMMATIZATION ----

['Creating', 'subjective', 'objective', 'sentence', 'classifier', 'unannotated', 'text', '.']

 TOTAL LEMMATIZE WORDS ==> 8

************************************************************************************************************************

47 --> Computational Linguistics and Intelligent Text Processing, 2005, pp. 


 ---- TOKENS ----

 ['Computational', 'Linguistics', 'and', 'Intelligent', 'Text', 'Processing', ',', '2005', ',', 'pp', '.'] 

 TOTAL TOKENS ==> 11

 ---- POST ----

 [('Computational', 'JJ'), ('Linguistics', 'NNS'), ('and', 'CC'), ('Intelligent', 'NNP'), ('Text', 'NNP'), ('Processing', 'NNP'), (',', ','), ('2005', 'CD'), (',', ','), ('pp', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Computational', 'Linguistics', 'Intelligent', 'Text', 'Processing', ',', '2005', ',', 'pp', '.']

 TOTAL FILTERED TOKENS ==>  10

 ---- POST FOR FILTERED TOKENS ----

 [('Computational', 'JJ'), ('Linguistics', 'NNP'), ('Intelligent', 'NNP'), ('Text', 'NNP'), ('Processing', 'NNP'), (',', ','), ('2005', 'CD'), (',', ','), ('pp', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Computational Linguistics', 'Linguistics Intelligent', 'Intelligent Text', 'Text Processing', 'Processing ,', ', 2005', '2005 ,', ', pp', 'pp .'] 

 TOTAL BIGRAMS --> 9 



 ---- TRI-GRAMS ---- 

 ['Computational Linguistics Intelligent', 'Linguistics Intelligent Text', 'Intelligent Text Processing', 'Text Processing ,', 'Processing , 2005', ', 2005 ,', '2005 , pp', ', pp .'] 

 TOTAL TRIGRAMS --> 8 



 ---- NOUN PHRASES ---- 

 ['pp'] 

 TOTAL NOUN PHRASES --> 1 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['comput', 'linguist', 'intellig', 'text', 'process', ',', '2005', ',', 'pp', '.']

 TOTAL PORTER STEM WORDS ==> 10



 ---- SNOWBALL STEMMING ----

['comput', 'linguist', 'intellig', 'text', 'process', ',', '2005', ',', 'pp', '.']

 TOTAL SNOWBALL STEM WORDS ==> 10



 ---- LEMMATIZATION ----

['Computational', 'Linguistics', 'Intelligent', 'Text', 'Processing', ',', '2005', ',', 'pp', '.']

 TOTAL LEMMATIZE WORDS ==> 10

************************************************************************************************************************

48 --> 486-497. 


 ---- TOKENS ----

 ['486-497', '.'] 

 TOTAL TOKENS ==> 2

 ---- POST ----

 [('486-497', 'JJ'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['486-497', '.']

 TOTAL FILTERED TOKENS ==>  2

 ---- POST FOR FILTERED TOKENS ----

 [('486-497', 'JJ'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['486-497 .'] 

 TOTAL BIGRAMS --> 1 



 ---- TRI-GRAMS ---- 

 [] 

 TOTAL TRIGRAMS --> 0 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['486-497', '.']

 TOTAL PORTER STEM WORDS ==> 2



 ---- SNOWBALL STEMMING ----

['486-497', '.']

 TOTAL SNOWBALL STEM WORDS ==> 2



 ---- LEMMATIZATION ----

['486-497', '.']

 TOTAL LEMMATIZE WORDS ==> 2

************************************************************************************************************************

49 --> 3. 


 ---- TOKENS ----

 ['3', '.'] 

 TOTAL TOKENS ==> 2

 ---- POST ----

 [('3', 'CD'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['3', '.']

 TOTAL FILTERED TOKENS ==>  2

 ---- POST FOR FILTERED TOKENS ----

 [('3', 'CD'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['3 .'] 

 TOTAL BIGRAMS --> 1 



 ---- TRI-GRAMS ---- 

 [] 

 TOTAL TRIGRAMS --> 0 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['3', '.']

 TOTAL PORTER STEM WORDS ==> 2



 ---- SNOWBALL STEMMING ----

['3', '.']

 TOTAL SNOWBALL STEM WORDS ==> 2



 ---- LEMMATIZATION ----

['3', '.']

 TOTAL LEMMATIZE WORDS ==> 2

************************************************************************************************************************

50 --> Nasukawa,T. 


 ---- TOKENS ----

 ['Nasukawa', ',', 'T', '.'] 

 TOTAL TOKENS ==> 4

 ---- POST ----

 [('Nasukawa', 'NNP'), (',', ','), ('T', 'NNP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Nasukawa', ',', '.']

 TOTAL FILTERED TOKENS ==>  3

 ---- POST FOR FILTERED TOKENS ----

 [('Nasukawa', 'NNP'), (',', ','), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Nasukawa ,', ', .'] 

 TOTAL BIGRAMS --> 2 



 ---- TRI-GRAMS ---- 

 ['Nasukawa , .'] 

 TOTAL TRIGRAMS --> 1 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> ['Nasukawa']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['nasukawa', ',', '.']

 TOTAL PORTER STEM WORDS ==> 3



 ---- SNOWBALL STEMMING ----

['nasukawa', ',', '.']

 TOTAL SNOWBALL STEM WORDS ==> 3



 ---- LEMMATIZATION ----

['Nasukawa', ',', '.']

 TOTAL LEMMATIZE WORDS ==> 3

************************************************************************************************************************

51 --> & Yi, J. 


 ---- TOKENS ----

 ['&', 'Yi', ',', 'J', '.'] 

 TOTAL TOKENS ==> 5

 ---- POST ----

 [('&', 'CC'), ('Yi', 'NNP'), (',', ','), ('J', 'NNP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['&', 'Yi', ',', 'J', '.']

 TOTAL FILTERED TOKENS ==>  5

 ---- POST FOR FILTERED TOKENS ----

 [('&', 'CC'), ('Yi', 'NNP'), (',', ','), ('J', 'NNP'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['& Yi', 'Yi ,', ', J', 'J .'] 

 TOTAL BIGRAMS --> 4 



 ---- TRI-GRAMS ---- 

 ['& Yi ,', 'Yi , J', ', J .'] 

 TOTAL TRIGRAMS --> 3 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['&', 'yi', ',', 'j', '.']

 TOTAL PORTER STEM WORDS ==> 5



 ---- SNOWBALL STEMMING ----

['&', 'yi', ',', 'j', '.']

 TOTAL SNOWBALL STEM WORDS ==> 5



 ---- LEMMATIZATION ----

['&', 'Yi', ',', 'J', '.']

 TOTAL LEMMATIZE WORDS ==> 5

************************************************************************************************************************

52 --> 2003. 


 ---- TOKENS ----

 ['2003', '.'] 

 TOTAL TOKENS ==> 2

 ---- POST ----

 [('2003', 'CD'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['2003', '.']

 TOTAL FILTERED TOKENS ==>  2

 ---- POST FOR FILTERED TOKENS ----

 [('2003', 'CD'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['2003 .'] 

 TOTAL BIGRAMS --> 1 



 ---- TRI-GRAMS ---- 

 [] 

 TOTAL TRIGRAMS --> 0 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['2003', '.']

 TOTAL PORTER STEM WORDS ==> 2



 ---- SNOWBALL STEMMING ----

['2003', '.']

 TOTAL SNOWBALL STEM WORDS ==> 2



 ---- LEMMATIZATION ----

['2003', '.']

 TOTAL LEMMATIZE WORDS ==> 2

************************************************************************************************************************

53 --> Sentiment analysis: capturing favorability using natural language processing. 


 ---- TOKENS ----

 ['Sentiment', 'analysis', ':', 'capturing', 'favorability', 'using', 'natural', 'language', 'processing', '.'] 

 TOTAL TOKENS ==> 10

 ---- POST ----

 [('Sentiment', 'JJ'), ('analysis', 'NN'), (':', ':'), ('capturing', 'NN'), ('favorability', 'NN'), ('using', 'VBG'), ('natural', 'JJ'), ('language', 'NN'), ('processing', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Sentiment', 'analysis', ':', 'capturing', 'favorability', 'using', 'natural', 'language', 'processing', '.']

 TOTAL FILTERED TOKENS ==>  10

 ---- POST FOR FILTERED TOKENS ----

 [('Sentiment', 'JJ'), ('analysis', 'NN'), (':', ':'), ('capturing', 'NN'), ('favorability', 'NN'), ('using', 'VBG'), ('natural', 'JJ'), ('language', 'NN'), ('processing', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Sentiment analysis', 'analysis :', ': capturing', 'capturing favorability', 'favorability using', 'using natural', 'natural language', 'language processing', 'processing .'] 

 TOTAL BIGRAMS --> 9 



 ---- TRI-GRAMS ---- 

 ['Sentiment analysis :', 'analysis : capturing', ': capturing favorability', 'capturing favorability using', 'favorability using natural', 'using natural language', 'natural language processing', 'language processing .'] 

 TOTAL TRIGRAMS --> 8 



 ---- NOUN PHRASES ---- 

 ['Sentiment analysis', 'capturing', 'favorability', 'natural language', 'processing'] 

 TOTAL NOUN PHRASES --> 5 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['sentiment', 'analysi', ':', 'captur', 'favor', 'use', 'natur', 'languag', 'process', '.']

 TOTAL PORTER STEM WORDS ==> 10



 ---- SNOWBALL STEMMING ----

['sentiment', 'analysi', ':', 'captur', 'favor', 'use', 'natur', 'languag', 'process', '.']

 TOTAL SNOWBALL STEM WORDS ==> 10



 ---- LEMMATIZATION ----

['Sentiment', 'analysis', ':', 'capturing', 'favorability', 'using', 'natural', 'language', 'processing', '.']

 TOTAL LEMMATIZE WORDS ==> 10

************************************************************************************************************************

54 --> In Proceedings of the 2nd international conference on Knowledge capture, October 23–25, 2003. 


 ---- TOKENS ----

 ['In', 'Proceedings', 'of', 'the', '2nd', 'international', 'conference', 'on', 'Knowledge', 'capture', ',', 'October', '23–25', ',', '2003', '.'] 

 TOTAL TOKENS ==> 16

 ---- POST ----

 [('In', 'IN'), ('Proceedings', 'NNP'), ('of', 'IN'), ('the', 'DT'), ('2nd', 'CD'), ('international', 'JJ'), ('conference', 'NN'), ('on', 'IN'), ('Knowledge', 'NNP'), ('capture', 'NN'), (',', ','), ('October', 'NNP'), ('23–25', 'CD'), (',', ','), ('2003', 'CD'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Proceedings', '2nd', 'international', 'conference', 'Knowledge', 'capture', ',', 'October', '23–25', ',', '2003', '.']

 TOTAL FILTERED TOKENS ==>  12

 ---- POST FOR FILTERED TOKENS ----

 [('Proceedings', 'NNS'), ('2nd', 'CD'), ('international', 'JJ'), ('conference', 'NN'), ('Knowledge', 'NNP'), ('capture', 'NN'), (',', ','), ('October', 'NNP'), ('23–25', 'CD'), (',', ','), ('2003', 'CD'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Proceedings 2nd', '2nd international', 'international conference', 'conference Knowledge', 'Knowledge capture', 'capture ,', ', October', 'October 23–25', '23–25 ,', ', 2003', '2003 .'] 

 TOTAL BIGRAMS --> 11 



 ---- TRI-GRAMS ---- 

 ['Proceedings 2nd international', '2nd international conference', 'international conference Knowledge', 'conference Knowledge capture', 'Knowledge capture ,', 'capture , October', ', October 23–25', 'October 23–25 ,', '23–25 , 2003', ', 2003 .'] 

 TOTAL TRIGRAMS --> 10 



 ---- NOUN PHRASES ---- 

 ['international conference', 'capture'] 

 TOTAL NOUN PHRASES --> 2 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['proceed', '2nd', 'intern', 'confer', 'knowledg', 'captur', ',', 'octob', '23–25', ',', '2003', '.']

 TOTAL PORTER STEM WORDS ==> 12



 ---- SNOWBALL STEMMING ----

['proceed', '2nd', 'intern', 'confer', 'knowledg', 'captur', ',', 'octob', '23–25', ',', '2003', '.']

 TOTAL SNOWBALL STEM WORDS ==> 12



 ---- LEMMATIZATION ----

['Proceedings', '2nd', 'international', 'conference', 'Knowledge', 'capture', ',', 'October', '23–25', ',', '2003', '.']

 TOTAL LEMMATIZE WORDS ==> 12

************************************************************************************************************************

55 --> (pp. 


 ---- TOKENS ----

 ['(', 'pp', '.'] 

 TOTAL TOKENS ==> 3

 ---- POST ----

 [('(', '('), ('pp', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['(', 'pp', '.']

 TOTAL FILTERED TOKENS ==>  3

 ---- POST FOR FILTERED TOKENS ----

 [('(', '('), ('pp', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['( pp', 'pp .'] 

 TOTAL BIGRAMS --> 2 



 ---- TRI-GRAMS ---- 

 ['( pp .'] 

 TOTAL TRIGRAMS --> 1 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 []

 ---- PORTER STEMMING ----

['(', 'pp', '.']

 TOTAL PORTER STEM WORDS ==> 3



 ---- SNOWBALL STEMMING ----

['(', 'pp', '.']

 TOTAL SNOWBALL STEM WORDS ==> 3



 ---- LEMMATIZATION ----

['(', 'pp', '.']

 TOTAL LEMMATIZE WORDS ==> 3

************************************************************************************************************************

56 --> 70– 77). 


 ---- TOKENS ----

 ['70–', '77', ')', '.'] 

 TOTAL TOKENS ==> 4

 ---- POST ----

 [('70–', 'CD'), ('77', 'CD'), (')', ')'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['70–', '77', ')', '.']

 TOTAL FILTERED TOKENS ==>  4

 ---- POST FOR FILTERED TOKENS ----

 [('70–', 'CD'), ('77', 'CD'), (')', ')'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['70– 77', '77 )', ') .'] 

 TOTAL BIGRAMS --> 3 



 ---- TRI-GRAMS ---- 

 ['70– 77 )', '77 ) .'] 

 TOTAL TRIGRAMS --> 2 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 []

 ---- PORTER STEMMING ----

['70–', '77', ')', '.']

 TOTAL PORTER STEM WORDS ==> 4



 ---- SNOWBALL STEMMING ----

['70–', '77', ')', '.']

 TOTAL SNOWBALL STEM WORDS ==> 4



 ---- LEMMATIZATION ----

['70–', '77', ')', '.']

 TOTAL LEMMATIZE WORDS ==> 4

************************************************************************************************************************

57 --> Florida, USA. 


 ---- TOKENS ----

 ['Florida', ',', 'USA', '.'] 

 TOTAL TOKENS ==> 4

 ---- POST ----

 [('Florida', 'NNP'), (',', ','), ('USA', 'NNP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Florida', ',', 'USA', '.']

 TOTAL FILTERED TOKENS ==>  4

 ---- POST FOR FILTERED TOKENS ----

 [('Florida', 'NNP'), (',', ','), ('USA', 'NNP'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Florida ,', ', USA', 'USA .'] 

 TOTAL BIGRAMS --> 3 



 ---- TRI-GRAMS ---- 

 ['Florida , USA', ', USA .'] 

 TOTAL TRIGRAMS --> 2 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> ['USA']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> ['Florida']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['florida', ',', 'usa', '.']

 TOTAL PORTER STEM WORDS ==> 4



 ---- SNOWBALL STEMMING ----

['florida', ',', 'usa', '.']

 TOTAL SNOWBALL STEM WORDS ==> 4



 ---- LEMMATIZATION ----

['Florida', ',', 'USA', '.']

 TOTAL LEMMATIZE WORDS ==> 4

************************************************************************************************************************

58 --> 4. 


 ---- TOKENS ----

 ['4', '.'] 

 TOTAL TOKENS ==> 2

 ---- POST ----

 [('4', 'CD'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['4', '.']

 TOTAL FILTERED TOKENS ==>  2

 ---- POST FOR FILTERED TOKENS ----

 [('4', 'CD'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['4 .'] 

 TOTAL BIGRAMS --> 1 



 ---- TRI-GRAMS ---- 

 [] 

 TOTAL TRIGRAMS --> 0 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['4', '.']

 TOTAL PORTER STEM WORDS ==> 2



 ---- SNOWBALL STEMMING ----

['4', '.']

 TOTAL SNOWBALL STEM WORDS ==> 2



 ---- LEMMATIZATION ----

['4', '.']

 TOTAL LEMMATIZE WORDS ==> 2

************************************************************************************************************************

59 --> Morinaga, S., Yamanishi, K., Tateishi, K., Fukushima, T. 2002. 


 ---- TOKENS ----

 ['Morinaga', ',', 'S.', ',', 'Yamanishi', ',', 'K.', ',', 'Tateishi', ',', 'K.', ',', 'Fukushima', ',', 'T.', '2002', '.'] 

 TOTAL TOKENS ==> 17

 ---- POST ----

 [('Morinaga', 'NNP'), (',', ','), ('S.', 'NNP'), (',', ','), ('Yamanishi', 'NNP'), (',', ','), ('K.', 'NNP'), (',', ','), ('Tateishi', 'NNP'), (',', ','), ('K.', 'NNP'), (',', ','), ('Fukushima', 'NNP'), (',', ','), ('T.', 'NNP'), ('2002', 'CD'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Morinaga', ',', 'S.', ',', 'Yamanishi', ',', 'K.', ',', 'Tateishi', ',', 'K.', ',', 'Fukushima', ',', 'T.', '2002', '.']

 TOTAL FILTERED TOKENS ==>  17

 ---- POST FOR FILTERED TOKENS ----

 [('Morinaga', 'NNP'), (',', ','), ('S.', 'NNP'), (',', ','), ('Yamanishi', 'NNP'), (',', ','), ('K.', 'NNP'), (',', ','), ('Tateishi', 'NNP'), (',', ','), ('K.', 'NNP'), (',', ','), ('Fukushima', 'NNP'), (',', ','), ('T.', 'NNP'), ('2002', 'CD'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Morinaga ,', ', S.', 'S. ,', ', Yamanishi', 'Yamanishi ,', ', K.', 'K. ,', ', Tateishi', 'Tateishi ,', ', K.', 'K. ,', ', Fukushima', 'Fukushima ,', ', T.', 'T. 2002', '2002 .'] 

 TOTAL BIGRAMS --> 16 



 ---- TRI-GRAMS ---- 

 ['Morinaga , S.', ', S. ,', 'S. , Yamanishi', ', Yamanishi ,', 'Yamanishi , K.', ', K. ,', 'K. , Tateishi', ', Tateishi ,', 'Tateishi , K.', ', K. ,', 'K. , Fukushima', ', Fukushima ,', 'Fukushima , T.', ', T. 2002', 'T. 2002 .'] 

 TOTAL TRIGRAMS --> 15 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> ['Yamanishi', 'Tateishi', 'Fukushima']
 TOTAL PERSON ENTITY --> 3 


 GPE ---> ['Morinaga']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['morinaga', ',', 's.', ',', 'yamanishi', ',', 'k.', ',', 'tateishi', ',', 'k.', ',', 'fukushima', ',', 't.', '2002', '.']

 TOTAL PORTER STEM WORDS ==> 17



 ---- SNOWBALL STEMMING ----

['morinaga', ',', 's.', ',', 'yamanishi', ',', 'k.', ',', 'tateishi', ',', 'k.', ',', 'fukushima', ',', 't.', '2002', '.']

 TOTAL SNOWBALL STEM WORDS ==> 17



 ---- LEMMATIZATION ----

['Morinaga', ',', 'S.', ',', 'Yamanishi', ',', 'K.', ',', 'Tateishi', ',', 'K.', ',', 'Fukushima', ',', 'T.', '2002', '.']

 TOTAL LEMMATIZE WORDS ==> 17

************************************************************************************************************************

60 --> Mining product reputations on the web. 


 ---- TOKENS ----

 ['Mining', 'product', 'reputations', 'on', 'the', 'web', '.'] 

 TOTAL TOKENS ==> 7

 ---- POST ----

 [('Mining', 'VBG'), ('product', 'NN'), ('reputations', 'NNS'), ('on', 'IN'), ('the', 'DT'), ('web', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Mining', 'product', 'reputations', 'web', '.']

 TOTAL FILTERED TOKENS ==>  5

 ---- POST FOR FILTERED TOKENS ----

 [('Mining', 'VBG'), ('product', 'NN'), ('reputations', 'NNS'), ('web', 'VBP'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Mining product', 'product reputations', 'reputations web', 'web .'] 

 TOTAL BIGRAMS --> 4 



 ---- TRI-GRAMS ---- 

 ['Mining product reputations', 'product reputations web', 'reputations web .'] 

 TOTAL TRIGRAMS --> 3 



 ---- NOUN PHRASES ---- 

 ['product'] 

 TOTAL NOUN PHRASES --> 1 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['mine', 'product', 'reput', 'web', '.']

 TOTAL PORTER STEM WORDS ==> 5



 ---- SNOWBALL STEMMING ----

['mine', 'product', 'reput', 'web', '.']

 TOTAL SNOWBALL STEM WORDS ==> 5



 ---- LEMMATIZATION ----

['Mining', 'product', 'reputation', 'web', '.']

 TOTAL LEMMATIZE WORDS ==> 5

************************************************************************************************************************

61 --> In  Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data  mining, pp. 


 ---- TOKENS ----

 ['In', 'Proceedings', 'of', 'the', 'eighth', 'ACM', 'SIGKDD', 'international', 'conference', 'on', 'Knowledge', 'discovery', 'and', 'data', 'mining', ',', 'pp', '.'] 

 TOTAL TOKENS ==> 18

 ---- POST ----

 [('In', 'IN'), ('Proceedings', 'NNP'), ('of', 'IN'), ('the', 'DT'), ('eighth', 'JJ'), ('ACM', 'NNP'), ('SIGKDD', 'NNP'), ('international', 'JJ'), ('conference', 'NN'), ('on', 'IN'), ('Knowledge', 'NNP'), ('discovery', 'NN'), ('and', 'CC'), ('data', 'NN'), ('mining', 'NN'), (',', ','), ('pp', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Proceedings', 'eighth', 'ACM', 'SIGKDD', 'international', 'conference', 'Knowledge', 'discovery', 'data', 'mining', ',', 'pp', '.']

 TOTAL FILTERED TOKENS ==>  13

 ---- POST FOR FILTERED TOKENS ----

 [('Proceedings', 'NNS'), ('eighth', 'VBP'), ('ACM', 'NNP'), ('SIGKDD', 'NNP'), ('international', 'JJ'), ('conference', 'NN'), ('Knowledge', 'NNP'), ('discovery', 'NN'), ('data', 'NN'), ('mining', 'NN'), (',', ','), ('pp', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Proceedings eighth', 'eighth ACM', 'ACM SIGKDD', 'SIGKDD international', 'international conference', 'conference Knowledge', 'Knowledge discovery', 'discovery data', 'data mining', 'mining ,', ', pp', 'pp .'] 

 TOTAL BIGRAMS --> 12 



 ---- TRI-GRAMS ---- 

 ['Proceedings eighth ACM', 'eighth ACM SIGKDD', 'ACM SIGKDD international', 'SIGKDD international conference', 'international conference Knowledge', 'conference Knowledge discovery', 'Knowledge discovery data', 'discovery data mining', 'data mining ,', 'mining , pp', ', pp .'] 

 TOTAL TRIGRAMS --> 11 



 ---- NOUN PHRASES ---- 

 ['international conference', 'discovery', 'data', 'mining', 'pp'] 

 TOTAL NOUN PHRASES --> 5 



 ---- NER ----

 
 ORGANIZATION ---> ['ACM']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['proceed', 'eighth', 'acm', 'sigkdd', 'intern', 'confer', 'knowledg', 'discoveri', 'data', 'mine', ',', 'pp', '.']

 TOTAL PORTER STEM WORDS ==> 13



 ---- SNOWBALL STEMMING ----

['proceed', 'eighth', 'acm', 'sigkdd', 'intern', 'confer', 'knowledg', 'discoveri', 'data', 'mine', ',', 'pp', '.']

 TOTAL SNOWBALL STEM WORDS ==> 13



 ---- LEMMATIZATION ----

['Proceedings', 'eighth', 'ACM', 'SIGKDD', 'international', 'conference', 'Knowledge', 'discovery', 'data', 'mining', ',', 'pp', '.']

 TOTAL LEMMATIZE WORDS ==> 13

************************************************************************************************************************

62 --> 341-349. 


 ---- TOKENS ----

 ['341-349', '.'] 

 TOTAL TOKENS ==> 2

 ---- POST ----

 [('341-349', 'JJ'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['341-349', '.']

 TOTAL FILTERED TOKENS ==>  2

 ---- POST FOR FILTERED TOKENS ----

 [('341-349', 'JJ'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['341-349 .'] 

 TOTAL BIGRAMS --> 1 



 ---- TRI-GRAMS ---- 

 [] 

 TOTAL TRIGRAMS --> 0 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['341-349', '.']

 TOTAL PORTER STEM WORDS ==> 2



 ---- SNOWBALL STEMMING ----

['341-349', '.']

 TOTAL SNOWBALL STEM WORDS ==> 2



 ---- LEMMATIZATION ----

['341-349', '.']

 TOTAL LEMMATIZE WORDS ==> 2

************************************************************************************************************************

63 --> 5. 


 ---- TOKENS ----

 ['5', '.'] 

 TOTAL TOKENS ==> 2

 ---- POST ----

 [('5', 'CD'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['5', '.']

 TOTAL FILTERED TOKENS ==>  2

 ---- POST FOR FILTERED TOKENS ----

 [('5', 'CD'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['5 .'] 

 TOTAL BIGRAMS --> 1 



 ---- TRI-GRAMS ---- 

 [] 

 TOTAL TRIGRAMS --> 0 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['5', '.']

 TOTAL PORTER STEM WORDS ==> 2



 ---- SNOWBALL STEMMING ----

['5', '.']

 TOTAL SNOWBALL STEM WORDS ==> 2



 ---- LEMMATIZATION ----

['5', '.']

 TOTAL LEMMATIZE WORDS ==> 2

************************************************************************************************************************

64 --> Pang, B., Lee, L., Vaithyanathan, S. 2002. 


 ---- TOKENS ----

 ['Pang', ',', 'B.', ',', 'Lee', ',', 'L.', ',', 'Vaithyanathan', ',', 'S.', '2002', '.'] 

 TOTAL TOKENS ==> 13

 ---- POST ----

 [('Pang', 'NNP'), (',', ','), ('B.', 'NNP'), (',', ','), ('Lee', 'NNP'), (',', ','), ('L.', 'NNP'), (',', ','), ('Vaithyanathan', 'NNP'), (',', ','), ('S.', 'NNP'), ('2002', 'CD'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Pang', ',', 'B.', ',', 'Lee', ',', 'L.', ',', 'Vaithyanathan', ',', 'S.', '2002', '.']

 TOTAL FILTERED TOKENS ==>  13

 ---- POST FOR FILTERED TOKENS ----

 [('Pang', 'NNP'), (',', ','), ('B.', 'NNP'), (',', ','), ('Lee', 'NNP'), (',', ','), ('L.', 'NNP'), (',', ','), ('Vaithyanathan', 'NNP'), (',', ','), ('S.', 'NNP'), ('2002', 'CD'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Pang ,', ', B.', 'B. ,', ', Lee', 'Lee ,', ', L.', 'L. ,', ', Vaithyanathan', 'Vaithyanathan ,', ', S.', 'S. 2002', '2002 .'] 

 TOTAL BIGRAMS --> 12 



 ---- TRI-GRAMS ---- 

 ['Pang , B.', ', B. ,', 'B. , Lee', ', Lee ,', 'Lee , L.', ', L. ,', 'L. , Vaithyanathan', ', Vaithyanathan ,', 'Vaithyanathan , S.', ', S. 2002', 'S. 2002 .'] 

 TOTAL TRIGRAMS --> 11 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> ['Lee']
 TOTAL PERSON ENTITY --> 1 


 GPE ---> ['Pang', 'Vaithyanathan']
 TOTAL GPE ENTITY --> 2 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['pang', ',', 'b.', ',', 'lee', ',', 'l.', ',', 'vaithyanathan', ',', 's.', '2002', '.']

 TOTAL PORTER STEM WORDS ==> 13



 ---- SNOWBALL STEMMING ----

['pang', ',', 'b.', ',', 'lee', ',', 'l.', ',', 'vaithyanathan', ',', 's.', '2002', '.']

 TOTAL SNOWBALL STEM WORDS ==> 13



 ---- LEMMATIZATION ----

['Pang', ',', 'B.', ',', 'Lee', ',', 'L.', ',', 'Vaithyanathan', ',', 'S.', '2002', '.']

 TOTAL LEMMATIZE WORDS ==> 13

************************************************************************************************************************

65 --> Thumbs up? 


 ---- TOKENS ----

 ['Thumbs', 'up', '?'] 

 TOTAL TOKENS ==> 3

 ---- POST ----

 [('Thumbs', 'VB'), ('up', 'RP'), ('?', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Thumbs', '?']

 TOTAL FILTERED TOKENS ==>  2

 ---- POST FOR FILTERED TOKENS ----

 [('Thumbs', 'NNS'), ('?', '.')] 



 ---- BI-GRAMS ---- 

 ['Thumbs ?'] 

 TOTAL BIGRAMS --> 1 



 ---- TRI-GRAMS ---- 

 [] 

 TOTAL TRIGRAMS --> 0 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['thumb', '?']

 TOTAL PORTER STEM WORDS ==> 2



 ---- SNOWBALL STEMMING ----

['thumb', '?']

 TOTAL SNOWBALL STEM WORDS ==> 2



 ---- LEMMATIZATION ----

['Thumbs', '?']

 TOTAL LEMMATIZE WORDS ==> 2

************************************************************************************************************************

66 --> Sentiment Classification using Machine Learning  Techniques. 


 ---- TOKENS ----

 ['Sentiment', 'Classification', 'using', 'Machine', 'Learning', 'Techniques', '.'] 

 TOTAL TOKENS ==> 7

 ---- POST ----

 [('Sentiment', 'NN'), ('Classification', 'NNP'), ('using', 'VBG'), ('Machine', 'NNP'), ('Learning', 'NNP'), ('Techniques', 'NNP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Sentiment', 'Classification', 'using', 'Machine', 'Learning', 'Techniques', '.']

 TOTAL FILTERED TOKENS ==>  7

 ---- POST FOR FILTERED TOKENS ----

 [('Sentiment', 'NN'), ('Classification', 'NNP'), ('using', 'VBG'), ('Machine', 'NNP'), ('Learning', 'NNP'), ('Techniques', 'NNP'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Sentiment Classification', 'Classification using', 'using Machine', 'Machine Learning', 'Learning Techniques', 'Techniques .'] 

 TOTAL BIGRAMS --> 6 



 ---- TRI-GRAMS ---- 

 ['Sentiment Classification using', 'Classification using Machine', 'using Machine Learning', 'Machine Learning Techniques', 'Learning Techniques .'] 

 TOTAL TRIGRAMS --> 5 



 ---- NOUN PHRASES ---- 

 ['Sentiment'] 

 TOTAL NOUN PHRASES --> 1 



 ---- NER ----

 
 ORGANIZATION ---> ['Sentiment Classification']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> ['Machine Learning Techniques']
 TOTAL PERSON ENTITY --> 1 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['sentiment', 'classif', 'use', 'machin', 'learn', 'techniqu', '.']

 TOTAL PORTER STEM WORDS ==> 7



 ---- SNOWBALL STEMMING ----

['sentiment', 'classif', 'use', 'machin', 'learn', 'techniqu', '.']

 TOTAL SNOWBALL STEM WORDS ==> 7



 ---- LEMMATIZATION ----

['Sentiment', 'Classification', 'using', 'Machine', 'Learning', 'Techniques', '.']

 TOTAL LEMMATIZE WORDS ==> 7

************************************************************************************************************************

67 --> Proc. 


 ---- TOKENS ----

 ['Proc', '.'] 

 TOTAL TOKENS ==> 2

 ---- POST ----

 [('Proc', 'NNP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Proc', '.']

 TOTAL FILTERED TOKENS ==>  2

 ---- POST FOR FILTERED TOKENS ----

 [('Proc', 'NNP'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Proc .'] 

 TOTAL BIGRAMS --> 1 



 ---- TRI-GRAMS ---- 

 [] 

 TOTAL TRIGRAMS --> 0 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> ['Proc']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['proc', '.']

 TOTAL PORTER STEM WORDS ==> 2



 ---- SNOWBALL STEMMING ----

['proc', '.']

 TOTAL SNOWBALL STEM WORDS ==> 2



 ---- LEMMATIZATION ----

['Proc', '.']

 TOTAL LEMMATIZE WORDS ==> 2

************************************************************************************************************************

68 --> of 7th EMNLP, pp.79-86. 


 ---- TOKENS ----

 ['of', '7th', 'EMNLP', ',', 'pp.79-86', '.'] 

 TOTAL TOKENS ==> 6

 ---- POST ----

 [('of', 'IN'), ('7th', 'CD'), ('EMNLP', 'NNP'), (',', ','), ('pp.79-86', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['7th', 'EMNLP', ',', 'pp.79-86', '.']

 TOTAL FILTERED TOKENS ==>  5

 ---- POST FOR FILTERED TOKENS ----

 [('7th', 'CD'), ('EMNLP', 'NNP'), (',', ','), ('pp.79-86', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['7th EMNLP', 'EMNLP ,', ', pp.79-86', 'pp.79-86 .'] 

 TOTAL BIGRAMS --> 4 



 ---- TRI-GRAMS ---- 

 ['7th EMNLP ,', 'EMNLP , pp.79-86', ', pp.79-86 .'] 

 TOTAL TRIGRAMS --> 3 



 ---- NOUN PHRASES ---- 

 ['pp.79-86'] 

 TOTAL NOUN PHRASES --> 1 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['7th', 'emnlp', ',', 'pp.79-86', '.']

 TOTAL PORTER STEM WORDS ==> 5



 ---- SNOWBALL STEMMING ----

['7th', 'emnlp', ',', 'pp.79-86', '.']

 TOTAL SNOWBALL STEM WORDS ==> 5



 ---- LEMMATIZATION ----

['7th', 'EMNLP', ',', 'pp.79-86', '.']

 TOTAL LEMMATIZE WORDS ==> 5

************************************************************************************************************************

69 --> White Paper – Sentiments Analysis  Page 4 of 5    6. 


 ---- TOKENS ----

 ['White', 'Paper', '–', 'Sentiments', 'Analysis', 'Page', '4', 'of', '5', '6', '.'] 

 TOTAL TOKENS ==> 11

 ---- POST ----

 [('White', 'NNP'), ('Paper', 'NNP'), ('–', 'NN'), ('Sentiments', 'NNP'), ('Analysis', 'NNP'), ('Page', 'NNP'), ('4', 'CD'), ('of', 'IN'), ('5', 'CD'), ('6', 'CD'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['White', 'Paper', '–', 'Sentiments', 'Analysis', 'Page', '4', '5', '6', '.']

 TOTAL FILTERED TOKENS ==>  10

 ---- POST FOR FILTERED TOKENS ----

 [('White', 'NNP'), ('Paper', 'NNP'), ('–', 'NN'), ('Sentiments', 'NNP'), ('Analysis', 'NNP'), ('Page', 'NNP'), ('4', 'CD'), ('5', 'CD'), ('6', 'CD'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['White Paper', 'Paper –', '– Sentiments', 'Sentiments Analysis', 'Analysis Page', 'Page 4', '4 5', '5 6', '6 .'] 

 TOTAL BIGRAMS --> 9 



 ---- TRI-GRAMS ---- 

 ['White Paper –', 'Paper – Sentiments', '– Sentiments Analysis', 'Sentiments Analysis Page', 'Analysis Page 4', 'Page 4 5', '4 5 6', '5 6 .'] 

 TOTAL TRIGRAMS --> 8 



 ---- NOUN PHRASES ---- 

 ['–'] 

 TOTAL NOUN PHRASES --> 1 



 ---- NER ----

 
 ORGANIZATION ---> ['Paper', 'Sentiments Analysis']
 TOTAL ORGANIZATION ENTITY --> 2 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['white', 'paper', '–', 'sentiment', 'analysi', 'page', '4', '5', '6', '.']

 TOTAL PORTER STEM WORDS ==> 10



 ---- SNOWBALL STEMMING ----

['white', 'paper', '–', 'sentiment', 'analysi', 'page', '4', '5', '6', '.']

 TOTAL SNOWBALL STEM WORDS ==> 10



 ---- LEMMATIZATION ----

['White', 'Paper', '–', 'Sentiments', 'Analysis', 'Page', '4', '5', '6', '.']

 TOTAL LEMMATIZE WORDS ==> 10

************************************************************************************************************************

70 --> Tong, R.M. 


 ---- TOKENS ----

 ['Tong', ',', 'R.M', '.'] 

 TOTAL TOKENS ==> 4

 ---- POST ----

 [('Tong', 'NNP'), (',', ','), ('R.M', 'NNP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Tong', ',', 'R.M', '.']

 TOTAL FILTERED TOKENS ==>  4

 ---- POST FOR FILTERED TOKENS ----

 [('Tong', 'NNP'), (',', ','), ('R.M', 'NNP'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Tong ,', ', R.M', 'R.M .'] 

 TOTAL BIGRAMS --> 3 



 ---- TRI-GRAMS ---- 

 ['Tong , R.M', ', R.M .'] 

 TOTAL TRIGRAMS --> 2 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> ['Tong']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['tong', ',', 'r.m', '.']

 TOTAL PORTER STEM WORDS ==> 4



 ---- SNOWBALL STEMMING ----

['tong', ',', 'r.m', '.']

 TOTAL SNOWBALL STEM WORDS ==> 4



 ---- LEMMATIZATION ----

['Tong', ',', 'R.M', '.']

 TOTAL LEMMATIZE WORDS ==> 4

************************************************************************************************************************

71 --> 2001. 


 ---- TOKENS ----

 ['2001', '.'] 

 TOTAL TOKENS ==> 2

 ---- POST ----

 [('2001', 'CD'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['2001', '.']

 TOTAL FILTERED TOKENS ==>  2

 ---- POST FOR FILTERED TOKENS ----

 [('2001', 'CD'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['2001 .'] 

 TOTAL BIGRAMS --> 1 



 ---- TRI-GRAMS ---- 

 [] 

 TOTAL TRIGRAMS --> 0 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['2001', '.']

 TOTAL PORTER STEM WORDS ==> 2



 ---- SNOWBALL STEMMING ----

['2001', '.']

 TOTAL SNOWBALL STEM WORDS ==> 2



 ---- LEMMATIZATION ----

['2001', '.']

 TOTAL LEMMATIZE WORDS ==> 2

************************************************************************************************************************

72 --> An operational system for detecting and tracking opinions in on-line discussion. 


 ---- TOKENS ----

 ['An', 'operational', 'system', 'for', 'detecting', 'and', 'tracking', 'opinions', 'in', 'on-line', 'discussion', '.'] 

 TOTAL TOKENS ==> 12

 ---- POST ----

 [('An', 'DT'), ('operational', 'JJ'), ('system', 'NN'), ('for', 'IN'), ('detecting', 'VBG'), ('and', 'CC'), ('tracking', 'VBG'), ('opinions', 'NNS'), ('in', 'IN'), ('on-line', 'JJ'), ('discussion', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['operational', 'system', 'detecting', 'tracking', 'opinions', 'on-line', 'discussion', '.']

 TOTAL FILTERED TOKENS ==>  8

 ---- POST FOR FILTERED TOKENS ----

 [('operational', 'JJ'), ('system', 'NN'), ('detecting', 'VBG'), ('tracking', 'VBG'), ('opinions', 'NNS'), ('on-line', 'JJ'), ('discussion', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['operational system', 'system detecting', 'detecting tracking', 'tracking opinions', 'opinions on-line', 'on-line discussion', 'discussion .'] 

 TOTAL BIGRAMS --> 7 



 ---- TRI-GRAMS ---- 

 ['operational system detecting', 'system detecting tracking', 'detecting tracking opinions', 'tracking opinions on-line', 'opinions on-line discussion', 'on-line discussion .'] 

 TOTAL TRIGRAMS --> 6 



 ---- NOUN PHRASES ---- 

 ['operational system', 'on-line discussion'] 

 TOTAL NOUN PHRASES --> 2 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['oper', 'system', 'detect', 'track', 'opinion', 'on-lin', 'discuss', '.']

 TOTAL PORTER STEM WORDS ==> 8



 ---- SNOWBALL STEMMING ----

['oper', 'system', 'detect', 'track', 'opinion', 'on-lin', 'discuss', '.']

 TOTAL SNOWBALL STEM WORDS ==> 8



 ---- LEMMATIZATION ----

['operational', 'system', 'detecting', 'tracking', 'opinion', 'on-line', 'discussion', '.']

 TOTAL LEMMATIZE WORDS ==> 8

************************************************************************************************************************

73 --> In  Proceedings of SIGIR Workshop on Operational Text Classification. 


 ---- TOKENS ----

 ['In', 'Proceedings', 'of', 'SIGIR', 'Workshop', 'on', 'Operational', 'Text', 'Classification', '.'] 

 TOTAL TOKENS ==> 10

 ---- POST ----

 [('In', 'IN'), ('Proceedings', 'NNP'), ('of', 'IN'), ('SIGIR', 'NNP'), ('Workshop', 'NNP'), ('on', 'IN'), ('Operational', 'NNP'), ('Text', 'NNP'), ('Classification', 'NNP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Proceedings', 'SIGIR', 'Workshop', 'Operational', 'Text', 'Classification', '.']

 TOTAL FILTERED TOKENS ==>  7

 ---- POST FOR FILTERED TOKENS ----

 [('Proceedings', 'NNS'), ('SIGIR', 'NNP'), ('Workshop', 'NNP'), ('Operational', 'NNP'), ('Text', 'NNP'), ('Classification', 'NNP'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Proceedings SIGIR', 'SIGIR Workshop', 'Workshop Operational', 'Operational Text', 'Text Classification', 'Classification .'] 

 TOTAL BIGRAMS --> 6 



 ---- TRI-GRAMS ---- 

 ['Proceedings SIGIR Workshop', 'SIGIR Workshop Operational', 'Workshop Operational Text', 'Operational Text Classification', 'Text Classification .'] 

 TOTAL TRIGRAMS --> 5 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> ['SIGIR Workshop']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['proceed', 'sigir', 'workshop', 'oper', 'text', 'classif', '.']

 TOTAL PORTER STEM WORDS ==> 7



 ---- SNOWBALL STEMMING ----

['proceed', 'sigir', 'workshop', 'oper', 'text', 'classif', '.']

 TOTAL SNOWBALL STEM WORDS ==> 7



 ---- LEMMATIZATION ----

['Proceedings', 'SIGIR', 'Workshop', 'Operational', 'Text', 'Classification', '.']

 TOTAL LEMMATIZE WORDS ==> 7

************************************************************************************************************************

74 --> 7. 


 ---- TOKENS ----

 ['7', '.'] 

 TOTAL TOKENS ==> 2

 ---- POST ----

 [('7', 'CD'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['7', '.']

 TOTAL FILTERED TOKENS ==>  2

 ---- POST FOR FILTERED TOKENS ----

 [('7', 'CD'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['7 .'] 

 TOTAL BIGRAMS --> 1 



 ---- TRI-GRAMS ---- 

 [] 

 TOTAL TRIGRAMS --> 0 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['7', '.']

 TOTAL PORTER STEM WORDS ==> 2



 ---- SNOWBALL STEMMING ----

['7', '.']

 TOTAL SNOWBALL STEM WORDS ==> 2



 ---- LEMMATIZATION ----

['7', '.']

 TOTAL LEMMATIZE WORDS ==> 2

************************************************************************************************************************

75 --> Turney, P. 2002. 


 ---- TOKENS ----

 ['Turney', ',', 'P.', '2002', '.'] 

 TOTAL TOKENS ==> 5

 ---- POST ----

 [('Turney', 'NNP'), (',', ','), ('P.', 'NNP'), ('2002', 'CD'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Turney', ',', 'P.', '2002', '.']

 TOTAL FILTERED TOKENS ==>  5

 ---- POST FOR FILTERED TOKENS ----

 [('Turney', 'NNP'), (',', ','), ('P.', 'NNP'), ('2002', 'CD'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Turney ,', ', P.', 'P. 2002', '2002 .'] 

 TOTAL BIGRAMS --> 4 



 ---- TRI-GRAMS ---- 

 ['Turney , P.', ', P. 2002', 'P. 2002 .'] 

 TOTAL TRIGRAMS --> 3 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> ['Turney']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['turney', ',', 'p.', '2002', '.']

 TOTAL PORTER STEM WORDS ==> 5



 ---- SNOWBALL STEMMING ----

['turney', ',', 'p.', '2002', '.']

 TOTAL SNOWBALL STEM WORDS ==> 5



 ---- LEMMATIZATION ----

['Turney', ',', 'P.', '2002', '.']

 TOTAL LEMMATIZE WORDS ==> 5

************************************************************************************************************************

76 --> Thumbs up or thumbs down? 


 ---- TOKENS ----

 ['Thumbs', 'up', 'or', 'thumbs', 'down', '?'] 

 TOTAL TOKENS ==> 6

 ---- POST ----

 [('Thumbs', 'VB'), ('up', 'RP'), ('or', 'CC'), ('thumbs', 'VB'), ('down', 'RP'), ('?', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Thumbs', 'thumbs', '?']

 TOTAL FILTERED TOKENS ==>  3

 ---- POST FOR FILTERED TOKENS ----

 [('Thumbs', 'NNP'), ('thumbs', 'NNS'), ('?', '.')] 



 ---- BI-GRAMS ---- 

 ['Thumbs thumbs', 'thumbs ?'] 

 TOTAL BIGRAMS --> 2 



 ---- TRI-GRAMS ---- 

 ['Thumbs thumbs ?'] 

 TOTAL TRIGRAMS --> 1 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> ['Thumbs']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['thumb', 'thumb', '?']

 TOTAL PORTER STEM WORDS ==> 3



 ---- SNOWBALL STEMMING ----

['thumb', 'thumb', '?']

 TOTAL SNOWBALL STEM WORDS ==> 3



 ---- LEMMATIZATION ----

['Thumbs', 'thumb', '?']

 TOTAL LEMMATIZE WORDS ==> 3

************************************************************************************************************************

77 --> Semantic orientation applied to unsupervised classification  of reviews. 


 ---- TOKENS ----

 ['Semantic', 'orientation', 'applied', 'to', 'unsupervised', 'classification', 'of', 'reviews', '.'] 

 TOTAL TOKENS ==> 9

 ---- POST ----

 [('Semantic', 'JJ'), ('orientation', 'NN'), ('applied', 'VBD'), ('to', 'TO'), ('unsupervised', 'JJ'), ('classification', 'NN'), ('of', 'IN'), ('reviews', 'NNS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Semantic', 'orientation', 'applied', 'unsupervised', 'classification', 'reviews', '.']

 TOTAL FILTERED TOKENS ==>  7

 ---- POST FOR FILTERED TOKENS ----

 [('Semantic', 'JJ'), ('orientation', 'NN'), ('applied', 'VBD'), ('unsupervised', 'JJ'), ('classification', 'NN'), ('reviews', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Semantic orientation', 'orientation applied', 'applied unsupervised', 'unsupervised classification', 'classification reviews', 'reviews .'] 

 TOTAL BIGRAMS --> 6 



 ---- TRI-GRAMS ---- 

 ['Semantic orientation applied', 'orientation applied unsupervised', 'applied unsupervised classification', 'unsupervised classification reviews', 'classification reviews .'] 

 TOTAL TRIGRAMS --> 5 



 ---- NOUN PHRASES ---- 

 ['Semantic orientation', 'unsupervised classification'] 

 TOTAL NOUN PHRASES --> 2 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> ['Semantic']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['semant', 'orient', 'appli', 'unsupervis', 'classif', 'review', '.']

 TOTAL PORTER STEM WORDS ==> 7



 ---- SNOWBALL STEMMING ----

['semant', 'orient', 'appli', 'unsupervis', 'classif', 'review', '.']

 TOTAL SNOWBALL STEM WORDS ==> 7



 ---- LEMMATIZATION ----

['Semantic', 'orientation', 'applied', 'unsupervised', 'classification', 'review', '.']

 TOTAL LEMMATIZE WORDS ==> 7

************************************************************************************************************************

78 --> In Proceedings of the 40th ACL, pp. 


 ---- TOKENS ----

 ['In', 'Proceedings', 'of', 'the', '40th', 'ACL', ',', 'pp', '.'] 

 TOTAL TOKENS ==> 9

 ---- POST ----

 [('In', 'IN'), ('Proceedings', 'NNP'), ('of', 'IN'), ('the', 'DT'), ('40th', 'JJ'), ('ACL', 'NNP'), (',', ','), ('pp', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Proceedings', '40th', 'ACL', ',', 'pp', '.']

 TOTAL FILTERED TOKENS ==>  6

 ---- POST FOR FILTERED TOKENS ----

 [('Proceedings', 'NNS'), ('40th', 'CD'), ('ACL', 'NNP'), (',', ','), ('pp', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Proceedings 40th', '40th ACL', 'ACL ,', ', pp', 'pp .'] 

 TOTAL BIGRAMS --> 5 



 ---- TRI-GRAMS ---- 

 ['Proceedings 40th ACL', '40th ACL ,', 'ACL , pp', ', pp .'] 

 TOTAL TRIGRAMS --> 4 



 ---- NOUN PHRASES ---- 

 ['pp'] 

 TOTAL NOUN PHRASES --> 1 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['proceed', '40th', 'acl', ',', 'pp', '.']

 TOTAL PORTER STEM WORDS ==> 6



 ---- SNOWBALL STEMMING ----

['proceed', '40th', 'acl', ',', 'pp', '.']

 TOTAL SNOWBALL STEM WORDS ==> 6



 ---- LEMMATIZATION ----

['Proceedings', '40th', 'ACL', ',', 'pp', '.']

 TOTAL LEMMATIZE WORDS ==> 6

************************************************************************************************************************

79 --> 417-424. 


 ---- TOKENS ----

 ['417-424', '.'] 

 TOTAL TOKENS ==> 2

 ---- POST ----

 [('417-424', 'JJ'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['417-424', '.']

 TOTAL FILTERED TOKENS ==>  2

 ---- POST FOR FILTERED TOKENS ----

 [('417-424', 'JJ'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['417-424 .'] 

 TOTAL BIGRAMS --> 1 



 ---- TRI-GRAMS ---- 

 [] 

 TOTAL TRIGRAMS --> 0 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['417-424', '.']

 TOTAL PORTER STEM WORDS ==> 2



 ---- SNOWBALL STEMMING ----

['417-424', '.']

 TOTAL SNOWBALL STEM WORDS ==> 2



 ---- LEMMATIZATION ----

['417-424', '.']

 TOTAL LEMMATIZE WORDS ==> 2

************************************************************************************************************************

80 --> 8. 


 ---- TOKENS ----

 ['8', '.'] 

 TOTAL TOKENS ==> 2

 ---- POST ----

 [('8', 'CD'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['8', '.']

 TOTAL FILTERED TOKENS ==>  2

 ---- POST FOR FILTERED TOKENS ----

 [('8', 'CD'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['8 .'] 

 TOTAL BIGRAMS --> 1 



 ---- TRI-GRAMS ---- 

 [] 

 TOTAL TRIGRAMS --> 0 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['8', '.']

 TOTAL PORTER STEM WORDS ==> 2



 ---- SNOWBALL STEMMING ----

['8', '.']

 TOTAL SNOWBALL STEM WORDS ==> 2



 ---- LEMMATIZATION ----

['8', '.']

 TOTAL LEMMATIZE WORDS ==> 2

************************************************************************************************************************

81 --> Wiebe, J. 


 ---- TOKENS ----

 ['Wiebe', ',', 'J', '.'] 

 TOTAL TOKENS ==> 4

 ---- POST ----

 [('Wiebe', 'NNP'), (',', ','), ('J', 'NNP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Wiebe', ',', 'J', '.']

 TOTAL FILTERED TOKENS ==>  4

 ---- POST FOR FILTERED TOKENS ----

 [('Wiebe', 'NNP'), (',', ','), ('J', 'NNP'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Wiebe ,', ', J', 'J .'] 

 TOTAL BIGRAMS --> 3 



 ---- TRI-GRAMS ---- 

 ['Wiebe , J', ', J .'] 

 TOTAL TRIGRAMS --> 2 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> ['Wiebe']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['wieb', ',', 'j', '.']

 TOTAL PORTER STEM WORDS ==> 4



 ---- SNOWBALL STEMMING ----

['wieb', ',', 'j', '.']

 TOTAL SNOWBALL STEM WORDS ==> 4



 ---- LEMMATIZATION ----

['Wiebe', ',', 'J', '.']

 TOTAL LEMMATIZE WORDS ==> 4

************************************************************************************************************************

82 --> (2000) Learning subjective adjectives from corpora. 


 ---- TOKENS ----

 ['(', '2000', ')', 'Learning', 'subjective', 'adjectives', 'from', 'corpora', '.'] 

 TOTAL TOKENS ==> 9

 ---- POST ----

 [('(', '('), ('2000', 'CD'), (')', ')'), ('Learning', 'VBG'), ('subjective', 'JJ'), ('adjectives', 'NNS'), ('from', 'IN'), ('corpora', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['(', '2000', ')', 'Learning', 'subjective', 'adjectives', 'corpora', '.']

 TOTAL FILTERED TOKENS ==>  8

 ---- POST FOR FILTERED TOKENS ----

 [('(', '('), ('2000', 'CD'), (')', ')'), ('Learning', 'VBG'), ('subjective', 'JJ'), ('adjectives', 'NNS'), ('corpora', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['( 2000', '2000 )', ') Learning', 'Learning subjective', 'subjective adjectives', 'adjectives corpora', 'corpora .'] 

 TOTAL BIGRAMS --> 7 



 ---- TRI-GRAMS ---- 

 ['( 2000 )', '2000 ) Learning', ') Learning subjective', 'Learning subjective adjectives', 'subjective adjectives corpora', 'adjectives corpora .'] 

 TOTAL TRIGRAMS --> 6 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['(', '2000', ')', 'learn', 'subject', 'adject', 'corpora', '.']

 TOTAL PORTER STEM WORDS ==> 8



 ---- SNOWBALL STEMMING ----

['(', '2000', ')', 'learn', 'subject', 'adject', 'corpora', '.']

 TOTAL SNOWBALL STEM WORDS ==> 8



 ---- LEMMATIZATION ----

['(', '2000', ')', 'Learning', 'subjective', 'adjective', 'corpus', '.']

 TOTAL LEMMATIZE WORDS ==> 8

************************************************************************************************************************

83 --> In Proceedings of National Conference on  Artificial Intelligence. 


 ---- TOKENS ----

 ['In', 'Proceedings', 'of', 'National', 'Conference', 'on', 'Artificial', 'Intelligence', '.'] 

 TOTAL TOKENS ==> 9

 ---- POST ----

 [('In', 'IN'), ('Proceedings', 'NNP'), ('of', 'IN'), ('National', 'NNP'), ('Conference', 'NNP'), ('on', 'IN'), ('Artificial', 'NNP'), ('Intelligence', 'NNP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Proceedings', 'National', 'Conference', 'Artificial', 'Intelligence', '.']

 TOTAL FILTERED TOKENS ==>  6

 ---- POST FOR FILTERED TOKENS ----

 [('Proceedings', 'NNS'), ('National', 'NNP'), ('Conference', 'NNP'), ('Artificial', 'NNP'), ('Intelligence', 'NNP'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Proceedings National', 'National Conference', 'Conference Artificial', 'Artificial Intelligence', 'Intelligence .'] 

 TOTAL BIGRAMS --> 5 



 ---- TRI-GRAMS ---- 

 ['Proceedings National Conference', 'National Conference Artificial', 'Conference Artificial Intelligence', 'Artificial Intelligence .'] 

 TOTAL TRIGRAMS --> 4 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> ['National Conference Artificial Intelligence']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['proceed', 'nation', 'confer', 'artifici', 'intellig', '.']

 TOTAL PORTER STEM WORDS ==> 6



 ---- SNOWBALL STEMMING ----

['proceed', 'nation', 'confer', 'artifici', 'intellig', '.']

 TOTAL SNOWBALL STEM WORDS ==> 6



 ---- LEMMATIZATION ----

['Proceedings', 'National', 'Conference', 'Artificial', 'Intelligence', '.']

 TOTAL LEMMATIZE WORDS ==> 6

************************************************************************************************************************

84 --> 9. 


 ---- TOKENS ----

 ['9', '.'] 

 TOTAL TOKENS ==> 2

 ---- POST ----

 [('9', 'CD'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['9', '.']

 TOTAL FILTERED TOKENS ==>  2

 ---- POST FOR FILTERED TOKENS ----

 [('9', 'CD'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['9 .'] 

 TOTAL BIGRAMS --> 1 



 ---- TRI-GRAMS ---- 

 [] 

 TOTAL TRIGRAMS --> 0 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['9', '.']

 TOTAL PORTER STEM WORDS ==> 2



 ---- SNOWBALL STEMMING ----

['9', '.']

 TOTAL SNOWBALL STEM WORDS ==> 2



 ---- LEMMATIZATION ----

['9', '.']

 TOTAL LEMMATIZE WORDS ==> 2

************************************************************************************************************************

85 --> Wilson, T., Wiebe, J., Hoffmann, P. 2009. 


 ---- TOKENS ----

 ['Wilson', ',', 'T.', ',', 'Wiebe', ',', 'J.', ',', 'Hoffmann', ',', 'P.', '2009', '.'] 

 TOTAL TOKENS ==> 13

 ---- POST ----

 [('Wilson', 'NNP'), (',', ','), ('T.', 'NNP'), (',', ','), ('Wiebe', 'NNP'), (',', ','), ('J.', 'NNP'), (',', ','), ('Hoffmann', 'NNP'), (',', ','), ('P.', 'NNP'), ('2009', 'CD'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Wilson', ',', 'T.', ',', 'Wiebe', ',', 'J.', ',', 'Hoffmann', ',', 'P.', '2009', '.']

 TOTAL FILTERED TOKENS ==>  13

 ---- POST FOR FILTERED TOKENS ----

 [('Wilson', 'NNP'), (',', ','), ('T.', 'NNP'), (',', ','), ('Wiebe', 'NNP'), (',', ','), ('J.', 'NNP'), (',', ','), ('Hoffmann', 'NNP'), (',', ','), ('P.', 'NNP'), ('2009', 'CD'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Wilson ,', ', T.', 'T. ,', ', Wiebe', 'Wiebe ,', ', J.', 'J. ,', ', Hoffmann', 'Hoffmann ,', ', P.', 'P. 2009', '2009 .'] 

 TOTAL BIGRAMS --> 12 



 ---- TRI-GRAMS ---- 

 ['Wilson , T.', ', T. ,', 'T. , Wiebe', ', Wiebe ,', 'Wiebe , J.', ', J. ,', 'J. , Hoffmann', ', Hoffmann ,', 'Hoffmann , P.', ', P. 2009', 'P. 2009 .'] 

 TOTAL TRIGRAMS --> 11 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> ['Wilson', 'Wiebe']
 TOTAL PERSON ENTITY --> 2 


 GPE ---> ['Hoffmann']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['wilson', ',', 't.', ',', 'wieb', ',', 'j.', ',', 'hoffmann', ',', 'p.', '2009', '.']

 TOTAL PORTER STEM WORDS ==> 13



 ---- SNOWBALL STEMMING ----

['wilson', ',', 't.', ',', 'wieb', ',', 'j.', ',', 'hoffmann', ',', 'p.', '2009', '.']

 TOTAL SNOWBALL STEM WORDS ==> 13



 ---- LEMMATIZATION ----

['Wilson', ',', 'T.', ',', 'Wiebe', ',', 'J.', ',', 'Hoffmann', ',', 'P.', '2009', '.']

 TOTAL LEMMATIZE WORDS ==> 13

************************************************************************************************************************

86 --> Recognizing contextual polarity: An exploration of features for  phrase level sentiment analysis. 


 ---- TOKENS ----

 ['Recognizing', 'contextual', 'polarity', ':', 'An', 'exploration', 'of', 'features', 'for', 'phrase', 'level', 'sentiment', 'analysis', '.'] 

 TOTAL TOKENS ==> 14

 ---- POST ----

 [('Recognizing', 'VBG'), ('contextual', 'JJ'), ('polarity', 'NN'), (':', ':'), ('An', 'DT'), ('exploration', 'NN'), ('of', 'IN'), ('features', 'NNS'), ('for', 'IN'), ('phrase', 'NN'), ('level', 'NN'), ('sentiment', 'NN'), ('analysis', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Recognizing', 'contextual', 'polarity', ':', 'exploration', 'features', 'phrase', 'level', 'sentiment', 'analysis', '.']

 TOTAL FILTERED TOKENS ==>  11

 ---- POST FOR FILTERED TOKENS ----

 [('Recognizing', 'VBG'), ('contextual', 'JJ'), ('polarity', 'NN'), (':', ':'), ('exploration', 'NN'), ('features', 'VBZ'), ('phrase', 'JJ'), ('level', 'NN'), ('sentiment', 'NN'), ('analysis', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Recognizing contextual', 'contextual polarity', 'polarity :', ': exploration', 'exploration features', 'features phrase', 'phrase level', 'level sentiment', 'sentiment analysis', 'analysis .'] 

 TOTAL BIGRAMS --> 10 



 ---- TRI-GRAMS ---- 

 ['Recognizing contextual polarity', 'contextual polarity :', 'polarity : exploration', ': exploration features', 'exploration features phrase', 'features phrase level', 'phrase level sentiment', 'level sentiment analysis', 'sentiment analysis .'] 

 TOTAL TRIGRAMS --> 9 



 ---- NOUN PHRASES ---- 

 ['contextual polarity', 'exploration', 'phrase level', 'sentiment', 'analysis'] 

 TOTAL NOUN PHRASES --> 5 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['recogn', 'contextu', 'polar', ':', 'explor', 'featur', 'phrase', 'level', 'sentiment', 'analysi', '.']

 TOTAL PORTER STEM WORDS ==> 11



 ---- SNOWBALL STEMMING ----

['recogn', 'contextu', 'polar', ':', 'explor', 'featur', 'phrase', 'level', 'sentiment', 'analysi', '.']

 TOTAL SNOWBALL STEM WORDS ==> 11



 ---- LEMMATIZATION ----

['Recognizing', 'contextual', 'polarity', ':', 'exploration', 'feature', 'phrase', 'level', 'sentiment', 'analysis', '.']

 TOTAL LEMMATIZE WORDS ==> 11

************************************************************************************************************************

87 --> Computational Linguistics, 35(3), pp. 


 ---- TOKENS ----

 ['Computational', 'Linguistics', ',', '35', '(', '3', ')', ',', 'pp', '.'] 

 TOTAL TOKENS ==> 10

 ---- POST ----

 [('Computational', 'JJ'), ('Linguistics', 'NNP'), (',', ','), ('35', 'CD'), ('(', '('), ('3', 'CD'), (')', ')'), (',', ','), ('pp', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Computational', 'Linguistics', ',', '35', '(', '3', ')', ',', 'pp', '.']

 TOTAL FILTERED TOKENS ==>  10

 ---- POST FOR FILTERED TOKENS ----

 [('Computational', 'JJ'), ('Linguistics', 'NNP'), (',', ','), ('35', 'CD'), ('(', '('), ('3', 'CD'), (')', ')'), (',', ','), ('pp', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Computational Linguistics', 'Linguistics ,', ', 35', '35 (', '( 3', '3 )', ') ,', ', pp', 'pp .'] 

 TOTAL BIGRAMS --> 9 



 ---- TRI-GRAMS ---- 

 ['Computational Linguistics ,', 'Linguistics , 35', ', 35 (', '35 ( 3', '( 3 )', '3 ) ,', ') , pp', ', pp .'] 

 TOTAL TRIGRAMS --> 8 



 ---- NOUN PHRASES ---- 

 ['pp'] 

 TOTAL NOUN PHRASES --> 1 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['comput', 'linguist', ',', '35', '(', '3', ')', ',', 'pp', '.']

 TOTAL PORTER STEM WORDS ==> 10



 ---- SNOWBALL STEMMING ----

['comput', 'linguist', ',', '35', '(', '3', ')', ',', 'pp', '.']

 TOTAL SNOWBALL STEM WORDS ==> 10



 ---- LEMMATIZATION ----

['Computational', 'Linguistics', ',', '35', '(', '3', ')', ',', 'pp', '.']

 TOTAL LEMMATIZE WORDS ==> 10

************************************************************************************************************************

88 --> 399-433. 


 ---- TOKENS ----

 ['399-433', '.'] 

 TOTAL TOKENS ==> 2

 ---- POST ----

 [('399-433', 'JJ'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['399-433', '.']

 TOTAL FILTERED TOKENS ==>  2

 ---- POST FOR FILTERED TOKENS ----

 [('399-433', 'JJ'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['399-433 .'] 

 TOTAL BIGRAMS --> 1 



 ---- TRI-GRAMS ---- 

 [] 

 TOTAL TRIGRAMS --> 0 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['399-433', '.']

 TOTAL PORTER STEM WORDS ==> 2



 ---- SNOWBALL STEMMING ----

['399-433', '.']

 TOTAL SNOWBALL STEM WORDS ==> 2



 ---- LEMMATIZATION ----

['399-433', '.']

 TOTAL LEMMATIZE WORDS ==> 2

************************************************************************************************************************

89 --> 10. 


 ---- TOKENS ----

 ['10', '.'] 

 TOTAL TOKENS ==> 2

 ---- POST ----

 [('10', 'CD'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['10', '.']

 TOTAL FILTERED TOKENS ==>  2

 ---- POST FOR FILTERED TOKENS ----

 [('10', 'CD'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['10 .'] 

 TOTAL BIGRAMS --> 1 



 ---- TRI-GRAMS ---- 

 [] 

 TOTAL TRIGRAMS --> 0 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['10', '.']

 TOTAL PORTER STEM WORDS ==> 2



 ---- SNOWBALL STEMMING ----

['10', '.']

 TOTAL SNOWBALL STEM WORDS ==> 2



 ---- LEMMATIZATION ----

['10', '.']

 TOTAL LEMMATIZE WORDS ==> 2

************************************************************************************************************************

90 --> Hatzivassiloglou, V. & McKeown, K.R. 


 ---- TOKENS ----

 ['Hatzivassiloglou', ',', 'V.', '&', 'McKeown', ',', 'K.R', '.'] 

 TOTAL TOKENS ==> 8

 ---- POST ----

 [('Hatzivassiloglou', 'NNP'), (',', ','), ('V.', 'NNP'), ('&', 'CC'), ('McKeown', 'NNP'), (',', ','), ('K.R', 'NNP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Hatzivassiloglou', ',', 'V.', '&', 'McKeown', ',', 'K.R', '.']

 TOTAL FILTERED TOKENS ==>  8

 ---- POST FOR FILTERED TOKENS ----

 [('Hatzivassiloglou', 'NNP'), (',', ','), ('V.', 'NNP'), ('&', 'CC'), ('McKeown', 'NNP'), (',', ','), ('K.R', 'NNP'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Hatzivassiloglou ,', ', V.', 'V. &', '& McKeown', 'McKeown ,', ', K.R', 'K.R .'] 

 TOTAL BIGRAMS --> 7 



 ---- TRI-GRAMS ---- 

 ['Hatzivassiloglou , V.', ', V. &', 'V. & McKeown', '& McKeown ,', 'McKeown , K.R', ', K.R .'] 

 TOTAL TRIGRAMS --> 6 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> ['McKeown']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> ['Hatzivassiloglou']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['hatzivassilogl', ',', 'v.', '&', 'mckeown', ',', 'k.r', '.']

 TOTAL PORTER STEM WORDS ==> 8



 ---- SNOWBALL STEMMING ----

['hatzivassiloglou', ',', 'v.', '&', 'mckeown', ',', 'k.r', '.']

 TOTAL SNOWBALL STEM WORDS ==> 8



 ---- LEMMATIZATION ----

['Hatzivassiloglou', ',', 'V.', '&', 'McKeown', ',', 'K.R', '.']

 TOTAL LEMMATIZE WORDS ==> 8

************************************************************************************************************************

91 --> 1997. 


 ---- TOKENS ----

 ['1997', '.'] 

 TOTAL TOKENS ==> 2

 ---- POST ----

 [('1997', 'CD'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['1997', '.']

 TOTAL FILTERED TOKENS ==>  2

 ---- POST FOR FILTERED TOKENS ----

 [('1997', 'CD'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['1997 .'] 

 TOTAL BIGRAMS --> 1 



 ---- TRI-GRAMS ---- 

 [] 

 TOTAL TRIGRAMS --> 0 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['1997', '.']

 TOTAL PORTER STEM WORDS ==> 2



 ---- SNOWBALL STEMMING ----

['1997', '.']

 TOTAL SNOWBALL STEM WORDS ==> 2



 ---- LEMMATIZATION ----

['1997', '.']

 TOTAL LEMMATIZE WORDS ==> 2

************************************************************************************************************************

92 --> Predicting the semantic orientation of adjectives. 


 ---- TOKENS ----

 ['Predicting', 'the', 'semantic', 'orientation', 'of', 'adjectives', '.'] 

 TOTAL TOKENS ==> 7

 ---- POST ----

 [('Predicting', 'VBG'), ('the', 'DT'), ('semantic', 'JJ'), ('orientation', 'NN'), ('of', 'IN'), ('adjectives', 'NNS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Predicting', 'semantic', 'orientation', 'adjectives', '.']

 TOTAL FILTERED TOKENS ==>  5

 ---- POST FOR FILTERED TOKENS ----

 [('Predicting', 'VBG'), ('semantic', 'JJ'), ('orientation', 'NN'), ('adjectives', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Predicting semantic', 'semantic orientation', 'orientation adjectives', 'adjectives .'] 

 TOTAL BIGRAMS --> 4 



 ---- TRI-GRAMS ---- 

 ['Predicting semantic orientation', 'semantic orientation adjectives', 'orientation adjectives .'] 

 TOTAL TRIGRAMS --> 3 



 ---- NOUN PHRASES ---- 

 ['semantic orientation'] 

 TOTAL NOUN PHRASES --> 1 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['predict', 'semant', 'orient', 'adject', '.']

 TOTAL PORTER STEM WORDS ==> 5



 ---- SNOWBALL STEMMING ----

['predict', 'semant', 'orient', 'adject', '.']

 TOTAL SNOWBALL STEM WORDS ==> 5



 ---- LEMMATIZATION ----

['Predicting', 'semantic', 'orientation', 'adjective', '.']

 TOTAL LEMMATIZE WORDS ==> 5

************************************************************************************************************************

93 --> In  Proceedings of the 8th conference on European chapter of the association for computational linguistics  Madrid, Spain, pp.174-181. 


 ---- TOKENS ----

 ['In', 'Proceedings', 'of', 'the', '8th', 'conference', 'on', 'European', 'chapter', 'of', 'the', 'association', 'for', 'computational', 'linguistics', 'Madrid', ',', 'Spain', ',', 'pp.174-181', '.'] 

 TOTAL TOKENS ==> 21

 ---- POST ----

 [('In', 'IN'), ('Proceedings', 'NNP'), ('of', 'IN'), ('the', 'DT'), ('8th', 'CD'), ('conference', 'NN'), ('on', 'IN'), ('European', 'JJ'), ('chapter', 'NN'), ('of', 'IN'), ('the', 'DT'), ('association', 'NN'), ('for', 'IN'), ('computational', 'JJ'), ('linguistics', 'NNS'), ('Madrid', 'NNP'), (',', ','), ('Spain', 'NNP'), (',', ','), ('pp.174-181', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Proceedings', '8th', 'conference', 'European', 'chapter', 'association', 'computational', 'linguistics', 'Madrid', ',', 'Spain', ',', 'pp.174-181', '.']

 TOTAL FILTERED TOKENS ==>  14

 ---- POST FOR FILTERED TOKENS ----

 [('Proceedings', 'NNS'), ('8th', 'CD'), ('conference', 'NN'), ('European', 'NNP'), ('chapter', 'NN'), ('association', 'NN'), ('computational', 'JJ'), ('linguistics', 'NNS'), ('Madrid', 'NNP'), (',', ','), ('Spain', 'NNP'), (',', ','), ('pp.174-181', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Proceedings 8th', '8th conference', 'conference European', 'European chapter', 'chapter association', 'association computational', 'computational linguistics', 'linguistics Madrid', 'Madrid ,', ', Spain', 'Spain ,', ', pp.174-181', 'pp.174-181 .'] 

 TOTAL BIGRAMS --> 13 



 ---- TRI-GRAMS ---- 

 ['Proceedings 8th conference', '8th conference European', 'conference European chapter', 'European chapter association', 'chapter association computational', 'association computational linguistics', 'computational linguistics Madrid', 'linguistics Madrid ,', 'Madrid , Spain', ', Spain ,', 'Spain , pp.174-181', ', pp.174-181 .'] 

 TOTAL TRIGRAMS --> 12 



 ---- NOUN PHRASES ---- 

 ['conference', 'chapter', 'association', 'pp.174-181'] 

 TOTAL NOUN PHRASES --> 4 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> ['European', 'Madrid', 'Spain']
 TOTAL GPE ENTITY --> 3 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['proceed', '8th', 'confer', 'european', 'chapter', 'associ', 'comput', 'linguist', 'madrid', ',', 'spain', ',', 'pp.174-181', '.']

 TOTAL PORTER STEM WORDS ==> 14



 ---- SNOWBALL STEMMING ----

['proceed', '8th', 'confer', 'european', 'chapter', 'associ', 'comput', 'linguist', 'madrid', ',', 'spain', ',', 'pp.174-181', '.']

 TOTAL SNOWBALL STEM WORDS ==> 14



 ---- LEMMATIZATION ----

['Proceedings', '8th', 'conference', 'European', 'chapter', 'association', 'computational', 'linguistics', 'Madrid', ',', 'Spain', ',', 'pp.174-181', '.']

 TOTAL LEMMATIZE WORDS ==> 14

************************************************************************************************************************

94 --> 11. 


 ---- TOKENS ----

 ['11', '.'] 

 TOTAL TOKENS ==> 2

 ---- POST ----

 [('11', 'CD'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['11', '.']

 TOTAL FILTERED TOKENS ==>  2

 ---- POST FOR FILTERED TOKENS ----

 [('11', 'CD'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['11 .'] 

 TOTAL BIGRAMS --> 1 



 ---- TRI-GRAMS ---- 

 [] 

 TOTAL TRIGRAMS --> 0 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['11', '.']

 TOTAL PORTER STEM WORDS ==> 2



 ---- SNOWBALL STEMMING ----

['11', '.']

 TOTAL SNOWBALL STEM WORDS ==> 2



 ---- LEMMATIZATION ----

['11', '.']

 TOTAL LEMMATIZE WORDS ==> 2

************************************************************************************************************************

95 --> Pang, B., & Lee, L. 2004. 


 ---- TOKENS ----

 ['Pang', ',', 'B.', ',', '&', 'Lee', ',', 'L.', '2004', '.'] 

 TOTAL TOKENS ==> 10

 ---- POST ----

 [('Pang', 'NNP'), (',', ','), ('B.', 'NNP'), (',', ','), ('&', 'CC'), ('Lee', 'NNP'), (',', ','), ('L.', 'NNP'), ('2004', 'CD'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Pang', ',', 'B.', ',', '&', 'Lee', ',', 'L.', '2004', '.']

 TOTAL FILTERED TOKENS ==>  10

 ---- POST FOR FILTERED TOKENS ----

 [('Pang', 'NNP'), (',', ','), ('B.', 'NNP'), (',', ','), ('&', 'CC'), ('Lee', 'NNP'), (',', ','), ('L.', 'NNP'), ('2004', 'CD'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Pang ,', ', B.', 'B. ,', ', &', '& Lee', 'Lee ,', ', L.', 'L. 2004', '2004 .'] 

 TOTAL BIGRAMS --> 9 



 ---- TRI-GRAMS ---- 

 ['Pang , B.', ', B. ,', 'B. , &', ', & Lee', '& Lee ,', 'Lee , L.', ', L. 2004', 'L. 2004 .'] 

 TOTAL TRIGRAMS --> 8 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> ['Lee']
 TOTAL PERSON ENTITY --> 1 


 GPE ---> ['Pang']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['pang', ',', 'b.', ',', '&', 'lee', ',', 'l.', '2004', '.']

 TOTAL PORTER STEM WORDS ==> 10



 ---- SNOWBALL STEMMING ----

['pang', ',', 'b.', ',', '&', 'lee', ',', 'l.', '2004', '.']

 TOTAL SNOWBALL STEM WORDS ==> 10



 ---- LEMMATIZATION ----

['Pang', ',', 'B.', ',', '&', 'Lee', ',', 'L.', '2004', '.']

 TOTAL LEMMATIZE WORDS ==> 10

************************************************************************************************************************

96 --> A sentimental education: sentiment analysis using subjectivity summarization  based on minimum cuts. 


 ---- TOKENS ----

 ['A', 'sentimental', 'education', ':', 'sentiment', 'analysis', 'using', 'subjectivity', 'summarization', 'based', 'on', 'minimum', 'cuts', '.'] 

 TOTAL TOKENS ==> 14

 ---- POST ----

 [('A', 'DT'), ('sentimental', 'JJ'), ('education', 'NN'), (':', ':'), ('sentiment', 'NN'), ('analysis', 'NN'), ('using', 'VBG'), ('subjectivity', 'NN'), ('summarization', 'NN'), ('based', 'VBN'), ('on', 'IN'), ('minimum', 'JJ'), ('cuts', 'NNS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['sentimental', 'education', ':', 'sentiment', 'analysis', 'using', 'subjectivity', 'summarization', 'based', 'minimum', 'cuts', '.']

 TOTAL FILTERED TOKENS ==>  12

 ---- POST FOR FILTERED TOKENS ----

 [('sentimental', 'JJ'), ('education', 'NN'), (':', ':'), ('sentiment', 'NN'), ('analysis', 'NN'), ('using', 'VBG'), ('subjectivity', 'NN'), ('summarization', 'NN'), ('based', 'VBN'), ('minimum', 'JJ'), ('cuts', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['sentimental education', 'education :', ': sentiment', 'sentiment analysis', 'analysis using', 'using subjectivity', 'subjectivity summarization', 'summarization based', 'based minimum', 'minimum cuts', 'cuts .'] 

 TOTAL BIGRAMS --> 11 



 ---- TRI-GRAMS ---- 

 ['sentimental education :', 'education : sentiment', ': sentiment analysis', 'sentiment analysis using', 'analysis using subjectivity', 'using subjectivity summarization', 'subjectivity summarization based', 'summarization based minimum', 'based minimum cuts', 'minimum cuts .'] 

 TOTAL TRIGRAMS --> 10 



 ---- NOUN PHRASES ---- 

 ['sentimental education', 'sentiment', 'analysis', 'subjectivity', 'summarization'] 

 TOTAL NOUN PHRASES --> 5 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['sentiment', 'educ', ':', 'sentiment', 'analysi', 'use', 'subject', 'summar', 'base', 'minimum', 'cut', '.']

 TOTAL PORTER STEM WORDS ==> 12



 ---- SNOWBALL STEMMING ----

['sentiment', 'educ', ':', 'sentiment', 'analysi', 'use', 'subject', 'summar', 'base', 'minimum', 'cut', '.']

 TOTAL SNOWBALL STEM WORDS ==> 12



 ---- LEMMATIZATION ----

['sentimental', 'education', ':', 'sentiment', 'analysis', 'using', 'subjectivity', 'summarization', 'based', 'minimum', 'cut', '.']

 TOTAL LEMMATIZE WORDS ==> 12

************************************************************************************************************************

97 --> In Proceedings of the 42nd annual meeting of the Association for Computational  Linguistics (ACL), pp. 


 ---- TOKENS ----

 ['In', 'Proceedings', 'of', 'the', '42nd', 'annual', 'meeting', 'of', 'the', 'Association', 'for', 'Computational', 'Linguistics', '(', 'ACL', ')', ',', 'pp', '.'] 

 TOTAL TOKENS ==> 19

 ---- POST ----

 [('In', 'IN'), ('Proceedings', 'NNP'), ('of', 'IN'), ('the', 'DT'), ('42nd', 'CD'), ('annual', 'JJ'), ('meeting', 'NN'), ('of', 'IN'), ('the', 'DT'), ('Association', 'NNP'), ('for', 'IN'), ('Computational', 'NNP'), ('Linguistics', 'NNP'), ('(', '('), ('ACL', 'NNP'), (')', ')'), (',', ','), ('pp', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Proceedings', '42nd', 'annual', 'meeting', 'Association', 'Computational', 'Linguistics', '(', 'ACL', ')', ',', 'pp', '.']

 TOTAL FILTERED TOKENS ==>  13

 ---- POST FOR FILTERED TOKENS ----

 [('Proceedings', 'NNS'), ('42nd', 'CD'), ('annual', 'JJ'), ('meeting', 'NN'), ('Association', 'NNP'), ('Computational', 'NNP'), ('Linguistics', 'NNP'), ('(', '('), ('ACL', 'NNP'), (')', ')'), (',', ','), ('pp', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Proceedings 42nd', '42nd annual', 'annual meeting', 'meeting Association', 'Association Computational', 'Computational Linguistics', 'Linguistics (', '( ACL', 'ACL )', ') ,', ', pp', 'pp .'] 

 TOTAL BIGRAMS --> 12 



 ---- TRI-GRAMS ---- 

 ['Proceedings 42nd annual', '42nd annual meeting', 'annual meeting Association', 'meeting Association Computational', 'Association Computational Linguistics', 'Computational Linguistics (', 'Linguistics ( ACL', '( ACL )', 'ACL ) ,', ') , pp', ', pp .'] 

 TOTAL TRIGRAMS --> 11 



 ---- NOUN PHRASES ---- 

 ['annual meeting', 'pp'] 

 TOTAL NOUN PHRASES --> 2 



 ---- NER ----

 
 ORGANIZATION ---> ['Association Computational Linguistics']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['proceed', '42nd', 'annual', 'meet', 'associ', 'comput', 'linguist', '(', 'acl', ')', ',', 'pp', '.']

 TOTAL PORTER STEM WORDS ==> 13



 ---- SNOWBALL STEMMING ----

['proceed', '42nd', 'annual', 'meet', 'associ', 'comput', 'linguist', '(', 'acl', ')', ',', 'pp', '.']

 TOTAL SNOWBALL STEM WORDS ==> 13



 ---- LEMMATIZATION ----

['Proceedings', '42nd', 'annual', 'meeting', 'Association', 'Computational', 'Linguistics', '(', 'ACL', ')', ',', 'pp', '.']

 TOTAL LEMMATIZE WORDS ==> 13

************************************************************************************************************************

98 --> 271–278. 


 ---- TOKENS ----

 ['271–278', '.'] 

 TOTAL TOKENS ==> 2

 ---- POST ----

 [('271–278', 'CD'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['271–278', '.']

 TOTAL FILTERED TOKENS ==>  2

 ---- POST FOR FILTERED TOKENS ----

 [('271–278', 'CD'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['271–278 .'] 

 TOTAL BIGRAMS --> 1 



 ---- TRI-GRAMS ---- 

 [] 

 TOTAL TRIGRAMS --> 0 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['271–278', '.']

 TOTAL PORTER STEM WORDS ==> 2



 ---- SNOWBALL STEMMING ----

['271–278', '.']

 TOTAL SNOWBALL STEM WORDS ==> 2



 ---- LEMMATIZATION ----

['271–278', '.']

 TOTAL LEMMATIZE WORDS ==> 2

************************************************************************************************************************

99 --> Barcelona, Spain  12. 


 ---- TOKENS ----

 ['Barcelona', ',', 'Spain', '12', '.'] 

 TOTAL TOKENS ==> 5

 ---- POST ----

 [('Barcelona', 'NNP'), (',', ','), ('Spain', 'NNP'), ('12', 'CD'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Barcelona', ',', 'Spain', '12', '.']

 TOTAL FILTERED TOKENS ==>  5

 ---- POST FOR FILTERED TOKENS ----

 [('Barcelona', 'NNP'), (',', ','), ('Spain', 'NNP'), ('12', 'CD'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Barcelona ,', ', Spain', 'Spain 12', '12 .'] 

 TOTAL BIGRAMS --> 4 



 ---- TRI-GRAMS ---- 

 ['Barcelona , Spain', ', Spain 12', 'Spain 12 .'] 

 TOTAL TRIGRAMS --> 3 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> ['Barcelona', 'Spain']
 TOTAL GPE ENTITY --> 2 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['barcelona', ',', 'spain', '12', '.']

 TOTAL PORTER STEM WORDS ==> 5



 ---- SNOWBALL STEMMING ----

['barcelona', ',', 'spain', '12', '.']

 TOTAL SNOWBALL STEM WORDS ==> 5



 ---- LEMMATIZATION ----

['Barcelona', ',', 'Spain', '12', '.']

 TOTAL LEMMATIZE WORDS ==> 5

************************************************************************************************************************

100 --> Yi, J., Nasukawa, T., Niblack, W., Bunescu, R.  2003. 


 ---- TOKENS ----

 ['Yi', ',', 'J.', ',', 'Nasukawa', ',', 'T.', ',', 'Niblack', ',', 'W.', ',', 'Bunescu', ',', 'R.', '2003', '.'] 

 TOTAL TOKENS ==> 17

 ---- POST ----

 [('Yi', 'NN'), (',', ','), ('J.', 'NNP'), (',', ','), ('Nasukawa', 'NNP'), (',', ','), ('T.', 'NNP'), (',', ','), ('Niblack', 'NNP'), (',', ','), ('W.', 'NNP'), (',', ','), ('Bunescu', 'NNP'), (',', ','), ('R.', 'NNP'), ('2003', 'CD'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Yi', ',', 'J.', ',', 'Nasukawa', ',', 'T.', ',', 'Niblack', ',', 'W.', ',', 'Bunescu', ',', 'R.', '2003', '.']

 TOTAL FILTERED TOKENS ==>  17

 ---- POST FOR FILTERED TOKENS ----

 [('Yi', 'NN'), (',', ','), ('J.', 'NNP'), (',', ','), ('Nasukawa', 'NNP'), (',', ','), ('T.', 'NNP'), (',', ','), ('Niblack', 'NNP'), (',', ','), ('W.', 'NNP'), (',', ','), ('Bunescu', 'NNP'), (',', ','), ('R.', 'NNP'), ('2003', 'CD'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Yi ,', ', J.', 'J. ,', ', Nasukawa', 'Nasukawa ,', ', T.', 'T. ,', ', Niblack', 'Niblack ,', ', W.', 'W. ,', ', Bunescu', 'Bunescu ,', ', R.', 'R. 2003', '2003 .'] 

 TOTAL BIGRAMS --> 16 



 ---- TRI-GRAMS ---- 

 ['Yi , J.', ', J. ,', 'J. , Nasukawa', ', Nasukawa ,', 'Nasukawa , T.', ', T. ,', 'T. , Niblack', ', Niblack ,', 'Niblack , W.', ', W. ,', 'W. , Bunescu', ', Bunescu ,', 'Bunescu , R.', ', R. 2003', 'R. 2003 .'] 

 TOTAL TRIGRAMS --> 15 



 ---- NOUN PHRASES ---- 

 ['Yi'] 

 TOTAL NOUN PHRASES --> 1 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> ['Bunescu']
 TOTAL PERSON ENTITY --> 1 


 GPE ---> ['Yi', 'Nasukawa', 'Niblack']
 TOTAL GPE ENTITY --> 3 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['yi', ',', 'j.', ',', 'nasukawa', ',', 't.', ',', 'niblack', ',', 'w.', ',', 'bunescu', ',', 'r.', '2003', '.']

 TOTAL PORTER STEM WORDS ==> 17



 ---- SNOWBALL STEMMING ----

['yi', ',', 'j.', ',', 'nasukawa', ',', 't.', ',', 'niblack', ',', 'w.', ',', 'bunescu', ',', 'r.', '2003', '.']

 TOTAL SNOWBALL STEM WORDS ==> 17



 ---- LEMMATIZATION ----

['Yi', ',', 'J.', ',', 'Nasukawa', ',', 'T.', ',', 'Niblack', ',', 'W.', ',', 'Bunescu', ',', 'R.', '2003', '.']

 TOTAL LEMMATIZE WORDS ==> 17

************************************************************************************************************************

101 --> Sentiment analyzer:  extracting sentiments about a  given topic using natural language processing techniques. 


 ---- TOKENS ----

 ['Sentiment', 'analyzer', ':', 'extracting', 'sentiments', 'about', 'a', 'given', 'topic', 'using', 'natural', 'language', 'processing', 'techniques', '.'] 

 TOTAL TOKENS ==> 15

 ---- POST ----

 [('Sentiment', 'JJ'), ('analyzer', 'NN'), (':', ':'), ('extracting', 'NN'), ('sentiments', 'NNS'), ('about', 'IN'), ('a', 'DT'), ('given', 'VBN'), ('topic', 'NN'), ('using', 'VBG'), ('natural', 'JJ'), ('language', 'NN'), ('processing', 'NN'), ('techniques', 'NNS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Sentiment', 'analyzer', ':', 'extracting', 'sentiments', 'given', 'topic', 'using', 'natural', 'language', 'processing', 'techniques', '.']

 TOTAL FILTERED TOKENS ==>  13

 ---- POST FOR FILTERED TOKENS ----

 [('Sentiment', 'JJ'), ('analyzer', 'NN'), (':', ':'), ('extracting', 'NN'), ('sentiments', 'NNS'), ('given', 'VBN'), ('topic', 'RP'), ('using', 'VBG'), ('natural', 'JJ'), ('language', 'NN'), ('processing', 'NN'), ('techniques', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Sentiment analyzer', 'analyzer :', ': extracting', 'extracting sentiments', 'sentiments given', 'given topic', 'topic using', 'using natural', 'natural language', 'language processing', 'processing techniques', 'techniques .'] 

 TOTAL BIGRAMS --> 12 



 ---- TRI-GRAMS ---- 

 ['Sentiment analyzer :', 'analyzer : extracting', ': extracting sentiments', 'extracting sentiments given', 'sentiments given topic', 'given topic using', 'topic using natural', 'using natural language', 'natural language processing', 'language processing techniques', 'processing techniques .'] 

 TOTAL TRIGRAMS --> 11 



 ---- NOUN PHRASES ---- 

 ['Sentiment analyzer', 'extracting', 'natural language', 'processing'] 

 TOTAL NOUN PHRASES --> 4 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['sentiment', 'analyz', ':', 'extract', 'sentiment', 'given', 'topic', 'use', 'natur', 'languag', 'process', 'techniqu', '.']

 TOTAL PORTER STEM WORDS ==> 13



 ---- SNOWBALL STEMMING ----

['sentiment', 'analyz', ':', 'extract', 'sentiment', 'given', 'topic', 'use', 'natur', 'languag', 'process', 'techniqu', '.']

 TOTAL SNOWBALL STEM WORDS ==> 13



 ---- LEMMATIZATION ----

['Sentiment', 'analyzer', ':', 'extracting', 'sentiment', 'given', 'topic', 'using', 'natural', 'language', 'processing', 'technique', '.']

 TOTAL LEMMATIZE WORDS ==> 13

************************************************************************************************************************

102 --> In Proceedings of the 3rd IEEE international  conference on data mining (ICDM 2003), November 19–22, 2003, pp. 


 ---- TOKENS ----

 ['In', 'Proceedings', 'of', 'the', '3rd', 'IEEE', 'international', 'conference', 'on', 'data', 'mining', '(', 'ICDM', '2003', ')', ',', 'November', '19–22', ',', '2003', ',', 'pp', '.'] 

 TOTAL TOKENS ==> 23

 ---- POST ----

 [('In', 'IN'), ('Proceedings', 'NNP'), ('of', 'IN'), ('the', 'DT'), ('3rd', 'CD'), ('IEEE', 'NNP'), ('international', 'JJ'), ('conference', 'NN'), ('on', 'IN'), ('data', 'NNS'), ('mining', 'NN'), ('(', '('), ('ICDM', 'NNP'), ('2003', 'CD'), (')', ')'), (',', ','), ('November', 'NNP'), ('19–22', 'CD'), (',', ','), ('2003', 'CD'), (',', ','), ('pp', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Proceedings', '3rd', 'IEEE', 'international', 'conference', 'data', 'mining', '(', 'ICDM', '2003', ')', ',', 'November', '19–22', ',', '2003', ',', 'pp', '.']

 TOTAL FILTERED TOKENS ==>  19

 ---- POST FOR FILTERED TOKENS ----

 [('Proceedings', 'NNS'), ('3rd', 'CD'), ('IEEE', 'NNP'), ('international', 'JJ'), ('conference', 'NN'), ('data', 'NNS'), ('mining', 'NN'), ('(', '('), ('ICDM', 'NNP'), ('2003', 'CD'), (')', ')'), (',', ','), ('November', 'NNP'), ('19–22', 'CD'), (',', ','), ('2003', 'CD'), (',', ','), ('pp', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Proceedings 3rd', '3rd IEEE', 'IEEE international', 'international conference', 'conference data', 'data mining', 'mining (', '( ICDM', 'ICDM 2003', '2003 )', ') ,', ', November', 'November 19–22', '19–22 ,', ', 2003', '2003 ,', ', pp', 'pp .'] 

 TOTAL BIGRAMS --> 18 



 ---- TRI-GRAMS ---- 

 ['Proceedings 3rd IEEE', '3rd IEEE international', 'IEEE international conference', 'international conference data', 'conference data mining', 'data mining (', 'mining ( ICDM', '( ICDM 2003', 'ICDM 2003 )', '2003 ) ,', ') , November', ', November 19–22', 'November 19–22 ,', '19–22 , 2003', ', 2003 ,', '2003 , pp', ', pp .'] 

 TOTAL TRIGRAMS --> 17 



 ---- NOUN PHRASES ---- 

 ['international conference', 'mining', 'pp'] 

 TOTAL NOUN PHRASES --> 3 



 ---- NER ----

 
 ORGANIZATION ---> ['IEEE']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['proceed', '3rd', 'ieee', 'intern', 'confer', 'data', 'mine', '(', 'icdm', '2003', ')', ',', 'novemb', '19–22', ',', '2003', ',', 'pp', '.']

 TOTAL PORTER STEM WORDS ==> 19



 ---- SNOWBALL STEMMING ----

['proceed', '3rd', 'ieee', 'intern', 'confer', 'data', 'mine', '(', 'icdm', '2003', ')', ',', 'novemb', '19–22', ',', '2003', ',', 'pp', '.']

 TOTAL SNOWBALL STEM WORDS ==> 19



 ---- LEMMATIZATION ----

['Proceedings', '3rd', 'IEEE', 'international', 'conference', 'data', 'mining', '(', 'ICDM', '2003', ')', ',', 'November', '19–22', ',', '2003', ',', 'pp', '.']

 TOTAL LEMMATIZE WORDS ==> 19

************************************************************************************************************************

103 --> 427-434 Florida, USA. 


 ---- TOKENS ----

 ['427-434', 'Florida', ',', 'USA', '.'] 

 TOTAL TOKENS ==> 5

 ---- POST ----

 [('427-434', 'JJ'), ('Florida', 'NNP'), (',', ','), ('USA', 'NNP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['427-434', 'Florida', ',', 'USA', '.']

 TOTAL FILTERED TOKENS ==>  5

 ---- POST FOR FILTERED TOKENS ----

 [('427-434', 'JJ'), ('Florida', 'NNP'), (',', ','), ('USA', 'NNP'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['427-434 Florida', 'Florida ,', ', USA', 'USA .'] 

 TOTAL BIGRAMS --> 4 



 ---- TRI-GRAMS ---- 

 ['427-434 Florida ,', 'Florida , USA', ', USA .'] 

 TOTAL TRIGRAMS --> 3 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> ['USA']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> ['Florida']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['427-434', 'florida', ',', 'usa', '.']

 TOTAL PORTER STEM WORDS ==> 5



 ---- SNOWBALL STEMMING ----

['427-434', 'florida', ',', 'usa', '.']

 TOTAL SNOWBALL STEM WORDS ==> 5



 ---- LEMMATIZATION ----

['427-434', 'Florida', ',', 'USA', '.']

 TOTAL LEMMATIZE WORDS ==> 5

************************************************************************************************************************

104 --> 13. 


 ---- TOKENS ----

 ['13', '.'] 

 TOTAL TOKENS ==> 2

 ---- POST ----

 [('13', 'CD'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['13', '.']

 TOTAL FILTERED TOKENS ==>  2

 ---- POST FOR FILTERED TOKENS ----

 [('13', 'CD'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['13 .'] 

 TOTAL BIGRAMS --> 1 



 ---- TRI-GRAMS ---- 

 [] 

 TOTAL TRIGRAMS --> 0 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['13', '.']

 TOTAL PORTER STEM WORDS ==> 2



 ---- SNOWBALL STEMMING ----

['13', '.']

 TOTAL SNOWBALL STEM WORDS ==> 2



 ---- LEMMATIZATION ----

['13', '.']

 TOTAL LEMMATIZE WORDS ==> 2

************************************************************************************************************************

105 --> Hiroshi, K., Tetsuya, N., Hideo, W. 2004. 


 ---- TOKENS ----

 ['Hiroshi', ',', 'K.', ',', 'Tetsuya', ',', 'N.', ',', 'Hideo', ',', 'W.', '2004', '.'] 

 TOTAL TOKENS ==> 13

 ---- POST ----

 [('Hiroshi', 'NNP'), (',', ','), ('K.', 'NNP'), (',', ','), ('Tetsuya', 'NNP'), (',', ','), ('N.', 'NNP'), (',', ','), ('Hideo', 'NNP'), (',', ','), ('W.', 'NNP'), ('2004', 'CD'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Hiroshi', ',', 'K.', ',', 'Tetsuya', ',', 'N.', ',', 'Hideo', ',', 'W.', '2004', '.']

 TOTAL FILTERED TOKENS ==>  13

 ---- POST FOR FILTERED TOKENS ----

 [('Hiroshi', 'NNP'), (',', ','), ('K.', 'NNP'), (',', ','), ('Tetsuya', 'NNP'), (',', ','), ('N.', 'NNP'), (',', ','), ('Hideo', 'NNP'), (',', ','), ('W.', 'NNP'), ('2004', 'CD'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Hiroshi ,', ', K.', 'K. ,', ', Tetsuya', 'Tetsuya ,', ', N.', 'N. ,', ', Hideo', 'Hideo ,', ', W.', 'W. 2004', '2004 .'] 

 TOTAL BIGRAMS --> 12 



 ---- TRI-GRAMS ---- 

 ['Hiroshi , K.', ', K. ,', 'K. , Tetsuya', ', Tetsuya ,', 'Tetsuya , N.', ', N. ,', 'N. , Hideo', ', Hideo ,', 'Hideo , W.', ', W. 2004', 'W. 2004 .'] 

 TOTAL TRIGRAMS --> 11 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> ['Tetsuya', 'Hideo']
 TOTAL PERSON ENTITY --> 2 


 GPE ---> ['Hiroshi']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['hiroshi', ',', 'k.', ',', 'tetsuya', ',', 'n.', ',', 'hideo', ',', 'w.', '2004', '.']

 TOTAL PORTER STEM WORDS ==> 13



 ---- SNOWBALL STEMMING ----

['hiroshi', ',', 'k.', ',', 'tetsuya', ',', 'n.', ',', 'hideo', ',', 'w.', '2004', '.']

 TOTAL SNOWBALL STEM WORDS ==> 13



 ---- LEMMATIZATION ----

['Hiroshi', ',', 'K.', ',', 'Tetsuya', ',', 'N.', ',', 'Hideo', ',', 'W.', '2004', '.']

 TOTAL LEMMATIZE WORDS ==> 13

************************************************************************************************************************

106 --> Deeper sentiment analysis using machine translation technology. 


 ---- TOKENS ----

 ['Deeper', 'sentiment', 'analysis', 'using', 'machine', 'translation', 'technology', '.'] 

 TOTAL TOKENS ==> 8

 ---- POST ----

 [('Deeper', 'NNP'), ('sentiment', 'NN'), ('analysis', 'NN'), ('using', 'VBG'), ('machine', 'NN'), ('translation', 'NN'), ('technology', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Deeper', 'sentiment', 'analysis', 'using', 'machine', 'translation', 'technology', '.']

 TOTAL FILTERED TOKENS ==>  8

 ---- POST FOR FILTERED TOKENS ----

 [('Deeper', 'NNP'), ('sentiment', 'NN'), ('analysis', 'NN'), ('using', 'VBG'), ('machine', 'NN'), ('translation', 'NN'), ('technology', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Deeper sentiment', 'sentiment analysis', 'analysis using', 'using machine', 'machine translation', 'translation technology', 'technology .'] 

 TOTAL BIGRAMS --> 7 



 ---- TRI-GRAMS ---- 

 ['Deeper sentiment analysis', 'sentiment analysis using', 'analysis using machine', 'using machine translation', 'machine translation technology', 'translation technology .'] 

 TOTAL TRIGRAMS --> 6 



 ---- NOUN PHRASES ---- 

 ['sentiment', 'analysis', 'machine', 'translation', 'technology'] 

 TOTAL NOUN PHRASES --> 5 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> ['Deeper']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['deeper', 'sentiment', 'analysi', 'use', 'machin', 'translat', 'technolog', '.']

 TOTAL PORTER STEM WORDS ==> 8



 ---- SNOWBALL STEMMING ----

['deeper', 'sentiment', 'analysi', 'use', 'machin', 'translat', 'technolog', '.']

 TOTAL SNOWBALL STEM WORDS ==> 8



 ---- LEMMATIZATION ----

['Deeper', 'sentiment', 'analysis', 'using', 'machine', 'translation', 'technology', '.']

 TOTAL LEMMATIZE WORDS ==> 8

************************************************************************************************************************

107 --> In Proceedings of the 20th international conference on computational linguistics (COLING 2004), August  23- 27, pp. 


 ---- TOKENS ----

 ['In', 'Proceedings', 'of', 'the', '20th', 'international', 'conference', 'on', 'computational', 'linguistics', '(', 'COLING', '2004', ')', ',', 'August', '23-', '27', ',', 'pp', '.'] 

 TOTAL TOKENS ==> 21

 ---- POST ----

 [('In', 'IN'), ('Proceedings', 'NNP'), ('of', 'IN'), ('the', 'DT'), ('20th', 'JJ'), ('international', 'JJ'), ('conference', 'NN'), ('on', 'IN'), ('computational', 'JJ'), ('linguistics', 'NNS'), ('(', '('), ('COLING', 'NNP'), ('2004', 'CD'), (')', ')'), (',', ','), ('August', 'NNP'), ('23-', 'CD'), ('27', 'CD'), (',', ','), ('pp', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Proceedings', '20th', 'international', 'conference', 'computational', 'linguistics', '(', 'COLING', '2004', ')', ',', 'August', '23-', '27', ',', 'pp', '.']

 TOTAL FILTERED TOKENS ==>  17

 ---- POST FOR FILTERED TOKENS ----

 [('Proceedings', 'NNS'), ('20th', 'CD'), ('international', 'JJ'), ('conference', 'NN'), ('computational', 'JJ'), ('linguistics', 'NNS'), ('(', '('), ('COLING', 'NNP'), ('2004', 'CD'), (')', ')'), (',', ','), ('August', 'NNP'), ('23-', 'CD'), ('27', 'CD'), (',', ','), ('pp', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Proceedings 20th', '20th international', 'international conference', 'conference computational', 'computational linguistics', 'linguistics (', '( COLING', 'COLING 2004', '2004 )', ') ,', ', August', 'August 23-', '23- 27', '27 ,', ', pp', 'pp .'] 

 TOTAL BIGRAMS --> 16 



 ---- TRI-GRAMS ---- 

 ['Proceedings 20th international', '20th international conference', 'international conference computational', 'conference computational linguistics', 'computational linguistics (', 'linguistics ( COLING', '( COLING 2004', 'COLING 2004 )', '2004 ) ,', ') , August', ', August 23-', 'August 23- 27', '23- 27 ,', '27 , pp', ', pp .'] 

 TOTAL TRIGRAMS --> 15 



 ---- NOUN PHRASES ---- 

 ['international conference', 'pp'] 

 TOTAL NOUN PHRASES --> 2 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['proceed', '20th', 'intern', 'confer', 'comput', 'linguist', '(', 'cole', '2004', ')', ',', 'august', '23-', '27', ',', 'pp', '.']

 TOTAL PORTER STEM WORDS ==> 17



 ---- SNOWBALL STEMMING ----

['proceed', '20th', 'intern', 'confer', 'comput', 'linguist', '(', 'cole', '2004', ')', ',', 'august', '23-', '27', ',', 'pp', '.']

 TOTAL SNOWBALL STEM WORDS ==> 17



 ---- LEMMATIZATION ----

['Proceedings', '20th', 'international', 'conference', 'computational', 'linguistics', '(', 'COLING', '2004', ')', ',', 'August', '23-', '27', ',', 'pp', '.']

 TOTAL LEMMATIZE WORDS ==> 17

************************************************************************************************************************

108 --> 494-500, Geneva, Switzerland. 


 ---- TOKENS ----

 ['494-500', ',', 'Geneva', ',', 'Switzerland', '.'] 

 TOTAL TOKENS ==> 6

 ---- POST ----

 [('494-500', 'CD'), (',', ','), ('Geneva', 'NNP'), (',', ','), ('Switzerland', 'NNP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['494-500', ',', 'Geneva', ',', 'Switzerland', '.']

 TOTAL FILTERED TOKENS ==>  6

 ---- POST FOR FILTERED TOKENS ----

 [('494-500', 'CD'), (',', ','), ('Geneva', 'NNP'), (',', ','), ('Switzerland', 'NNP'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['494-500 ,', ', Geneva', 'Geneva ,', ', Switzerland', 'Switzerland .'] 

 TOTAL BIGRAMS --> 5 



 ---- TRI-GRAMS ---- 

 ['494-500 , Geneva', ', Geneva ,', 'Geneva , Switzerland', ', Switzerland .'] 

 TOTAL TRIGRAMS --> 4 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> ['Geneva', 'Switzerland']
 TOTAL GPE ENTITY --> 2 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['494-500', ',', 'geneva', ',', 'switzerland', '.']

 TOTAL PORTER STEM WORDS ==> 6



 ---- SNOWBALL STEMMING ----

['494-500', ',', 'geneva', ',', 'switzerland', '.']

 TOTAL SNOWBALL STEM WORDS ==> 6



 ---- LEMMATIZATION ----

['494-500', ',', 'Geneva', ',', 'Switzerland', '.']

 TOTAL LEMMATIZE WORDS ==> 6

************************************************************************************************************************

109 --> 14. 


 ---- TOKENS ----

 ['14', '.'] 

 TOTAL TOKENS ==> 2

 ---- POST ----

 [('14', 'CD'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['14', '.']

 TOTAL FILTERED TOKENS ==>  2

 ---- POST FOR FILTERED TOKENS ----

 [('14', 'CD'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['14 .'] 

 TOTAL BIGRAMS --> 1 



 ---- TRI-GRAMS ---- 

 [] 

 TOTAL TRIGRAMS --> 0 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['14', '.']

 TOTAL PORTER STEM WORDS ==> 2



 ---- SNOWBALL STEMMING ----

['14', '.']

 TOTAL SNOWBALL STEM WORDS ==> 2



 ---- LEMMATIZATION ----

['14', '.']

 TOTAL LEMMATIZE WORDS ==> 2

************************************************************************************************************************

110 --> Bollen, J., Mao, H., Zeng, X. 


 ---- TOKENS ----

 ['Bollen', ',', 'J.', ',', 'Mao', ',', 'H.', ',', 'Zeng', ',', 'X', '.'] 

 TOTAL TOKENS ==> 12

 ---- POST ----

 [('Bollen', 'NNP'), (',', ','), ('J.', 'NNP'), (',', ','), ('Mao', 'NNP'), (',', ','), ('H.', 'NNP'), (',', ','), ('Zeng', 'NNP'), (',', ','), ('X', 'NNP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Bollen', ',', 'J.', ',', 'Mao', ',', 'H.', ',', 'Zeng', ',', 'X', '.']

 TOTAL FILTERED TOKENS ==>  12

 ---- POST FOR FILTERED TOKENS ----

 [('Bollen', 'NNP'), (',', ','), ('J.', 'NNP'), (',', ','), ('Mao', 'NNP'), (',', ','), ('H.', 'NNP'), (',', ','), ('Zeng', 'NNP'), (',', ','), ('X', 'NNP'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Bollen ,', ', J.', 'J. ,', ', Mao', 'Mao ,', ', H.', 'H. ,', ', Zeng', 'Zeng ,', ', X', 'X .'] 

 TOTAL BIGRAMS --> 11 



 ---- TRI-GRAMS ---- 

 ['Bollen , J.', ', J. ,', 'J. , Mao', ', Mao ,', 'Mao , H.', ', H. ,', 'H. , Zeng', ', Zeng ,', 'Zeng , X', ', X .'] 

 TOTAL TRIGRAMS --> 10 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> ['Mao', 'Zeng']
 TOTAL PERSON ENTITY --> 2 


 GPE ---> ['Bollen']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['bollen', ',', 'j.', ',', 'mao', ',', 'h.', ',', 'zeng', ',', 'x', '.']

 TOTAL PORTER STEM WORDS ==> 12



 ---- SNOWBALL STEMMING ----

['bollen', ',', 'j.', ',', 'mao', ',', 'h.', ',', 'zeng', ',', 'x', '.']

 TOTAL SNOWBALL STEM WORDS ==> 12



 ---- LEMMATIZATION ----

['Bollen', ',', 'J.', ',', 'Mao', ',', 'H.', ',', 'Zeng', ',', 'X', '.']

 TOTAL LEMMATIZE WORDS ==> 12

************************************************************************************************************************

111 --> 2011. 


 ---- TOKENS ----

 ['2011', '.'] 

 TOTAL TOKENS ==> 2

 ---- POST ----

 [('2011', 'CD'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['2011', '.']

 TOTAL FILTERED TOKENS ==>  2

 ---- POST FOR FILTERED TOKENS ----

 [('2011', 'CD'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['2011 .'] 

 TOTAL BIGRAMS --> 1 



 ---- TRI-GRAMS ---- 

 [] 

 TOTAL TRIGRAMS --> 0 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['2011', '.']

 TOTAL PORTER STEM WORDS ==> 2



 ---- SNOWBALL STEMMING ----

['2011', '.']

 TOTAL SNOWBALL STEM WORDS ==> 2



 ---- LEMMATIZATION ----

['2011', '.']

 TOTAL LEMMATIZE WORDS ==> 2

************************************************************************************************************************

112 --> Twitter mood predicts the stock market. 


 ---- TOKENS ----

 ['Twitter', 'mood', 'predicts', 'the', 'stock', 'market', '.'] 

 TOTAL TOKENS ==> 7

 ---- POST ----

 [('Twitter', 'NNP'), ('mood', 'NN'), ('predicts', 'VBZ'), ('the', 'DT'), ('stock', 'NN'), ('market', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Twitter', 'mood', 'predicts', 'stock', 'market', '.']

 TOTAL FILTERED TOKENS ==>  6

 ---- POST FOR FILTERED TOKENS ----

 [('Twitter', 'NNP'), ('mood', 'NN'), ('predicts', 'NNS'), ('stock', 'NN'), ('market', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Twitter mood', 'mood predicts', 'predicts stock', 'stock market', 'market .'] 

 TOTAL BIGRAMS --> 5 



 ---- TRI-GRAMS ---- 

 ['Twitter mood predicts', 'mood predicts stock', 'predicts stock market', 'stock market .'] 

 TOTAL TRIGRAMS --> 4 



 ---- NOUN PHRASES ---- 

 ['mood', 'stock', 'market'] 

 TOTAL NOUN PHRASES --> 3 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['twitter', 'mood', 'predict', 'stock', 'market', '.']

 TOTAL PORTER STEM WORDS ==> 6



 ---- SNOWBALL STEMMING ----

['twitter', 'mood', 'predict', 'stock', 'market', '.']

 TOTAL SNOWBALL STEM WORDS ==> 6



 ---- LEMMATIZATION ----

['Twitter', 'mood', 'predicts', 'stock', 'market', '.']

 TOTAL LEMMATIZE WORDS ==> 6

************************************************************************************************************************

113 --> Journal of Computational  Science, 2(1), pp. 


 ---- TOKENS ----

 ['Journal', 'of', 'Computational', 'Science', ',', '2', '(', '1', ')', ',', 'pp', '.'] 

 TOTAL TOKENS ==> 12

 ---- POST ----

 [('Journal', 'NNP'), ('of', 'IN'), ('Computational', 'NNP'), ('Science', 'NNP'), (',', ','), ('2', 'CD'), ('(', '('), ('1', 'CD'), (')', ')'), (',', ','), ('pp', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Journal', 'Computational', 'Science', ',', '2', '(', '1', ')', ',', 'pp', '.']

 TOTAL FILTERED TOKENS ==>  11

 ---- POST FOR FILTERED TOKENS ----

 [('Journal', 'NNP'), ('Computational', 'NNP'), ('Science', 'NNP'), (',', ','), ('2', 'CD'), ('(', '('), ('1', 'CD'), (')', ')'), (',', ','), ('pp', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Journal Computational', 'Computational Science', 'Science ,', ', 2', '2 (', '( 1', '1 )', ') ,', ', pp', 'pp .'] 

 TOTAL BIGRAMS --> 10 



 ---- TRI-GRAMS ---- 

 ['Journal Computational Science', 'Computational Science ,', 'Science , 2', ', 2 (', '2 ( 1', '( 1 )', '1 ) ,', ') , pp', ', pp .'] 

 TOTAL TRIGRAMS --> 9 



 ---- NOUN PHRASES ---- 

 ['pp'] 

 TOTAL NOUN PHRASES --> 1 



 ---- NER ----

 
 ORGANIZATION ---> ['Computational Science']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> ['Journal']
 TOTAL PERSON ENTITY --> 1 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['journal', 'comput', 'scienc', ',', '2', '(', '1', ')', ',', 'pp', '.']

 TOTAL PORTER STEM WORDS ==> 11



 ---- SNOWBALL STEMMING ----

['journal', 'comput', 'scienc', ',', '2', '(', '1', ')', ',', 'pp', '.']

 TOTAL SNOWBALL STEM WORDS ==> 11



 ---- LEMMATIZATION ----

['Journal', 'Computational', 'Science', ',', '2', '(', '1', ')', ',', 'pp', '.']

 TOTAL LEMMATIZE WORDS ==> 11

************************************************************************************************************************

114 --> 1-8. 


 ---- TOKENS ----

 ['1-8', '.'] 

 TOTAL TOKENS ==> 2

 ---- POST ----

 [('1-8', 'JJ'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['1-8', '.']

 TOTAL FILTERED TOKENS ==>  2

 ---- POST FOR FILTERED TOKENS ----

 [('1-8', 'JJ'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['1-8 .'] 

 TOTAL BIGRAMS --> 1 



 ---- TRI-GRAMS ---- 

 [] 

 TOTAL TRIGRAMS --> 0 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['1-8', '.']

 TOTAL PORTER STEM WORDS ==> 2



 ---- SNOWBALL STEMMING ----

['1-8', '.']

 TOTAL SNOWBALL STEM WORDS ==> 2



 ---- LEMMATIZATION ----

['1-8', '.']

 TOTAL LEMMATIZE WORDS ==> 2

************************************************************************************************************************

115 --> White Paper – Sentiments Analysis  Page 5 of 5 


 ---- TOKENS ----

 ['White', 'Paper', '–', 'Sentiments', 'Analysis', 'Page', '5', 'of', '5'] 

 TOTAL TOKENS ==> 9

 ---- POST ----

 [('White', 'NNP'), ('Paper', 'NNP'), ('–', 'NN'), ('Sentiments', 'NNP'), ('Analysis', 'NNP'), ('Page', 'NNP'), ('5', 'CD'), ('of', 'IN'), ('5', 'CD')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['White', 'Paper', '–', 'Sentiments', 'Analysis', 'Page', '5', '5']

 TOTAL FILTERED TOKENS ==>  8

 ---- POST FOR FILTERED TOKENS ----

 [('White', 'NNP'), ('Paper', 'NNP'), ('–', 'NN'), ('Sentiments', 'NNP'), ('Analysis', 'NNP'), ('Page', 'NNP'), ('5', 'CD'), ('5', 'CD')] 



 ---- BI-GRAMS ---- 

 ['White Paper', 'Paper –', '– Sentiments', 'Sentiments Analysis', 'Analysis Page', 'Page 5', '5 5'] 

 TOTAL BIGRAMS --> 7 



 ---- TRI-GRAMS ---- 

 ['White Paper –', 'Paper – Sentiments', '– Sentiments Analysis', 'Sentiments Analysis Page', 'Analysis Page 5', 'Page 5 5'] 

 TOTAL TRIGRAMS --> 6 



 ---- NOUN PHRASES ---- 

 ['–'] 

 TOTAL NOUN PHRASES --> 1 



 ---- NER ----

 
 ORGANIZATION ---> ['Paper', 'Sentiments Analysis']
 TOTAL ORGANIZATION ENTITY --> 2 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['white', 'paper', '–', 'sentiment', 'analysi', 'page', '5', '5']

 TOTAL PORTER STEM WORDS ==> 8



 ---- SNOWBALL STEMMING ----

['white', 'paper', '–', 'sentiment', 'analysi', 'page', '5', '5']

 TOTAL SNOWBALL STEM WORDS ==> 8



 ---- LEMMATIZATION ----

['White', 'Paper', '–', 'Sentiments', 'Analysis', 'Page', '5', '5']

 TOTAL LEMMATIZE WORDS ==> 8

************************************************************************************************************************

