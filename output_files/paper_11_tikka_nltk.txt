1 --> See discussions, stats, and author profiles for this publication at: https://www.researchgate.net/publication/319164243 Natural Language Processing: State of The Art, Current Trends and Challenges Article · August 2017 CITATIONS 54 READS 28,980 4 authors, including: Some of the authors of this publication are also working on these related projects: Poisson-Exponential Distribution: problems of estimation and prediction View project Lognormal distribution from Bayesian view point View project Kiran Khatter BML MUNJAL UNIVERSITY 28 PUBLICATIONS   171 CITATIONS    SEE PROFILE Sukhdev Singh Indian Institute of Technology Patna 20 PUBLICATIONS   243 CITATIONS    SEE PROFILE All content following this page was uploaded by Kiran Khatter on 01 April 2018. 


 ---- TOKENS ----

 ['See', 'discussions', ',', 'stats', ',', 'and', 'author', 'profiles', 'for', 'this', 'publication', 'at', ':', 'https', ':', '//www.researchgate.net/publication/319164243', 'Natural', 'Language', 'Processing', ':', 'State', 'of', 'The', 'Art', ',', 'Current', 'Trends', 'and', 'Challenges', 'Article', '·', 'August', '2017', 'CITATIONS', '54', 'READS', '28,980', '4', 'authors', ',', 'including', ':', 'Some', 'of', 'the', 'authors', 'of', 'this', 'publication', 'are', 'also', 'working', 'on', 'these', 'related', 'projects', ':', 'Poisson-Exponential', 'Distribution', ':', 'problems', 'of', 'estimation', 'and', 'prediction', 'View', 'project', 'Lognormal', 'distribution', 'from', 'Bayesian', 'view', 'point', 'View', 'project', 'Kiran', 'Khatter', 'BML', 'MUNJAL', 'UNIVERSITY', '28', 'PUBLICATIONS', '171', 'CITATIONS', 'SEE', 'PROFILE', 'Sukhdev', 'Singh', 'Indian', 'Institute', 'of', 'Technology', 'Patna', '20', 'PUBLICATIONS', '243', 'CITATIONS', 'SEE', 'PROFILE', 'All', 'content', 'following', 'this', 'page', 'was', 'uploaded', 'by', 'Kiran', 'Khatter', 'on', '01', 'April', '2018', '.'] 

 TOTAL TOKENS ==> 114

 ---- POST ----

 [('See', 'JJ'), ('discussions', 'NNS'), (',', ','), ('stats', 'NNS'), (',', ','), ('and', 'CC'), ('author', 'NN'), ('profiles', 'NNS'), ('for', 'IN'), ('this', 'DT'), ('publication', 'NN'), ('at', 'IN'), (':', ':'), ('https', 'NN'), (':', ':'), ('//www.researchgate.net/publication/319164243', 'JJ'), ('Natural', 'NNP'), ('Language', 'NNP'), ('Processing', 'NN'), (':', ':'), ('State', 'NN'), ('of', 'IN'), ('The', 'DT'), ('Art', 'NNP'), (',', ','), ('Current', 'NNP'), ('Trends', 'NNP'), ('and', 'CC'), ('Challenges', 'NNP'), ('Article', 'NNP'), ('·', 'NNP'), ('August', 'NNP'), ('2017', 'CD'), ('CITATIONS', 'NNP'), ('54', 'CD'), ('READS', 'NNP'), ('28,980', 'CD'), ('4', 'CD'), ('authors', 'NNS'), (',', ','), ('including', 'VBG'), (':', ':'), ('Some', 'DT'), ('of', 'IN'), ('the', 'DT'), ('authors', 'NNS'), ('of', 'IN'), ('this', 'DT'), ('publication', 'NN'), ('are', 'VBP'), ('also', 'RB'), ('working', 'VBG'), ('on', 'IN'), ('these', 'DT'), ('related', 'JJ'), ('projects', 'NNS'), (':', ':'), ('Poisson-Exponential', 'JJ'), ('Distribution', 'NN'), (':', ':'), ('problems', 'NNS'), ('of', 'IN'), ('estimation', 'NN'), ('and', 'CC'), ('prediction', 'NN'), ('View', 'NNP'), ('project', 'NN'), ('Lognormal', 'NNP'), ('distribution', 'NN'), ('from', 'IN'), ('Bayesian', 'JJ'), ('view', 'NN'), ('point', 'NN'), ('View', 'NNP'), ('project', 'NN'), ('Kiran', 'NNP'), ('Khatter', 'NNP'), ('BML', 'NNP'), ('MUNJAL', 'NNP'), ('UNIVERSITY', 'NNP'), ('28', 'CD'), ('PUBLICATIONS', 'NNP'), ('171', 'CD'), ('CITATIONS', 'NNP'), ('SEE', 'NNP'), ('PROFILE', 'NNP'), ('Sukhdev', 'NNP'), ('Singh', 'NNP'), ('Indian', 'NNP'), ('Institute', 'NNP'), ('of', 'IN'), ('Technology', 'NNP'), ('Patna', 'NNP'), ('20', 'CD'), ('PUBLICATIONS', 'NNP'), ('243', 'CD'), ('CITATIONS', 'NNP'), ('SEE', 'NNP'), ('PROFILE', 'NNP'), ('All', 'NNP'), ('content', 'NN'), ('following', 'VBG'), ('this', 'DT'), ('page', 'NN'), ('was', 'VBD'), ('uploaded', 'VBN'), ('by', 'IN'), ('Kiran', 'NNP'), ('Khatter', 'NNP'), ('on', 'IN'), ('01', 'CD'), ('April', 'NNP'), ('2018', 'CD'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['See', 'discussions', ',', 'stats', ',', 'author', 'profiles', 'publication', ':', 'https', ':', '//www.researchgate.net/publication/319164243', 'Natural', 'Language', 'Processing', ':', 'State', 'Art', ',', 'Current', 'Trends', 'Challenges', 'Article', '·', 'August', '2017', 'CITATIONS', '54', 'READS', '28,980', '4', 'authors', ',', 'including', ':', 'authors', 'publication', 'also', 'working', 'related', 'projects', ':', 'Poisson-Exponential', 'Distribution', ':', 'problems', 'estimation', 'prediction', 'View', 'project', 'Lognormal', 'distribution', 'Bayesian', 'view', 'point', 'View', 'project', 'Kiran', 'Khatter', 'BML', 'MUNJAL', 'UNIVERSITY', '28', 'PUBLICATIONS', '171', 'CITATIONS', 'SEE', 'PROFILE', 'Sukhdev', 'Singh', 'Indian', 'Institute', 'Technology', 'Patna', '20', 'PUBLICATIONS', '243', 'CITATIONS', 'SEE', 'PROFILE', 'content', 'following', 'page', 'uploaded', 'Kiran', 'Khatter', '01', 'April', '2018', '.']

 TOTAL FILTERED TOKENS ==>  90

 ---- POST FOR FILTERED TOKENS ----

 [('See', 'JJ'), ('discussions', 'NNS'), (',', ','), ('stats', 'NNS'), (',', ','), ('author', 'NN'), ('profiles', 'NNS'), ('publication', 'NN'), (':', ':'), ('https', 'NN'), (':', ':'), ('//www.researchgate.net/publication/319164243', 'JJ'), ('Natural', 'NNP'), ('Language', 'NNP'), ('Processing', 'NN'), (':', ':'), ('State', 'NNP'), ('Art', 'NNP'), (',', ','), ('Current', 'NNP'), ('Trends', 'NNP'), ('Challenges', 'NNP'), ('Article', 'NNP'), ('·', 'NNP'), ('August', 'NNP'), ('2017', 'CD'), ('CITATIONS', 'NNP'), ('54', 'CD'), ('READS', 'NNP'), ('28,980', 'CD'), ('4', 'CD'), ('authors', 'NNS'), (',', ','), ('including', 'VBG'), (':', ':'), ('authors', 'NNS'), ('publication', 'VBP'), ('also', 'RB'), ('working', 'VBG'), ('related', 'JJ'), ('projects', 'NNS'), (':', ':'), ('Poisson-Exponential', 'JJ'), ('Distribution', 'NN'), (':', ':'), ('problems', 'NNS'), ('estimation', 'VBP'), ('prediction', 'NN'), ('View', 'NNP'), ('project', 'NN'), ('Lognormal', 'NNP'), ('distribution', 'NN'), ('Bayesian', 'NNP'), ('view', 'NN'), ('point', 'NN'), ('View', 'NNP'), ('project', 'NN'), ('Kiran', 'NNP'), ('Khatter', 'NNP'), ('BML', 'NNP'), ('MUNJAL', 'NNP'), ('UNIVERSITY', 'NNP'), ('28', 'CD'), ('PUBLICATIONS', 'NNP'), ('171', 'CD'), ('CITATIONS', 'NNP'), ('SEE', 'NNP'), ('PROFILE', 'NNP'), ('Sukhdev', 'NNP'), ('Singh', 'NNP'), ('Indian', 'NNP'), ('Institute', 'NNP'), ('Technology', 'NNP'), ('Patna', 'NNP'), ('20', 'CD'), ('PUBLICATIONS', 'NNP'), ('243', 'CD'), ('CITATIONS', 'NNP'), ('SEE', 'NNP'), ('PROFILE', 'NNP'), ('content', 'NN'), ('following', 'VBG'), ('page', 'NN'), ('uploaded', 'VBD'), ('Kiran', 'NNP'), ('Khatter', 'NNP'), ('01', 'CD'), ('April', 'NNP'), ('2018', 'CD'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['See discussions', 'discussions ,', ', stats', 'stats ,', ', author', 'author profiles', 'profiles publication', 'publication :', ': https', 'https :', ': //www.researchgate.net/publication/319164243', '//www.researchgate.net/publication/319164243 Natural', 'Natural Language', 'Language Processing', 'Processing :', ': State', 'State Art', 'Art ,', ', Current', 'Current Trends', 'Trends Challenges', 'Challenges Article', 'Article ·', '· August', 'August 2017', '2017 CITATIONS', 'CITATIONS 54', '54 READS', 'READS 28,980', '28,980 4', '4 authors', 'authors ,', ', including', 'including :', ': authors', 'authors publication', 'publication also', 'also working', 'working related', 'related projects', 'projects :', ': Poisson-Exponential', 'Poisson-Exponential Distribution', 'Distribution :', ': problems', 'problems estimation', 'estimation prediction', 'prediction View', 'View project', 'project Lognormal', 'Lognormal distribution', 'distribution Bayesian', 'Bayesian view', 'view point', 'point View', 'View project', 'project Kiran', 'Kiran Khatter', 'Khatter BML', 'BML MUNJAL', 'MUNJAL UNIVERSITY', 'UNIVERSITY 28', '28 PUBLICATIONS', 'PUBLICATIONS 171', '171 CITATIONS', 'CITATIONS SEE', 'SEE PROFILE', 'PROFILE Sukhdev', 'Sukhdev Singh', 'Singh Indian', 'Indian Institute', 'Institute Technology', 'Technology Patna', 'Patna 20', '20 PUBLICATIONS', 'PUBLICATIONS 243', '243 CITATIONS', 'CITATIONS SEE', 'SEE PROFILE', 'PROFILE content', 'content following', 'following page', 'page uploaded', 'uploaded Kiran', 'Kiran Khatter', 'Khatter 01', '01 April', 'April 2018', '2018 .'] 

 TOTAL BIGRAMS --> 89 



 ---- TRI-GRAMS ---- 

 ['See discussions ,', 'discussions , stats', ', stats ,', 'stats , author', ', author profiles', 'author profiles publication', 'profiles publication :', 'publication : https', ': https :', 'https : //www.researchgate.net/publication/319164243', ': //www.researchgate.net/publication/319164243 Natural', '//www.researchgate.net/publication/319164243 Natural Language', 'Natural Language Processing', 'Language Processing :', 'Processing : State', ': State Art', 'State Art ,', 'Art , Current', ', Current Trends', 'Current Trends Challenges', 'Trends Challenges Article', 'Challenges Article ·', 'Article · August', '· August 2017', 'August 2017 CITATIONS', '2017 CITATIONS 54', 'CITATIONS 54 READS', '54 READS 28,980', 'READS 28,980 4', '28,980 4 authors', '4 authors ,', 'authors , including', ', including :', 'including : authors', ': authors publication', 'authors publication also', 'publication also working', 'also working related', 'working related projects', 'related projects :', 'projects : Poisson-Exponential', ': Poisson-Exponential Distribution', 'Poisson-Exponential Distribution :', 'Distribution : problems', ': problems estimation', 'problems estimation prediction', 'estimation prediction View', 'prediction View project', 'View project Lognormal', 'project Lognormal distribution', 'Lognormal distribution Bayesian', 'distribution Bayesian view', 'Bayesian view point', 'view point View', 'point View project', 'View project Kiran', 'project Kiran Khatter', 'Kiran Khatter BML', 'Khatter BML MUNJAL', 'BML MUNJAL UNIVERSITY', 'MUNJAL UNIVERSITY 28', 'UNIVERSITY 28 PUBLICATIONS', '28 PUBLICATIONS 171', 'PUBLICATIONS 171 CITATIONS', '171 CITATIONS SEE', 'CITATIONS SEE PROFILE', 'SEE PROFILE Sukhdev', 'PROFILE Sukhdev Singh', 'Sukhdev Singh Indian', 'Singh Indian Institute', 'Indian Institute Technology', 'Institute Technology Patna', 'Technology Patna 20', 'Patna 20 PUBLICATIONS', '20 PUBLICATIONS 243', 'PUBLICATIONS 243 CITATIONS', '243 CITATIONS SEE', 'CITATIONS SEE PROFILE', 'SEE PROFILE content', 'PROFILE content following', 'content following page', 'following page uploaded', 'page uploaded Kiran', 'uploaded Kiran Khatter', 'Kiran Khatter 01', 'Khatter 01 April', '01 April 2018', 'April 2018 .'] 

 TOTAL TRIGRAMS --> 88 



 ---- NOUN PHRASES ---- 

 ['author', 'publication', 'https', 'Processing', 'Poisson-Exponential Distribution', 'prediction', 'project', 'distribution', 'view', 'point', 'project', 'content', 'page'] 

 TOTAL NOUN PHRASES --> 13 



 ---- NER ----

 
 ORGANIZATION ---> ['Natural Language', 'MUNJAL', 'CITATIONS', 'CITATIONS']
 TOTAL ORGANIZATION ENTITY --> 4 


 PERSON ---> ['Current Trends Challenges Article', 'Lognormal', 'Kiran Khatter', 'Sukhdev Singh Indian Institute Technology', 'Kiran Khatter']
 TOTAL PERSON ENTITY --> 5 


 GPE ---> ['Bayesian']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['see', 'discuss', ',', 'stat', ',', 'author', 'profil', 'public', ':', 'http', ':', '//www.researchgate.net/publication/319164243', 'natur', 'languag', 'process', ':', 'state', 'art', ',', 'current', 'trend', 'challeng', 'articl', '·', 'august', '2017', 'citat', '54', 'read', '28,980', '4', 'author', ',', 'includ', ':', 'author', 'public', 'also', 'work', 'relat', 'project', ':', 'poisson-exponenti', 'distribut', ':', 'problem', 'estim', 'predict', 'view', 'project', 'lognorm', 'distribut', 'bayesian', 'view', 'point', 'view', 'project', 'kiran', 'khatter', 'bml', 'munjal', 'univers', '28', 'public', '171', 'citat', 'see', 'profil', 'sukhdev', 'singh', 'indian', 'institut', 'technolog', 'patna', '20', 'public', '243', 'citat', 'see', 'profil', 'content', 'follow', 'page', 'upload', 'kiran', 'khatter', '01', 'april', '2018', '.']

 TOTAL PORTER STEM WORDS ==> 90



 ---- SNOWBALL STEMMING ----

['see', 'discuss', ',', 'stat', ',', 'author', 'profil', 'public', ':', 'https', ':', '//www.researchgate.net/publication/319164243', 'natur', 'languag', 'process', ':', 'state', 'art', ',', 'current', 'trend', 'challeng', 'articl', '·', 'august', '2017', 'citat', '54', 'read', '28,980', '4', 'author', ',', 'includ', ':', 'author', 'public', 'also', 'work', 'relat', 'project', ':', 'poisson-exponenti', 'distribut', ':', 'problem', 'estim', 'predict', 'view', 'project', 'lognorm', 'distribut', 'bayesian', 'view', 'point', 'view', 'project', 'kiran', 'khatter', 'bml', 'munjal', 'univers', '28', 'public', '171', 'citat', 'see', 'profil', 'sukhdev', 'singh', 'indian', 'institut', 'technolog', 'patna', '20', 'public', '243', 'citat', 'see', 'profil', 'content', 'follow', 'page', 'upload', 'kiran', 'khatter', '01', 'april', '2018', '.']

 TOTAL SNOWBALL STEM WORDS ==> 90



 ---- LEMMATIZATION ----

['See', 'discussion', ',', 'stats', ',', 'author', 'profile', 'publication', ':', 'http', ':', '//www.researchgate.net/publication/319164243', 'Natural', 'Language', 'Processing', ':', 'State', 'Art', ',', 'Current', 'Trends', 'Challenges', 'Article', '·', 'August', '2017', 'CITATIONS', '54', 'READS', '28,980', '4', 'author', ',', 'including', ':', 'author', 'publication', 'also', 'working', 'related', 'project', ':', 'Poisson-Exponential', 'Distribution', ':', 'problem', 'estimation', 'prediction', 'View', 'project', 'Lognormal', 'distribution', 'Bayesian', 'view', 'point', 'View', 'project', 'Kiran', 'Khatter', 'BML', 'MUNJAL', 'UNIVERSITY', '28', 'PUBLICATIONS', '171', 'CITATIONS', 'SEE', 'PROFILE', 'Sukhdev', 'Singh', 'Indian', 'Institute', 'Technology', 'Patna', '20', 'PUBLICATIONS', '243', 'CITATIONS', 'SEE', 'PROFILE', 'content', 'following', 'page', 'uploaded', 'Kiran', 'Khatter', '01', 'April', '2018', '.']

 TOTAL LEMMATIZE WORDS ==> 90

************************************************************************************************************************

2 --> The user has requested enhancement of the downloaded file. 


 ---- TOKENS ----

 ['The', 'user', 'has', 'requested', 'enhancement', 'of', 'the', 'downloaded', 'file', '.'] 

 TOTAL TOKENS ==> 10

 ---- POST ----

 [('The', 'DT'), ('user', 'NN'), ('has', 'VBZ'), ('requested', 'VBN'), ('enhancement', 'NN'), ('of', 'IN'), ('the', 'DT'), ('downloaded', 'VBN'), ('file', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['user', 'requested', 'enhancement', 'downloaded', 'file', '.']

 TOTAL FILTERED TOKENS ==>  6

 ---- POST FOR FILTERED TOKENS ----

 [('user', 'RB'), ('requested', 'VBN'), ('enhancement', 'NN'), ('downloaded', 'VBN'), ('file', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['user requested', 'requested enhancement', 'enhancement downloaded', 'downloaded file', 'file .'] 

 TOTAL BIGRAMS --> 5 



 ---- TRI-GRAMS ---- 

 ['user requested enhancement', 'requested enhancement downloaded', 'enhancement downloaded file', 'downloaded file .'] 

 TOTAL TRIGRAMS --> 4 



 ---- NOUN PHRASES ---- 

 ['enhancement', 'file'] 

 TOTAL NOUN PHRASES --> 2 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['user', 'request', 'enhanc', 'download', 'file', '.']

 TOTAL PORTER STEM WORDS ==> 6



 ---- SNOWBALL STEMMING ----

['user', 'request', 'enhanc', 'download', 'file', '.']

 TOTAL SNOWBALL STEM WORDS ==> 6



 ---- LEMMATIZATION ----

['user', 'requested', 'enhancement', 'downloaded', 'file', '.']

 TOTAL LEMMATIZE WORDS ==> 6

************************************************************************************************************************

3 --> https://www.researchgate.net/publication/319164243_Natural_Language_Processing_State_of_The_Art_Current_Trends_and_Challenges?enrichId=rgreq-b679ffd100c6b00b6348f225c923452e-XXX&enrichSource=Y292ZXJQYWdlOzMxOTE2NDI0MztBUzo2MTA1NzA2ODg1NDQ3NjhAMTUyMjU4MjgwNDM4Mg%3D%3D&el=1_x_2&_esc=publicationCoverPdf https://www.researchgate.net/publication/319164243_Natural_Language_Processing_State_of_The_Art_Current_Trends_and_Challenges?enrichId=rgreq-b679ffd100c6b00b6348f225c923452e-XXX&enrichSource=Y292ZXJQYWdlOzMxOTE2NDI0MztBUzo2MTA1NzA2ODg1NDQ3NjhAMTUyMjU4MjgwNDM4Mg%3D%3D&el=1_x_3&_esc=publicationCoverPdf https://www.researchgate.net/project/Poisson-Exponential-Distribution-problems-of-estimation-and-prediction?enrichId=rgreq-b679ffd100c6b00b6348f225c923452e-XXX&enrichSource=Y292ZXJQYWdlOzMxOTE2NDI0MztBUzo2MTA1NzA2ODg1NDQ3NjhAMTUyMjU4MjgwNDM4Mg%3D%3D&el=1_x_9&_esc=publicationCoverPdf https://www.researchgate.net/project/Lognormal-distribution-from-Bayesian-view-point?enrichId=rgreq-b679ffd100c6b00b6348f225c923452e-XXX&enrichSource=Y292ZXJQYWdlOzMxOTE2NDI0MztBUzo2MTA1NzA2ODg1NDQ3NjhAMTUyMjU4MjgwNDM4Mg%3D%3D&el=1_x_9&_esc=publicationCoverPdf https://www.researchgate.net/?enrichId=rgreq-b679ffd100c6b00b6348f225c923452e-XXX&enrichSource=Y292ZXJQYWdlOzMxOTE2NDI0MztBUzo2MTA1NzA2ODg1NDQ3NjhAMTUyMjU4MjgwNDM4Mg%3D%3D&el=1_x_1&_esc=publicationCoverPdf https://www.researchgate.net/profile/Kiran-Khatter?enrichId=rgreq-b679ffd100c6b00b6348f225c923452e-XXX&enrichSource=Y292ZXJQYWdlOzMxOTE2NDI0MztBUzo2MTA1NzA2ODg1NDQ3NjhAMTUyMjU4MjgwNDM4Mg%3D%3D&el=1_x_4&_esc=publicationCoverPdf https://www.researchgate.net/profile/Kiran-Khatter?enrichId=rgreq-b679ffd100c6b00b6348f225c923452e-XXX&enrichSource=Y292ZXJQYWdlOzMxOTE2NDI0MztBUzo2MTA1NzA2ODg1NDQ3NjhAMTUyMjU4MjgwNDM4Mg%3D%3D&el=1_x_5&_esc=publicationCoverPdf https://www.researchgate.net/institution/BML-MUNJAL-UNIVERSITY?enrichId=rgreq-b679ffd100c6b00b6348f225c923452e-XXX&enrichSource=Y292ZXJQYWdlOzMxOTE2NDI0MztBUzo2MTA1NzA2ODg1NDQ3NjhAMTUyMjU4MjgwNDM4Mg%3D%3D&el=1_x_6&_esc=publicationCoverPdf https://www.researchgate.net/profile/Kiran-Khatter?enrichId=rgreq-b679ffd100c6b00b6348f225c923452e-XXX&enrichSource=Y292ZXJQYWdlOzMxOTE2NDI0MztBUzo2MTA1NzA2ODg1NDQ3NjhAMTUyMjU4MjgwNDM4Mg%3D%3D&el=1_x_7&_esc=publicationCoverPdf https://www.researchgate.net/profile/Sukhdev-Singh?enrichId=rgreq-b679ffd100c6b00b6348f225c923452e-XXX&enrichSource=Y292ZXJQYWdlOzMxOTE2NDI0MztBUzo2MTA1NzA2ODg1NDQ3NjhAMTUyMjU4MjgwNDM4Mg%3D%3D&el=1_x_4&_esc=publicationCoverPdf https://www.researchgate.net/profile/Sukhdev-Singh?enrichId=rgreq-b679ffd100c6b00b6348f225c923452e-XXX&enrichSource=Y292ZXJQYWdlOzMxOTE2NDI0MztBUzo2MTA1NzA2ODg1NDQ3NjhAMTUyMjU4MjgwNDM4Mg%3D%3D&el=1_x_5&_esc=publicationCoverPdf https://www.researchgate.net/institution/Indian-Institute-of-Technology-Patna?enrichId=rgreq-b679ffd100c6b00b6348f225c923452e-XXX&enrichSource=Y292ZXJQYWdlOzMxOTE2NDI0MztBUzo2MTA1NzA2ODg1NDQ3NjhAMTUyMjU4MjgwNDM4Mg%3D%3D&el=1_x_6&_esc=publicationCoverPdf https://www.researchgate.net/profile/Sukhdev-Singh?enrichId=rgreq-b679ffd100c6b00b6348f225c923452e-XXX&enrichSource=Y292ZXJQYWdlOzMxOTE2NDI0MztBUzo2MTA1NzA2ODg1NDQ3NjhAMTUyMjU4MjgwNDM4Mg%3D%3D&el=1_x_7&_esc=publicationCoverPdf https://www.researchgate.net/profile/Kiran-Khatter?enrichId=rgreq-b679ffd100c6b00b6348f225c923452e-XXX&enrichSource=Y292ZXJQYWdlOzMxOTE2NDI0MztBUzo2MTA1NzA2ODg1NDQ3NjhAMTUyMjU4MjgwNDM4Mg%3D%3D&el=1_x_10&_esc=publicationCoverPdf  Natural Language Processing: State of The Art, Current Trends and  Challenges  Diksha Khurana 1 , Aditya Koli 1 , Kiran Khatter 1,2  and Sukhdev Singh 1,2   1 Department of Computer Science and Engineering  Manav Rachna International University, Faridabad-121004, India  2 Accendere Knowledge Management Services Pvt. 


 ---- TOKENS ----

 ['https', ':', '//www.researchgate.net/publication/319164243_Natural_Language_Processing_State_of_The_Art_Current_Trends_and_Challenges', '?', 'enrichId=rgreq-b679ffd100c6b00b6348f225c923452e-XXX', '&', 'enrichSource=Y292ZXJQYWdlOzMxOTE2NDI0MztBUzo2MTA1NzA2ODg1NDQ3NjhAMTUyMjU4MjgwNDM4Mg', '%', '3D', '%', '3D', '&', 'el=1_x_2', '&', '_esc=publicationCoverPdf', 'https', ':', '//www.researchgate.net/publication/319164243_Natural_Language_Processing_State_of_The_Art_Current_Trends_and_Challenges', '?', 'enrichId=rgreq-b679ffd100c6b00b6348f225c923452e-XXX', '&', 'enrichSource=Y292ZXJQYWdlOzMxOTE2NDI0MztBUzo2MTA1NzA2ODg1NDQ3NjhAMTUyMjU4MjgwNDM4Mg', '%', '3D', '%', '3D', '&', 'el=1_x_3', '&', '_esc=publicationCoverPdf', 'https', ':', '//www.researchgate.net/project/Poisson-Exponential-Distribution-problems-of-estimation-and-prediction', '?', 'enrichId=rgreq-b679ffd100c6b00b6348f225c923452e-XXX', '&', 'enrichSource=Y292ZXJQYWdlOzMxOTE2NDI0MztBUzo2MTA1NzA2ODg1NDQ3NjhAMTUyMjU4MjgwNDM4Mg', '%', '3D', '%', '3D', '&', 'el=1_x_9', '&', '_esc=publicationCoverPdf', 'https', ':', '//www.researchgate.net/project/Lognormal-distribution-from-Bayesian-view-point', '?', 'enrichId=rgreq-b679ffd100c6b00b6348f225c923452e-XXX', '&', 'enrichSource=Y292ZXJQYWdlOzMxOTE2NDI0MztBUzo2MTA1NzA2ODg1NDQ3NjhAMTUyMjU4MjgwNDM4Mg', '%', '3D', '%', '3D', '&', 'el=1_x_9', '&', '_esc=publicationCoverPdf', 'https', ':', '//www.researchgate.net/', '?', 'enrichId=rgreq-b679ffd100c6b00b6348f225c923452e-XXX', '&', 'enrichSource=Y292ZXJQYWdlOzMxOTE2NDI0MztBUzo2MTA1NzA2ODg1NDQ3NjhAMTUyMjU4MjgwNDM4Mg', '%', '3D', '%', '3D', '&', 'el=1_x_1', '&', '_esc=publicationCoverPdf', 'https', ':', '//www.researchgate.net/profile/Kiran-Khatter', '?', 'enrichId=rgreq-b679ffd100c6b00b6348f225c923452e-XXX', '&', 'enrichSource=Y292ZXJQYWdlOzMxOTE2NDI0MztBUzo2MTA1NzA2ODg1NDQ3NjhAMTUyMjU4MjgwNDM4Mg', '%', '3D', '%', '3D', '&', 'el=1_x_4', '&', '_esc=publicationCoverPdf', 'https', ':', '//www.researchgate.net/profile/Kiran-Khatter', '?', 'enrichId=rgreq-b679ffd100c6b00b6348f225c923452e-XXX', '&', 'enrichSource=Y292ZXJQYWdlOzMxOTE2NDI0MztBUzo2MTA1NzA2ODg1NDQ3NjhAMTUyMjU4MjgwNDM4Mg', '%', '3D', '%', '3D', '&', 'el=1_x_5', '&', '_esc=publicationCoverPdf', 'https', ':', '//www.researchgate.net/institution/BML-MUNJAL-UNIVERSITY', '?', 'enrichId=rgreq-b679ffd100c6b00b6348f225c923452e-XXX', '&', 'enrichSource=Y292ZXJQYWdlOzMxOTE2NDI0MztBUzo2MTA1NzA2ODg1NDQ3NjhAMTUyMjU4MjgwNDM4Mg', '%', '3D', '%', '3D', '&', 'el=1_x_6', '&', '_esc=publicationCoverPdf', 'https', ':', '//www.researchgate.net/profile/Kiran-Khatter', '?', 'enrichId=rgreq-b679ffd100c6b00b6348f225c923452e-XXX', '&', 'enrichSource=Y292ZXJQYWdlOzMxOTE2NDI0MztBUzo2MTA1NzA2ODg1NDQ3NjhAMTUyMjU4MjgwNDM4Mg', '%', '3D', '%', '3D', '&', 'el=1_x_7', '&', '_esc=publicationCoverPdf', 'https', ':', '//www.researchgate.net/profile/Sukhdev-Singh', '?', 'enrichId=rgreq-b679ffd100c6b00b6348f225c923452e-XXX', '&', 'enrichSource=Y292ZXJQYWdlOzMxOTE2NDI0MztBUzo2MTA1NzA2ODg1NDQ3NjhAMTUyMjU4MjgwNDM4Mg', '%', '3D', '%', '3D', '&', 'el=1_x_4', '&', '_esc=publicationCoverPdf', 'https', ':', '//www.researchgate.net/profile/Sukhdev-Singh', '?', 'enrichId=rgreq-b679ffd100c6b00b6348f225c923452e-XXX', '&', 'enrichSource=Y292ZXJQYWdlOzMxOTE2NDI0MztBUzo2MTA1NzA2ODg1NDQ3NjhAMTUyMjU4MjgwNDM4Mg', '%', '3D', '%', '3D', '&', 'el=1_x_5', '&', '_esc=publicationCoverPdf', 'https', ':', '//www.researchgate.net/institution/Indian-Institute-of-Technology-Patna', '?', 'enrichId=rgreq-b679ffd100c6b00b6348f225c923452e-XXX', '&', 'enrichSource=Y292ZXJQYWdlOzMxOTE2NDI0MztBUzo2MTA1NzA2ODg1NDQ3NjhAMTUyMjU4MjgwNDM4Mg', '%', '3D', '%', '3D', '&', 'el=1_x_6', '&', '_esc=publicationCoverPdf', 'https', ':', '//www.researchgate.net/profile/Sukhdev-Singh', '?', 'enrichId=rgreq-b679ffd100c6b00b6348f225c923452e-XXX', '&', 'enrichSource=Y292ZXJQYWdlOzMxOTE2NDI0MztBUzo2MTA1NzA2ODg1NDQ3NjhAMTUyMjU4MjgwNDM4Mg', '%', '3D', '%', '3D', '&', 'el=1_x_7', '&', '_esc=publicationCoverPdf', 'https', ':', '//www.researchgate.net/profile/Kiran-Khatter', '?', 'enrichId=rgreq-b679ffd100c6b00b6348f225c923452e-XXX', '&', 'enrichSource=Y292ZXJQYWdlOzMxOTE2NDI0MztBUzo2MTA1NzA2ODg1NDQ3NjhAMTUyMjU4MjgwNDM4Mg', '%', '3D', '%', '3D', '&', 'el=1_x_10', '&', '_esc=publicationCoverPdf', 'Natural', 'Language', 'Processing', ':', 'State', 'of', 'The', 'Art', ',', 'Current', 'Trends', 'and', 'Challenges', 'Diksha', 'Khurana', '1', ',', 'Aditya', 'Koli', '1', ',', 'Kiran', 'Khatter', '1,2', 'and', 'Sukhdev', 'Singh', '1,2', '1', 'Department', 'of', 'Computer', 'Science', 'and', 'Engineering', 'Manav', 'Rachna', 'International', 'University', ',', 'Faridabad-121004', ',', 'India', '2', 'Accendere', 'Knowledge', 'Management', 'Services', 'Pvt', '.'] 

 TOTAL TOKENS ==> 260

 ---- POST ----

 [('https', 'NN'), (':', ':'), ('//www.researchgate.net/publication/319164243_Natural_Language_Processing_State_of_The_Art_Current_Trends_and_Challenges', 'NNS'), ('?', '.'), ('enrichId=rgreq-b679ffd100c6b00b6348f225c923452e-XXX', 'JJ'), ('&', 'CC'), ('enrichSource=Y292ZXJQYWdlOzMxOTE2NDI0MztBUzo2MTA1NzA2ODg1NDQ3NjhAMTUyMjU4MjgwNDM4Mg', 'JJ'), ('%', 'NN'), ('3D', 'CD'), ('%', 'NN'), ('3D', 'CD'), ('&', 'CC'), ('el=1_x_2', 'NNP'), ('&', 'CC'), ('_esc=publicationCoverPdf', 'NNP'), ('https', 'NN'), (':', ':'), ('//www.researchgate.net/publication/319164243_Natural_Language_Processing_State_of_The_Art_Current_Trends_and_Challenges', 'NNS'), ('?', '.'), ('enrichId=rgreq-b679ffd100c6b00b6348f225c923452e-XXX', 'JJ'), ('&', 'CC'), ('enrichSource=Y292ZXJQYWdlOzMxOTE2NDI0MztBUzo2MTA1NzA2ODg1NDQ3NjhAMTUyMjU4MjgwNDM4Mg', 'JJ'), ('%', 'NN'), ('3D', 'CD'), ('%', 'NN'), ('3D', 'CD'), ('&', 'CC'), ('el=1_x_3', 'NNP'), ('&', 'CC'), ('_esc=publicationCoverPdf', 'NNP'), ('https', 'NN'), (':', ':'), ('//www.researchgate.net/project/Poisson-Exponential-Distribution-problems-of-estimation-and-prediction', 'NN'), ('?', '.'), ('enrichId=rgreq-b679ffd100c6b00b6348f225c923452e-XXX', 'JJ'), ('&', 'CC'), ('enrichSource=Y292ZXJQYWdlOzMxOTE2NDI0MztBUzo2MTA1NzA2ODg1NDQ3NjhAMTUyMjU4MjgwNDM4Mg', 'JJ'), ('%', 'NN'), ('3D', 'CD'), ('%', 'NN'), ('3D', 'CD'), ('&', 'CC'), ('el=1_x_9', 'NNP'), ('&', 'CC'), ('_esc=publicationCoverPdf', 'NNP'), ('https', 'NN'), (':', ':'), ('//www.researchgate.net/project/Lognormal-distribution-from-Bayesian-view-point', 'JJ'), ('?', '.'), ('enrichId=rgreq-b679ffd100c6b00b6348f225c923452e-XXX', 'JJ'), ('&', 'CC'), ('enrichSource=Y292ZXJQYWdlOzMxOTE2NDI0MztBUzo2MTA1NzA2ODg1NDQ3NjhAMTUyMjU4MjgwNDM4Mg', 'JJ'), ('%', 'NN'), ('3D', 'CD'), ('%', 'NN'), ('3D', 'CD'), ('&', 'CC'), ('el=1_x_9', 'NNP'), ('&', 'CC'), ('_esc=publicationCoverPdf', 'NNP'), ('https', 'NN'), (':', ':'), ('//www.researchgate.net/', 'NN'), ('?', '.'), ('enrichId=rgreq-b679ffd100c6b00b6348f225c923452e-XXX', 'JJ'), ('&', 'CC'), ('enrichSource=Y292ZXJQYWdlOzMxOTE2NDI0MztBUzo2MTA1NzA2ODg1NDQ3NjhAMTUyMjU4MjgwNDM4Mg', 'JJ'), ('%', 'NN'), ('3D', 'CD'), ('%', 'NN'), ('3D', 'CD'), ('&', 'CC'), ('el=1_x_1', 'NNP'), ('&', 'CC'), ('_esc=publicationCoverPdf', 'NNP'), ('https', 'NN'), (':', ':'), ('//www.researchgate.net/profile/Kiran-Khatter', 'JJ'), ('?', '.'), ('enrichId=rgreq-b679ffd100c6b00b6348f225c923452e-XXX', 'JJ'), ('&', 'CC'), ('enrichSource=Y292ZXJQYWdlOzMxOTE2NDI0MztBUzo2MTA1NzA2ODg1NDQ3NjhAMTUyMjU4MjgwNDM4Mg', 'JJ'), ('%', 'NN'), ('3D', 'CD'), ('%', 'NN'), ('3D', 'CD'), ('&', 'CC'), ('el=1_x_4', 'NNP'), ('&', 'CC'), ('_esc=publicationCoverPdf', 'NNP'), ('https', 'NN'), (':', ':'), ('//www.researchgate.net/profile/Kiran-Khatter', 'JJ'), ('?', '.'), ('enrichId=rgreq-b679ffd100c6b00b6348f225c923452e-XXX', 'JJ'), ('&', 'CC'), ('enrichSource=Y292ZXJQYWdlOzMxOTE2NDI0MztBUzo2MTA1NzA2ODg1NDQ3NjhAMTUyMjU4MjgwNDM4Mg', 'JJ'), ('%', 'NN'), ('3D', 'CD'), ('%', 'NN'), ('3D', 'CD'), ('&', 'CC'), ('el=1_x_5', 'NNP'), ('&', 'CC'), ('_esc=publicationCoverPdf', 'NNP'), ('https', 'NN'), (':', ':'), ('//www.researchgate.net/institution/BML-MUNJAL-UNIVERSITY', 'JJ'), ('?', '.'), ('enrichId=rgreq-b679ffd100c6b00b6348f225c923452e-XXX', 'JJ'), ('&', 'CC'), ('enrichSource=Y292ZXJQYWdlOzMxOTE2NDI0MztBUzo2MTA1NzA2ODg1NDQ3NjhAMTUyMjU4MjgwNDM4Mg', 'JJ'), ('%', 'NN'), ('3D', 'CD'), ('%', 'NN'), ('3D', 'CD'), ('&', 'CC'), ('el=1_x_6', 'NNP'), ('&', 'CC'), ('_esc=publicationCoverPdf', 'NNP'), ('https', 'NN'), (':', ':'), ('//www.researchgate.net/profile/Kiran-Khatter', 'JJ'), ('?', '.'), ('enrichId=rgreq-b679ffd100c6b00b6348f225c923452e-XXX', 'JJ'), ('&', 'CC'), ('enrichSource=Y292ZXJQYWdlOzMxOTE2NDI0MztBUzo2MTA1NzA2ODg1NDQ3NjhAMTUyMjU4MjgwNDM4Mg', 'JJ'), ('%', 'NN'), ('3D', 'CD'), ('%', 'NN'), ('3D', 'CD'), ('&', 'CC'), ('el=1_x_7', 'NNP'), ('&', 'CC'), ('_esc=publicationCoverPdf', 'NNP'), ('https', 'NN'), (':', ':'), ('//www.researchgate.net/profile/Sukhdev-Singh', 'JJ'), ('?', '.'), ('enrichId=rgreq-b679ffd100c6b00b6348f225c923452e-XXX', 'JJ'), ('&', 'CC'), ('enrichSource=Y292ZXJQYWdlOzMxOTE2NDI0MztBUzo2MTA1NzA2ODg1NDQ3NjhAMTUyMjU4MjgwNDM4Mg', 'JJ'), ('%', 'NN'), ('3D', 'CD'), ('%', 'NN'), ('3D', 'CD'), ('&', 'CC'), ('el=1_x_4', 'NNP'), ('&', 'CC'), ('_esc=publicationCoverPdf', 'NNP'), ('https', 'NN'), (':', ':'), ('//www.researchgate.net/profile/Sukhdev-Singh', 'JJ'), ('?', '.'), ('enrichId=rgreq-b679ffd100c6b00b6348f225c923452e-XXX', 'JJ'), ('&', 'CC'), ('enrichSource=Y292ZXJQYWdlOzMxOTE2NDI0MztBUzo2MTA1NzA2ODg1NDQ3NjhAMTUyMjU4MjgwNDM4Mg', 'JJ'), ('%', 'NN'), ('3D', 'CD'), ('%', 'NN'), ('3D', 'CD'), ('&', 'CC'), ('el=1_x_5', 'NNP'), ('&', 'CC'), ('_esc=publicationCoverPdf', 'NNP'), ('https', 'NN'), (':', ':'), ('//www.researchgate.net/institution/Indian-Institute-of-Technology-Patna', 'JJ'), ('?', '.'), ('enrichId=rgreq-b679ffd100c6b00b6348f225c923452e-XXX', 'JJ'), ('&', 'CC'), ('enrichSource=Y292ZXJQYWdlOzMxOTE2NDI0MztBUzo2MTA1NzA2ODg1NDQ3NjhAMTUyMjU4MjgwNDM4Mg', 'JJ'), ('%', 'NN'), ('3D', 'CD'), ('%', 'NN'), ('3D', 'CD'), ('&', 'CC'), ('el=1_x_6', 'NNP'), ('&', 'CC'), ('_esc=publicationCoverPdf', 'NNP'), ('https', 'NN'), (':', ':'), ('//www.researchgate.net/profile/Sukhdev-Singh', 'JJ'), ('?', '.'), ('enrichId=rgreq-b679ffd100c6b00b6348f225c923452e-XXX', 'JJ'), ('&', 'CC'), ('enrichSource=Y292ZXJQYWdlOzMxOTE2NDI0MztBUzo2MTA1NzA2ODg1NDQ3NjhAMTUyMjU4MjgwNDM4Mg', 'JJ'), ('%', 'NN'), ('3D', 'CD'), ('%', 'NN'), ('3D', 'CD'), ('&', 'CC'), ('el=1_x_7', 'NNP'), ('&', 'CC'), ('_esc=publicationCoverPdf', 'NNP'), ('https', 'NN'), (':', ':'), ('//www.researchgate.net/profile/Kiran-Khatter', 'JJ'), ('?', '.'), ('enrichId=rgreq-b679ffd100c6b00b6348f225c923452e-XXX', 'JJ'), ('&', 'CC'), ('enrichSource=Y292ZXJQYWdlOzMxOTE2NDI0MztBUzo2MTA1NzA2ODg1NDQ3NjhAMTUyMjU4MjgwNDM4Mg', 'JJ'), ('%', 'NN'), ('3D', 'CD'), ('%', 'NN'), ('3D', 'CD'), ('&', 'CC'), ('el=1_x_10', 'NNP'), ('&', 'CC'), ('_esc=publicationCoverPdf', 'NNP'), ('Natural', 'NNP'), ('Language', 'NNP'), ('Processing', 'NN'), (':', ':'), ('State', 'NN'), ('of', 'IN'), ('The', 'DT'), ('Art', 'NNP'), (',', ','), ('Current', 'NNP'), ('Trends', 'NNP'), ('and', 'CC'), ('Challenges', 'NNP'), ('Diksha', 'NNP'), ('Khurana', 'NNP'), ('1', 'CD'), (',', ','), ('Aditya', 'NNP'), ('Koli', 'NNP'), ('1', 'CD'), (',', ','), ('Kiran', 'NNP'), ('Khatter', 'NNP'), ('1,2', 'CD'), ('and', 'CC'), ('Sukhdev', 'NNP'), ('Singh', 'NNP'), ('1,2', 'CD'), ('1', 'CD'), ('Department', 'NNP'), ('of', 'IN'), ('Computer', 'NNP'), ('Science', 'NNP'), ('and', 'CC'), ('Engineering', 'NNP'), ('Manav', 'NNP'), ('Rachna', 'NNP'), ('International', 'NNP'), ('University', 'NNP'), (',', ','), ('Faridabad-121004', 'NNP'), (',', ','), ('India', 'NNP'), ('2', 'CD'), ('Accendere', 'NNP'), ('Knowledge', 'NNP'), ('Management', 'NNP'), ('Services', 'NNPS'), ('Pvt', 'NNP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['https', ':', '//www.researchgate.net/publication/319164243_Natural_Language_Processing_State_of_The_Art_Current_Trends_and_Challenges', '?', 'enrichId=rgreq-b679ffd100c6b00b6348f225c923452e-XXX', '&', 'enrichSource=Y292ZXJQYWdlOzMxOTE2NDI0MztBUzo2MTA1NzA2ODg1NDQ3NjhAMTUyMjU4MjgwNDM4Mg', '%', '3D', '%', '3D', '&', 'el=1_x_2', '&', '_esc=publicationCoverPdf', 'https', ':', '//www.researchgate.net/publication/319164243_Natural_Language_Processing_State_of_The_Art_Current_Trends_and_Challenges', '?', 'enrichId=rgreq-b679ffd100c6b00b6348f225c923452e-XXX', '&', 'enrichSource=Y292ZXJQYWdlOzMxOTE2NDI0MztBUzo2MTA1NzA2ODg1NDQ3NjhAMTUyMjU4MjgwNDM4Mg', '%', '3D', '%', '3D', '&', 'el=1_x_3', '&', '_esc=publicationCoverPdf', 'https', ':', '//www.researchgate.net/project/Poisson-Exponential-Distribution-problems-of-estimation-and-prediction', '?', 'enrichId=rgreq-b679ffd100c6b00b6348f225c923452e-XXX', '&', 'enrichSource=Y292ZXJQYWdlOzMxOTE2NDI0MztBUzo2MTA1NzA2ODg1NDQ3NjhAMTUyMjU4MjgwNDM4Mg', '%', '3D', '%', '3D', '&', 'el=1_x_9', '&', '_esc=publicationCoverPdf', 'https', ':', '//www.researchgate.net/project/Lognormal-distribution-from-Bayesian-view-point', '?', 'enrichId=rgreq-b679ffd100c6b00b6348f225c923452e-XXX', '&', 'enrichSource=Y292ZXJQYWdlOzMxOTE2NDI0MztBUzo2MTA1NzA2ODg1NDQ3NjhAMTUyMjU4MjgwNDM4Mg', '%', '3D', '%', '3D', '&', 'el=1_x_9', '&', '_esc=publicationCoverPdf', 'https', ':', '//www.researchgate.net/', '?', 'enrichId=rgreq-b679ffd100c6b00b6348f225c923452e-XXX', '&', 'enrichSource=Y292ZXJQYWdlOzMxOTE2NDI0MztBUzo2MTA1NzA2ODg1NDQ3NjhAMTUyMjU4MjgwNDM4Mg', '%', '3D', '%', '3D', '&', 'el=1_x_1', '&', '_esc=publicationCoverPdf', 'https', ':', '//www.researchgate.net/profile/Kiran-Khatter', '?', 'enrichId=rgreq-b679ffd100c6b00b6348f225c923452e-XXX', '&', 'enrichSource=Y292ZXJQYWdlOzMxOTE2NDI0MztBUzo2MTA1NzA2ODg1NDQ3NjhAMTUyMjU4MjgwNDM4Mg', '%', '3D', '%', '3D', '&', 'el=1_x_4', '&', '_esc=publicationCoverPdf', 'https', ':', '//www.researchgate.net/profile/Kiran-Khatter', '?', 'enrichId=rgreq-b679ffd100c6b00b6348f225c923452e-XXX', '&', 'enrichSource=Y292ZXJQYWdlOzMxOTE2NDI0MztBUzo2MTA1NzA2ODg1NDQ3NjhAMTUyMjU4MjgwNDM4Mg', '%', '3D', '%', '3D', '&', 'el=1_x_5', '&', '_esc=publicationCoverPdf', 'https', ':', '//www.researchgate.net/institution/BML-MUNJAL-UNIVERSITY', '?', 'enrichId=rgreq-b679ffd100c6b00b6348f225c923452e-XXX', '&', 'enrichSource=Y292ZXJQYWdlOzMxOTE2NDI0MztBUzo2MTA1NzA2ODg1NDQ3NjhAMTUyMjU4MjgwNDM4Mg', '%', '3D', '%', '3D', '&', 'el=1_x_6', '&', '_esc=publicationCoverPdf', 'https', ':', '//www.researchgate.net/profile/Kiran-Khatter', '?', 'enrichId=rgreq-b679ffd100c6b00b6348f225c923452e-XXX', '&', 'enrichSource=Y292ZXJQYWdlOzMxOTE2NDI0MztBUzo2MTA1NzA2ODg1NDQ3NjhAMTUyMjU4MjgwNDM4Mg', '%', '3D', '%', '3D', '&', 'el=1_x_7', '&', '_esc=publicationCoverPdf', 'https', ':', '//www.researchgate.net/profile/Sukhdev-Singh', '?', 'enrichId=rgreq-b679ffd100c6b00b6348f225c923452e-XXX', '&', 'enrichSource=Y292ZXJQYWdlOzMxOTE2NDI0MztBUzo2MTA1NzA2ODg1NDQ3NjhAMTUyMjU4MjgwNDM4Mg', '%', '3D', '%', '3D', '&', 'el=1_x_4', '&', '_esc=publicationCoverPdf', 'https', ':', '//www.researchgate.net/profile/Sukhdev-Singh', '?', 'enrichId=rgreq-b679ffd100c6b00b6348f225c923452e-XXX', '&', 'enrichSource=Y292ZXJQYWdlOzMxOTE2NDI0MztBUzo2MTA1NzA2ODg1NDQ3NjhAMTUyMjU4MjgwNDM4Mg', '%', '3D', '%', '3D', '&', 'el=1_x_5', '&', '_esc=publicationCoverPdf', 'https', ':', '//www.researchgate.net/institution/Indian-Institute-of-Technology-Patna', '?', 'enrichId=rgreq-b679ffd100c6b00b6348f225c923452e-XXX', '&', 'enrichSource=Y292ZXJQYWdlOzMxOTE2NDI0MztBUzo2MTA1NzA2ODg1NDQ3NjhAMTUyMjU4MjgwNDM4Mg', '%', '3D', '%', '3D', '&', 'el=1_x_6', '&', '_esc=publicationCoverPdf', 'https', ':', '//www.researchgate.net/profile/Sukhdev-Singh', '?', 'enrichId=rgreq-b679ffd100c6b00b6348f225c923452e-XXX', '&', 'enrichSource=Y292ZXJQYWdlOzMxOTE2NDI0MztBUzo2MTA1NzA2ODg1NDQ3NjhAMTUyMjU4MjgwNDM4Mg', '%', '3D', '%', '3D', '&', 'el=1_x_7', '&', '_esc=publicationCoverPdf', 'https', ':', '//www.researchgate.net/profile/Kiran-Khatter', '?', 'enrichId=rgreq-b679ffd100c6b00b6348f225c923452e-XXX', '&', 'enrichSource=Y292ZXJQYWdlOzMxOTE2NDI0MztBUzo2MTA1NzA2ODg1NDQ3NjhAMTUyMjU4MjgwNDM4Mg', '%', '3D', '%', '3D', '&', 'el=1_x_10', '&', '_esc=publicationCoverPdf', 'Natural', 'Language', 'Processing', ':', 'State', 'Art', ',', 'Current', 'Trends', 'Challenges', 'Diksha', 'Khurana', '1', ',', 'Aditya', 'Koli', '1', ',', 'Kiran', 'Khatter', '1,2', 'Sukhdev', 'Singh', '1,2', '1', 'Department', 'Computer', 'Science', 'Engineering', 'Manav', 'Rachna', 'International', 'University', ',', 'Faridabad-121004', ',', 'India', '2', 'Accendere', 'Knowledge', 'Management', 'Services', 'Pvt', '.']

 TOTAL FILTERED TOKENS ==>  254

 ---- POST FOR FILTERED TOKENS ----

 [('https', 'NN'), (':', ':'), ('//www.researchgate.net/publication/319164243_Natural_Language_Processing_State_of_The_Art_Current_Trends_and_Challenges', 'NNS'), ('?', '.'), ('enrichId=rgreq-b679ffd100c6b00b6348f225c923452e-XXX', 'JJ'), ('&', 'CC'), ('enrichSource=Y292ZXJQYWdlOzMxOTE2NDI0MztBUzo2MTA1NzA2ODg1NDQ3NjhAMTUyMjU4MjgwNDM4Mg', 'JJ'), ('%', 'NN'), ('3D', 'CD'), ('%', 'NN'), ('3D', 'CD'), ('&', 'CC'), ('el=1_x_2', 'NNP'), ('&', 'CC'), ('_esc=publicationCoverPdf', 'NNP'), ('https', 'NN'), (':', ':'), ('//www.researchgate.net/publication/319164243_Natural_Language_Processing_State_of_The_Art_Current_Trends_and_Challenges', 'NNS'), ('?', '.'), ('enrichId=rgreq-b679ffd100c6b00b6348f225c923452e-XXX', 'JJ'), ('&', 'CC'), ('enrichSource=Y292ZXJQYWdlOzMxOTE2NDI0MztBUzo2MTA1NzA2ODg1NDQ3NjhAMTUyMjU4MjgwNDM4Mg', 'JJ'), ('%', 'NN'), ('3D', 'CD'), ('%', 'NN'), ('3D', 'CD'), ('&', 'CC'), ('el=1_x_3', 'NNP'), ('&', 'CC'), ('_esc=publicationCoverPdf', 'NNP'), ('https', 'NN'), (':', ':'), ('//www.researchgate.net/project/Poisson-Exponential-Distribution-problems-of-estimation-and-prediction', 'NN'), ('?', '.'), ('enrichId=rgreq-b679ffd100c6b00b6348f225c923452e-XXX', 'JJ'), ('&', 'CC'), ('enrichSource=Y292ZXJQYWdlOzMxOTE2NDI0MztBUzo2MTA1NzA2ODg1NDQ3NjhAMTUyMjU4MjgwNDM4Mg', 'JJ'), ('%', 'NN'), ('3D', 'CD'), ('%', 'NN'), ('3D', 'CD'), ('&', 'CC'), ('el=1_x_9', 'NNP'), ('&', 'CC'), ('_esc=publicationCoverPdf', 'NNP'), ('https', 'NN'), (':', ':'), ('//www.researchgate.net/project/Lognormal-distribution-from-Bayesian-view-point', 'JJ'), ('?', '.'), ('enrichId=rgreq-b679ffd100c6b00b6348f225c923452e-XXX', 'JJ'), ('&', 'CC'), ('enrichSource=Y292ZXJQYWdlOzMxOTE2NDI0MztBUzo2MTA1NzA2ODg1NDQ3NjhAMTUyMjU4MjgwNDM4Mg', 'JJ'), ('%', 'NN'), ('3D', 'CD'), ('%', 'NN'), ('3D', 'CD'), ('&', 'CC'), ('el=1_x_9', 'NNP'), ('&', 'CC'), ('_esc=publicationCoverPdf', 'NNP'), ('https', 'NN'), (':', ':'), ('//www.researchgate.net/', 'NN'), ('?', '.'), ('enrichId=rgreq-b679ffd100c6b00b6348f225c923452e-XXX', 'JJ'), ('&', 'CC'), ('enrichSource=Y292ZXJQYWdlOzMxOTE2NDI0MztBUzo2MTA1NzA2ODg1NDQ3NjhAMTUyMjU4MjgwNDM4Mg', 'JJ'), ('%', 'NN'), ('3D', 'CD'), ('%', 'NN'), ('3D', 'CD'), ('&', 'CC'), ('el=1_x_1', 'NNP'), ('&', 'CC'), ('_esc=publicationCoverPdf', 'NNP'), ('https', 'NN'), (':', ':'), ('//www.researchgate.net/profile/Kiran-Khatter', 'JJ'), ('?', '.'), ('enrichId=rgreq-b679ffd100c6b00b6348f225c923452e-XXX', 'JJ'), ('&', 'CC'), ('enrichSource=Y292ZXJQYWdlOzMxOTE2NDI0MztBUzo2MTA1NzA2ODg1NDQ3NjhAMTUyMjU4MjgwNDM4Mg', 'JJ'), ('%', 'NN'), ('3D', 'CD'), ('%', 'NN'), ('3D', 'CD'), ('&', 'CC'), ('el=1_x_4', 'NNP'), ('&', 'CC'), ('_esc=publicationCoverPdf', 'NNP'), ('https', 'NN'), (':', ':'), ('//www.researchgate.net/profile/Kiran-Khatter', 'JJ'), ('?', '.'), ('enrichId=rgreq-b679ffd100c6b00b6348f225c923452e-XXX', 'JJ'), ('&', 'CC'), ('enrichSource=Y292ZXJQYWdlOzMxOTE2NDI0MztBUzo2MTA1NzA2ODg1NDQ3NjhAMTUyMjU4MjgwNDM4Mg', 'JJ'), ('%', 'NN'), ('3D', 'CD'), ('%', 'NN'), ('3D', 'CD'), ('&', 'CC'), ('el=1_x_5', 'NNP'), ('&', 'CC'), ('_esc=publicationCoverPdf', 'NNP'), ('https', 'NN'), (':', ':'), ('//www.researchgate.net/institution/BML-MUNJAL-UNIVERSITY', 'JJ'), ('?', '.'), ('enrichId=rgreq-b679ffd100c6b00b6348f225c923452e-XXX', 'JJ'), ('&', 'CC'), ('enrichSource=Y292ZXJQYWdlOzMxOTE2NDI0MztBUzo2MTA1NzA2ODg1NDQ3NjhAMTUyMjU4MjgwNDM4Mg', 'JJ'), ('%', 'NN'), ('3D', 'CD'), ('%', 'NN'), ('3D', 'CD'), ('&', 'CC'), ('el=1_x_6', 'NNP'), ('&', 'CC'), ('_esc=publicationCoverPdf', 'NNP'), ('https', 'NN'), (':', ':'), ('//www.researchgate.net/profile/Kiran-Khatter', 'JJ'), ('?', '.'), ('enrichId=rgreq-b679ffd100c6b00b6348f225c923452e-XXX', 'JJ'), ('&', 'CC'), ('enrichSource=Y292ZXJQYWdlOzMxOTE2NDI0MztBUzo2MTA1NzA2ODg1NDQ3NjhAMTUyMjU4MjgwNDM4Mg', 'JJ'), ('%', 'NN'), ('3D', 'CD'), ('%', 'NN'), ('3D', 'CD'), ('&', 'CC'), ('el=1_x_7', 'NNP'), ('&', 'CC'), ('_esc=publicationCoverPdf', 'NNP'), ('https', 'NN'), (':', ':'), ('//www.researchgate.net/profile/Sukhdev-Singh', 'JJ'), ('?', '.'), ('enrichId=rgreq-b679ffd100c6b00b6348f225c923452e-XXX', 'JJ'), ('&', 'CC'), ('enrichSource=Y292ZXJQYWdlOzMxOTE2NDI0MztBUzo2MTA1NzA2ODg1NDQ3NjhAMTUyMjU4MjgwNDM4Mg', 'JJ'), ('%', 'NN'), ('3D', 'CD'), ('%', 'NN'), ('3D', 'CD'), ('&', 'CC'), ('el=1_x_4', 'NNP'), ('&', 'CC'), ('_esc=publicationCoverPdf', 'NNP'), ('https', 'NN'), (':', ':'), ('//www.researchgate.net/profile/Sukhdev-Singh', 'JJ'), ('?', '.'), ('enrichId=rgreq-b679ffd100c6b00b6348f225c923452e-XXX', 'JJ'), ('&', 'CC'), ('enrichSource=Y292ZXJQYWdlOzMxOTE2NDI0MztBUzo2MTA1NzA2ODg1NDQ3NjhAMTUyMjU4MjgwNDM4Mg', 'JJ'), ('%', 'NN'), ('3D', 'CD'), ('%', 'NN'), ('3D', 'CD'), ('&', 'CC'), ('el=1_x_5', 'NNP'), ('&', 'CC'), ('_esc=publicationCoverPdf', 'NNP'), ('https', 'NN'), (':', ':'), ('//www.researchgate.net/institution/Indian-Institute-of-Technology-Patna', 'JJ'), ('?', '.'), ('enrichId=rgreq-b679ffd100c6b00b6348f225c923452e-XXX', 'JJ'), ('&', 'CC'), ('enrichSource=Y292ZXJQYWdlOzMxOTE2NDI0MztBUzo2MTA1NzA2ODg1NDQ3NjhAMTUyMjU4MjgwNDM4Mg', 'JJ'), ('%', 'NN'), ('3D', 'CD'), ('%', 'NN'), ('3D', 'CD'), ('&', 'CC'), ('el=1_x_6', 'NNP'), ('&', 'CC'), ('_esc=publicationCoverPdf', 'NNP'), ('https', 'NN'), (':', ':'), ('//www.researchgate.net/profile/Sukhdev-Singh', 'JJ'), ('?', '.'), ('enrichId=rgreq-b679ffd100c6b00b6348f225c923452e-XXX', 'JJ'), ('&', 'CC'), ('enrichSource=Y292ZXJQYWdlOzMxOTE2NDI0MztBUzo2MTA1NzA2ODg1NDQ3NjhAMTUyMjU4MjgwNDM4Mg', 'JJ'), ('%', 'NN'), ('3D', 'CD'), ('%', 'NN'), ('3D', 'CD'), ('&', 'CC'), ('el=1_x_7', 'NNP'), ('&', 'CC'), ('_esc=publicationCoverPdf', 'NNP'), ('https', 'NN'), (':', ':'), ('//www.researchgate.net/profile/Kiran-Khatter', 'JJ'), ('?', '.'), ('enrichId=rgreq-b679ffd100c6b00b6348f225c923452e-XXX', 'JJ'), ('&', 'CC'), ('enrichSource=Y292ZXJQYWdlOzMxOTE2NDI0MztBUzo2MTA1NzA2ODg1NDQ3NjhAMTUyMjU4MjgwNDM4Mg', 'JJ'), ('%', 'NN'), ('3D', 'CD'), ('%', 'NN'), ('3D', 'CD'), ('&', 'CC'), ('el=1_x_10', 'NNP'), ('&', 'CC'), ('_esc=publicationCoverPdf', 'NNP'), ('Natural', 'NNP'), ('Language', 'NNP'), ('Processing', 'NN'), (':', ':'), ('State', 'NNP'), ('Art', 'NNP'), (',', ','), ('Current', 'NNP'), ('Trends', 'NNP'), ('Challenges', 'NNP'), ('Diksha', 'NNP'), ('Khurana', 'NNP'), ('1', 'CD'), (',', ','), ('Aditya', 'NNP'), ('Koli', 'NNP'), ('1', 'CD'), (',', ','), ('Kiran', 'NNP'), ('Khatter', 'NNP'), ('1,2', 'CD'), ('Sukhdev', 'NNP'), ('Singh', 'NNP'), ('1,2', 'CD'), ('1', 'CD'), ('Department', 'NNP'), ('Computer', 'NNP'), ('Science', 'NNP'), ('Engineering', 'NNP'), ('Manav', 'NNP'), ('Rachna', 'NNP'), ('International', 'NNP'), ('University', 'NNP'), (',', ','), ('Faridabad-121004', 'NNP'), (',', ','), ('India', 'NNP'), ('2', 'CD'), ('Accendere', 'NNP'), ('Knowledge', 'NNP'), ('Management', 'NNP'), ('Services', 'NNPS'), ('Pvt', 'NNP'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['https :', ': //www.researchgate.net/publication/319164243_Natural_Language_Processing_State_of_The_Art_Current_Trends_and_Challenges', '//www.researchgate.net/publication/319164243_Natural_Language_Processing_State_of_The_Art_Current_Trends_and_Challenges ?', '? enrichId=rgreq-b679ffd100c6b00b6348f225c923452e-XXX', 'enrichId=rgreq-b679ffd100c6b00b6348f225c923452e-XXX &', '& enrichSource=Y292ZXJQYWdlOzMxOTE2NDI0MztBUzo2MTA1NzA2ODg1NDQ3NjhAMTUyMjU4MjgwNDM4Mg', 'enrichSource=Y292ZXJQYWdlOzMxOTE2NDI0MztBUzo2MTA1NzA2ODg1NDQ3NjhAMTUyMjU4MjgwNDM4Mg %', '% 3D', '3D %', '% 3D', '3D &', '& el=1_x_2', 'el=1_x_2 &', '& _esc=publicationCoverPdf', '_esc=publicationCoverPdf https', 'https :', ': //www.researchgate.net/publication/319164243_Natural_Language_Processing_State_of_The_Art_Current_Trends_and_Challenges', '//www.researchgate.net/publication/319164243_Natural_Language_Processing_State_of_The_Art_Current_Trends_and_Challenges ?', '? enrichId=rgreq-b679ffd100c6b00b6348f225c923452e-XXX', 'enrichId=rgreq-b679ffd100c6b00b6348f225c923452e-XXX &', '& enrichSource=Y292ZXJQYWdlOzMxOTE2NDI0MztBUzo2MTA1NzA2ODg1NDQ3NjhAMTUyMjU4MjgwNDM4Mg', 'enrichSource=Y292ZXJQYWdlOzMxOTE2NDI0MztBUzo2MTA1NzA2ODg1NDQ3NjhAMTUyMjU4MjgwNDM4Mg %', '% 3D', '3D %', '% 3D', '3D &', '& el=1_x_3', 'el=1_x_3 &', '& _esc=publicationCoverPdf', '_esc=publicationCoverPdf https', 'https :', ': //www.researchgate.net/project/Poisson-Exponential-Distribution-problems-of-estimation-and-prediction', '//www.researchgate.net/project/Poisson-Exponential-Distribution-problems-of-estimation-and-prediction ?', '? enrichId=rgreq-b679ffd100c6b00b6348f225c923452e-XXX', 'enrichId=rgreq-b679ffd100c6b00b6348f225c923452e-XXX &', '& enrichSource=Y292ZXJQYWdlOzMxOTE2NDI0MztBUzo2MTA1NzA2ODg1NDQ3NjhAMTUyMjU4MjgwNDM4Mg', 'enrichSource=Y292ZXJQYWdlOzMxOTE2NDI0MztBUzo2MTA1NzA2ODg1NDQ3NjhAMTUyMjU4MjgwNDM4Mg %', '% 3D', '3D %', '% 3D', '3D &', '& el=1_x_9', 'el=1_x_9 &', '& _esc=publicationCoverPdf', '_esc=publicationCoverPdf https', 'https :', ': //www.researchgate.net/project/Lognormal-distribution-from-Bayesian-view-point', '//www.researchgate.net/project/Lognormal-distribution-from-Bayesian-view-point ?', '? enrichId=rgreq-b679ffd100c6b00b6348f225c923452e-XXX', 'enrichId=rgreq-b679ffd100c6b00b6348f225c923452e-XXX &', '& enrichSource=Y292ZXJQYWdlOzMxOTE2NDI0MztBUzo2MTA1NzA2ODg1NDQ3NjhAMTUyMjU4MjgwNDM4Mg', 'enrichSource=Y292ZXJQYWdlOzMxOTE2NDI0MztBUzo2MTA1NzA2ODg1NDQ3NjhAMTUyMjU4MjgwNDM4Mg %', '% 3D', '3D %', '% 3D', '3D &', '& el=1_x_9', 'el=1_x_9 &', '& _esc=publicationCoverPdf', '_esc=publicationCoverPdf https', 'https :', ': //www.researchgate.net/', '//www.researchgate.net/ ?', '? enrichId=rgreq-b679ffd100c6b00b6348f225c923452e-XXX', 'enrichId=rgreq-b679ffd100c6b00b6348f225c923452e-XXX &', '& enrichSource=Y292ZXJQYWdlOzMxOTE2NDI0MztBUzo2MTA1NzA2ODg1NDQ3NjhAMTUyMjU4MjgwNDM4Mg', 'enrichSource=Y292ZXJQYWdlOzMxOTE2NDI0MztBUzo2MTA1NzA2ODg1NDQ3NjhAMTUyMjU4MjgwNDM4Mg %', '% 3D', '3D %', '% 3D', '3D &', '& el=1_x_1', 'el=1_x_1 &', '& _esc=publicationCoverPdf', '_esc=publicationCoverPdf https', 'https :', ': //www.researchgate.net/profile/Kiran-Khatter', '//www.researchgate.net/profile/Kiran-Khatter ?', '? enrichId=rgreq-b679ffd100c6b00b6348f225c923452e-XXX', 'enrichId=rgreq-b679ffd100c6b00b6348f225c923452e-XXX &', '& enrichSource=Y292ZXJQYWdlOzMxOTE2NDI0MztBUzo2MTA1NzA2ODg1NDQ3NjhAMTUyMjU4MjgwNDM4Mg', 'enrichSource=Y292ZXJQYWdlOzMxOTE2NDI0MztBUzo2MTA1NzA2ODg1NDQ3NjhAMTUyMjU4MjgwNDM4Mg %', '% 3D', '3D %', '% 3D', '3D &', '& el=1_x_4', 'el=1_x_4 &', '& _esc=publicationCoverPdf', '_esc=publicationCoverPdf https', 'https :', ': //www.researchgate.net/profile/Kiran-Khatter', '//www.researchgate.net/profile/Kiran-Khatter ?', '? enrichId=rgreq-b679ffd100c6b00b6348f225c923452e-XXX', 'enrichId=rgreq-b679ffd100c6b00b6348f225c923452e-XXX &', '& enrichSource=Y292ZXJQYWdlOzMxOTE2NDI0MztBUzo2MTA1NzA2ODg1NDQ3NjhAMTUyMjU4MjgwNDM4Mg', 'enrichSource=Y292ZXJQYWdlOzMxOTE2NDI0MztBUzo2MTA1NzA2ODg1NDQ3NjhAMTUyMjU4MjgwNDM4Mg %', '% 3D', '3D %', '% 3D', '3D &', '& el=1_x_5', 'el=1_x_5 &', '& _esc=publicationCoverPdf', '_esc=publicationCoverPdf https', 'https :', ': //www.researchgate.net/institution/BML-MUNJAL-UNIVERSITY', '//www.researchgate.net/institution/BML-MUNJAL-UNIVERSITY ?', '? enrichId=rgreq-b679ffd100c6b00b6348f225c923452e-XXX', 'enrichId=rgreq-b679ffd100c6b00b6348f225c923452e-XXX &', '& enrichSource=Y292ZXJQYWdlOzMxOTE2NDI0MztBUzo2MTA1NzA2ODg1NDQ3NjhAMTUyMjU4MjgwNDM4Mg', 'enrichSource=Y292ZXJQYWdlOzMxOTE2NDI0MztBUzo2MTA1NzA2ODg1NDQ3NjhAMTUyMjU4MjgwNDM4Mg %', '% 3D', '3D %', '% 3D', '3D &', '& el=1_x_6', 'el=1_x_6 &', '& _esc=publicationCoverPdf', '_esc=publicationCoverPdf https', 'https :', ': //www.researchgate.net/profile/Kiran-Khatter', '//www.researchgate.net/profile/Kiran-Khatter ?', '? enrichId=rgreq-b679ffd100c6b00b6348f225c923452e-XXX', 'enrichId=rgreq-b679ffd100c6b00b6348f225c923452e-XXX &', '& enrichSource=Y292ZXJQYWdlOzMxOTE2NDI0MztBUzo2MTA1NzA2ODg1NDQ3NjhAMTUyMjU4MjgwNDM4Mg', 'enrichSource=Y292ZXJQYWdlOzMxOTE2NDI0MztBUzo2MTA1NzA2ODg1NDQ3NjhAMTUyMjU4MjgwNDM4Mg %', '% 3D', '3D %', '% 3D', '3D &', '& el=1_x_7', 'el=1_x_7 &', '& _esc=publicationCoverPdf', '_esc=publicationCoverPdf https', 'https :', ': //www.researchgate.net/profile/Sukhdev-Singh', '//www.researchgate.net/profile/Sukhdev-Singh ?', '? enrichId=rgreq-b679ffd100c6b00b6348f225c923452e-XXX', 'enrichId=rgreq-b679ffd100c6b00b6348f225c923452e-XXX &', '& enrichSource=Y292ZXJQYWdlOzMxOTE2NDI0MztBUzo2MTA1NzA2ODg1NDQ3NjhAMTUyMjU4MjgwNDM4Mg', 'enrichSource=Y292ZXJQYWdlOzMxOTE2NDI0MztBUzo2MTA1NzA2ODg1NDQ3NjhAMTUyMjU4MjgwNDM4Mg %', '% 3D', '3D %', '% 3D', '3D &', '& el=1_x_4', 'el=1_x_4 &', '& _esc=publicationCoverPdf', '_esc=publicationCoverPdf https', 'https :', ': //www.researchgate.net/profile/Sukhdev-Singh', '//www.researchgate.net/profile/Sukhdev-Singh ?', '? enrichId=rgreq-b679ffd100c6b00b6348f225c923452e-XXX', 'enrichId=rgreq-b679ffd100c6b00b6348f225c923452e-XXX &', '& enrichSource=Y292ZXJQYWdlOzMxOTE2NDI0MztBUzo2MTA1NzA2ODg1NDQ3NjhAMTUyMjU4MjgwNDM4Mg', 'enrichSource=Y292ZXJQYWdlOzMxOTE2NDI0MztBUzo2MTA1NzA2ODg1NDQ3NjhAMTUyMjU4MjgwNDM4Mg %', '% 3D', '3D %', '% 3D', '3D &', '& el=1_x_5', 'el=1_x_5 &', '& _esc=publicationCoverPdf', '_esc=publicationCoverPdf https', 'https :', ': //www.researchgate.net/institution/Indian-Institute-of-Technology-Patna', '//www.researchgate.net/institution/Indian-Institute-of-Technology-Patna ?', '? enrichId=rgreq-b679ffd100c6b00b6348f225c923452e-XXX', 'enrichId=rgreq-b679ffd100c6b00b6348f225c923452e-XXX &', '& enrichSource=Y292ZXJQYWdlOzMxOTE2NDI0MztBUzo2MTA1NzA2ODg1NDQ3NjhAMTUyMjU4MjgwNDM4Mg', 'enrichSource=Y292ZXJQYWdlOzMxOTE2NDI0MztBUzo2MTA1NzA2ODg1NDQ3NjhAMTUyMjU4MjgwNDM4Mg %', '% 3D', '3D %', '% 3D', '3D &', '& el=1_x_6', 'el=1_x_6 &', '& _esc=publicationCoverPdf', '_esc=publicationCoverPdf https', 'https :', ': //www.researchgate.net/profile/Sukhdev-Singh', '//www.researchgate.net/profile/Sukhdev-Singh ?', '? enrichId=rgreq-b679ffd100c6b00b6348f225c923452e-XXX', 'enrichId=rgreq-b679ffd100c6b00b6348f225c923452e-XXX &', '& enrichSource=Y292ZXJQYWdlOzMxOTE2NDI0MztBUzo2MTA1NzA2ODg1NDQ3NjhAMTUyMjU4MjgwNDM4Mg', 'enrichSource=Y292ZXJQYWdlOzMxOTE2NDI0MztBUzo2MTA1NzA2ODg1NDQ3NjhAMTUyMjU4MjgwNDM4Mg %', '% 3D', '3D %', '% 3D', '3D &', '& el=1_x_7', 'el=1_x_7 &', '& _esc=publicationCoverPdf', '_esc=publicationCoverPdf https', 'https :', ': //www.researchgate.net/profile/Kiran-Khatter', '//www.researchgate.net/profile/Kiran-Khatter ?', '? enrichId=rgreq-b679ffd100c6b00b6348f225c923452e-XXX', 'enrichId=rgreq-b679ffd100c6b00b6348f225c923452e-XXX &', '& enrichSource=Y292ZXJQYWdlOzMxOTE2NDI0MztBUzo2MTA1NzA2ODg1NDQ3NjhAMTUyMjU4MjgwNDM4Mg', 'enrichSource=Y292ZXJQYWdlOzMxOTE2NDI0MztBUzo2MTA1NzA2ODg1NDQ3NjhAMTUyMjU4MjgwNDM4Mg %', '% 3D', '3D %', '% 3D', '3D &', '& el=1_x_10', 'el=1_x_10 &', '& _esc=publicationCoverPdf', '_esc=publicationCoverPdf Natural', 'Natural Language', 'Language Processing', 'Processing :', ': State', 'State Art', 'Art ,', ', Current', 'Current Trends', 'Trends Challenges', 'Challenges Diksha', 'Diksha Khurana', 'Khurana 1', '1 ,', ', Aditya', 'Aditya Koli', 'Koli 1', '1 ,', ', Kiran', 'Kiran Khatter', 'Khatter 1,2', '1,2 Sukhdev', 'Sukhdev Singh', 'Singh 1,2', '1,2 1', '1 Department', 'Department Computer', 'Computer Science', 'Science Engineering', 'Engineering Manav', 'Manav Rachna', 'Rachna International', 'International University', 'University ,', ', Faridabad-121004', 'Faridabad-121004 ,', ', India', 'India 2', '2 Accendere', 'Accendere Knowledge', 'Knowledge Management', 'Management Services', 'Services Pvt', 'Pvt .'] 

 TOTAL BIGRAMS --> 253 



 ---- TRI-GRAMS ---- 

 ['https : //www.researchgate.net/publication/319164243_Natural_Language_Processing_State_of_The_Art_Current_Trends_and_Challenges', ': //www.researchgate.net/publication/319164243_Natural_Language_Processing_State_of_The_Art_Current_Trends_and_Challenges ?', '//www.researchgate.net/publication/319164243_Natural_Language_Processing_State_of_The_Art_Current_Trends_and_Challenges ? enrichId=rgreq-b679ffd100c6b00b6348f225c923452e-XXX', '? enrichId=rgreq-b679ffd100c6b00b6348f225c923452e-XXX &', 'enrichId=rgreq-b679ffd100c6b00b6348f225c923452e-XXX & enrichSource=Y292ZXJQYWdlOzMxOTE2NDI0MztBUzo2MTA1NzA2ODg1NDQ3NjhAMTUyMjU4MjgwNDM4Mg', '& enrichSource=Y292ZXJQYWdlOzMxOTE2NDI0MztBUzo2MTA1NzA2ODg1NDQ3NjhAMTUyMjU4MjgwNDM4Mg %', 'enrichSource=Y292ZXJQYWdlOzMxOTE2NDI0MztBUzo2MTA1NzA2ODg1NDQ3NjhAMTUyMjU4MjgwNDM4Mg % 3D', '% 3D %', '3D % 3D', '% 3D &', '3D & el=1_x_2', '& el=1_x_2 &', 'el=1_x_2 & _esc=publicationCoverPdf', '& _esc=publicationCoverPdf https', '_esc=publicationCoverPdf https :', 'https : //www.researchgate.net/publication/319164243_Natural_Language_Processing_State_of_The_Art_Current_Trends_and_Challenges', ': //www.researchgate.net/publication/319164243_Natural_Language_Processing_State_of_The_Art_Current_Trends_and_Challenges ?', '//www.researchgate.net/publication/319164243_Natural_Language_Processing_State_of_The_Art_Current_Trends_and_Challenges ? enrichId=rgreq-b679ffd100c6b00b6348f225c923452e-XXX', '? enrichId=rgreq-b679ffd100c6b00b6348f225c923452e-XXX &', 'enrichId=rgreq-b679ffd100c6b00b6348f225c923452e-XXX & enrichSource=Y292ZXJQYWdlOzMxOTE2NDI0MztBUzo2MTA1NzA2ODg1NDQ3NjhAMTUyMjU4MjgwNDM4Mg', '& enrichSource=Y292ZXJQYWdlOzMxOTE2NDI0MztBUzo2MTA1NzA2ODg1NDQ3NjhAMTUyMjU4MjgwNDM4Mg %', 'enrichSource=Y292ZXJQYWdlOzMxOTE2NDI0MztBUzo2MTA1NzA2ODg1NDQ3NjhAMTUyMjU4MjgwNDM4Mg % 3D', '% 3D %', '3D % 3D', '% 3D &', '3D & el=1_x_3', '& el=1_x_3 &', 'el=1_x_3 & _esc=publicationCoverPdf', '& _esc=publicationCoverPdf https', '_esc=publicationCoverPdf https :', 'https : //www.researchgate.net/project/Poisson-Exponential-Distribution-problems-of-estimation-and-prediction', ': //www.researchgate.net/project/Poisson-Exponential-Distribution-problems-of-estimation-and-prediction ?', '//www.researchgate.net/project/Poisson-Exponential-Distribution-problems-of-estimation-and-prediction ? enrichId=rgreq-b679ffd100c6b00b6348f225c923452e-XXX', '? enrichId=rgreq-b679ffd100c6b00b6348f225c923452e-XXX &', 'enrichId=rgreq-b679ffd100c6b00b6348f225c923452e-XXX & enrichSource=Y292ZXJQYWdlOzMxOTE2NDI0MztBUzo2MTA1NzA2ODg1NDQ3NjhAMTUyMjU4MjgwNDM4Mg', '& enrichSource=Y292ZXJQYWdlOzMxOTE2NDI0MztBUzo2MTA1NzA2ODg1NDQ3NjhAMTUyMjU4MjgwNDM4Mg %', 'enrichSource=Y292ZXJQYWdlOzMxOTE2NDI0MztBUzo2MTA1NzA2ODg1NDQ3NjhAMTUyMjU4MjgwNDM4Mg % 3D', '% 3D %', '3D % 3D', '% 3D &', '3D & el=1_x_9', '& el=1_x_9 &', 'el=1_x_9 & _esc=publicationCoverPdf', '& _esc=publicationCoverPdf https', '_esc=publicationCoverPdf https :', 'https : //www.researchgate.net/project/Lognormal-distribution-from-Bayesian-view-point', ': //www.researchgate.net/project/Lognormal-distribution-from-Bayesian-view-point ?', '//www.researchgate.net/project/Lognormal-distribution-from-Bayesian-view-point ? enrichId=rgreq-b679ffd100c6b00b6348f225c923452e-XXX', '? enrichId=rgreq-b679ffd100c6b00b6348f225c923452e-XXX &', 'enrichId=rgreq-b679ffd100c6b00b6348f225c923452e-XXX & enrichSource=Y292ZXJQYWdlOzMxOTE2NDI0MztBUzo2MTA1NzA2ODg1NDQ3NjhAMTUyMjU4MjgwNDM4Mg', '& enrichSource=Y292ZXJQYWdlOzMxOTE2NDI0MztBUzo2MTA1NzA2ODg1NDQ3NjhAMTUyMjU4MjgwNDM4Mg %', 'enrichSource=Y292ZXJQYWdlOzMxOTE2NDI0MztBUzo2MTA1NzA2ODg1NDQ3NjhAMTUyMjU4MjgwNDM4Mg % 3D', '% 3D %', '3D % 3D', '% 3D &', '3D & el=1_x_9', '& el=1_x_9 &', 'el=1_x_9 & _esc=publicationCoverPdf', '& _esc=publicationCoverPdf https', '_esc=publicationCoverPdf https :', 'https : //www.researchgate.net/', ': //www.researchgate.net/ ?', '//www.researchgate.net/ ? enrichId=rgreq-b679ffd100c6b00b6348f225c923452e-XXX', '? enrichId=rgreq-b679ffd100c6b00b6348f225c923452e-XXX &', 'enrichId=rgreq-b679ffd100c6b00b6348f225c923452e-XXX & enrichSource=Y292ZXJQYWdlOzMxOTE2NDI0MztBUzo2MTA1NzA2ODg1NDQ3NjhAMTUyMjU4MjgwNDM4Mg', '& enrichSource=Y292ZXJQYWdlOzMxOTE2NDI0MztBUzo2MTA1NzA2ODg1NDQ3NjhAMTUyMjU4MjgwNDM4Mg %', 'enrichSource=Y292ZXJQYWdlOzMxOTE2NDI0MztBUzo2MTA1NzA2ODg1NDQ3NjhAMTUyMjU4MjgwNDM4Mg % 3D', '% 3D %', '3D % 3D', '% 3D &', '3D & el=1_x_1', '& el=1_x_1 &', 'el=1_x_1 & _esc=publicationCoverPdf', '& _esc=publicationCoverPdf https', '_esc=publicationCoverPdf https :', 'https : //www.researchgate.net/profile/Kiran-Khatter', ': //www.researchgate.net/profile/Kiran-Khatter ?', '//www.researchgate.net/profile/Kiran-Khatter ? enrichId=rgreq-b679ffd100c6b00b6348f225c923452e-XXX', '? enrichId=rgreq-b679ffd100c6b00b6348f225c923452e-XXX &', 'enrichId=rgreq-b679ffd100c6b00b6348f225c923452e-XXX & enrichSource=Y292ZXJQYWdlOzMxOTE2NDI0MztBUzo2MTA1NzA2ODg1NDQ3NjhAMTUyMjU4MjgwNDM4Mg', '& enrichSource=Y292ZXJQYWdlOzMxOTE2NDI0MztBUzo2MTA1NzA2ODg1NDQ3NjhAMTUyMjU4MjgwNDM4Mg %', 'enrichSource=Y292ZXJQYWdlOzMxOTE2NDI0MztBUzo2MTA1NzA2ODg1NDQ3NjhAMTUyMjU4MjgwNDM4Mg % 3D', '% 3D %', '3D % 3D', '% 3D &', '3D & el=1_x_4', '& el=1_x_4 &', 'el=1_x_4 & _esc=publicationCoverPdf', '& _esc=publicationCoverPdf https', '_esc=publicationCoverPdf https :', 'https : //www.researchgate.net/profile/Kiran-Khatter', ': //www.researchgate.net/profile/Kiran-Khatter ?', '//www.researchgate.net/profile/Kiran-Khatter ? enrichId=rgreq-b679ffd100c6b00b6348f225c923452e-XXX', '? enrichId=rgreq-b679ffd100c6b00b6348f225c923452e-XXX &', 'enrichId=rgreq-b679ffd100c6b00b6348f225c923452e-XXX & enrichSource=Y292ZXJQYWdlOzMxOTE2NDI0MztBUzo2MTA1NzA2ODg1NDQ3NjhAMTUyMjU4MjgwNDM4Mg', '& enrichSource=Y292ZXJQYWdlOzMxOTE2NDI0MztBUzo2MTA1NzA2ODg1NDQ3NjhAMTUyMjU4MjgwNDM4Mg %', 'enrichSource=Y292ZXJQYWdlOzMxOTE2NDI0MztBUzo2MTA1NzA2ODg1NDQ3NjhAMTUyMjU4MjgwNDM4Mg % 3D', '% 3D %', '3D % 3D', '% 3D &', '3D & el=1_x_5', '& el=1_x_5 &', 'el=1_x_5 & _esc=publicationCoverPdf', '& _esc=publicationCoverPdf https', '_esc=publicationCoverPdf https :', 'https : //www.researchgate.net/institution/BML-MUNJAL-UNIVERSITY', ': //www.researchgate.net/institution/BML-MUNJAL-UNIVERSITY ?', '//www.researchgate.net/institution/BML-MUNJAL-UNIVERSITY ? enrichId=rgreq-b679ffd100c6b00b6348f225c923452e-XXX', '? enrichId=rgreq-b679ffd100c6b00b6348f225c923452e-XXX &', 'enrichId=rgreq-b679ffd100c6b00b6348f225c923452e-XXX & enrichSource=Y292ZXJQYWdlOzMxOTE2NDI0MztBUzo2MTA1NzA2ODg1NDQ3NjhAMTUyMjU4MjgwNDM4Mg', '& enrichSource=Y292ZXJQYWdlOzMxOTE2NDI0MztBUzo2MTA1NzA2ODg1NDQ3NjhAMTUyMjU4MjgwNDM4Mg %', 'enrichSource=Y292ZXJQYWdlOzMxOTE2NDI0MztBUzo2MTA1NzA2ODg1NDQ3NjhAMTUyMjU4MjgwNDM4Mg % 3D', '% 3D %', '3D % 3D', '% 3D &', '3D & el=1_x_6', '& el=1_x_6 &', 'el=1_x_6 & _esc=publicationCoverPdf', '& _esc=publicationCoverPdf https', '_esc=publicationCoverPdf https :', 'https : //www.researchgate.net/profile/Kiran-Khatter', ': //www.researchgate.net/profile/Kiran-Khatter ?', '//www.researchgate.net/profile/Kiran-Khatter ? enrichId=rgreq-b679ffd100c6b00b6348f225c923452e-XXX', '? enrichId=rgreq-b679ffd100c6b00b6348f225c923452e-XXX &', 'enrichId=rgreq-b679ffd100c6b00b6348f225c923452e-XXX & enrichSource=Y292ZXJQYWdlOzMxOTE2NDI0MztBUzo2MTA1NzA2ODg1NDQ3NjhAMTUyMjU4MjgwNDM4Mg', '& enrichSource=Y292ZXJQYWdlOzMxOTE2NDI0MztBUzo2MTA1NzA2ODg1NDQ3NjhAMTUyMjU4MjgwNDM4Mg %', 'enrichSource=Y292ZXJQYWdlOzMxOTE2NDI0MztBUzo2MTA1NzA2ODg1NDQ3NjhAMTUyMjU4MjgwNDM4Mg % 3D', '% 3D %', '3D % 3D', '% 3D &', '3D & el=1_x_7', '& el=1_x_7 &', 'el=1_x_7 & _esc=publicationCoverPdf', '& _esc=publicationCoverPdf https', '_esc=publicationCoverPdf https :', 'https : //www.researchgate.net/profile/Sukhdev-Singh', ': //www.researchgate.net/profile/Sukhdev-Singh ?', '//www.researchgate.net/profile/Sukhdev-Singh ? enrichId=rgreq-b679ffd100c6b00b6348f225c923452e-XXX', '? enrichId=rgreq-b679ffd100c6b00b6348f225c923452e-XXX &', 'enrichId=rgreq-b679ffd100c6b00b6348f225c923452e-XXX & enrichSource=Y292ZXJQYWdlOzMxOTE2NDI0MztBUzo2MTA1NzA2ODg1NDQ3NjhAMTUyMjU4MjgwNDM4Mg', '& enrichSource=Y292ZXJQYWdlOzMxOTE2NDI0MztBUzo2MTA1NzA2ODg1NDQ3NjhAMTUyMjU4MjgwNDM4Mg %', 'enrichSource=Y292ZXJQYWdlOzMxOTE2NDI0MztBUzo2MTA1NzA2ODg1NDQ3NjhAMTUyMjU4MjgwNDM4Mg % 3D', '% 3D %', '3D % 3D', '% 3D &', '3D & el=1_x_4', '& el=1_x_4 &', 'el=1_x_4 & _esc=publicationCoverPdf', '& _esc=publicationCoverPdf https', '_esc=publicationCoverPdf https :', 'https : //www.researchgate.net/profile/Sukhdev-Singh', ': //www.researchgate.net/profile/Sukhdev-Singh ?', '//www.researchgate.net/profile/Sukhdev-Singh ? enrichId=rgreq-b679ffd100c6b00b6348f225c923452e-XXX', '? enrichId=rgreq-b679ffd100c6b00b6348f225c923452e-XXX &', 'enrichId=rgreq-b679ffd100c6b00b6348f225c923452e-XXX & enrichSource=Y292ZXJQYWdlOzMxOTE2NDI0MztBUzo2MTA1NzA2ODg1NDQ3NjhAMTUyMjU4MjgwNDM4Mg', '& enrichSource=Y292ZXJQYWdlOzMxOTE2NDI0MztBUzo2MTA1NzA2ODg1NDQ3NjhAMTUyMjU4MjgwNDM4Mg %', 'enrichSource=Y292ZXJQYWdlOzMxOTE2NDI0MztBUzo2MTA1NzA2ODg1NDQ3NjhAMTUyMjU4MjgwNDM4Mg % 3D', '% 3D %', '3D % 3D', '% 3D &', '3D & el=1_x_5', '& el=1_x_5 &', 'el=1_x_5 & _esc=publicationCoverPdf', '& _esc=publicationCoverPdf https', '_esc=publicationCoverPdf https :', 'https : //www.researchgate.net/institution/Indian-Institute-of-Technology-Patna', ': //www.researchgate.net/institution/Indian-Institute-of-Technology-Patna ?', '//www.researchgate.net/institution/Indian-Institute-of-Technology-Patna ? enrichId=rgreq-b679ffd100c6b00b6348f225c923452e-XXX', '? enrichId=rgreq-b679ffd100c6b00b6348f225c923452e-XXX &', 'enrichId=rgreq-b679ffd100c6b00b6348f225c923452e-XXX & enrichSource=Y292ZXJQYWdlOzMxOTE2NDI0MztBUzo2MTA1NzA2ODg1NDQ3NjhAMTUyMjU4MjgwNDM4Mg', '& enrichSource=Y292ZXJQYWdlOzMxOTE2NDI0MztBUzo2MTA1NzA2ODg1NDQ3NjhAMTUyMjU4MjgwNDM4Mg %', 'enrichSource=Y292ZXJQYWdlOzMxOTE2NDI0MztBUzo2MTA1NzA2ODg1NDQ3NjhAMTUyMjU4MjgwNDM4Mg % 3D', '% 3D %', '3D % 3D', '% 3D &', '3D & el=1_x_6', '& el=1_x_6 &', 'el=1_x_6 & _esc=publicationCoverPdf', '& _esc=publicationCoverPdf https', '_esc=publicationCoverPdf https :', 'https : //www.researchgate.net/profile/Sukhdev-Singh', ': //www.researchgate.net/profile/Sukhdev-Singh ?', '//www.researchgate.net/profile/Sukhdev-Singh ? enrichId=rgreq-b679ffd100c6b00b6348f225c923452e-XXX', '? enrichId=rgreq-b679ffd100c6b00b6348f225c923452e-XXX &', 'enrichId=rgreq-b679ffd100c6b00b6348f225c923452e-XXX & enrichSource=Y292ZXJQYWdlOzMxOTE2NDI0MztBUzo2MTA1NzA2ODg1NDQ3NjhAMTUyMjU4MjgwNDM4Mg', '& enrichSource=Y292ZXJQYWdlOzMxOTE2NDI0MztBUzo2MTA1NzA2ODg1NDQ3NjhAMTUyMjU4MjgwNDM4Mg %', 'enrichSource=Y292ZXJQYWdlOzMxOTE2NDI0MztBUzo2MTA1NzA2ODg1NDQ3NjhAMTUyMjU4MjgwNDM4Mg % 3D', '% 3D %', '3D % 3D', '% 3D &', '3D & el=1_x_7', '& el=1_x_7 &', 'el=1_x_7 & _esc=publicationCoverPdf', '& _esc=publicationCoverPdf https', '_esc=publicationCoverPdf https :', 'https : //www.researchgate.net/profile/Kiran-Khatter', ': //www.researchgate.net/profile/Kiran-Khatter ?', '//www.researchgate.net/profile/Kiran-Khatter ? enrichId=rgreq-b679ffd100c6b00b6348f225c923452e-XXX', '? enrichId=rgreq-b679ffd100c6b00b6348f225c923452e-XXX &', 'enrichId=rgreq-b679ffd100c6b00b6348f225c923452e-XXX & enrichSource=Y292ZXJQYWdlOzMxOTE2NDI0MztBUzo2MTA1NzA2ODg1NDQ3NjhAMTUyMjU4MjgwNDM4Mg', '& enrichSource=Y292ZXJQYWdlOzMxOTE2NDI0MztBUzo2MTA1NzA2ODg1NDQ3NjhAMTUyMjU4MjgwNDM4Mg %', 'enrichSource=Y292ZXJQYWdlOzMxOTE2NDI0MztBUzo2MTA1NzA2ODg1NDQ3NjhAMTUyMjU4MjgwNDM4Mg % 3D', '% 3D %', '3D % 3D', '% 3D &', '3D & el=1_x_10', '& el=1_x_10 &', 'el=1_x_10 & _esc=publicationCoverPdf', '& _esc=publicationCoverPdf Natural', '_esc=publicationCoverPdf Natural Language', 'Natural Language Processing', 'Language Processing :', 'Processing : State', ': State Art', 'State Art ,', 'Art , Current', ', Current Trends', 'Current Trends Challenges', 'Trends Challenges Diksha', 'Challenges Diksha Khurana', 'Diksha Khurana 1', 'Khurana 1 ,', '1 , Aditya', ', Aditya Koli', 'Aditya Koli 1', 'Koli 1 ,', '1 , Kiran', ', Kiran Khatter', 'Kiran Khatter 1,2', 'Khatter 1,2 Sukhdev', '1,2 Sukhdev Singh', 'Sukhdev Singh 1,2', 'Singh 1,2 1', '1,2 1 Department', '1 Department Computer', 'Department Computer Science', 'Computer Science Engineering', 'Science Engineering Manav', 'Engineering Manav Rachna', 'Manav Rachna International', 'Rachna International University', 'International University ,', 'University , Faridabad-121004', ', Faridabad-121004 ,', 'Faridabad-121004 , India', ', India 2', 'India 2 Accendere', '2 Accendere Knowledge', 'Accendere Knowledge Management', 'Knowledge Management Services', 'Management Services Pvt', 'Services Pvt .'] 

 TOTAL TRIGRAMS --> 252 



 ---- NOUN PHRASES ---- 

 ['https', 'enrichSource=Y292ZXJQYWdlOzMxOTE2NDI0MztBUzo2MTA1NzA2ODg1NDQ3NjhAMTUyMjU4MjgwNDM4Mg %', '%', 'https', 'enrichSource=Y292ZXJQYWdlOzMxOTE2NDI0MztBUzo2MTA1NzA2ODg1NDQ3NjhAMTUyMjU4MjgwNDM4Mg %', '%', 'https', '', 'enrichSource=Y292ZXJQYWdlOzMxOTE2NDI0MztBUzo2MTA1NzA2ODg1NDQ3NjhAMTUyMjU4MjgwNDM4Mg %', '%', 'https', 'enrichSource=Y292ZXJQYWdlOzMxOTE2NDI0MztBUzo2MTA1NzA2ODg1NDQ3NjhAMTUyMjU4MjgwNDM4Mg %', '%', 'https', '', 'enrichSource=Y292ZXJQYWdlOzMxOTE2NDI0MztBUzo2MTA1NzA2ODg1NDQ3NjhAMTUyMjU4MjgwNDM4Mg %', '%', 'https', 'enrichSource=Y292ZXJQYWdlOzMxOTE2NDI0MztBUzo2MTA1NzA2ODg1NDQ3NjhAMTUyMjU4MjgwNDM4Mg %', '%', 'https', 'enrichSource=Y292ZXJQYWdlOzMxOTE2NDI0MztBUzo2MTA1NzA2ODg1NDQ3NjhAMTUyMjU4MjgwNDM4Mg %', '%', 'https', 'enrichSource=Y292ZXJQYWdlOzMxOTE2NDI0MztBUzo2MTA1NzA2ODg1NDQ3NjhAMTUyMjU4MjgwNDM4Mg %', '%', 'https', 'enrichSource=Y292ZXJQYWdlOzMxOTE2NDI0MztBUzo2MTA1NzA2ODg1NDQ3NjhAMTUyMjU4MjgwNDM4Mg %', '%', 'https', 'enrichSource=Y292ZXJQYWdlOzMxOTE2NDI0MztBUzo2MTA1NzA2ODg1NDQ3NjhAMTUyMjU4MjgwNDM4Mg %', '%', 'https', 'enrichSource=Y292ZXJQYWdlOzMxOTE2NDI0MztBUzo2MTA1NzA2ODg1NDQ3NjhAMTUyMjU4MjgwNDM4Mg %', '%', 'https', 'enrichSource=Y292ZXJQYWdlOzMxOTE2NDI0MztBUzo2MTA1NzA2ODg1NDQ3NjhAMTUyMjU4MjgwNDM4Mg %', '%', 'https', 'enrichSource=Y292ZXJQYWdlOzMxOTE2NDI0MztBUzo2MTA1NzA2ODg1NDQ3NjhAMTUyMjU4MjgwNDM4Mg %', '%', 'https', 'enrichSource=Y292ZXJQYWdlOzMxOTE2NDI0MztBUzo2MTA1NzA2ODg1NDQ3NjhAMTUyMjU4MjgwNDM4Mg %', '%', 'Processing'] 

 TOTAL NOUN PHRASES --> 45 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> ['Current Trends Challenges Diksha Khurana', 'Aditya Koli', 'Kiran Khatter', 'Sukhdev Singh', 'Rachna International University']
 TOTAL PERSON ENTITY --> 5 


 GPE ---> ['India']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['http', ':', '//www.researchgate.net/publication/319164243_natural_language_processing_state_of_the_art_current_trends_and_challeng', '?', 'enrichid=rgreq-b679ffd100c6b00b6348f225c923452e-xxx', '&', 'enrichsource=y292zxjqywdlozmxote2ndi0mztbuzo2mta1nza2odg1ndq3njhamtuymju4mjgwndm4mg', '%', '3d', '%', '3d', '&', 'el=1_x_2', '&', '_esc=publicationcoverpdf', 'http', ':', '//www.researchgate.net/publication/319164243_natural_language_processing_state_of_the_art_current_trends_and_challeng', '?', 'enrichid=rgreq-b679ffd100c6b00b6348f225c923452e-xxx', '&', 'enrichsource=y292zxjqywdlozmxote2ndi0mztbuzo2mta1nza2odg1ndq3njhamtuymju4mjgwndm4mg', '%', '3d', '%', '3d', '&', 'el=1_x_3', '&', '_esc=publicationcoverpdf', 'http', ':', '//www.researchgate.net/project/poisson-exponential-distribution-problems-of-estimation-and-predict', '?', 'enrichid=rgreq-b679ffd100c6b00b6348f225c923452e-xxx', '&', 'enrichsource=y292zxjqywdlozmxote2ndi0mztbuzo2mta1nza2odg1ndq3njhamtuymju4mjgwndm4mg', '%', '3d', '%', '3d', '&', 'el=1_x_9', '&', '_esc=publicationcoverpdf', 'http', ':', '//www.researchgate.net/project/lognormal-distribution-from-bayesian-view-point', '?', 'enrichid=rgreq-b679ffd100c6b00b6348f225c923452e-xxx', '&', 'enrichsource=y292zxjqywdlozmxote2ndi0mztbuzo2mta1nza2odg1ndq3njhamtuymju4mjgwndm4mg', '%', '3d', '%', '3d', '&', 'el=1_x_9', '&', '_esc=publicationcoverpdf', 'http', ':', '//www.researchgate.net/', '?', 'enrichid=rgreq-b679ffd100c6b00b6348f225c923452e-xxx', '&', 'enrichsource=y292zxjqywdlozmxote2ndi0mztbuzo2mta1nza2odg1ndq3njhamtuymju4mjgwndm4mg', '%', '3d', '%', '3d', '&', 'el=1_x_1', '&', '_esc=publicationcoverpdf', 'http', ':', '//www.researchgate.net/profile/kiran-khatt', '?', 'enrichid=rgreq-b679ffd100c6b00b6348f225c923452e-xxx', '&', 'enrichsource=y292zxjqywdlozmxote2ndi0mztbuzo2mta1nza2odg1ndq3njhamtuymju4mjgwndm4mg', '%', '3d', '%', '3d', '&', 'el=1_x_4', '&', '_esc=publicationcoverpdf', 'http', ':', '//www.researchgate.net/profile/kiran-khatt', '?', 'enrichid=rgreq-b679ffd100c6b00b6348f225c923452e-xxx', '&', 'enrichsource=y292zxjqywdlozmxote2ndi0mztbuzo2mta1nza2odg1ndq3njhamtuymju4mjgwndm4mg', '%', '3d', '%', '3d', '&', 'el=1_x_5', '&', '_esc=publicationcoverpdf', 'http', ':', '//www.researchgate.net/institution/bml-munjal-univers', '?', 'enrichid=rgreq-b679ffd100c6b00b6348f225c923452e-xxx', '&', 'enrichsource=y292zxjqywdlozmxote2ndi0mztbuzo2mta1nza2odg1ndq3njhamtuymju4mjgwndm4mg', '%', '3d', '%', '3d', '&', 'el=1_x_6', '&', '_esc=publicationcoverpdf', 'http', ':', '//www.researchgate.net/profile/kiran-khatt', '?', 'enrichid=rgreq-b679ffd100c6b00b6348f225c923452e-xxx', '&', 'enrichsource=y292zxjqywdlozmxote2ndi0mztbuzo2mta1nza2odg1ndq3njhamtuymju4mjgwndm4mg', '%', '3d', '%', '3d', '&', 'el=1_x_7', '&', '_esc=publicationcoverpdf', 'http', ':', '//www.researchgate.net/profile/sukhdev-singh', '?', 'enrichid=rgreq-b679ffd100c6b00b6348f225c923452e-xxx', '&', 'enrichsource=y292zxjqywdlozmxote2ndi0mztbuzo2mta1nza2odg1ndq3njhamtuymju4mjgwndm4mg', '%', '3d', '%', '3d', '&', 'el=1_x_4', '&', '_esc=publicationcoverpdf', 'http', ':', '//www.researchgate.net/profile/sukhdev-singh', '?', 'enrichid=rgreq-b679ffd100c6b00b6348f225c923452e-xxx', '&', 'enrichsource=y292zxjqywdlozmxote2ndi0mztbuzo2mta1nza2odg1ndq3njhamtuymju4mjgwndm4mg', '%', '3d', '%', '3d', '&', 'el=1_x_5', '&', '_esc=publicationcoverpdf', 'http', ':', '//www.researchgate.net/institution/indian-institute-of-technology-patna', '?', 'enrichid=rgreq-b679ffd100c6b00b6348f225c923452e-xxx', '&', 'enrichsource=y292zxjqywdlozmxote2ndi0mztbuzo2mta1nza2odg1ndq3njhamtuymju4mjgwndm4mg', '%', '3d', '%', '3d', '&', 'el=1_x_6', '&', '_esc=publicationcoverpdf', 'http', ':', '//www.researchgate.net/profile/sukhdev-singh', '?', 'enrichid=rgreq-b679ffd100c6b00b6348f225c923452e-xxx', '&', 'enrichsource=y292zxjqywdlozmxote2ndi0mztbuzo2mta1nza2odg1ndq3njhamtuymju4mjgwndm4mg', '%', '3d', '%', '3d', '&', 'el=1_x_7', '&', '_esc=publicationcoverpdf', 'http', ':', '//www.researchgate.net/profile/kiran-khatt', '?', 'enrichid=rgreq-b679ffd100c6b00b6348f225c923452e-xxx', '&', 'enrichsource=y292zxjqywdlozmxote2ndi0mztbuzo2mta1nza2odg1ndq3njhamtuymju4mjgwndm4mg', '%', '3d', '%', '3d', '&', 'el=1_x_10', '&', '_esc=publicationcoverpdf', 'natur', 'languag', 'process', ':', 'state', 'art', ',', 'current', 'trend', 'challeng', 'diksha', 'khurana', '1', ',', 'aditya', 'koli', '1', ',', 'kiran', 'khatter', '1,2', 'sukhdev', 'singh', '1,2', '1', 'depart', 'comput', 'scienc', 'engin', 'manav', 'rachna', 'intern', 'univers', ',', 'faridabad-121004', ',', 'india', '2', 'accender', 'knowledg', 'manag', 'servic', 'pvt', '.']

 TOTAL PORTER STEM WORDS ==> 254



 ---- SNOWBALL STEMMING ----

['https', ':', '//www.researchgate.net/publication/319164243_natural_language_processing_state_of_the_art_current_trends_and_challeng', '?', 'enrichid=rgreq-b679ffd100c6b00b6348f225c923452e-xxx', '&', 'enrichsource=y292zxjqywdlozmxote2ndi0mztbuzo2mta1nza2odg1ndq3njhamtuymju4mjgwndm4mg', '%', '3d', '%', '3d', '&', 'el=1_x_2', '&', '_esc=publicationcoverpdf', 'https', ':', '//www.researchgate.net/publication/319164243_natural_language_processing_state_of_the_art_current_trends_and_challeng', '?', 'enrichid=rgreq-b679ffd100c6b00b6348f225c923452e-xxx', '&', 'enrichsource=y292zxjqywdlozmxote2ndi0mztbuzo2mta1nza2odg1ndq3njhamtuymju4mjgwndm4mg', '%', '3d', '%', '3d', '&', 'el=1_x_3', '&', '_esc=publicationcoverpdf', 'https', ':', '//www.researchgate.net/project/poisson-exponential-distribution-problems-of-estimation-and-predict', '?', 'enrichid=rgreq-b679ffd100c6b00b6348f225c923452e-xxx', '&', 'enrichsource=y292zxjqywdlozmxote2ndi0mztbuzo2mta1nza2odg1ndq3njhamtuymju4mjgwndm4mg', '%', '3d', '%', '3d', '&', 'el=1_x_9', '&', '_esc=publicationcoverpdf', 'https', ':', '//www.researchgate.net/project/lognormal-distribution-from-bayesian-view-point', '?', 'enrichid=rgreq-b679ffd100c6b00b6348f225c923452e-xxx', '&', 'enrichsource=y292zxjqywdlozmxote2ndi0mztbuzo2mta1nza2odg1ndq3njhamtuymju4mjgwndm4mg', '%', '3d', '%', '3d', '&', 'el=1_x_9', '&', '_esc=publicationcoverpdf', 'https', ':', '//www.researchgate.net/', '?', 'enrichid=rgreq-b679ffd100c6b00b6348f225c923452e-xxx', '&', 'enrichsource=y292zxjqywdlozmxote2ndi0mztbuzo2mta1nza2odg1ndq3njhamtuymju4mjgwndm4mg', '%', '3d', '%', '3d', '&', 'el=1_x_1', '&', '_esc=publicationcoverpdf', 'https', ':', '//www.researchgate.net/profile/kiran-khatt', '?', 'enrichid=rgreq-b679ffd100c6b00b6348f225c923452e-xxx', '&', 'enrichsource=y292zxjqywdlozmxote2ndi0mztbuzo2mta1nza2odg1ndq3njhamtuymju4mjgwndm4mg', '%', '3d', '%', '3d', '&', 'el=1_x_4', '&', '_esc=publicationcoverpdf', 'https', ':', '//www.researchgate.net/profile/kiran-khatt', '?', 'enrichid=rgreq-b679ffd100c6b00b6348f225c923452e-xxx', '&', 'enrichsource=y292zxjqywdlozmxote2ndi0mztbuzo2mta1nza2odg1ndq3njhamtuymju4mjgwndm4mg', '%', '3d', '%', '3d', '&', 'el=1_x_5', '&', '_esc=publicationcoverpdf', 'https', ':', '//www.researchgate.net/institution/bml-munjal-univers', '?', 'enrichid=rgreq-b679ffd100c6b00b6348f225c923452e-xxx', '&', 'enrichsource=y292zxjqywdlozmxote2ndi0mztbuzo2mta1nza2odg1ndq3njhamtuymju4mjgwndm4mg', '%', '3d', '%', '3d', '&', 'el=1_x_6', '&', '_esc=publicationcoverpdf', 'https', ':', '//www.researchgate.net/profile/kiran-khatt', '?', 'enrichid=rgreq-b679ffd100c6b00b6348f225c923452e-xxx', '&', 'enrichsource=y292zxjqywdlozmxote2ndi0mztbuzo2mta1nza2odg1ndq3njhamtuymju4mjgwndm4mg', '%', '3d', '%', '3d', '&', 'el=1_x_7', '&', '_esc=publicationcoverpdf', 'https', ':', '//www.researchgate.net/profile/sukhdev-singh', '?', 'enrichid=rgreq-b679ffd100c6b00b6348f225c923452e-xxx', '&', 'enrichsource=y292zxjqywdlozmxote2ndi0mztbuzo2mta1nza2odg1ndq3njhamtuymju4mjgwndm4mg', '%', '3d', '%', '3d', '&', 'el=1_x_4', '&', '_esc=publicationcoverpdf', 'https', ':', '//www.researchgate.net/profile/sukhdev-singh', '?', 'enrichid=rgreq-b679ffd100c6b00b6348f225c923452e-xxx', '&', 'enrichsource=y292zxjqywdlozmxote2ndi0mztbuzo2mta1nza2odg1ndq3njhamtuymju4mjgwndm4mg', '%', '3d', '%', '3d', '&', 'el=1_x_5', '&', '_esc=publicationcoverpdf', 'https', ':', '//www.researchgate.net/institution/indian-institute-of-technology-patna', '?', 'enrichid=rgreq-b679ffd100c6b00b6348f225c923452e-xxx', '&', 'enrichsource=y292zxjqywdlozmxote2ndi0mztbuzo2mta1nza2odg1ndq3njhamtuymju4mjgwndm4mg', '%', '3d', '%', '3d', '&', 'el=1_x_6', '&', '_esc=publicationcoverpdf', 'https', ':', '//www.researchgate.net/profile/sukhdev-singh', '?', 'enrichid=rgreq-b679ffd100c6b00b6348f225c923452e-xxx', '&', 'enrichsource=y292zxjqywdlozmxote2ndi0mztbuzo2mta1nza2odg1ndq3njhamtuymju4mjgwndm4mg', '%', '3d', '%', '3d', '&', 'el=1_x_7', '&', '_esc=publicationcoverpdf', 'https', ':', '//www.researchgate.net/profile/kiran-khatt', '?', 'enrichid=rgreq-b679ffd100c6b00b6348f225c923452e-xxx', '&', 'enrichsource=y292zxjqywdlozmxote2ndi0mztbuzo2mta1nza2odg1ndq3njhamtuymju4mjgwndm4mg', '%', '3d', '%', '3d', '&', 'el=1_x_10', '&', '_esc=publicationcoverpdf', 'natur', 'languag', 'process', ':', 'state', 'art', ',', 'current', 'trend', 'challeng', 'diksha', 'khurana', '1', ',', 'aditya', 'koli', '1', ',', 'kiran', 'khatter', '1,2', 'sukhdev', 'singh', '1,2', '1', 'depart', 'comput', 'scienc', 'engin', 'manav', 'rachna', 'intern', 'univers', ',', 'faridabad-121004', ',', 'india', '2', 'accender', 'knowledg', 'manag', 'servic', 'pvt', '.']

 TOTAL SNOWBALL STEM WORDS ==> 254



 ---- LEMMATIZATION ----

['http', ':', '//www.researchgate.net/publication/319164243_Natural_Language_Processing_State_of_The_Art_Current_Trends_and_Challenges', '?', 'enrichId=rgreq-b679ffd100c6b00b6348f225c923452e-XXX', '&', 'enrichSource=Y292ZXJQYWdlOzMxOTE2NDI0MztBUzo2MTA1NzA2ODg1NDQ3NjhAMTUyMjU4MjgwNDM4Mg', '%', '3D', '%', '3D', '&', 'el=1_x_2', '&', '_esc=publicationCoverPdf', 'http', ':', '//www.researchgate.net/publication/319164243_Natural_Language_Processing_State_of_The_Art_Current_Trends_and_Challenges', '?', 'enrichId=rgreq-b679ffd100c6b00b6348f225c923452e-XXX', '&', 'enrichSource=Y292ZXJQYWdlOzMxOTE2NDI0MztBUzo2MTA1NzA2ODg1NDQ3NjhAMTUyMjU4MjgwNDM4Mg', '%', '3D', '%', '3D', '&', 'el=1_x_3', '&', '_esc=publicationCoverPdf', 'http', ':', '//www.researchgate.net/project/Poisson-Exponential-Distribution-problems-of-estimation-and-prediction', '?', 'enrichId=rgreq-b679ffd100c6b00b6348f225c923452e-XXX', '&', 'enrichSource=Y292ZXJQYWdlOzMxOTE2NDI0MztBUzo2MTA1NzA2ODg1NDQ3NjhAMTUyMjU4MjgwNDM4Mg', '%', '3D', '%', '3D', '&', 'el=1_x_9', '&', '_esc=publicationCoverPdf', 'http', ':', '//www.researchgate.net/project/Lognormal-distribution-from-Bayesian-view-point', '?', 'enrichId=rgreq-b679ffd100c6b00b6348f225c923452e-XXX', '&', 'enrichSource=Y292ZXJQYWdlOzMxOTE2NDI0MztBUzo2MTA1NzA2ODg1NDQ3NjhAMTUyMjU4MjgwNDM4Mg', '%', '3D', '%', '3D', '&', 'el=1_x_9', '&', '_esc=publicationCoverPdf', 'http', ':', '//www.researchgate.net/', '?', 'enrichId=rgreq-b679ffd100c6b00b6348f225c923452e-XXX', '&', 'enrichSource=Y292ZXJQYWdlOzMxOTE2NDI0MztBUzo2MTA1NzA2ODg1NDQ3NjhAMTUyMjU4MjgwNDM4Mg', '%', '3D', '%', '3D', '&', 'el=1_x_1', '&', '_esc=publicationCoverPdf', 'http', ':', '//www.researchgate.net/profile/Kiran-Khatter', '?', 'enrichId=rgreq-b679ffd100c6b00b6348f225c923452e-XXX', '&', 'enrichSource=Y292ZXJQYWdlOzMxOTE2NDI0MztBUzo2MTA1NzA2ODg1NDQ3NjhAMTUyMjU4MjgwNDM4Mg', '%', '3D', '%', '3D', '&', 'el=1_x_4', '&', '_esc=publicationCoverPdf', 'http', ':', '//www.researchgate.net/profile/Kiran-Khatter', '?', 'enrichId=rgreq-b679ffd100c6b00b6348f225c923452e-XXX', '&', 'enrichSource=Y292ZXJQYWdlOzMxOTE2NDI0MztBUzo2MTA1NzA2ODg1NDQ3NjhAMTUyMjU4MjgwNDM4Mg', '%', '3D', '%', '3D', '&', 'el=1_x_5', '&', '_esc=publicationCoverPdf', 'http', ':', '//www.researchgate.net/institution/BML-MUNJAL-UNIVERSITY', '?', 'enrichId=rgreq-b679ffd100c6b00b6348f225c923452e-XXX', '&', 'enrichSource=Y292ZXJQYWdlOzMxOTE2NDI0MztBUzo2MTA1NzA2ODg1NDQ3NjhAMTUyMjU4MjgwNDM4Mg', '%', '3D', '%', '3D', '&', 'el=1_x_6', '&', '_esc=publicationCoverPdf', 'http', ':', '//www.researchgate.net/profile/Kiran-Khatter', '?', 'enrichId=rgreq-b679ffd100c6b00b6348f225c923452e-XXX', '&', 'enrichSource=Y292ZXJQYWdlOzMxOTE2NDI0MztBUzo2MTA1NzA2ODg1NDQ3NjhAMTUyMjU4MjgwNDM4Mg', '%', '3D', '%', '3D', '&', 'el=1_x_7', '&', '_esc=publicationCoverPdf', 'http', ':', '//www.researchgate.net/profile/Sukhdev-Singh', '?', 'enrichId=rgreq-b679ffd100c6b00b6348f225c923452e-XXX', '&', 'enrichSource=Y292ZXJQYWdlOzMxOTE2NDI0MztBUzo2MTA1NzA2ODg1NDQ3NjhAMTUyMjU4MjgwNDM4Mg', '%', '3D', '%', '3D', '&', 'el=1_x_4', '&', '_esc=publicationCoverPdf', 'http', ':', '//www.researchgate.net/profile/Sukhdev-Singh', '?', 'enrichId=rgreq-b679ffd100c6b00b6348f225c923452e-XXX', '&', 'enrichSource=Y292ZXJQYWdlOzMxOTE2NDI0MztBUzo2MTA1NzA2ODg1NDQ3NjhAMTUyMjU4MjgwNDM4Mg', '%', '3D', '%', '3D', '&', 'el=1_x_5', '&', '_esc=publicationCoverPdf', 'http', ':', '//www.researchgate.net/institution/Indian-Institute-of-Technology-Patna', '?', 'enrichId=rgreq-b679ffd100c6b00b6348f225c923452e-XXX', '&', 'enrichSource=Y292ZXJQYWdlOzMxOTE2NDI0MztBUzo2MTA1NzA2ODg1NDQ3NjhAMTUyMjU4MjgwNDM4Mg', '%', '3D', '%', '3D', '&', 'el=1_x_6', '&', '_esc=publicationCoverPdf', 'http', ':', '//www.researchgate.net/profile/Sukhdev-Singh', '?', 'enrichId=rgreq-b679ffd100c6b00b6348f225c923452e-XXX', '&', 'enrichSource=Y292ZXJQYWdlOzMxOTE2NDI0MztBUzo2MTA1NzA2ODg1NDQ3NjhAMTUyMjU4MjgwNDM4Mg', '%', '3D', '%', '3D', '&', 'el=1_x_7', '&', '_esc=publicationCoverPdf', 'http', ':', '//www.researchgate.net/profile/Kiran-Khatter', '?', 'enrichId=rgreq-b679ffd100c6b00b6348f225c923452e-XXX', '&', 'enrichSource=Y292ZXJQYWdlOzMxOTE2NDI0MztBUzo2MTA1NzA2ODg1NDQ3NjhAMTUyMjU4MjgwNDM4Mg', '%', '3D', '%', '3D', '&', 'el=1_x_10', '&', '_esc=publicationCoverPdf', 'Natural', 'Language', 'Processing', ':', 'State', 'Art', ',', 'Current', 'Trends', 'Challenges', 'Diksha', 'Khurana', '1', ',', 'Aditya', 'Koli', '1', ',', 'Kiran', 'Khatter', '1,2', 'Sukhdev', 'Singh', '1,2', '1', 'Department', 'Computer', 'Science', 'Engineering', 'Manav', 'Rachna', 'International', 'University', ',', 'Faridabad-121004', ',', 'India', '2', 'Accendere', 'Knowledge', 'Management', 'Services', 'Pvt', '.']

 TOTAL LEMMATIZE WORDS ==> 254

************************************************************************************************************************

4 --> Ltd., India    Abstract   Natural language processing (NLP) has recently gained much attention for representing and  analysing human language computationally. 


 ---- TOKENS ----

 ['Ltd.', ',', 'India', 'Abstract', 'Natural', 'language', 'processing', '(', 'NLP', ')', 'has', 'recently', 'gained', 'much', 'attention', 'for', 'representing', 'and', 'analysing', 'human', 'language', 'computationally', '.'] 

 TOTAL TOKENS ==> 23

 ---- POST ----

 [('Ltd.', 'NNP'), (',', ','), ('India', 'NNP'), ('Abstract', 'NNP'), ('Natural', 'NNP'), ('language', 'NN'), ('processing', 'NN'), ('(', '('), ('NLP', 'NNP'), (')', ')'), ('has', 'VBZ'), ('recently', 'RB'), ('gained', 'VBN'), ('much', 'JJ'), ('attention', 'NN'), ('for', 'IN'), ('representing', 'VBG'), ('and', 'CC'), ('analysing', 'VBG'), ('human', 'JJ'), ('language', 'NN'), ('computationally', 'RB'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Ltd.', ',', 'India', 'Abstract', 'Natural', 'language', 'processing', '(', 'NLP', ')', 'recently', 'gained', 'much', 'attention', 'representing', 'analysing', 'human', 'language', 'computationally', '.']

 TOTAL FILTERED TOKENS ==>  20

 ---- POST FOR FILTERED TOKENS ----

 [('Ltd.', 'NNP'), (',', ','), ('India', 'NNP'), ('Abstract', 'NNP'), ('Natural', 'NNP'), ('language', 'NN'), ('processing', 'NN'), ('(', '('), ('NLP', 'NNP'), (')', ')'), ('recently', 'RB'), ('gained', 'VBD'), ('much', 'JJ'), ('attention', 'NN'), ('representing', 'VBG'), ('analysing', 'VBG'), ('human', 'JJ'), ('language', 'NN'), ('computationally', 'RB'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Ltd. ,', ', India', 'India Abstract', 'Abstract Natural', 'Natural language', 'language processing', 'processing (', '( NLP', 'NLP )', ') recently', 'recently gained', 'gained much', 'much attention', 'attention representing', 'representing analysing', 'analysing human', 'human language', 'language computationally', 'computationally .'] 

 TOTAL BIGRAMS --> 19 



 ---- TRI-GRAMS ---- 

 ['Ltd. , India', ', India Abstract', 'India Abstract Natural', 'Abstract Natural language', 'Natural language processing', 'language processing (', 'processing ( NLP', '( NLP )', 'NLP ) recently', ') recently gained', 'recently gained much', 'gained much attention', 'much attention representing', 'attention representing analysing', 'representing analysing human', 'analysing human language', 'human language computationally', 'language computationally .'] 

 TOTAL TRIGRAMS --> 18 



 ---- NOUN PHRASES ---- 

 ['language', 'processing', 'much attention', 'human language'] 

 TOTAL NOUN PHRASES --> 4 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> ['India']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['ltd.', ',', 'india', 'abstract', 'natur', 'languag', 'process', '(', 'nlp', ')', 'recent', 'gain', 'much', 'attent', 'repres', 'analys', 'human', 'languag', 'comput', '.']

 TOTAL PORTER STEM WORDS ==> 20



 ---- SNOWBALL STEMMING ----

['ltd.', ',', 'india', 'abstract', 'natur', 'languag', 'process', '(', 'nlp', ')', 'recent', 'gain', 'much', 'attent', 'repres', 'analys', 'human', 'languag', 'comput', '.']

 TOTAL SNOWBALL STEM WORDS ==> 20



 ---- LEMMATIZATION ----

['Ltd.', ',', 'India', 'Abstract', 'Natural', 'language', 'processing', '(', 'NLP', ')', 'recently', 'gained', 'much', 'attention', 'representing', 'analysing', 'human', 'language', 'computationally', '.']

 TOTAL LEMMATIZE WORDS ==> 20

************************************************************************************************************************

5 --> It has spread its applications in various fields  such as machine translation, email spam detection, information extraction, summarization,  medical, and question answering etc. 


 ---- TOKENS ----

 ['It', 'has', 'spread', 'its', 'applications', 'in', 'various', 'fields', 'such', 'as', 'machine', 'translation', ',', 'email', 'spam', 'detection', ',', 'information', 'extraction', ',', 'summarization', ',', 'medical', ',', 'and', 'question', 'answering', 'etc', '.'] 

 TOTAL TOKENS ==> 29

 ---- POST ----

 [('It', 'PRP'), ('has', 'VBZ'), ('spread', 'VBN'), ('its', 'PRP$'), ('applications', 'NNS'), ('in', 'IN'), ('various', 'JJ'), ('fields', 'NNS'), ('such', 'JJ'), ('as', 'IN'), ('machine', 'NN'), ('translation', 'NN'), (',', ','), ('email', 'VBP'), ('spam', 'JJ'), ('detection', 'NN'), (',', ','), ('information', 'NN'), ('extraction', 'NN'), (',', ','), ('summarization', 'NN'), (',', ','), ('medical', 'JJ'), (',', ','), ('and', 'CC'), ('question', 'NN'), ('answering', 'VBG'), ('etc', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['spread', 'applications', 'various', 'fields', 'machine', 'translation', ',', 'email', 'spam', 'detection', ',', 'information', 'extraction', ',', 'summarization', ',', 'medical', ',', 'question', 'answering', 'etc', '.']

 TOTAL FILTERED TOKENS ==>  22

 ---- POST FOR FILTERED TOKENS ----

 [('spread', 'NN'), ('applications', 'NNS'), ('various', 'JJ'), ('fields', 'NNS'), ('machine', 'NN'), ('translation', 'NN'), (',', ','), ('email', 'VBP'), ('spam', 'JJ'), ('detection', 'NN'), (',', ','), ('information', 'NN'), ('extraction', 'NN'), (',', ','), ('summarization', 'NN'), (',', ','), ('medical', 'JJ'), (',', ','), ('question', 'NN'), ('answering', 'VBG'), ('etc', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['spread applications', 'applications various', 'various fields', 'fields machine', 'machine translation', 'translation ,', ', email', 'email spam', 'spam detection', 'detection ,', ', information', 'information extraction', 'extraction ,', ', summarization', 'summarization ,', ', medical', 'medical ,', ', question', 'question answering', 'answering etc', 'etc .'] 

 TOTAL BIGRAMS --> 21 



 ---- TRI-GRAMS ---- 

 ['spread applications various', 'applications various fields', 'various fields machine', 'fields machine translation', 'machine translation ,', 'translation , email', ', email spam', 'email spam detection', 'spam detection ,', 'detection , information', ', information extraction', 'information extraction ,', 'extraction , summarization', ', summarization ,', 'summarization , medical', ', medical ,', 'medical , question', ', question answering', 'question answering etc', 'answering etc .'] 

 TOTAL TRIGRAMS --> 20 



 ---- NOUN PHRASES ---- 

 ['spread', 'machine', 'translation', 'spam detection', 'information', 'extraction', 'summarization', 'question', 'etc'] 

 TOTAL NOUN PHRASES --> 9 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['spread', 'applic', 'variou', 'field', 'machin', 'translat', ',', 'email', 'spam', 'detect', ',', 'inform', 'extract', ',', 'summar', ',', 'medic', ',', 'question', 'answer', 'etc', '.']

 TOTAL PORTER STEM WORDS ==> 22



 ---- SNOWBALL STEMMING ----

['spread', 'applic', 'various', 'field', 'machin', 'translat', ',', 'email', 'spam', 'detect', ',', 'inform', 'extract', ',', 'summar', ',', 'medic', ',', 'question', 'answer', 'etc', '.']

 TOTAL SNOWBALL STEM WORDS ==> 22



 ---- LEMMATIZATION ----

['spread', 'application', 'various', 'field', 'machine', 'translation', ',', 'email', 'spam', 'detection', ',', 'information', 'extraction', ',', 'summarization', ',', 'medical', ',', 'question', 'answering', 'etc', '.']

 TOTAL LEMMATIZE WORDS ==> 22

************************************************************************************************************************

6 --> The paper distinguishes four phases by discussing  different levels of NLP and components of Natural Language Generation (NLG) followed by  presenting the history and evolution of NLP, state of the art presenting the various  applications of NLP and current trends and challenges. 


 ---- TOKENS ----

 ['The', 'paper', 'distinguishes', 'four', 'phases', 'by', 'discussing', 'different', 'levels', 'of', 'NLP', 'and', 'components', 'of', 'Natural', 'Language', 'Generation', '(', 'NLG', ')', 'followed', 'by', 'presenting', 'the', 'history', 'and', 'evolution', 'of', 'NLP', ',', 'state', 'of', 'the', 'art', 'presenting', 'the', 'various', 'applications', 'of', 'NLP', 'and', 'current', 'trends', 'and', 'challenges', '.'] 

 TOTAL TOKENS ==> 46

 ---- POST ----

 [('The', 'DT'), ('paper', 'NN'), ('distinguishes', 'VBZ'), ('four', 'CD'), ('phases', 'NNS'), ('by', 'IN'), ('discussing', 'VBG'), ('different', 'JJ'), ('levels', 'NNS'), ('of', 'IN'), ('NLP', 'NNP'), ('and', 'CC'), ('components', 'NNS'), ('of', 'IN'), ('Natural', 'NNP'), ('Language', 'NNP'), ('Generation', 'NNP'), ('(', '('), ('NLG', 'NNP'), (')', ')'), ('followed', 'VBN'), ('by', 'IN'), ('presenting', 'VBG'), ('the', 'DT'), ('history', 'NN'), ('and', 'CC'), ('evolution', 'NN'), ('of', 'IN'), ('NLP', 'NNP'), (',', ','), ('state', 'NN'), ('of', 'IN'), ('the', 'DT'), ('art', 'NN'), ('presenting', 'VBG'), ('the', 'DT'), ('various', 'JJ'), ('applications', 'NNS'), ('of', 'IN'), ('NLP', 'NNP'), ('and', 'CC'), ('current', 'JJ'), ('trends', 'NNS'), ('and', 'CC'), ('challenges', 'NNS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['paper', 'distinguishes', 'four', 'phases', 'discussing', 'different', 'levels', 'NLP', 'components', 'Natural', 'Language', 'Generation', '(', 'NLG', ')', 'followed', 'presenting', 'history', 'evolution', 'NLP', ',', 'state', 'art', 'presenting', 'various', 'applications', 'NLP', 'current', 'trends', 'challenges', '.']

 TOTAL FILTERED TOKENS ==>  31

 ---- POST FOR FILTERED TOKENS ----

 [('paper', 'NN'), ('distinguishes', 'VBZ'), ('four', 'CD'), ('phases', 'NNS'), ('discussing', 'VBG'), ('different', 'JJ'), ('levels', 'NNS'), ('NLP', 'NNP'), ('components', 'NNS'), ('Natural', 'NNP'), ('Language', 'NNP'), ('Generation', 'NNP'), ('(', '('), ('NLG', 'NNP'), (')', ')'), ('followed', 'VBD'), ('presenting', 'VBG'), ('history', 'NN'), ('evolution', 'NN'), ('NLP', 'NNP'), (',', ','), ('state', 'NN'), ('art', 'NN'), ('presenting', 'VBG'), ('various', 'JJ'), ('applications', 'NNS'), ('NLP', 'NNP'), ('current', 'JJ'), ('trends', 'NNS'), ('challenges', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['paper distinguishes', 'distinguishes four', 'four phases', 'phases discussing', 'discussing different', 'different levels', 'levels NLP', 'NLP components', 'components Natural', 'Natural Language', 'Language Generation', 'Generation (', '( NLG', 'NLG )', ') followed', 'followed presenting', 'presenting history', 'history evolution', 'evolution NLP', 'NLP ,', ', state', 'state art', 'art presenting', 'presenting various', 'various applications', 'applications NLP', 'NLP current', 'current trends', 'trends challenges', 'challenges .'] 

 TOTAL BIGRAMS --> 30 



 ---- TRI-GRAMS ---- 

 ['paper distinguishes four', 'distinguishes four phases', 'four phases discussing', 'phases discussing different', 'discussing different levels', 'different levels NLP', 'levels NLP components', 'NLP components Natural', 'components Natural Language', 'Natural Language Generation', 'Language Generation (', 'Generation ( NLG', '( NLG )', 'NLG ) followed', ') followed presenting', 'followed presenting history', 'presenting history evolution', 'history evolution NLP', 'evolution NLP ,', 'NLP , state', ', state art', 'state art presenting', 'art presenting various', 'presenting various applications', 'various applications NLP', 'applications NLP current', 'NLP current trends', 'current trends challenges', 'trends challenges .'] 

 TOTAL TRIGRAMS --> 29 



 ---- NOUN PHRASES ---- 

 ['paper', 'history', 'evolution', 'state', 'art'] 

 TOTAL NOUN PHRASES --> 5 



 ---- NER ----

 
 ORGANIZATION ---> ['NLP', 'Natural Language Generation', 'NLP', 'NLP']
 TOTAL ORGANIZATION ENTITY --> 4 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['paper', 'distinguish', 'four', 'phase', 'discuss', 'differ', 'level', 'nlp', 'compon', 'natur', 'languag', 'gener', '(', 'nlg', ')', 'follow', 'present', 'histori', 'evolut', 'nlp', ',', 'state', 'art', 'present', 'variou', 'applic', 'nlp', 'current', 'trend', 'challeng', '.']

 TOTAL PORTER STEM WORDS ==> 31



 ---- SNOWBALL STEMMING ----

['paper', 'distinguish', 'four', 'phase', 'discuss', 'differ', 'level', 'nlp', 'compon', 'natur', 'languag', 'generat', '(', 'nlg', ')', 'follow', 'present', 'histori', 'evolut', 'nlp', ',', 'state', 'art', 'present', 'various', 'applic', 'nlp', 'current', 'trend', 'challeng', '.']

 TOTAL SNOWBALL STEM WORDS ==> 31



 ---- LEMMATIZATION ----

['paper', 'distinguishes', 'four', 'phase', 'discussing', 'different', 'level', 'NLP', 'component', 'Natural', 'Language', 'Generation', '(', 'NLG', ')', 'followed', 'presenting', 'history', 'evolution', 'NLP', ',', 'state', 'art', 'presenting', 'various', 'application', 'NLP', 'current', 'trend', 'challenge', '.']

 TOTAL LEMMATIZE WORDS ==> 31

************************************************************************************************************************

7 --> 1. 


 ---- TOKENS ----

 ['1', '.'] 

 TOTAL TOKENS ==> 2

 ---- POST ----

 [('1', 'CD'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['1', '.']

 TOTAL FILTERED TOKENS ==>  2

 ---- POST FOR FILTERED TOKENS ----

 [('1', 'CD'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['1 .'] 

 TOTAL BIGRAMS --> 1 



 ---- TRI-GRAMS ---- 

 [] 

 TOTAL TRIGRAMS --> 0 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['1', '.']

 TOTAL PORTER STEM WORDS ==> 2



 ---- SNOWBALL STEMMING ----

['1', '.']

 TOTAL SNOWBALL STEM WORDS ==> 2



 ---- LEMMATIZATION ----

['1', '.']

 TOTAL LEMMATIZE WORDS ==> 2

************************************************************************************************************************

8 --> Introduction  Natural Language Processing (NLP) is a tract of Artificial Intelligence and Linguistics,  devoted to make computers understand the statements or words written in human languages. 


 ---- TOKENS ----

 ['Introduction', 'Natural', 'Language', 'Processing', '(', 'NLP', ')', 'is', 'a', 'tract', 'of', 'Artificial', 'Intelligence', 'and', 'Linguistics', ',', 'devoted', 'to', 'make', 'computers', 'understand', 'the', 'statements', 'or', 'words', 'written', 'in', 'human', 'languages', '.'] 

 TOTAL TOKENS ==> 30

 ---- POST ----

 [('Introduction', 'NNP'), ('Natural', 'NNP'), ('Language', 'NNP'), ('Processing', 'NNP'), ('(', '('), ('NLP', 'NNP'), (')', ')'), ('is', 'VBZ'), ('a', 'DT'), ('tract', 'NN'), ('of', 'IN'), ('Artificial', 'NNP'), ('Intelligence', 'NNP'), ('and', 'CC'), ('Linguistics', 'NNP'), (',', ','), ('devoted', 'VBD'), ('to', 'TO'), ('make', 'VB'), ('computers', 'NNS'), ('understand', 'VB'), ('the', 'DT'), ('statements', 'NNS'), ('or', 'CC'), ('words', 'NNS'), ('written', 'VBN'), ('in', 'IN'), ('human', 'JJ'), ('languages', 'NNS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Introduction', 'Natural', 'Language', 'Processing', '(', 'NLP', ')', 'tract', 'Artificial', 'Intelligence', 'Linguistics', ',', 'devoted', 'make', 'computers', 'understand', 'statements', 'words', 'written', 'human', 'languages', '.']

 TOTAL FILTERED TOKENS ==>  22

 ---- POST FOR FILTERED TOKENS ----

 [('Introduction', 'NNP'), ('Natural', 'NNP'), ('Language', 'NNP'), ('Processing', 'NNP'), ('(', '('), ('NLP', 'NNP'), (')', ')'), ('tract', 'VBP'), ('Artificial', 'JJ'), ('Intelligence', 'NNP'), ('Linguistics', 'NNP'), (',', ','), ('devoted', 'VBD'), ('make', 'JJ'), ('computers', 'NNS'), ('understand', 'JJ'), ('statements', 'NNS'), ('words', 'NNS'), ('written', 'VBN'), ('human', 'JJ'), ('languages', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Introduction Natural', 'Natural Language', 'Language Processing', 'Processing (', '( NLP', 'NLP )', ') tract', 'tract Artificial', 'Artificial Intelligence', 'Intelligence Linguistics', 'Linguistics ,', ', devoted', 'devoted make', 'make computers', 'computers understand', 'understand statements', 'statements words', 'words written', 'written human', 'human languages', 'languages .'] 

 TOTAL BIGRAMS --> 21 



 ---- TRI-GRAMS ---- 

 ['Introduction Natural Language', 'Natural Language Processing', 'Language Processing (', 'Processing ( NLP', '( NLP )', 'NLP ) tract', ') tract Artificial', 'tract Artificial Intelligence', 'Artificial Intelligence Linguistics', 'Intelligence Linguistics ,', 'Linguistics , devoted', ', devoted make', 'devoted make computers', 'make computers understand', 'computers understand statements', 'understand statements words', 'statements words written', 'words written human', 'written human languages', 'human languages .'] 

 TOTAL TRIGRAMS --> 20 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> ['Artificial Intelligence Linguistics']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['introduct', 'natur', 'languag', 'process', '(', 'nlp', ')', 'tract', 'artifici', 'intellig', 'linguist', ',', 'devot', 'make', 'comput', 'understand', 'statement', 'word', 'written', 'human', 'languag', '.']

 TOTAL PORTER STEM WORDS ==> 22



 ---- SNOWBALL STEMMING ----

['introduct', 'natur', 'languag', 'process', '(', 'nlp', ')', 'tract', 'artifici', 'intellig', 'linguist', ',', 'devot', 'make', 'comput', 'understand', 'statement', 'word', 'written', 'human', 'languag', '.']

 TOTAL SNOWBALL STEM WORDS ==> 22



 ---- LEMMATIZATION ----

['Introduction', 'Natural', 'Language', 'Processing', '(', 'NLP', ')', 'tract', 'Artificial', 'Intelligence', 'Linguistics', ',', 'devoted', 'make', 'computer', 'understand', 'statement', 'word', 'written', 'human', 'language', '.']

 TOTAL LEMMATIZE WORDS ==> 22

************************************************************************************************************************

9 --> Natural language processing came into existence to ease the user’s work and to satisfy the  wish to communicate with the computer in natural language. 


 ---- TOKENS ----

 ['Natural', 'language', 'processing', 'came', 'into', 'existence', 'to', 'ease', 'the', 'user', '’', 's', 'work', 'and', 'to', 'satisfy', 'the', 'wish', 'to', 'communicate', 'with', 'the', 'computer', 'in', 'natural', 'language', '.'] 

 TOTAL TOKENS ==> 27

 ---- POST ----

 [('Natural', 'JJ'), ('language', 'NN'), ('processing', 'NN'), ('came', 'VBD'), ('into', 'IN'), ('existence', 'NN'), ('to', 'TO'), ('ease', 'VB'), ('the', 'DT'), ('user', 'NN'), ('’', 'NNP'), ('s', 'NN'), ('work', 'NN'), ('and', 'CC'), ('to', 'TO'), ('satisfy', 'VB'), ('the', 'DT'), ('wish', 'NN'), ('to', 'TO'), ('communicate', 'VB'), ('with', 'IN'), ('the', 'DT'), ('computer', 'NN'), ('in', 'IN'), ('natural', 'JJ'), ('language', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Natural', 'language', 'processing', 'came', 'existence', 'ease', 'user', '’', 'work', 'satisfy', 'wish', 'communicate', 'computer', 'natural', 'language', '.']

 TOTAL FILTERED TOKENS ==>  16

 ---- POST FOR FILTERED TOKENS ----

 [('Natural', 'JJ'), ('language', 'NN'), ('processing', 'NN'), ('came', 'VBD'), ('existence', 'RB'), ('ease', 'JJ'), ('user', 'NN'), ('’', 'NNP'), ('work', 'NN'), ('satisfy', 'NN'), ('wish', 'JJ'), ('communicate', 'NN'), ('computer', 'NN'), ('natural', 'JJ'), ('language', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Natural language', 'language processing', 'processing came', 'came existence', 'existence ease', 'ease user', 'user ’', '’ work', 'work satisfy', 'satisfy wish', 'wish communicate', 'communicate computer', 'computer natural', 'natural language', 'language .'] 

 TOTAL BIGRAMS --> 15 



 ---- TRI-GRAMS ---- 

 ['Natural language processing', 'language processing came', 'processing came existence', 'came existence ease', 'existence ease user', 'ease user ’', 'user ’ work', '’ work satisfy', 'work satisfy wish', 'satisfy wish communicate', 'wish communicate computer', 'communicate computer natural', 'computer natural language', 'natural language .'] 

 TOTAL TRIGRAMS --> 14 



 ---- NOUN PHRASES ---- 

 ['Natural language', 'processing', 'ease user', 'work', 'satisfy', 'wish communicate', 'computer', 'natural language'] 

 TOTAL NOUN PHRASES --> 8 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['natur', 'languag', 'process', 'came', 'exist', 'eas', 'user', '’', 'work', 'satisfi', 'wish', 'commun', 'comput', 'natur', 'languag', '.']

 TOTAL PORTER STEM WORDS ==> 16



 ---- SNOWBALL STEMMING ----

['natur', 'languag', 'process', 'came', 'exist', 'eas', 'user', '’', 'work', 'satisfi', 'wish', 'communic', 'comput', 'natur', 'languag', '.']

 TOTAL SNOWBALL STEM WORDS ==> 16



 ---- LEMMATIZATION ----

['Natural', 'language', 'processing', 'came', 'existence', 'ease', 'user', '’', 'work', 'satisfy', 'wish', 'communicate', 'computer', 'natural', 'language', '.']

 TOTAL LEMMATIZE WORDS ==> 16

************************************************************************************************************************

10 --> Since all the users may not be  well-versed in machine specific language, NLP caters those users who do not have enough  time to learn new languages or get perfection in it. 


 ---- TOKENS ----

 ['Since', 'all', 'the', 'users', 'may', 'not', 'be', 'well-versed', 'in', 'machine', 'specific', 'language', ',', 'NLP', 'caters', 'those', 'users', 'who', 'do', 'not', 'have', 'enough', 'time', 'to', 'learn', 'new', 'languages', 'or', 'get', 'perfection', 'in', 'it', '.'] 

 TOTAL TOKENS ==> 33

 ---- POST ----

 [('Since', 'IN'), ('all', 'PDT'), ('the', 'DT'), ('users', 'NNS'), ('may', 'MD'), ('not', 'RB'), ('be', 'VB'), ('well-versed', 'JJ'), ('in', 'IN'), ('machine', 'NN'), ('specific', 'JJ'), ('language', 'NN'), (',', ','), ('NLP', 'NNP'), ('caters', 'NNS'), ('those', 'DT'), ('users', 'NNS'), ('who', 'WP'), ('do', 'VBP'), ('not', 'RB'), ('have', 'VB'), ('enough', 'JJ'), ('time', 'NN'), ('to', 'TO'), ('learn', 'VB'), ('new', 'JJ'), ('languages', 'NNS'), ('or', 'CC'), ('get', 'VB'), ('perfection', 'NN'), ('in', 'IN'), ('it', 'PRP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Since', 'users', 'may', 'well-versed', 'machine', 'specific', 'language', ',', 'NLP', 'caters', 'users', 'enough', 'time', 'learn', 'new', 'languages', 'get', 'perfection', '.']

 TOTAL FILTERED TOKENS ==>  19

 ---- POST FOR FILTERED TOKENS ----

 [('Since', 'IN'), ('users', 'NNS'), ('may', 'MD'), ('well-versed', 'JJ'), ('machine', 'NN'), ('specific', 'JJ'), ('language', 'NN'), (',', ','), ('NLP', 'NNP'), ('caters', 'VBZ'), ('users', 'NNS'), ('enough', 'JJ'), ('time', 'NN'), ('learn', 'VB'), ('new', 'JJ'), ('languages', 'NNS'), ('get', 'VBP'), ('perfection', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Since users', 'users may', 'may well-versed', 'well-versed machine', 'machine specific', 'specific language', 'language ,', ', NLP', 'NLP caters', 'caters users', 'users enough', 'enough time', 'time learn', 'learn new', 'new languages', 'languages get', 'get perfection', 'perfection .'] 

 TOTAL BIGRAMS --> 18 



 ---- TRI-GRAMS ---- 

 ['Since users may', 'users may well-versed', 'may well-versed machine', 'well-versed machine specific', 'machine specific language', 'specific language ,', 'language , NLP', ', NLP caters', 'NLP caters users', 'caters users enough', 'users enough time', 'enough time learn', 'time learn new', 'learn new languages', 'new languages get', 'languages get perfection', 'get perfection .'] 

 TOTAL TRIGRAMS --> 17 



 ---- NOUN PHRASES ---- 

 ['well-versed machine', 'specific language', 'enough time', 'perfection'] 

 TOTAL NOUN PHRASES --> 4 



 ---- NER ----

 
 ORGANIZATION ---> ['NLP']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['sinc', 'user', 'may', 'well-vers', 'machin', 'specif', 'languag', ',', 'nlp', 'cater', 'user', 'enough', 'time', 'learn', 'new', 'languag', 'get', 'perfect', '.']

 TOTAL PORTER STEM WORDS ==> 19



 ---- SNOWBALL STEMMING ----

['sinc', 'user', 'may', 'well-vers', 'machin', 'specif', 'languag', ',', 'nlp', 'cater', 'user', 'enough', 'time', 'learn', 'new', 'languag', 'get', 'perfect', '.']

 TOTAL SNOWBALL STEM WORDS ==> 19



 ---- LEMMATIZATION ----

['Since', 'user', 'may', 'well-versed', 'machine', 'specific', 'language', ',', 'NLP', 'caters', 'user', 'enough', 'time', 'learn', 'new', 'language', 'get', 'perfection', '.']

 TOTAL LEMMATIZE WORDS ==> 19

************************************************************************************************************************

11 --> A language can be defined as a set of rules or set of symbol. 


 ---- TOKENS ----

 ['A', 'language', 'can', 'be', 'defined', 'as', 'a', 'set', 'of', 'rules', 'or', 'set', 'of', 'symbol', '.'] 

 TOTAL TOKENS ==> 15

 ---- POST ----

 [('A', 'DT'), ('language', 'NN'), ('can', 'MD'), ('be', 'VB'), ('defined', 'VBN'), ('as', 'IN'), ('a', 'DT'), ('set', 'NN'), ('of', 'IN'), ('rules', 'NNS'), ('or', 'CC'), ('set', 'NN'), ('of', 'IN'), ('symbol', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['language', 'defined', 'set', 'rules', 'set', 'symbol', '.']

 TOTAL FILTERED TOKENS ==>  7

 ---- POST FOR FILTERED TOKENS ----

 [('language', 'NN'), ('defined', 'VBD'), ('set', 'VBN'), ('rules', 'NNS'), ('set', 'VBD'), ('symbol', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['language defined', 'defined set', 'set rules', 'rules set', 'set symbol', 'symbol .'] 

 TOTAL BIGRAMS --> 6 



 ---- TRI-GRAMS ---- 

 ['language defined set', 'defined set rules', 'set rules set', 'rules set symbol', 'set symbol .'] 

 TOTAL TRIGRAMS --> 5 



 ---- NOUN PHRASES ---- 

 ['language', 'symbol'] 

 TOTAL NOUN PHRASES --> 2 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['languag', 'defin', 'set', 'rule', 'set', 'symbol', '.']

 TOTAL PORTER STEM WORDS ==> 7



 ---- SNOWBALL STEMMING ----

['languag', 'defin', 'set', 'rule', 'set', 'symbol', '.']

 TOTAL SNOWBALL STEM WORDS ==> 7



 ---- LEMMATIZATION ----

['language', 'defined', 'set', 'rule', 'set', 'symbol', '.']

 TOTAL LEMMATIZE WORDS ==> 7

************************************************************************************************************************

12 --> Symbol are combined and used  for conveying information or broadcasting the information. 


 ---- TOKENS ----

 ['Symbol', 'are', 'combined', 'and', 'used', 'for', 'conveying', 'information', 'or', 'broadcasting', 'the', 'information', '.'] 

 TOTAL TOKENS ==> 13

 ---- POST ----

 [('Symbol', 'NN'), ('are', 'VBP'), ('combined', 'VBN'), ('and', 'CC'), ('used', 'VBN'), ('for', 'IN'), ('conveying', 'VBG'), ('information', 'NN'), ('or', 'CC'), ('broadcasting', 'VBG'), ('the', 'DT'), ('information', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Symbol', 'combined', 'used', 'conveying', 'information', 'broadcasting', 'information', '.']

 TOTAL FILTERED TOKENS ==>  8

 ---- POST FOR FILTERED TOKENS ----

 [('Symbol', 'NN'), ('combined', 'VBD'), ('used', 'JJ'), ('conveying', 'VBG'), ('information', 'NN'), ('broadcasting', 'VBG'), ('information', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Symbol combined', 'combined used', 'used conveying', 'conveying information', 'information broadcasting', 'broadcasting information', 'information .'] 

 TOTAL BIGRAMS --> 7 



 ---- TRI-GRAMS ---- 

 ['Symbol combined used', 'combined used conveying', 'used conveying information', 'conveying information broadcasting', 'information broadcasting information', 'broadcasting information .'] 

 TOTAL TRIGRAMS --> 6 



 ---- NOUN PHRASES ---- 

 ['Symbol', 'information', 'information'] 

 TOTAL NOUN PHRASES --> 3 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> ['Symbol']
 TOTAL PERSON ENTITY --> 1 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['symbol', 'combin', 'use', 'convey', 'inform', 'broadcast', 'inform', '.']

 TOTAL PORTER STEM WORDS ==> 8



 ---- SNOWBALL STEMMING ----

['symbol', 'combin', 'use', 'convey', 'inform', 'broadcast', 'inform', '.']

 TOTAL SNOWBALL STEM WORDS ==> 8



 ---- LEMMATIZATION ----

['Symbol', 'combined', 'used', 'conveying', 'information', 'broadcasting', 'information', '.']

 TOTAL LEMMATIZE WORDS ==> 8

************************************************************************************************************************

13 --> Symbols are tyrannized by the  Rules. 


 ---- TOKENS ----

 ['Symbols', 'are', 'tyrannized', 'by', 'the', 'Rules', '.'] 

 TOTAL TOKENS ==> 7

 ---- POST ----

 [('Symbols', 'NNS'), ('are', 'VBP'), ('tyrannized', 'VBN'), ('by', 'IN'), ('the', 'DT'), ('Rules', 'NNP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Symbols', 'tyrannized', 'Rules', '.']

 TOTAL FILTERED TOKENS ==>  4

 ---- POST FOR FILTERED TOKENS ----

 [('Symbols', 'NNP'), ('tyrannized', 'VBD'), ('Rules', 'NNP'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Symbols tyrannized', 'tyrannized Rules', 'Rules .'] 

 TOTAL BIGRAMS --> 3 



 ---- TRI-GRAMS ---- 

 ['Symbols tyrannized Rules', 'tyrannized Rules .'] 

 TOTAL TRIGRAMS --> 2 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> ['Symbols', 'Rules']
 TOTAL PERSON ENTITY --> 2 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['symbol', 'tyrann', 'rule', '.']

 TOTAL PORTER STEM WORDS ==> 4



 ---- SNOWBALL STEMMING ----

['symbol', 'tyrann', 'rule', '.']

 TOTAL SNOWBALL STEM WORDS ==> 4



 ---- LEMMATIZATION ----

['Symbols', 'tyrannized', 'Rules', '.']

 TOTAL LEMMATIZE WORDS ==> 4

************************************************************************************************************************

14 --> Natural Language Processing basically can be classified into two parts i.e. 


 ---- TOKENS ----

 ['Natural', 'Language', 'Processing', 'basically', 'can', 'be', 'classified', 'into', 'two', 'parts', 'i.e', '.'] 

 TOTAL TOKENS ==> 12

 ---- POST ----

 [('Natural', 'JJ'), ('Language', 'NNP'), ('Processing', 'NNP'), ('basically', 'RB'), ('can', 'MD'), ('be', 'VB'), ('classified', 'VBN'), ('into', 'IN'), ('two', 'CD'), ('parts', 'NNS'), ('i.e', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Natural', 'Language', 'Processing', 'basically', 'classified', 'two', 'parts', 'i.e', '.']

 TOTAL FILTERED TOKENS ==>  9

 ---- POST FOR FILTERED TOKENS ----

 [('Natural', 'JJ'), ('Language', 'NNP'), ('Processing', 'NNP'), ('basically', 'RB'), ('classified', 'VBD'), ('two', 'CD'), ('parts', 'NNS'), ('i.e', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Natural Language', 'Language Processing', 'Processing basically', 'basically classified', 'classified two', 'two parts', 'parts i.e', 'i.e .'] 

 TOTAL BIGRAMS --> 8 



 ---- TRI-GRAMS ---- 

 ['Natural Language Processing', 'Language Processing basically', 'Processing basically classified', 'basically classified two', 'classified two parts', 'two parts i.e', 'parts i.e .'] 

 TOTAL TRIGRAMS --> 7 



 ---- NOUN PHRASES ---- 

 ['i.e'] 

 TOTAL NOUN PHRASES --> 1 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['natur', 'languag', 'process', 'basic', 'classifi', 'two', 'part', 'i.e', '.']

 TOTAL PORTER STEM WORDS ==> 9



 ---- SNOWBALL STEMMING ----

['natur', 'languag', 'process', 'basic', 'classifi', 'two', 'part', 'i.e', '.']

 TOTAL SNOWBALL STEM WORDS ==> 9



 ---- LEMMATIZATION ----

['Natural', 'Language', 'Processing', 'basically', 'classified', 'two', 'part', 'i.e', '.']

 TOTAL LEMMATIZE WORDS ==> 9

************************************************************************************************************************

15 --> Natural  Language Understanding and Natural Language Generation which evolves the task to  understand and generate the text (Figure 1). 


 ---- TOKENS ----

 ['Natural', 'Language', 'Understanding', 'and', 'Natural', 'Language', 'Generation', 'which', 'evolves', 'the', 'task', 'to', 'understand', 'and', 'generate', 'the', 'text', '(', 'Figure', '1', ')', '.'] 

 TOTAL TOKENS ==> 22

 ---- POST ----

 [('Natural', 'JJ'), ('Language', 'NNP'), ('Understanding', 'NNP'), ('and', 'CC'), ('Natural', 'NNP'), ('Language', 'NNP'), ('Generation', 'NNP'), ('which', 'WDT'), ('evolves', 'VBZ'), ('the', 'DT'), ('task', 'NN'), ('to', 'TO'), ('understand', 'VB'), ('and', 'CC'), ('generate', 'VB'), ('the', 'DT'), ('text', 'NN'), ('(', '('), ('Figure', 'NNP'), ('1', 'CD'), (')', ')'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Natural', 'Language', 'Understanding', 'Natural', 'Language', 'Generation', 'evolves', 'task', 'understand', 'generate', 'text', '(', 'Figure', '1', ')', '.']

 TOTAL FILTERED TOKENS ==>  16

 ---- POST FOR FILTERED TOKENS ----

 [('Natural', 'JJ'), ('Language', 'NNP'), ('Understanding', 'NNP'), ('Natural', 'NNP'), ('Language', 'NNP'), ('Generation', 'NNP'), ('evolves', 'VBZ'), ('task', 'JJ'), ('understand', 'JJ'), ('generate', 'NN'), ('text', 'NN'), ('(', '('), ('Figure', 'NNP'), ('1', 'CD'), (')', ')'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Natural Language', 'Language Understanding', 'Understanding Natural', 'Natural Language', 'Language Generation', 'Generation evolves', 'evolves task', 'task understand', 'understand generate', 'generate text', 'text (', '( Figure', 'Figure 1', '1 )', ') .'] 

 TOTAL BIGRAMS --> 15 



 ---- TRI-GRAMS ---- 

 ['Natural Language Understanding', 'Language Understanding Natural', 'Understanding Natural Language', 'Natural Language Generation', 'Language Generation evolves', 'Generation evolves task', 'evolves task understand', 'task understand generate', 'understand generate text', 'generate text (', 'text ( Figure', '( Figure 1', 'Figure 1 )', '1 ) .'] 

 TOTAL TRIGRAMS --> 14 



 ---- NOUN PHRASES ---- 

 ['task understand generate', 'text'] 

 TOTAL NOUN PHRASES --> 2 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['natur', 'languag', 'understand', 'natur', 'languag', 'gener', 'evolv', 'task', 'understand', 'gener', 'text', '(', 'figur', '1', ')', '.']

 TOTAL PORTER STEM WORDS ==> 16



 ---- SNOWBALL STEMMING ----

['natur', 'languag', 'understand', 'natur', 'languag', 'generat', 'evolv', 'task', 'understand', 'generat', 'text', '(', 'figur', '1', ')', '.']

 TOTAL SNOWBALL STEM WORDS ==> 16



 ---- LEMMATIZATION ----

['Natural', 'Language', 'Understanding', 'Natural', 'Language', 'Generation', 'evolves', 'task', 'understand', 'generate', 'text', '(', 'Figure', '1', ')', '.']

 TOTAL LEMMATIZE WORDS ==> 16

************************************************************************************************************************

16 --> Figure 1. 


 ---- TOKENS ----

 ['Figure', '1', '.'] 

 TOTAL TOKENS ==> 3

 ---- POST ----

 [('Figure', 'NN'), ('1', 'CD'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Figure', '1', '.']

 TOTAL FILTERED TOKENS ==>  3

 ---- POST FOR FILTERED TOKENS ----

 [('Figure', 'NN'), ('1', 'CD'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Figure 1', '1 .'] 

 TOTAL BIGRAMS --> 2 



 ---- TRI-GRAMS ---- 

 ['Figure 1 .'] 

 TOTAL TRIGRAMS --> 1 



 ---- NOUN PHRASES ---- 

 ['Figure'] 

 TOTAL NOUN PHRASES --> 1 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['figur', '1', '.']

 TOTAL PORTER STEM WORDS ==> 3



 ---- SNOWBALL STEMMING ----

['figur', '1', '.']

 TOTAL SNOWBALL STEM WORDS ==> 3



 ---- LEMMATIZATION ----

['Figure', '1', '.']

 TOTAL LEMMATIZE WORDS ==> 3

************************************************************************************************************************

17 --> Broad Classification of NLP  Linguistics is the science of language which includes Phonology that refers to sound,  Morphology word formation, Syntax sentence structure, Semantics syntax and Pragmatics  which refers to understanding. 


 ---- TOKENS ----

 ['Broad', 'Classification', 'of', 'NLP', 'Linguistics', 'is', 'the', 'science', 'of', 'language', 'which', 'includes', 'Phonology', 'that', 'refers', 'to', 'sound', ',', 'Morphology', 'word', 'formation', ',', 'Syntax', 'sentence', 'structure', ',', 'Semantics', 'syntax', 'and', 'Pragmatics', 'which', 'refers', 'to', 'understanding', '.'] 

 TOTAL TOKENS ==> 35

 ---- POST ----

 [('Broad', 'NNP'), ('Classification', 'NNP'), ('of', 'IN'), ('NLP', 'NNP'), ('Linguistics', 'NNP'), ('is', 'VBZ'), ('the', 'DT'), ('science', 'NN'), ('of', 'IN'), ('language', 'NN'), ('which', 'WDT'), ('includes', 'VBZ'), ('Phonology', 'NNP'), ('that', 'WDT'), ('refers', 'VBZ'), ('to', 'TO'), ('sound', 'VB'), (',', ','), ('Morphology', 'NNP'), ('word', 'NN'), ('formation', 'NN'), (',', ','), ('Syntax', 'NNP'), ('sentence', 'NN'), ('structure', 'NN'), (',', ','), ('Semantics', 'NNP'), ('syntax', 'NN'), ('and', 'CC'), ('Pragmatics', 'NNP'), ('which', 'WDT'), ('refers', 'VBZ'), ('to', 'TO'), ('understanding', 'VBG'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Broad', 'Classification', 'NLP', 'Linguistics', 'science', 'language', 'includes', 'Phonology', 'refers', 'sound', ',', 'Morphology', 'word', 'formation', ',', 'Syntax', 'sentence', 'structure', ',', 'Semantics', 'syntax', 'Pragmatics', 'refers', 'understanding', '.']

 TOTAL FILTERED TOKENS ==>  25

 ---- POST FOR FILTERED TOKENS ----

 [('Broad', 'NNP'), ('Classification', 'NNP'), ('NLP', 'NNP'), ('Linguistics', 'NNP'), ('science', 'NN'), ('language', 'NN'), ('includes', 'VBZ'), ('Phonology', 'NNP'), ('refers', 'NNS'), ('sound', 'VBD'), (',', ','), ('Morphology', 'NNP'), ('word', 'NN'), ('formation', 'NN'), (',', ','), ('Syntax', 'NNP'), ('sentence', 'NN'), ('structure', 'NN'), (',', ','), ('Semantics', 'NNP'), ('syntax', 'NN'), ('Pragmatics', 'NNPS'), ('refers', 'NNS'), ('understanding', 'VBG'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Broad Classification', 'Classification NLP', 'NLP Linguistics', 'Linguistics science', 'science language', 'language includes', 'includes Phonology', 'Phonology refers', 'refers sound', 'sound ,', ', Morphology', 'Morphology word', 'word formation', 'formation ,', ', Syntax', 'Syntax sentence', 'sentence structure', 'structure ,', ', Semantics', 'Semantics syntax', 'syntax Pragmatics', 'Pragmatics refers', 'refers understanding', 'understanding .'] 

 TOTAL BIGRAMS --> 24 



 ---- TRI-GRAMS ---- 

 ['Broad Classification NLP', 'Classification NLP Linguistics', 'NLP Linguistics science', 'Linguistics science language', 'science language includes', 'language includes Phonology', 'includes Phonology refers', 'Phonology refers sound', 'refers sound ,', 'sound , Morphology', ', Morphology word', 'Morphology word formation', 'word formation ,', 'formation , Syntax', ', Syntax sentence', 'Syntax sentence structure', 'sentence structure ,', 'structure , Semantics', ', Semantics syntax', 'Semantics syntax Pragmatics', 'syntax Pragmatics refers', 'Pragmatics refers understanding', 'refers understanding .'] 

 TOTAL TRIGRAMS --> 23 



 ---- NOUN PHRASES ---- 

 ['science', 'language', 'word', 'formation', 'sentence', 'structure', 'syntax'] 

 TOTAL NOUN PHRASES --> 7 



 ---- NER ----

 
 ORGANIZATION ---> ['Phonology']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> ['Broad', 'Semantics']
 TOTAL PERSON ENTITY --> 2 


 GPE ---> ['Morphology', 'Syntax']
 TOTAL GPE ENTITY --> 2 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['broad', 'classif', 'nlp', 'linguist', 'scienc', 'languag', 'includ', 'phonolog', 'refer', 'sound', ',', 'morpholog', 'word', 'format', ',', 'syntax', 'sentenc', 'structur', ',', 'semant', 'syntax', 'pragmat', 'refer', 'understand', '.']

 TOTAL PORTER STEM WORDS ==> 25



 ---- SNOWBALL STEMMING ----

['broad', 'classif', 'nlp', 'linguist', 'scienc', 'languag', 'includ', 'phonolog', 'refer', 'sound', ',', 'morpholog', 'word', 'format', ',', 'syntax', 'sentenc', 'structur', ',', 'semant', 'syntax', 'pragmat', 'refer', 'understand', '.']

 TOTAL SNOWBALL STEM WORDS ==> 25



 ---- LEMMATIZATION ----

['Broad', 'Classification', 'NLP', 'Linguistics', 'science', 'language', 'includes', 'Phonology', 'refers', 'sound', ',', 'Morphology', 'word', 'formation', ',', 'Syntax', 'sentence', 'structure', ',', 'Semantics', 'syntax', 'Pragmatics', 'refers', 'understanding', '.']

 TOTAL LEMMATIZE WORDS ==> 25

************************************************************************************************************************

18 --> Noah Chomsky, one of the first linguists of twelfth century that started syntactic theories,  marked a unique position in the field of theoretical linguistics because he revolutionised the  area of syntax (Chomsky, 1965) [1]. 


 ---- TOKENS ----

 ['Noah', 'Chomsky', ',', 'one', 'of', 'the', 'first', 'linguists', 'of', 'twelfth', 'century', 'that', 'started', 'syntactic', 'theories', ',', 'marked', 'a', 'unique', 'position', 'in', 'the', 'field', 'of', 'theoretical', 'linguistics', 'because', 'he', 'revolutionised', 'the', 'area', 'of', 'syntax', '(', 'Chomsky', ',', '1965', ')', '[', '1', ']', '.'] 

 TOTAL TOKENS ==> 42

 ---- POST ----

 [('Noah', 'NNP'), ('Chomsky', 'NNP'), (',', ','), ('one', 'CD'), ('of', 'IN'), ('the', 'DT'), ('first', 'JJ'), ('linguists', 'NNS'), ('of', 'IN'), ('twelfth', 'JJ'), ('century', 'NN'), ('that', 'WDT'), ('started', 'VBD'), ('syntactic', 'JJ'), ('theories', 'NNS'), (',', ','), ('marked', 'VBD'), ('a', 'DT'), ('unique', 'JJ'), ('position', 'NN'), ('in', 'IN'), ('the', 'DT'), ('field', 'NN'), ('of', 'IN'), ('theoretical', 'JJ'), ('linguistics', 'NNS'), ('because', 'IN'), ('he', 'PRP'), ('revolutionised', 'VBD'), ('the', 'DT'), ('area', 'NN'), ('of', 'IN'), ('syntax', 'NN'), ('(', '('), ('Chomsky', 'NNP'), (',', ','), ('1965', 'CD'), (')', ')'), ('[', 'VBD'), ('1', 'CD'), (']', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Noah', 'Chomsky', ',', 'one', 'first', 'linguists', 'twelfth', 'century', 'started', 'syntactic', 'theories', ',', 'marked', 'unique', 'position', 'field', 'theoretical', 'linguistics', 'revolutionised', 'area', 'syntax', '(', 'Chomsky', ',', '1965', ')', '[', '1', ']', '.']

 TOTAL FILTERED TOKENS ==>  30

 ---- POST FOR FILTERED TOKENS ----

 [('Noah', 'NNP'), ('Chomsky', 'NNP'), (',', ','), ('one', 'CD'), ('first', 'JJ'), ('linguists', 'VBZ'), ('twelfth', 'JJ'), ('century', 'NN'), ('started', 'VBD'), ('syntactic', 'JJ'), ('theories', 'NNS'), (',', ','), ('marked', 'VBD'), ('unique', 'JJ'), ('position', 'NN'), ('field', 'NN'), ('theoretical', 'JJ'), ('linguistics', 'NNS'), ('revolutionised', 'VBD'), ('area', 'NN'), ('syntax', 'NN'), ('(', '('), ('Chomsky', 'NNP'), (',', ','), ('1965', 'CD'), (')', ')'), ('[', 'VBD'), ('1', 'CD'), (']', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Noah Chomsky', 'Chomsky ,', ', one', 'one first', 'first linguists', 'linguists twelfth', 'twelfth century', 'century started', 'started syntactic', 'syntactic theories', 'theories ,', ', marked', 'marked unique', 'unique position', 'position field', 'field theoretical', 'theoretical linguistics', 'linguistics revolutionised', 'revolutionised area', 'area syntax', 'syntax (', '( Chomsky', 'Chomsky ,', ', 1965', '1965 )', ') [', '[ 1', '1 ]', '] .'] 

 TOTAL BIGRAMS --> 29 



 ---- TRI-GRAMS ---- 

 ['Noah Chomsky ,', 'Chomsky , one', ', one first', 'one first linguists', 'first linguists twelfth', 'linguists twelfth century', 'twelfth century started', 'century started syntactic', 'started syntactic theories', 'syntactic theories ,', 'theories , marked', ', marked unique', 'marked unique position', 'unique position field', 'position field theoretical', 'field theoretical linguistics', 'theoretical linguistics revolutionised', 'linguistics revolutionised area', 'revolutionised area syntax', 'area syntax (', 'syntax ( Chomsky', '( Chomsky ,', 'Chomsky , 1965', ', 1965 )', '1965 ) [', ') [ 1', '[ 1 ]', '1 ] .'] 

 TOTAL TRIGRAMS --> 28 



 ---- NOUN PHRASES ---- 

 ['twelfth century', 'unique position', 'field', 'area', 'syntax', ']'] 

 TOTAL NOUN PHRASES --> 6 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> ['Noah Chomsky']
 TOTAL PERSON ENTITY --> 1 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['noah', 'chomski', ',', 'one', 'first', 'linguist', 'twelfth', 'centuri', 'start', 'syntact', 'theori', ',', 'mark', 'uniqu', 'posit', 'field', 'theoret', 'linguist', 'revolutionis', 'area', 'syntax', '(', 'chomski', ',', '1965', ')', '[', '1', ']', '.']

 TOTAL PORTER STEM WORDS ==> 30



 ---- SNOWBALL STEMMING ----

['noah', 'chomski', ',', 'one', 'first', 'linguist', 'twelfth', 'centuri', 'start', 'syntact', 'theori', ',', 'mark', 'uniqu', 'posit', 'field', 'theoret', 'linguist', 'revolutionis', 'area', 'syntax', '(', 'chomski', ',', '1965', ')', '[', '1', ']', '.']

 TOTAL SNOWBALL STEM WORDS ==> 30



 ---- LEMMATIZATION ----

['Noah', 'Chomsky', ',', 'one', 'first', 'linguist', 'twelfth', 'century', 'started', 'syntactic', 'theory', ',', 'marked', 'unique', 'position', 'field', 'theoretical', 'linguistics', 'revolutionised', 'area', 'syntax', '(', 'Chomsky', ',', '1965', ')', '[', '1', ']', '.']

 TOTAL LEMMATIZE WORDS ==> 30

************************************************************************************************************************

19 --> Which can be broadly categorized into two levels Higher  Level which include speech recognition and Lower Level which corresponds to natural  language. 


 ---- TOKENS ----

 ['Which', 'can', 'be', 'broadly', 'categorized', 'into', 'two', 'levels', 'Higher', 'Level', 'which', 'include', 'speech', 'recognition', 'and', 'Lower', 'Level', 'which', 'corresponds', 'to', 'natural', 'language', '.'] 

 TOTAL TOKENS ==> 23

 ---- POST ----

 [('Which', 'WDT'), ('can', 'MD'), ('be', 'VB'), ('broadly', 'RB'), ('categorized', 'VBN'), ('into', 'IN'), ('two', 'CD'), ('levels', 'NNS'), ('Higher', 'JJR'), ('Level', 'NNP'), ('which', 'WDT'), ('include', 'VBP'), ('speech', 'JJ'), ('recognition', 'NN'), ('and', 'CC'), ('Lower', 'NNP'), ('Level', 'NNP'), ('which', 'WDT'), ('corresponds', 'VBZ'), ('to', 'TO'), ('natural', 'JJ'), ('language', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['broadly', 'categorized', 'two', 'levels', 'Higher', 'Level', 'include', 'speech', 'recognition', 'Lower', 'Level', 'corresponds', 'natural', 'language', '.']

 TOTAL FILTERED TOKENS ==>  15

 ---- POST FOR FILTERED TOKENS ----

 [('broadly', 'RB'), ('categorized', 'VBN'), ('two', 'CD'), ('levels', 'NNS'), ('Higher', 'RBR'), ('Level', 'NNP'), ('include', 'VBP'), ('speech', 'JJ'), ('recognition', 'NN'), ('Lower', 'NNP'), ('Level', 'NNP'), ('corresponds', 'VBZ'), ('natural', 'JJ'), ('language', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['broadly categorized', 'categorized two', 'two levels', 'levels Higher', 'Higher Level', 'Level include', 'include speech', 'speech recognition', 'recognition Lower', 'Lower Level', 'Level corresponds', 'corresponds natural', 'natural language', 'language .'] 

 TOTAL BIGRAMS --> 14 



 ---- TRI-GRAMS ---- 

 ['broadly categorized two', 'categorized two levels', 'two levels Higher', 'levels Higher Level', 'Higher Level include', 'Level include speech', 'include speech recognition', 'speech recognition Lower', 'recognition Lower Level', 'Lower Level corresponds', 'Level corresponds natural', 'corresponds natural language', 'natural language .'] 

 TOTAL TRIGRAMS --> 13 



 ---- NOUN PHRASES ---- 

 ['speech recognition', 'natural language'] 

 TOTAL NOUN PHRASES --> 2 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> ['Lower Level']
 TOTAL PERSON ENTITY --> 1 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['broadli', 'categor', 'two', 'level', 'higher', 'level', 'includ', 'speech', 'recognit', 'lower', 'level', 'correspond', 'natur', 'languag', '.']

 TOTAL PORTER STEM WORDS ==> 15



 ---- SNOWBALL STEMMING ----

['broad', 'categor', 'two', 'level', 'higher', 'level', 'includ', 'speech', 'recognit', 'lower', 'level', 'correspond', 'natur', 'languag', '.']

 TOTAL SNOWBALL STEM WORDS ==> 15



 ---- LEMMATIZATION ----

['broadly', 'categorized', 'two', 'level', 'Higher', 'Level', 'include', 'speech', 'recognition', 'Lower', 'Level', 'corresponds', 'natural', 'language', '.']

 TOTAL LEMMATIZE WORDS ==> 15

************************************************************************************************************************

20 --> Few of the researched tasks of NLP are Automatic Summarization, Co-Reference  Resolution, Discourse Analysis, Machine Translation, Morphological Segmentation, Named  Entity Recognition, Optical Character Recognition, Part Of Speech Tagging etc. 


 ---- TOKENS ----

 ['Few', 'of', 'the', 'researched', 'tasks', 'of', 'NLP', 'are', 'Automatic', 'Summarization', ',', 'Co-Reference', 'Resolution', ',', 'Discourse', 'Analysis', ',', 'Machine', 'Translation', ',', 'Morphological', 'Segmentation', ',', 'Named', 'Entity', 'Recognition', ',', 'Optical', 'Character', 'Recognition', ',', 'Part', 'Of', 'Speech', 'Tagging', 'etc', '.'] 

 TOTAL TOKENS ==> 37

 ---- POST ----

 [('Few', 'JJ'), ('of', 'IN'), ('the', 'DT'), ('researched', 'JJ'), ('tasks', 'NNS'), ('of', 'IN'), ('NLP', 'NNP'), ('are', 'VBP'), ('Automatic', 'NNP'), ('Summarization', 'NNP'), (',', ','), ('Co-Reference', 'NNP'), ('Resolution', 'NNP'), (',', ','), ('Discourse', 'NNP'), ('Analysis', 'NNP'), (',', ','), ('Machine', 'NNP'), ('Translation', 'NNP'), (',', ','), ('Morphological', 'NNP'), ('Segmentation', 'NNP'), (',', ','), ('Named', 'NNP'), ('Entity', 'NNP'), ('Recognition', 'NNP'), (',', ','), ('Optical', 'NNP'), ('Character', 'NNP'), ('Recognition', 'NNP'), (',', ','), ('Part', 'NNP'), ('Of', 'IN'), ('Speech', 'NNP'), ('Tagging', 'NNP'), ('etc', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['researched', 'tasks', 'NLP', 'Automatic', 'Summarization', ',', 'Co-Reference', 'Resolution', ',', 'Discourse', 'Analysis', ',', 'Machine', 'Translation', ',', 'Morphological', 'Segmentation', ',', 'Named', 'Entity', 'Recognition', ',', 'Optical', 'Character', 'Recognition', ',', 'Part', 'Speech', 'Tagging', 'etc', '.']

 TOTAL FILTERED TOKENS ==>  31

 ---- POST FOR FILTERED TOKENS ----

 [('researched', 'VBN'), ('tasks', 'NNS'), ('NLP', 'NNP'), ('Automatic', 'NNP'), ('Summarization', 'NNP'), (',', ','), ('Co-Reference', 'NNP'), ('Resolution', 'NNP'), (',', ','), ('Discourse', 'NNP'), ('Analysis', 'NNP'), (',', ','), ('Machine', 'NNP'), ('Translation', 'NNP'), (',', ','), ('Morphological', 'NNP'), ('Segmentation', 'NNP'), (',', ','), ('Named', 'NNP'), ('Entity', 'NNP'), ('Recognition', 'NNP'), (',', ','), ('Optical', 'NNP'), ('Character', 'NNP'), ('Recognition', 'NNP'), (',', ','), ('Part', 'NNP'), ('Speech', 'NNP'), ('Tagging', 'NNP'), ('etc', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['researched tasks', 'tasks NLP', 'NLP Automatic', 'Automatic Summarization', 'Summarization ,', ', Co-Reference', 'Co-Reference Resolution', 'Resolution ,', ', Discourse', 'Discourse Analysis', 'Analysis ,', ', Machine', 'Machine Translation', 'Translation ,', ', Morphological', 'Morphological Segmentation', 'Segmentation ,', ', Named', 'Named Entity', 'Entity Recognition', 'Recognition ,', ', Optical', 'Optical Character', 'Character Recognition', 'Recognition ,', ', Part', 'Part Speech', 'Speech Tagging', 'Tagging etc', 'etc .'] 

 TOTAL BIGRAMS --> 30 



 ---- TRI-GRAMS ---- 

 ['researched tasks NLP', 'tasks NLP Automatic', 'NLP Automatic Summarization', 'Automatic Summarization ,', 'Summarization , Co-Reference', ', Co-Reference Resolution', 'Co-Reference Resolution ,', 'Resolution , Discourse', ', Discourse Analysis', 'Discourse Analysis ,', 'Analysis , Machine', ', Machine Translation', 'Machine Translation ,', 'Translation , Morphological', ', Morphological Segmentation', 'Morphological Segmentation ,', 'Segmentation , Named', ', Named Entity', 'Named Entity Recognition', 'Entity Recognition ,', 'Recognition , Optical', ', Optical Character', 'Optical Character Recognition', 'Character Recognition ,', 'Recognition , Part', ', Part Speech', 'Part Speech Tagging', 'Speech Tagging etc', 'Tagging etc .'] 

 TOTAL TRIGRAMS --> 29 



 ---- NOUN PHRASES ---- 

 ['etc'] 

 TOTAL NOUN PHRASES --> 1 



 ---- NER ----

 
 ORGANIZATION ---> ['NLP Automatic Summarization']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> ['Discourse Analysis', 'Machine Translation', 'Morphological Segmentation', 'Named Entity Recognition', 'Optical Character Recognition', 'Part Speech Tagging']
 TOTAL PERSON ENTITY --> 6 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['research', 'task', 'nlp', 'automat', 'summar', ',', 'co-refer', 'resolut', ',', 'discours', 'analysi', ',', 'machin', 'translat', ',', 'morpholog', 'segment', ',', 'name', 'entiti', 'recognit', ',', 'optic', 'charact', 'recognit', ',', 'part', 'speech', 'tag', 'etc', '.']

 TOTAL PORTER STEM WORDS ==> 31



 ---- SNOWBALL STEMMING ----

['research', 'task', 'nlp', 'automat', 'summar', ',', 'co-refer', 'resolut', ',', 'discours', 'analysi', ',', 'machin', 'translat', ',', 'morpholog', 'segment', ',', 'name', 'entiti', 'recognit', ',', 'optic', 'charact', 'recognit', ',', 'part', 'speech', 'tag', 'etc', '.']

 TOTAL SNOWBALL STEM WORDS ==> 31



 ---- LEMMATIZATION ----

['researched', 'task', 'NLP', 'Automatic', 'Summarization', ',', 'Co-Reference', 'Resolution', ',', 'Discourse', 'Analysis', ',', 'Machine', 'Translation', ',', 'Morphological', 'Segmentation', ',', 'Named', 'Entity', 'Recognition', ',', 'Optical', 'Character', 'Recognition', ',', 'Part', 'Speech', 'Tagging', 'etc', '.']

 TOTAL LEMMATIZE WORDS ==> 31

************************************************************************************************************************

21 --> Some of  these tasks have direct real world applications such as Machine translation, Named entity  recognition, Optical character recognition etc. 


 ---- TOKENS ----

 ['Some', 'of', 'these', 'tasks', 'have', 'direct', 'real', 'world', 'applications', 'such', 'as', 'Machine', 'translation', ',', 'Named', 'entity', 'recognition', ',', 'Optical', 'character', 'recognition', 'etc', '.'] 

 TOTAL TOKENS ==> 23

 ---- POST ----

 [('Some', 'DT'), ('of', 'IN'), ('these', 'DT'), ('tasks', 'NNS'), ('have', 'VBP'), ('direct', 'JJ'), ('real', 'JJ'), ('world', 'NN'), ('applications', 'NNS'), ('such', 'JJ'), ('as', 'IN'), ('Machine', 'NNP'), ('translation', 'NN'), (',', ','), ('Named', 'NNP'), ('entity', 'NN'), ('recognition', 'NN'), (',', ','), ('Optical', 'NNP'), ('character', 'NN'), ('recognition', 'NN'), ('etc', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['tasks', 'direct', 'real', 'world', 'applications', 'Machine', 'translation', ',', 'Named', 'entity', 'recognition', ',', 'Optical', 'character', 'recognition', 'etc', '.']

 TOTAL FILTERED TOKENS ==>  17

 ---- POST FOR FILTERED TOKENS ----

 [('tasks', 'NNS'), ('direct', 'JJ'), ('real', 'JJ'), ('world', 'NN'), ('applications', 'NNS'), ('Machine', 'NNP'), ('translation', 'NN'), (',', ','), ('Named', 'NNP'), ('entity', 'NN'), ('recognition', 'NN'), (',', ','), ('Optical', 'NNP'), ('character', 'NN'), ('recognition', 'NN'), ('etc', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['tasks direct', 'direct real', 'real world', 'world applications', 'applications Machine', 'Machine translation', 'translation ,', ', Named', 'Named entity', 'entity recognition', 'recognition ,', ', Optical', 'Optical character', 'character recognition', 'recognition etc', 'etc .'] 

 TOTAL BIGRAMS --> 16 



 ---- TRI-GRAMS ---- 

 ['tasks direct real', 'direct real world', 'real world applications', 'world applications Machine', 'applications Machine translation', 'Machine translation ,', 'translation , Named', ', Named entity', 'Named entity recognition', 'entity recognition ,', 'recognition , Optical', ', Optical character', 'Optical character recognition', 'character recognition etc', 'recognition etc .'] 

 TOTAL TRIGRAMS --> 15 



 ---- NOUN PHRASES ---- 

 ['direct real world', 'translation', 'entity', 'recognition', 'character', 'recognition', 'etc'] 

 TOTAL NOUN PHRASES --> 7 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> ['Machine', 'Named']
 TOTAL PERSON ENTITY --> 2 


 GPE ---> ['Optical']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['task', 'direct', 'real', 'world', 'applic', 'machin', 'translat', ',', 'name', 'entiti', 'recognit', ',', 'optic', 'charact', 'recognit', 'etc', '.']

 TOTAL PORTER STEM WORDS ==> 17



 ---- SNOWBALL STEMMING ----

['task', 'direct', 'real', 'world', 'applic', 'machin', 'translat', ',', 'name', 'entiti', 'recognit', ',', 'optic', 'charact', 'recognit', 'etc', '.']

 TOTAL SNOWBALL STEM WORDS ==> 17



 ---- LEMMATIZATION ----

['task', 'direct', 'real', 'world', 'application', 'Machine', 'translation', ',', 'Named', 'entity', 'recognition', ',', 'Optical', 'character', 'recognition', 'etc', '.']

 TOTAL LEMMATIZE WORDS ==> 17

************************************************************************************************************************

22 --> Automatic summarization produces an  understandable summary of a set of text and provides summaries or detailed information of  text of a known type. 


 ---- TOKENS ----

 ['Automatic', 'summarization', 'produces', 'an', 'understandable', 'summary', 'of', 'a', 'set', 'of', 'text', 'and', 'provides', 'summaries', 'or', 'detailed', 'information', 'of', 'text', 'of', 'a', 'known', 'type', '.'] 

 TOTAL TOKENS ==> 24

 ---- POST ----

 [('Automatic', 'JJ'), ('summarization', 'NN'), ('produces', 'VBZ'), ('an', 'DT'), ('understandable', 'JJ'), ('summary', 'NN'), ('of', 'IN'), ('a', 'DT'), ('set', 'NN'), ('of', 'IN'), ('text', 'NN'), ('and', 'CC'), ('provides', 'VBZ'), ('summaries', 'NNS'), ('or', 'CC'), ('detailed', 'VBN'), ('information', 'NN'), ('of', 'IN'), ('text', 'NN'), ('of', 'IN'), ('a', 'DT'), ('known', 'VBN'), ('type', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Automatic', 'summarization', 'produces', 'understandable', 'summary', 'set', 'text', 'provides', 'summaries', 'detailed', 'information', 'text', 'known', 'type', '.']

 TOTAL FILTERED TOKENS ==>  15

 ---- POST FOR FILTERED TOKENS ----

 [('Automatic', 'JJ'), ('summarization', 'NN'), ('produces', 'VBZ'), ('understandable', 'JJ'), ('summary', 'JJ'), ('set', 'NN'), ('text', 'NN'), ('provides', 'VBZ'), ('summaries', 'NNS'), ('detailed', 'VBN'), ('information', 'NN'), ('text', 'NN'), ('known', 'VBN'), ('type', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Automatic summarization', 'summarization produces', 'produces understandable', 'understandable summary', 'summary set', 'set text', 'text provides', 'provides summaries', 'summaries detailed', 'detailed information', 'information text', 'text known', 'known type', 'type .'] 

 TOTAL BIGRAMS --> 14 



 ---- TRI-GRAMS ---- 

 ['Automatic summarization produces', 'summarization produces understandable', 'produces understandable summary', 'understandable summary set', 'summary set text', 'set text provides', 'text provides summaries', 'provides summaries detailed', 'summaries detailed information', 'detailed information text', 'information text known', 'text known type', 'known type .'] 

 TOTAL TRIGRAMS --> 13 



 ---- NOUN PHRASES ---- 

 ['Automatic summarization', 'understandable summary set', 'text', 'information', 'text', 'type'] 

 TOTAL NOUN PHRASES --> 6 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> ['Automatic']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['automat', 'summar', 'produc', 'understand', 'summari', 'set', 'text', 'provid', 'summari', 'detail', 'inform', 'text', 'known', 'type', '.']

 TOTAL PORTER STEM WORDS ==> 15



 ---- SNOWBALL STEMMING ----

['automat', 'summar', 'produc', 'understand', 'summari', 'set', 'text', 'provid', 'summari', 'detail', 'inform', 'text', 'known', 'type', '.']

 TOTAL SNOWBALL STEM WORDS ==> 15



 ---- LEMMATIZATION ----

['Automatic', 'summarization', 'produce', 'understandable', 'summary', 'set', 'text', 'provides', 'summary', 'detailed', 'information', 'text', 'known', 'type', '.']

 TOTAL LEMMATIZE WORDS ==> 15

************************************************************************************************************************

23 --> Co-reference resolution it refers to a sentence or larger set of text that  determines which word refer to the same object. 


 ---- TOKENS ----

 ['Co-reference', 'resolution', 'it', 'refers', 'to', 'a', 'sentence', 'or', 'larger', 'set', 'of', 'text', 'that', 'determines', 'which', 'word', 'refer', 'to', 'the', 'same', 'object', '.'] 

 TOTAL TOKENS ==> 22

 ---- POST ----

 [('Co-reference', 'NNP'), ('resolution', 'NN'), ('it', 'PRP'), ('refers', 'VBZ'), ('to', 'TO'), ('a', 'DT'), ('sentence', 'NN'), ('or', 'CC'), ('larger', 'JJR'), ('set', 'NN'), ('of', 'IN'), ('text', 'NN'), ('that', 'IN'), ('determines', 'VBZ'), ('which', 'WDT'), ('word', 'NN'), ('refer', 'NN'), ('to', 'TO'), ('the', 'DT'), ('same', 'JJ'), ('object', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Co-reference', 'resolution', 'refers', 'sentence', 'larger', 'set', 'text', 'determines', 'word', 'refer', 'object', '.']

 TOTAL FILTERED TOKENS ==>  12

 ---- POST FOR FILTERED TOKENS ----

 [('Co-reference', 'NNP'), ('resolution', 'NN'), ('refers', 'NNS'), ('sentence', 'NN'), ('larger', 'JJR'), ('set', 'VBN'), ('text', 'NN'), ('determines', 'NNS'), ('word', 'NN'), ('refer', 'NN'), ('object', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Co-reference resolution', 'resolution refers', 'refers sentence', 'sentence larger', 'larger set', 'set text', 'text determines', 'determines word', 'word refer', 'refer object', 'object .'] 

 TOTAL BIGRAMS --> 11 



 ---- TRI-GRAMS ---- 

 ['Co-reference resolution refers', 'resolution refers sentence', 'refers sentence larger', 'sentence larger set', 'larger set text', 'set text determines', 'text determines word', 'determines word refer', 'word refer object', 'refer object .'] 

 TOTAL TRIGRAMS --> 10 



 ---- NOUN PHRASES ---- 

 ['resolution', 'sentence', 'text', 'word', 'refer', 'object'] 

 TOTAL NOUN PHRASES --> 6 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['co-refer', 'resolut', 'refer', 'sentenc', 'larger', 'set', 'text', 'determin', 'word', 'refer', 'object', '.']

 TOTAL PORTER STEM WORDS ==> 12



 ---- SNOWBALL STEMMING ----

['co-refer', 'resolut', 'refer', 'sentenc', 'larger', 'set', 'text', 'determin', 'word', 'refer', 'object', '.']

 TOTAL SNOWBALL STEM WORDS ==> 12



 ---- LEMMATIZATION ----

['Co-reference', 'resolution', 'refers', 'sentence', 'larger', 'set', 'text', 'determines', 'word', 'refer', 'object', '.']

 TOTAL LEMMATIZE WORDS ==> 12

************************************************************************************************************************

24 --> Discourse analysis refers to the task of  identifying the discourse structure of connected text. 


 ---- TOKENS ----

 ['Discourse', 'analysis', 'refers', 'to', 'the', 'task', 'of', 'identifying', 'the', 'discourse', 'structure', 'of', 'connected', 'text', '.'] 

 TOTAL TOKENS ==> 15

 ---- POST ----

 [('Discourse', 'NNP'), ('analysis', 'NN'), ('refers', 'NNS'), ('to', 'TO'), ('the', 'DT'), ('task', 'NN'), ('of', 'IN'), ('identifying', 'VBG'), ('the', 'DT'), ('discourse', 'NN'), ('structure', 'NN'), ('of', 'IN'), ('connected', 'JJ'), ('text', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Discourse', 'analysis', 'refers', 'task', 'identifying', 'discourse', 'structure', 'connected', 'text', '.']

 TOTAL FILTERED TOKENS ==>  10

 ---- POST FOR FILTERED TOKENS ----

 [('Discourse', 'NNP'), ('analysis', 'NN'), ('refers', 'NNS'), ('task', 'VBP'), ('identifying', 'VBG'), ('discourse', 'NN'), ('structure', 'NN'), ('connected', 'VBN'), ('text', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Discourse analysis', 'analysis refers', 'refers task', 'task identifying', 'identifying discourse', 'discourse structure', 'structure connected', 'connected text', 'text .'] 

 TOTAL BIGRAMS --> 9 



 ---- TRI-GRAMS ---- 

 ['Discourse analysis refers', 'analysis refers task', 'refers task identifying', 'task identifying discourse', 'identifying discourse structure', 'discourse structure connected', 'structure connected text', 'connected text .'] 

 TOTAL TRIGRAMS --> 8 



 ---- NOUN PHRASES ---- 

 ['analysis', 'discourse', 'structure', 'text'] 

 TOTAL NOUN PHRASES --> 4 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> ['Discourse']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['discours', 'analysi', 'refer', 'task', 'identifi', 'discours', 'structur', 'connect', 'text', '.']

 TOTAL PORTER STEM WORDS ==> 10



 ---- SNOWBALL STEMMING ----

['discours', 'analysi', 'refer', 'task', 'identifi', 'discours', 'structur', 'connect', 'text', '.']

 TOTAL SNOWBALL STEM WORDS ==> 10



 ---- LEMMATIZATION ----

['Discourse', 'analysis', 'refers', 'task', 'identifying', 'discourse', 'structure', 'connected', 'text', '.']

 TOTAL LEMMATIZE WORDS ==> 10

************************************************************************************************************************

25 --> Machine translation which refers to  automatic translation of text from one human language to another. 


 ---- TOKENS ----

 ['Machine', 'translation', 'which', 'refers', 'to', 'automatic', 'translation', 'of', 'text', 'from', 'one', 'human', 'language', 'to', 'another', '.'] 

 TOTAL TOKENS ==> 16

 ---- POST ----

 [('Machine', 'NNP'), ('translation', 'NN'), ('which', 'WDT'), ('refers', 'VBZ'), ('to', 'TO'), ('automatic', 'JJ'), ('translation', 'NN'), ('of', 'IN'), ('text', 'NN'), ('from', 'IN'), ('one', 'CD'), ('human', 'JJ'), ('language', 'NN'), ('to', 'TO'), ('another', 'DT'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Machine', 'translation', 'refers', 'automatic', 'translation', 'text', 'one', 'human', 'language', 'another', '.']

 TOTAL FILTERED TOKENS ==>  11

 ---- POST FOR FILTERED TOKENS ----

 [('Machine', 'NN'), ('translation', 'NN'), ('refers', 'NNS'), ('automatic', 'JJ'), ('translation', 'NN'), ('text', 'IN'), ('one', 'CD'), ('human', 'JJ'), ('language', 'NN'), ('another', 'DT'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Machine translation', 'translation refers', 'refers automatic', 'automatic translation', 'translation text', 'text one', 'one human', 'human language', 'language another', 'another .'] 

 TOTAL BIGRAMS --> 10 



 ---- TRI-GRAMS ---- 

 ['Machine translation refers', 'translation refers automatic', 'refers automatic translation', 'automatic translation text', 'translation text one', 'text one human', 'one human language', 'human language another', 'language another .'] 

 TOTAL TRIGRAMS --> 9 



 ---- NOUN PHRASES ---- 

 ['Machine', 'translation', 'automatic translation', 'human language'] 

 TOTAL NOUN PHRASES --> 4 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> ['Machine']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['machin', 'translat', 'refer', 'automat', 'translat', 'text', 'one', 'human', 'languag', 'anoth', '.']

 TOTAL PORTER STEM WORDS ==> 11



 ---- SNOWBALL STEMMING ----

['machin', 'translat', 'refer', 'automat', 'translat', 'text', 'one', 'human', 'languag', 'anoth', '.']

 TOTAL SNOWBALL STEM WORDS ==> 11



 ---- LEMMATIZATION ----

['Machine', 'translation', 'refers', 'automatic', 'translation', 'text', 'one', 'human', 'language', 'another', '.']

 TOTAL LEMMATIZE WORDS ==> 11

************************************************************************************************************************

26 --> Morphological  segmentation which refers to separate word into individual morphemes and identify the class  of the morphemes. 


 ---- TOKENS ----

 ['Morphological', 'segmentation', 'which', 'refers', 'to', 'separate', 'word', 'into', 'individual', 'morphemes', 'and', 'identify', 'the', 'class', 'of', 'the', 'morphemes', '.'] 

 TOTAL TOKENS ==> 18

 ---- POST ----

 [('Morphological', 'NNP'), ('segmentation', 'NN'), ('which', 'WDT'), ('refers', 'NNS'), ('to', 'TO'), ('separate', 'VB'), ('word', 'NN'), ('into', 'IN'), ('individual', 'JJ'), ('morphemes', 'NNS'), ('and', 'CC'), ('identify', 'VB'), ('the', 'DT'), ('class', 'NN'), ('of', 'IN'), ('the', 'DT'), ('morphemes', 'NNS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Morphological', 'segmentation', 'refers', 'separate', 'word', 'individual', 'morphemes', 'identify', 'class', 'morphemes', '.']

 TOTAL FILTERED TOKENS ==>  11

 ---- POST FOR FILTERED TOKENS ----

 [('Morphological', 'JJ'), ('segmentation', 'NN'), ('refers', 'NNS'), ('separate', 'VBP'), ('word', 'NN'), ('individual', 'JJ'), ('morphemes', 'NNS'), ('identify', 'VBP'), ('class', 'NN'), ('morphemes', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Morphological segmentation', 'segmentation refers', 'refers separate', 'separate word', 'word individual', 'individual morphemes', 'morphemes identify', 'identify class', 'class morphemes', 'morphemes .'] 

 TOTAL BIGRAMS --> 10 



 ---- TRI-GRAMS ---- 

 ['Morphological segmentation refers', 'segmentation refers separate', 'refers separate word', 'separate word individual', 'word individual morphemes', 'individual morphemes identify', 'morphemes identify class', 'identify class morphemes', 'class morphemes .'] 

 TOTAL TRIGRAMS --> 9 



 ---- NOUN PHRASES ---- 

 ['Morphological segmentation', 'word', 'class'] 

 TOTAL NOUN PHRASES --> 3 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['morpholog', 'segment', 'refer', 'separ', 'word', 'individu', 'morphem', 'identifi', 'class', 'morphem', '.']

 TOTAL PORTER STEM WORDS ==> 11



 ---- SNOWBALL STEMMING ----

['morpholog', 'segment', 'refer', 'separ', 'word', 'individu', 'morphem', 'identifi', 'class', 'morphem', '.']

 TOTAL SNOWBALL STEM WORDS ==> 11



 ---- LEMMATIZATION ----

['Morphological', 'segmentation', 'refers', 'separate', 'word', 'individual', 'morpheme', 'identify', 'class', 'morpheme', '.']

 TOTAL LEMMATIZE WORDS ==> 11

************************************************************************************************************************

27 --> Named entity recognition (NER) it describes a stream of text, determine  which items in the text relates to proper names. 


 ---- TOKENS ----

 ['Named', 'entity', 'recognition', '(', 'NER', ')', 'it', 'describes', 'a', 'stream', 'of', 'text', ',', 'determine', 'which', 'items', 'in', 'the', 'text', 'relates', 'to', 'proper', 'names', '.'] 

 TOTAL TOKENS ==> 24

 ---- POST ----

 [('Named', 'VBN'), ('entity', 'NN'), ('recognition', 'NN'), ('(', '('), ('NER', 'NNP'), (')', ')'), ('it', 'PRP'), ('describes', 'VBZ'), ('a', 'DT'), ('stream', 'NN'), ('of', 'IN'), ('text', 'NN'), (',', ','), ('determine', 'NN'), ('which', 'WDT'), ('items', 'NNS'), ('in', 'IN'), ('the', 'DT'), ('text', 'NN'), ('relates', 'VBZ'), ('to', 'TO'), ('proper', 'VB'), ('names', 'NNS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Named', 'entity', 'recognition', '(', 'NER', ')', 'describes', 'stream', 'text', ',', 'determine', 'items', 'text', 'relates', 'proper', 'names', '.']

 TOTAL FILTERED TOKENS ==>  17

 ---- POST FOR FILTERED TOKENS ----

 [('Named', 'VBN'), ('entity', 'NN'), ('recognition', 'NN'), ('(', '('), ('NER', 'NNP'), (')', ')'), ('describes', 'VBZ'), ('stream', 'JJ'), ('text', 'NN'), (',', ','), ('determine', 'JJ'), ('items', 'NNS'), ('text', 'JJ'), ('relates', 'NNS'), ('proper', 'JJ'), ('names', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Named entity', 'entity recognition', 'recognition (', '( NER', 'NER )', ') describes', 'describes stream', 'stream text', 'text ,', ', determine', 'determine items', 'items text', 'text relates', 'relates proper', 'proper names', 'names .'] 

 TOTAL BIGRAMS --> 16 



 ---- TRI-GRAMS ---- 

 ['Named entity recognition', 'entity recognition (', 'recognition ( NER', '( NER )', 'NER ) describes', ') describes stream', 'describes stream text', 'stream text ,', 'text , determine', ', determine items', 'determine items text', 'items text relates', 'text relates proper', 'relates proper names', 'proper names .'] 

 TOTAL TRIGRAMS --> 15 



 ---- NOUN PHRASES ---- 

 ['entity', 'recognition', 'stream text'] 

 TOTAL NOUN PHRASES --> 3 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['name', 'entiti', 'recognit', '(', 'ner', ')', 'describ', 'stream', 'text', ',', 'determin', 'item', 'text', 'relat', 'proper', 'name', '.']

 TOTAL PORTER STEM WORDS ==> 17



 ---- SNOWBALL STEMMING ----

['name', 'entiti', 'recognit', '(', 'ner', ')', 'describ', 'stream', 'text', ',', 'determin', 'item', 'text', 'relat', 'proper', 'name', '.']

 TOTAL SNOWBALL STEM WORDS ==> 17



 ---- LEMMATIZATION ----

['Named', 'entity', 'recognition', '(', 'NER', ')', 'describes', 'stream', 'text', ',', 'determine', 'item', 'text', 'relates', 'proper', 'name', '.']

 TOTAL LEMMATIZE WORDS ==> 17

************************************************************************************************************************

28 --> Optical character recognition (OCR) it gives  an image representing printed text, which help in determining the corresponding or related  text. 


 ---- TOKENS ----

 ['Optical', 'character', 'recognition', '(', 'OCR', ')', 'it', 'gives', 'an', 'image', 'representing', 'printed', 'text', ',', 'which', 'help', 'in', 'determining', 'the', 'corresponding', 'or', 'related', 'text', '.'] 

 TOTAL TOKENS ==> 24

 ---- POST ----

 [('Optical', 'JJ'), ('character', 'NN'), ('recognition', 'NN'), ('(', '('), ('OCR', 'NNP'), (')', ')'), ('it', 'PRP'), ('gives', 'VBZ'), ('an', 'DT'), ('image', 'NN'), ('representing', 'VBG'), ('printed', 'VBN'), ('text', 'NN'), (',', ','), ('which', 'WDT'), ('help', 'VBP'), ('in', 'IN'), ('determining', 'VBG'), ('the', 'DT'), ('corresponding', 'NN'), ('or', 'CC'), ('related', 'VBN'), ('text', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Optical', 'character', 'recognition', '(', 'OCR', ')', 'gives', 'image', 'representing', 'printed', 'text', ',', 'help', 'determining', 'corresponding', 'related', 'text', '.']

 TOTAL FILTERED TOKENS ==>  18

 ---- POST FOR FILTERED TOKENS ----

 [('Optical', 'JJ'), ('character', 'NN'), ('recognition', 'NN'), ('(', '('), ('OCR', 'NNP'), (')', ')'), ('gives', 'VBZ'), ('image', 'NN'), ('representing', 'VBG'), ('printed', 'VBN'), ('text', 'NN'), (',', ','), ('help', 'NN'), ('determining', 'VBG'), ('corresponding', 'VBG'), ('related', 'JJ'), ('text', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Optical character', 'character recognition', 'recognition (', '( OCR', 'OCR )', ') gives', 'gives image', 'image representing', 'representing printed', 'printed text', 'text ,', ', help', 'help determining', 'determining corresponding', 'corresponding related', 'related text', 'text .'] 

 TOTAL BIGRAMS --> 17 



 ---- TRI-GRAMS ---- 

 ['Optical character recognition', 'character recognition (', 'recognition ( OCR', '( OCR )', 'OCR ) gives', ') gives image', 'gives image representing', 'image representing printed', 'representing printed text', 'printed text ,', 'text , help', ', help determining', 'help determining corresponding', 'determining corresponding related', 'corresponding related text', 'related text .'] 

 TOTAL TRIGRAMS --> 16 



 ---- NOUN PHRASES ---- 

 ['Optical character', 'recognition', 'image', 'text', 'help', 'related text'] 

 TOTAL NOUN PHRASES --> 6 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> ['Optical']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['optic', 'charact', 'recognit', '(', 'ocr', ')', 'give', 'imag', 'repres', 'print', 'text', ',', 'help', 'determin', 'correspond', 'relat', 'text', '.']

 TOTAL PORTER STEM WORDS ==> 18



 ---- SNOWBALL STEMMING ----

['optic', 'charact', 'recognit', '(', 'ocr', ')', 'give', 'imag', 'repres', 'print', 'text', ',', 'help', 'determin', 'correspond', 'relat', 'text', '.']

 TOTAL SNOWBALL STEM WORDS ==> 18



 ---- LEMMATIZATION ----

['Optical', 'character', 'recognition', '(', 'OCR', ')', 'give', 'image', 'representing', 'printed', 'text', ',', 'help', 'determining', 'corresponding', 'related', 'text', '.']

 TOTAL LEMMATIZE WORDS ==> 18

************************************************************************************************************************

29 --> Part of speech tagging it describes a sentence, determines the part of speech for each  word. 


 ---- TOKENS ----

 ['Part', 'of', 'speech', 'tagging', 'it', 'describes', 'a', 'sentence', ',', 'determines', 'the', 'part', 'of', 'speech', 'for', 'each', 'word', '.'] 

 TOTAL TOKENS ==> 18

 ---- POST ----

 [('Part', 'NN'), ('of', 'IN'), ('speech', 'NN'), ('tagging', 'VBG'), ('it', 'PRP'), ('describes', 'VBZ'), ('a', 'DT'), ('sentence', 'NN'), (',', ','), ('determines', 'VBZ'), ('the', 'DT'), ('part', 'NN'), ('of', 'IN'), ('speech', 'NN'), ('for', 'IN'), ('each', 'DT'), ('word', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Part', 'speech', 'tagging', 'describes', 'sentence', ',', 'determines', 'part', 'speech', 'word', '.']

 TOTAL FILTERED TOKENS ==>  11

 ---- POST FOR FILTERED TOKENS ----

 [('Part', 'NN'), ('speech', 'NN'), ('tagging', 'VBG'), ('describes', 'JJ'), ('sentence', 'NN'), (',', ','), ('determines', 'NNS'), ('part', 'NN'), ('speech', 'NN'), ('word', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Part speech', 'speech tagging', 'tagging describes', 'describes sentence', 'sentence ,', ', determines', 'determines part', 'part speech', 'speech word', 'word .'] 

 TOTAL BIGRAMS --> 10 



 ---- TRI-GRAMS ---- 

 ['Part speech tagging', 'speech tagging describes', 'tagging describes sentence', 'describes sentence ,', 'sentence , determines', ', determines part', 'determines part speech', 'part speech word', 'speech word .'] 

 TOTAL TRIGRAMS --> 9 



 ---- NOUN PHRASES ---- 

 ['Part', 'speech', 'describes sentence', 'part', 'speech', 'word'] 

 TOTAL NOUN PHRASES --> 6 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> ['Part']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['part', 'speech', 'tag', 'describ', 'sentenc', ',', 'determin', 'part', 'speech', 'word', '.']

 TOTAL PORTER STEM WORDS ==> 11



 ---- SNOWBALL STEMMING ----

['part', 'speech', 'tag', 'describ', 'sentenc', ',', 'determin', 'part', 'speech', 'word', '.']

 TOTAL SNOWBALL STEM WORDS ==> 11



 ---- LEMMATIZATION ----

['Part', 'speech', 'tagging', 'describes', 'sentence', ',', 'determines', 'part', 'speech', 'word', '.']

 TOTAL LEMMATIZE WORDS ==> 11

************************************************************************************************************************

30 --> Though NLP tasks are obviously very closely interweaved but they are used  frequently, for convenience. 


 ---- TOKENS ----

 ['Though', 'NLP', 'tasks', 'are', 'obviously', 'very', 'closely', 'interweaved', 'but', 'they', 'are', 'used', 'frequently', ',', 'for', 'convenience', '.'] 

 TOTAL TOKENS ==> 17

 ---- POST ----

 [('Though', 'IN'), ('NLP', 'NNP'), ('tasks', 'NNS'), ('are', 'VBP'), ('obviously', 'RB'), ('very', 'RB'), ('closely', 'RB'), ('interweaved', 'VBN'), ('but', 'CC'), ('they', 'PRP'), ('are', 'VBP'), ('used', 'VBN'), ('frequently', 'RB'), (',', ','), ('for', 'IN'), ('convenience', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Though', 'NLP', 'tasks', 'obviously', 'closely', 'interweaved', 'used', 'frequently', ',', 'convenience', '.']

 TOTAL FILTERED TOKENS ==>  11

 ---- POST FOR FILTERED TOKENS ----

 [('Though', 'IN'), ('NLP', 'NNP'), ('tasks', 'NNS'), ('obviously', 'RB'), ('closely', 'RB'), ('interweaved', 'VBN'), ('used', 'VBN'), ('frequently', 'RB'), (',', ','), ('convenience', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Though NLP', 'NLP tasks', 'tasks obviously', 'obviously closely', 'closely interweaved', 'interweaved used', 'used frequently', 'frequently ,', ', convenience', 'convenience .'] 

 TOTAL BIGRAMS --> 10 



 ---- TRI-GRAMS ---- 

 ['Though NLP tasks', 'NLP tasks obviously', 'tasks obviously closely', 'obviously closely interweaved', 'closely interweaved used', 'interweaved used frequently', 'used frequently ,', 'frequently , convenience', ', convenience .'] 

 TOTAL TRIGRAMS --> 9 



 ---- NOUN PHRASES ---- 

 ['convenience'] 

 TOTAL NOUN PHRASES --> 1 



 ---- NER ----

 
 ORGANIZATION ---> ['NLP']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['though', 'nlp', 'task', 'obvious', 'close', 'interweav', 'use', 'frequent', ',', 'conveni', '.']

 TOTAL PORTER STEM WORDS ==> 11



 ---- SNOWBALL STEMMING ----

['though', 'nlp', 'task', 'obvious', 'close', 'interweav', 'use', 'frequent', ',', 'conveni', '.']

 TOTAL SNOWBALL STEM WORDS ==> 11



 ---- LEMMATIZATION ----

['Though', 'NLP', 'task', 'obviously', 'closely', 'interweaved', 'used', 'frequently', ',', 'convenience', '.']

 TOTAL LEMMATIZE WORDS ==> 11

************************************************************************************************************************

31 --> Some of the task such as automatic summarisation, co-reference  analysis etc. 


 ---- TOKENS ----

 ['Some', 'of', 'the', 'task', 'such', 'as', 'automatic', 'summarisation', ',', 'co-reference', 'analysis', 'etc', '.'] 

 TOTAL TOKENS ==> 13

 ---- POST ----

 [('Some', 'DT'), ('of', 'IN'), ('the', 'DT'), ('task', 'NN'), ('such', 'JJ'), ('as', 'IN'), ('automatic', 'JJ'), ('summarisation', 'NN'), (',', ','), ('co-reference', 'NN'), ('analysis', 'NN'), ('etc', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['task', 'automatic', 'summarisation', ',', 'co-reference', 'analysis', 'etc', '.']

 TOTAL FILTERED TOKENS ==>  8

 ---- POST FOR FILTERED TOKENS ----

 [('task', 'NN'), ('automatic', 'JJ'), ('summarisation', 'NN'), (',', ','), ('co-reference', 'NN'), ('analysis', 'NN'), ('etc', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['task automatic', 'automatic summarisation', 'summarisation ,', ', co-reference', 'co-reference analysis', 'analysis etc', 'etc .'] 

 TOTAL BIGRAMS --> 7 



 ---- TRI-GRAMS ---- 

 ['task automatic summarisation', 'automatic summarisation ,', 'summarisation , co-reference', ', co-reference analysis', 'co-reference analysis etc', 'analysis etc .'] 

 TOTAL TRIGRAMS --> 6 



 ---- NOUN PHRASES ---- 

 ['task', 'automatic summarisation', 'co-reference', 'analysis', 'etc'] 

 TOTAL NOUN PHRASES --> 5 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['task', 'automat', 'summaris', ',', 'co-refer', 'analysi', 'etc', '.']

 TOTAL PORTER STEM WORDS ==> 8



 ---- SNOWBALL STEMMING ----

['task', 'automat', 'summaris', ',', 'co-refer', 'analysi', 'etc', '.']

 TOTAL SNOWBALL STEM WORDS ==> 8



 ---- LEMMATIZATION ----

['task', 'automatic', 'summarisation', ',', 'co-reference', 'analysis', 'etc', '.']

 TOTAL LEMMATIZE WORDS ==> 8

************************************************************************************************************************

32 --> act as subtask that are used in solving larger tasks. 


 ---- TOKENS ----

 ['act', 'as', 'subtask', 'that', 'are', 'used', 'in', 'solving', 'larger', 'tasks', '.'] 

 TOTAL TOKENS ==> 11

 ---- POST ----

 [('act', 'NN'), ('as', 'IN'), ('subtask', 'NN'), ('that', 'WDT'), ('are', 'VBP'), ('used', 'VBN'), ('in', 'IN'), ('solving', 'VBG'), ('larger', 'JJR'), ('tasks', 'NNS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['act', 'subtask', 'used', 'solving', 'larger', 'tasks', '.']

 TOTAL FILTERED TOKENS ==>  7

 ---- POST FOR FILTERED TOKENS ----

 [('act', 'NN'), ('subtask', 'NN'), ('used', 'VBN'), ('solving', 'VBG'), ('larger', 'JJR'), ('tasks', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['act subtask', 'subtask used', 'used solving', 'solving larger', 'larger tasks', 'tasks .'] 

 TOTAL BIGRAMS --> 6 



 ---- TRI-GRAMS ---- 

 ['act subtask used', 'subtask used solving', 'used solving larger', 'solving larger tasks', 'larger tasks .'] 

 TOTAL TRIGRAMS --> 5 



 ---- NOUN PHRASES ---- 

 ['act', 'subtask'] 

 TOTAL NOUN PHRASES --> 2 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['act', 'subtask', 'use', 'solv', 'larger', 'task', '.']

 TOTAL PORTER STEM WORDS ==> 7



 ---- SNOWBALL STEMMING ----

['act', 'subtask', 'use', 'solv', 'larger', 'task', '.']

 TOTAL SNOWBALL STEM WORDS ==> 7



 ---- LEMMATIZATION ----

['act', 'subtask', 'used', 'solving', 'larger', 'task', '.']

 TOTAL LEMMATIZE WORDS ==> 7

************************************************************************************************************************

33 --> The goal of Natural Language Processing is to accommodate one or more specialities of an  algorithm or system. 


 ---- TOKENS ----

 ['The', 'goal', 'of', 'Natural', 'Language', 'Processing', 'is', 'to', 'accommodate', 'one', 'or', 'more', 'specialities', 'of', 'an', 'algorithm', 'or', 'system', '.'] 

 TOTAL TOKENS ==> 19

 ---- POST ----

 [('The', 'DT'), ('goal', 'NN'), ('of', 'IN'), ('Natural', 'NNP'), ('Language', 'NNP'), ('Processing', 'NNP'), ('is', 'VBZ'), ('to', 'TO'), ('accommodate', 'VB'), ('one', 'CD'), ('or', 'CC'), ('more', 'JJR'), ('specialities', 'NNS'), ('of', 'IN'), ('an', 'DT'), ('algorithm', 'NN'), ('or', 'CC'), ('system', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['goal', 'Natural', 'Language', 'Processing', 'accommodate', 'one', 'specialities', 'algorithm', 'system', '.']

 TOTAL FILTERED TOKENS ==>  10

 ---- POST FOR FILTERED TOKENS ----

 [('goal', 'NN'), ('Natural', 'NNP'), ('Language', 'NNP'), ('Processing', 'NNP'), ('accommodate', 'VB'), ('one', 'CD'), ('specialities', 'NNS'), ('algorithm', 'VBZ'), ('system', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['goal Natural', 'Natural Language', 'Language Processing', 'Processing accommodate', 'accommodate one', 'one specialities', 'specialities algorithm', 'algorithm system', 'system .'] 

 TOTAL BIGRAMS --> 9 



 ---- TRI-GRAMS ---- 

 ['goal Natural Language', 'Natural Language Processing', 'Language Processing accommodate', 'Processing accommodate one', 'accommodate one specialities', 'one specialities algorithm', 'specialities algorithm system', 'algorithm system .'] 

 TOTAL TRIGRAMS --> 8 



 ---- NOUN PHRASES ---- 

 ['goal', 'system'] 

 TOTAL NOUN PHRASES --> 2 



 ---- NER ----

 
 ORGANIZATION ---> ['Natural Language']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['goal', 'natur', 'languag', 'process', 'accommod', 'one', 'special', 'algorithm', 'system', '.']

 TOTAL PORTER STEM WORDS ==> 10



 ---- SNOWBALL STEMMING ----

['goal', 'natur', 'languag', 'process', 'accommod', 'one', 'special', 'algorithm', 'system', '.']

 TOTAL SNOWBALL STEM WORDS ==> 10



 ---- LEMMATIZATION ----

['goal', 'Natural', 'Language', 'Processing', 'accommodate', 'one', 'speciality', 'algorithm', 'system', '.']

 TOTAL LEMMATIZE WORDS ==> 10

************************************************************************************************************************

34 --> The metric of NLP assess on an algorithmic system allows for the  integration of language understanding and language generation. 


 ---- TOKENS ----

 ['The', 'metric', 'of', 'NLP', 'assess', 'on', 'an', 'algorithmic', 'system', 'allows', 'for', 'the', 'integration', 'of', 'language', 'understanding', 'and', 'language', 'generation', '.'] 

 TOTAL TOKENS ==> 20

 ---- POST ----

 [('The', 'DT'), ('metric', 'JJ'), ('of', 'IN'), ('NLP', 'NNP'), ('assess', 'NN'), ('on', 'IN'), ('an', 'DT'), ('algorithmic', 'JJ'), ('system', 'NN'), ('allows', 'VBZ'), ('for', 'IN'), ('the', 'DT'), ('integration', 'NN'), ('of', 'IN'), ('language', 'NN'), ('understanding', 'NN'), ('and', 'CC'), ('language', 'NN'), ('generation', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['metric', 'NLP', 'assess', 'algorithmic', 'system', 'allows', 'integration', 'language', 'understanding', 'language', 'generation', '.']

 TOTAL FILTERED TOKENS ==>  12

 ---- POST FOR FILTERED TOKENS ----

 [('metric', 'JJ'), ('NLP', 'NNP'), ('assess', 'NN'), ('algorithmic', 'JJ'), ('system', 'NN'), ('allows', 'VBZ'), ('integration', 'NN'), ('language', 'NN'), ('understanding', 'JJ'), ('language', 'NN'), ('generation', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['metric NLP', 'NLP assess', 'assess algorithmic', 'algorithmic system', 'system allows', 'allows integration', 'integration language', 'language understanding', 'understanding language', 'language generation', 'generation .'] 

 TOTAL BIGRAMS --> 11 



 ---- TRI-GRAMS ---- 

 ['metric NLP assess', 'NLP assess algorithmic', 'assess algorithmic system', 'algorithmic system allows', 'system allows integration', 'allows integration language', 'integration language understanding', 'language understanding language', 'understanding language generation', 'language generation .'] 

 TOTAL TRIGRAMS --> 10 



 ---- NOUN PHRASES ---- 

 ['assess', 'algorithmic system', 'integration', 'language', 'understanding language', 'generation'] 

 TOTAL NOUN PHRASES --> 6 



 ---- NER ----

 
 ORGANIZATION ---> ['NLP']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['metric', 'nlp', 'assess', 'algorithm', 'system', 'allow', 'integr', 'languag', 'understand', 'languag', 'gener', '.']

 TOTAL PORTER STEM WORDS ==> 12



 ---- SNOWBALL STEMMING ----

['metric', 'nlp', 'assess', 'algorithm', 'system', 'allow', 'integr', 'languag', 'understand', 'languag', 'generat', '.']

 TOTAL SNOWBALL STEM WORDS ==> 12



 ---- LEMMATIZATION ----

['metric', 'NLP', 'ass', 'algorithmic', 'system', 'allows', 'integration', 'language', 'understanding', 'language', 'generation', '.']

 TOTAL LEMMATIZE WORDS ==> 12

************************************************************************************************************************

35 --> It is even used in  multilingual event detection Rospocher et al. 


 ---- TOKENS ----

 ['It', 'is', 'even', 'used', 'in', 'multilingual', 'event', 'detection', 'Rospocher', 'et', 'al', '.'] 

 TOTAL TOKENS ==> 12

 ---- POST ----

 [('It', 'PRP'), ('is', 'VBZ'), ('even', 'RB'), ('used', 'VBN'), ('in', 'IN'), ('multilingual', 'JJ'), ('event', 'NN'), ('detection', 'NN'), ('Rospocher', 'NNP'), ('et', 'CC'), ('al', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['even', 'used', 'multilingual', 'event', 'detection', 'Rospocher', 'et', 'al', '.']

 TOTAL FILTERED TOKENS ==>  9

 ---- POST FOR FILTERED TOKENS ----

 [('even', 'RB'), ('used', 'VBN'), ('multilingual', 'JJ'), ('event', 'NN'), ('detection', 'NN'), ('Rospocher', 'NNP'), ('et', 'CC'), ('al', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['even used', 'used multilingual', 'multilingual event', 'event detection', 'detection Rospocher', 'Rospocher et', 'et al', 'al .'] 

 TOTAL BIGRAMS --> 8 



 ---- TRI-GRAMS ---- 

 ['even used multilingual', 'used multilingual event', 'multilingual event detection', 'event detection Rospocher', 'detection Rospocher et', 'Rospocher et al', 'et al .'] 

 TOTAL TRIGRAMS --> 7 



 ---- NOUN PHRASES ---- 

 ['multilingual event', 'detection', 'al'] 

 TOTAL NOUN PHRASES --> 3 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> ['Rospocher']
 TOTAL PERSON ENTITY --> 1 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['even', 'use', 'multilingu', 'event', 'detect', 'rospoch', 'et', 'al', '.']

 TOTAL PORTER STEM WORDS ==> 9



 ---- SNOWBALL STEMMING ----

['even', 'use', 'multilingu', 'event', 'detect', 'rospoch', 'et', 'al', '.']

 TOTAL SNOWBALL STEM WORDS ==> 9



 ---- LEMMATIZATION ----

['even', 'used', 'multilingual', 'event', 'detection', 'Rospocher', 'et', 'al', '.']

 TOTAL LEMMATIZE WORDS ==> 9

************************************************************************************************************************

36 --> [2] purposed a novel modular system for cross- lingual event extraction for English, Dutch and Italian texts by using different pipelines for  different languages. 


 ---- TOKENS ----

 ['[', '2', ']', 'purposed', 'a', 'novel', 'modular', 'system', 'for', 'cross-', 'lingual', 'event', 'extraction', 'for', 'English', ',', 'Dutch', 'and', 'Italian', 'texts', 'by', 'using', 'different', 'pipelines', 'for', 'different', 'languages', '.'] 

 TOTAL TOKENS ==> 28

 ---- POST ----

 [('[', 'RB'), ('2', 'CD'), (']', 'NNS'), ('purposed', 'VBD'), ('a', 'DT'), ('novel', 'JJ'), ('modular', 'NN'), ('system', 'NN'), ('for', 'IN'), ('cross-', 'JJ'), ('lingual', 'JJ'), ('event', 'NN'), ('extraction', 'NN'), ('for', 'IN'), ('English', 'NNP'), (',', ','), ('Dutch', 'NNP'), ('and', 'CC'), ('Italian', 'JJ'), ('texts', 'NN'), ('by', 'IN'), ('using', 'VBG'), ('different', 'JJ'), ('pipelines', 'NNS'), ('for', 'IN'), ('different', 'JJ'), ('languages', 'NNS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['[', '2', ']', 'purposed', 'novel', 'modular', 'system', 'cross-', 'lingual', 'event', 'extraction', 'English', ',', 'Dutch', 'Italian', 'texts', 'using', 'different', 'pipelines', 'different', 'languages', '.']

 TOTAL FILTERED TOKENS ==>  22

 ---- POST FOR FILTERED TOKENS ----

 [('[', 'RB'), ('2', 'CD'), (']', 'NNS'), ('purposed', 'VBD'), ('novel', 'JJ'), ('modular', 'JJ'), ('system', 'NN'), ('cross-', 'JJ'), ('lingual', 'JJ'), ('event', 'NN'), ('extraction', 'NN'), ('English', 'NNP'), (',', ','), ('Dutch', 'NNP'), ('Italian', 'NNP'), ('texts', 'NN'), ('using', 'VBG'), ('different', 'JJ'), ('pipelines', 'NNS'), ('different', 'JJ'), ('languages', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['[ 2', '2 ]', '] purposed', 'purposed novel', 'novel modular', 'modular system', 'system cross-', 'cross- lingual', 'lingual event', 'event extraction', 'extraction English', 'English ,', ', Dutch', 'Dutch Italian', 'Italian texts', 'texts using', 'using different', 'different pipelines', 'pipelines different', 'different languages', 'languages .'] 

 TOTAL BIGRAMS --> 21 



 ---- TRI-GRAMS ---- 

 ['[ 2 ]', '2 ] purposed', '] purposed novel', 'purposed novel modular', 'novel modular system', 'modular system cross-', 'system cross- lingual', 'cross- lingual event', 'lingual event extraction', 'event extraction English', 'extraction English ,', 'English , Dutch', ', Dutch Italian', 'Dutch Italian texts', 'Italian texts using', 'texts using different', 'using different pipelines', 'different pipelines different', 'pipelines different languages', 'different languages .'] 

 TOTAL TRIGRAMS --> 20 



 ---- NOUN PHRASES ---- 

 ['novel modular system', 'cross- lingual event', 'extraction', 'texts'] 

 TOTAL NOUN PHRASES --> 4 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> ['Dutch Italian']
 TOTAL PERSON ENTITY --> 1 


 GPE ---> ['English']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['[', '2', ']', 'purpos', 'novel', 'modular', 'system', 'cross-', 'lingual', 'event', 'extract', 'english', ',', 'dutch', 'italian', 'text', 'use', 'differ', 'pipelin', 'differ', 'languag', '.']

 TOTAL PORTER STEM WORDS ==> 22



 ---- SNOWBALL STEMMING ----

['[', '2', ']', 'purpos', 'novel', 'modular', 'system', 'cross-', 'lingual', 'event', 'extract', 'english', ',', 'dutch', 'italian', 'text', 'use', 'differ', 'pipelin', 'differ', 'languag', '.']

 TOTAL SNOWBALL STEM WORDS ==> 22



 ---- LEMMATIZATION ----

['[', '2', ']', 'purposed', 'novel', 'modular', 'system', 'cross-', 'lingual', 'event', 'extraction', 'English', ',', 'Dutch', 'Italian', 'text', 'using', 'different', 'pipeline', 'different', 'language', '.']

 TOTAL LEMMATIZE WORDS ==> 22

************************************************************************************************************************

37 --> The system incorporates a modular set of foremost multilingual Natural  Language Processing (NLP) tools. 


 ---- TOKENS ----

 ['The', 'system', 'incorporates', 'a', 'modular', 'set', 'of', 'foremost', 'multilingual', 'Natural', 'Language', 'Processing', '(', 'NLP', ')', 'tools', '.'] 

 TOTAL TOKENS ==> 17

 ---- POST ----

 [('The', 'DT'), ('system', 'NN'), ('incorporates', 'VBZ'), ('a', 'DT'), ('modular', 'JJ'), ('set', 'NN'), ('of', 'IN'), ('foremost', 'JJ'), ('multilingual', 'JJ'), ('Natural', 'NNP'), ('Language', 'NNP'), ('Processing', 'NNP'), ('(', '('), ('NLP', 'NNP'), (')', ')'), ('tools', 'NNS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['system', 'incorporates', 'modular', 'set', 'foremost', 'multilingual', 'Natural', 'Language', 'Processing', '(', 'NLP', ')', 'tools', '.']

 TOTAL FILTERED TOKENS ==>  14

 ---- POST FOR FILTERED TOKENS ----

 [('system', 'NN'), ('incorporates', 'VBZ'), ('modular', 'JJ'), ('set', 'NN'), ('foremost', 'NN'), ('multilingual', 'JJ'), ('Natural', 'NNP'), ('Language', 'NNP'), ('Processing', 'NNP'), ('(', '('), ('NLP', 'NNP'), (')', ')'), ('tools', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['system incorporates', 'incorporates modular', 'modular set', 'set foremost', 'foremost multilingual', 'multilingual Natural', 'Natural Language', 'Language Processing', 'Processing (', '( NLP', 'NLP )', ') tools', 'tools .'] 

 TOTAL BIGRAMS --> 13 



 ---- TRI-GRAMS ---- 

 ['system incorporates modular', 'incorporates modular set', 'modular set foremost', 'set foremost multilingual', 'foremost multilingual Natural', 'multilingual Natural Language', 'Natural Language Processing', 'Language Processing (', 'Processing ( NLP', '( NLP )', 'NLP ) tools', ') tools .'] 

 TOTAL TRIGRAMS --> 12 



 ---- NOUN PHRASES ---- 

 ['system', 'modular set', 'foremost'] 

 TOTAL NOUN PHRASES --> 3 



 ---- NER ----

 
 ORGANIZATION ---> ['Natural Language']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['system', 'incorpor', 'modular', 'set', 'foremost', 'multilingu', 'natur', 'languag', 'process', '(', 'nlp', ')', 'tool', '.']

 TOTAL PORTER STEM WORDS ==> 14



 ---- SNOWBALL STEMMING ----

['system', 'incorpor', 'modular', 'set', 'foremost', 'multilingu', 'natur', 'languag', 'process', '(', 'nlp', ')', 'tool', '.']

 TOTAL SNOWBALL STEM WORDS ==> 14



 ---- LEMMATIZATION ----

['system', 'incorporates', 'modular', 'set', 'foremost', 'multilingual', 'Natural', 'Language', 'Processing', '(', 'NLP', ')', 'tool', '.']

 TOTAL LEMMATIZE WORDS ==> 14

************************************************************************************************************************

38 --> The pipeline integrates modules for basic NLP processing  as well as more advanced tasks such as cross-lingual named entity linking, semantic role  labelling and time normalization. 


 ---- TOKENS ----

 ['The', 'pipeline', 'integrates', 'modules', 'for', 'basic', 'NLP', 'processing', 'as', 'well', 'as', 'more', 'advanced', 'tasks', 'such', 'as', 'cross-lingual', 'named', 'entity', 'linking', ',', 'semantic', 'role', 'labelling', 'and', 'time', 'normalization', '.'] 

 TOTAL TOKENS ==> 28

 ---- POST ----

 [('The', 'DT'), ('pipeline', 'NN'), ('integrates', 'VBZ'), ('modules', 'NNS'), ('for', 'IN'), ('basic', 'JJ'), ('NLP', 'NNP'), ('processing', 'NN'), ('as', 'RB'), ('well', 'RB'), ('as', 'IN'), ('more', 'RBR'), ('advanced', 'JJ'), ('tasks', 'NNS'), ('such', 'JJ'), ('as', 'IN'), ('cross-lingual', 'JJ'), ('named', 'VBN'), ('entity', 'NN'), ('linking', 'VBG'), (',', ','), ('semantic', 'JJ'), ('role', 'NN'), ('labelling', 'NN'), ('and', 'CC'), ('time', 'NN'), ('normalization', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['pipeline', 'integrates', 'modules', 'basic', 'NLP', 'processing', 'well', 'advanced', 'tasks', 'cross-lingual', 'named', 'entity', 'linking', ',', 'semantic', 'role', 'labelling', 'time', 'normalization', '.']

 TOTAL FILTERED TOKENS ==>  20

 ---- POST FOR FILTERED TOKENS ----

 [('pipeline', 'NN'), ('integrates', 'VBZ'), ('modules', 'NNS'), ('basic', 'JJ'), ('NLP', 'NNP'), ('processing', 'NN'), ('well', 'RB'), ('advanced', 'JJ'), ('tasks', 'NNS'), ('cross-lingual', 'JJ'), ('named', 'VBN'), ('entity', 'NN'), ('linking', 'VBG'), (',', ','), ('semantic', 'JJ'), ('role', 'NN'), ('labelling', 'NN'), ('time', 'NN'), ('normalization', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['pipeline integrates', 'integrates modules', 'modules basic', 'basic NLP', 'NLP processing', 'processing well', 'well advanced', 'advanced tasks', 'tasks cross-lingual', 'cross-lingual named', 'named entity', 'entity linking', 'linking ,', ', semantic', 'semantic role', 'role labelling', 'labelling time', 'time normalization', 'normalization .'] 

 TOTAL BIGRAMS --> 19 



 ---- TRI-GRAMS ---- 

 ['pipeline integrates modules', 'integrates modules basic', 'modules basic NLP', 'basic NLP processing', 'NLP processing well', 'processing well advanced', 'well advanced tasks', 'advanced tasks cross-lingual', 'tasks cross-lingual named', 'cross-lingual named entity', 'named entity linking', 'entity linking ,', 'linking , semantic', ', semantic role', 'semantic role labelling', 'role labelling time', 'labelling time normalization', 'time normalization .'] 

 TOTAL TRIGRAMS --> 18 



 ---- NOUN PHRASES ---- 

 ['pipeline', 'processing', 'entity', 'semantic role', 'labelling', 'time', 'normalization'] 

 TOTAL NOUN PHRASES --> 7 



 ---- NER ----

 
 ORGANIZATION ---> ['NLP']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['pipelin', 'integr', 'modul', 'basic', 'nlp', 'process', 'well', 'advanc', 'task', 'cross-lingu', 'name', 'entiti', 'link', ',', 'semant', 'role', 'label', 'time', 'normal', '.']

 TOTAL PORTER STEM WORDS ==> 20



 ---- SNOWBALL STEMMING ----

['pipelin', 'integr', 'modul', 'basic', 'nlp', 'process', 'well', 'advanc', 'task', 'cross-lingu', 'name', 'entiti', 'link', ',', 'semant', 'role', 'label', 'time', 'normal', '.']

 TOTAL SNOWBALL STEM WORDS ==> 20



 ---- LEMMATIZATION ----

['pipeline', 'integrates', 'module', 'basic', 'NLP', 'processing', 'well', 'advanced', 'task', 'cross-lingual', 'named', 'entity', 'linking', ',', 'semantic', 'role', 'labelling', 'time', 'normalization', '.']

 TOTAL LEMMATIZE WORDS ==> 20

************************************************************************************************************************

39 --> Thus, the cross-lingual framework allows for the  interpretation of events, participants, locations and time, as well as the relations between  them. 


 ---- TOKENS ----

 ['Thus', ',', 'the', 'cross-lingual', 'framework', 'allows', 'for', 'the', 'interpretation', 'of', 'events', ',', 'participants', ',', 'locations', 'and', 'time', ',', 'as', 'well', 'as', 'the', 'relations', 'between', 'them', '.'] 

 TOTAL TOKENS ==> 26

 ---- POST ----

 [('Thus', 'RB'), (',', ','), ('the', 'DT'), ('cross-lingual', 'JJ'), ('framework', 'NN'), ('allows', 'NNS'), ('for', 'IN'), ('the', 'DT'), ('interpretation', 'NN'), ('of', 'IN'), ('events', 'NNS'), (',', ','), ('participants', 'NNS'), (',', ','), ('locations', 'NNS'), ('and', 'CC'), ('time', 'NN'), (',', ','), ('as', 'RB'), ('well', 'RB'), ('as', 'IN'), ('the', 'DT'), ('relations', 'NNS'), ('between', 'IN'), ('them', 'PRP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Thus', ',', 'cross-lingual', 'framework', 'allows', 'interpretation', 'events', ',', 'participants', ',', 'locations', 'time', ',', 'well', 'relations', '.']

 TOTAL FILTERED TOKENS ==>  16

 ---- POST FOR FILTERED TOKENS ----

 [('Thus', 'RB'), (',', ','), ('cross-lingual', 'JJ'), ('framework', 'NN'), ('allows', 'VBZ'), ('interpretation', 'NN'), ('events', 'NNS'), (',', ','), ('participants', 'NNS'), (',', ','), ('locations', 'NNS'), ('time', 'NN'), (',', ','), ('well', 'RB'), ('relations', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Thus ,', ', cross-lingual', 'cross-lingual framework', 'framework allows', 'allows interpretation', 'interpretation events', 'events ,', ', participants', 'participants ,', ', locations', 'locations time', 'time ,', ', well', 'well relations', 'relations .'] 

 TOTAL BIGRAMS --> 15 



 ---- TRI-GRAMS ---- 

 ['Thus , cross-lingual', ', cross-lingual framework', 'cross-lingual framework allows', 'framework allows interpretation', 'allows interpretation events', 'interpretation events ,', 'events , participants', ', participants ,', 'participants , locations', ', locations time', 'locations time ,', 'time , well', ', well relations', 'well relations .'] 

 TOTAL TRIGRAMS --> 14 



 ---- NOUN PHRASES ---- 

 ['cross-lingual framework', 'interpretation', 'time'] 

 TOTAL NOUN PHRASES --> 3 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['thu', ',', 'cross-lingu', 'framework', 'allow', 'interpret', 'event', ',', 'particip', ',', 'locat', 'time', ',', 'well', 'relat', '.']

 TOTAL PORTER STEM WORDS ==> 16



 ---- SNOWBALL STEMMING ----

['thus', ',', 'cross-lingu', 'framework', 'allow', 'interpret', 'event', ',', 'particip', ',', 'locat', 'time', ',', 'well', 'relat', '.']

 TOTAL SNOWBALL STEM WORDS ==> 16



 ---- LEMMATIZATION ----

['Thus', ',', 'cross-lingual', 'framework', 'allows', 'interpretation', 'event', ',', 'participant', ',', 'location', 'time', ',', 'well', 'relation', '.']

 TOTAL LEMMATIZE WORDS ==> 16

************************************************************************************************************************

40 --> Output of these individual pipelines is intended to be used as input for a system that  obtains event centric knowledge graphs. 


 ---- TOKENS ----

 ['Output', 'of', 'these', 'individual', 'pipelines', 'is', 'intended', 'to', 'be', 'used', 'as', 'input', 'for', 'a', 'system', 'that', 'obtains', 'event', 'centric', 'knowledge', 'graphs', '.'] 

 TOTAL TOKENS ==> 22

 ---- POST ----

 [('Output', 'NN'), ('of', 'IN'), ('these', 'DT'), ('individual', 'JJ'), ('pipelines', 'NNS'), ('is', 'VBZ'), ('intended', 'VBN'), ('to', 'TO'), ('be', 'VB'), ('used', 'VBN'), ('as', 'IN'), ('input', 'NN'), ('for', 'IN'), ('a', 'DT'), ('system', 'NN'), ('that', 'WDT'), ('obtains', 'VBZ'), ('event', 'NN'), ('centric', 'NN'), ('knowledge', 'NN'), ('graphs', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Output', 'individual', 'pipelines', 'intended', 'used', 'input', 'system', 'obtains', 'event', 'centric', 'knowledge', 'graphs', '.']

 TOTAL FILTERED TOKENS ==>  13

 ---- POST FOR FILTERED TOKENS ----

 [('Output', 'NNP'), ('individual', 'JJ'), ('pipelines', 'NNS'), ('intended', 'VBN'), ('used', 'VBN'), ('input', 'NN'), ('system', 'NN'), ('obtains', 'VBZ'), ('event', 'NN'), ('centric', 'NN'), ('knowledge', 'NN'), ('graphs', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Output individual', 'individual pipelines', 'pipelines intended', 'intended used', 'used input', 'input system', 'system obtains', 'obtains event', 'event centric', 'centric knowledge', 'knowledge graphs', 'graphs .'] 

 TOTAL BIGRAMS --> 12 



 ---- TRI-GRAMS ---- 

 ['Output individual pipelines', 'individual pipelines intended', 'pipelines intended used', 'intended used input', 'used input system', 'input system obtains', 'system obtains event', 'obtains event centric', 'event centric knowledge', 'centric knowledge graphs', 'knowledge graphs .'] 

 TOTAL TRIGRAMS --> 11 



 ---- NOUN PHRASES ---- 

 ['input', 'system', 'event', 'centric', 'knowledge', 'graphs'] 

 TOTAL NOUN PHRASES --> 6 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> ['Output']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['output', 'individu', 'pipelin', 'intend', 'use', 'input', 'system', 'obtain', 'event', 'centric', 'knowledg', 'graph', '.']

 TOTAL PORTER STEM WORDS ==> 13



 ---- SNOWBALL STEMMING ----

['output', 'individu', 'pipelin', 'intend', 'use', 'input', 'system', 'obtain', 'event', 'centric', 'knowledg', 'graph', '.']

 TOTAL SNOWBALL STEM WORDS ==> 13



 ---- LEMMATIZATION ----

['Output', 'individual', 'pipeline', 'intended', 'used', 'input', 'system', 'obtains', 'event', 'centric', 'knowledge', 'graph', '.']

 TOTAL LEMMATIZE WORDS ==> 13

************************************************************************************************************************

41 --> All modules behave like UNIX pipes: they all take  standard input, to do some annotation, and produce standard output which in turn is the input  for the next module pipelines are built as a data centric architecture so that modules can be  adapted and replaced. 


 ---- TOKENS ----

 ['All', 'modules', 'behave', 'like', 'UNIX', 'pipes', ':', 'they', 'all', 'take', 'standard', 'input', ',', 'to', 'do', 'some', 'annotation', ',', 'and', 'produce', 'standard', 'output', 'which', 'in', 'turn', 'is', 'the', 'input', 'for', 'the', 'next', 'module', 'pipelines', 'are', 'built', 'as', 'a', 'data', 'centric', 'architecture', 'so', 'that', 'modules', 'can', 'be', 'adapted', 'and', 'replaced', '.'] 

 TOTAL TOKENS ==> 49

 ---- POST ----

 [('All', 'DT'), ('modules', 'NNS'), ('behave', 'VBP'), ('like', 'IN'), ('UNIX', 'NNP'), ('pipes', 'NNS'), (':', ':'), ('they', 'PRP'), ('all', 'DT'), ('take', 'VBP'), ('standard', 'JJ'), ('input', 'NN'), (',', ','), ('to', 'TO'), ('do', 'VB'), ('some', 'DT'), ('annotation', 'NN'), (',', ','), ('and', 'CC'), ('produce', 'VB'), ('standard', 'NN'), ('output', 'NN'), ('which', 'WDT'), ('in', 'IN'), ('turn', 'NN'), ('is', 'VBZ'), ('the', 'DT'), ('input', 'NN'), ('for', 'IN'), ('the', 'DT'), ('next', 'JJ'), ('module', 'NN'), ('pipelines', 'NNS'), ('are', 'VBP'), ('built', 'VBN'), ('as', 'IN'), ('a', 'DT'), ('data', 'NN'), ('centric', 'NN'), ('architecture', 'NN'), ('so', 'IN'), ('that', 'IN'), ('modules', 'NNS'), ('can', 'MD'), ('be', 'VB'), ('adapted', 'VBN'), ('and', 'CC'), ('replaced', 'VBN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['modules', 'behave', 'like', 'UNIX', 'pipes', ':', 'take', 'standard', 'input', ',', 'annotation', ',', 'produce', 'standard', 'output', 'turn', 'input', 'next', 'module', 'pipelines', 'built', 'data', 'centric', 'architecture', 'modules', 'adapted', 'replaced', '.']

 TOTAL FILTERED TOKENS ==>  28

 ---- POST FOR FILTERED TOKENS ----

 [('modules', 'NNS'), ('behave', 'VBP'), ('like', 'IN'), ('UNIX', 'NNP'), ('pipes', 'NNS'), (':', ':'), ('take', 'VB'), ('standard', 'NN'), ('input', 'NN'), (',', ','), ('annotation', 'NN'), (',', ','), ('produce', 'VB'), ('standard', 'JJ'), ('output', 'NN'), ('turn', 'VBP'), ('input', 'VBN'), ('next', 'JJ'), ('module', 'NN'), ('pipelines', 'NNS'), ('built', 'VBN'), ('data', 'NNS'), ('centric', 'JJ'), ('architecture', 'NN'), ('modules', 'NNS'), ('adapted', 'VBD'), ('replaced', 'VBN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['modules behave', 'behave like', 'like UNIX', 'UNIX pipes', 'pipes :', ': take', 'take standard', 'standard input', 'input ,', ', annotation', 'annotation ,', ', produce', 'produce standard', 'standard output', 'output turn', 'turn input', 'input next', 'next module', 'module pipelines', 'pipelines built', 'built data', 'data centric', 'centric architecture', 'architecture modules', 'modules adapted', 'adapted replaced', 'replaced .'] 

 TOTAL BIGRAMS --> 27 



 ---- TRI-GRAMS ---- 

 ['modules behave like', 'behave like UNIX', 'like UNIX pipes', 'UNIX pipes :', 'pipes : take', ': take standard', 'take standard input', 'standard input ,', 'input , annotation', ', annotation ,', 'annotation , produce', ', produce standard', 'produce standard output', 'standard output turn', 'output turn input', 'turn input next', 'input next module', 'next module pipelines', 'module pipelines built', 'pipelines built data', 'built data centric', 'data centric architecture', 'centric architecture modules', 'architecture modules adapted', 'modules adapted replaced', 'adapted replaced .'] 

 TOTAL TRIGRAMS --> 26 



 ---- NOUN PHRASES ---- 

 ['standard', 'input', 'annotation', 'standard output', 'next module', 'centric architecture'] 

 TOTAL NOUN PHRASES --> 6 



 ---- NER ----

 
 ORGANIZATION ---> ['UNIX']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['modul', 'behav', 'like', 'unix', 'pipe', ':', 'take', 'standard', 'input', ',', 'annot', ',', 'produc', 'standard', 'output', 'turn', 'input', 'next', 'modul', 'pipelin', 'built', 'data', 'centric', 'architectur', 'modul', 'adapt', 'replac', '.']

 TOTAL PORTER STEM WORDS ==> 28



 ---- SNOWBALL STEMMING ----

['modul', 'behav', 'like', 'unix', 'pipe', ':', 'take', 'standard', 'input', ',', 'annot', ',', 'produc', 'standard', 'output', 'turn', 'input', 'next', 'modul', 'pipelin', 'built', 'data', 'centric', 'architectur', 'modul', 'adapt', 'replac', '.']

 TOTAL SNOWBALL STEM WORDS ==> 28



 ---- LEMMATIZATION ----

['module', 'behave', 'like', 'UNIX', 'pipe', ':', 'take', 'standard', 'input', ',', 'annotation', ',', 'produce', 'standard', 'output', 'turn', 'input', 'next', 'module', 'pipeline', 'built', 'data', 'centric', 'architecture', 'module', 'adapted', 'replaced', '.']

 TOTAL LEMMATIZE WORDS ==> 28

************************************************************************************************************************

42 --> Furthermore, modular architecture allows for different configurations  and for dynamic distribution. 


 ---- TOKENS ----

 ['Furthermore', ',', 'modular', 'architecture', 'allows', 'for', 'different', 'configurations', 'and', 'for', 'dynamic', 'distribution', '.'] 

 TOTAL TOKENS ==> 13

 ---- POST ----

 [('Furthermore', 'RB'), (',', ','), ('modular', 'JJ'), ('architecture', 'NN'), ('allows', 'VBZ'), ('for', 'IN'), ('different', 'JJ'), ('configurations', 'NNS'), ('and', 'CC'), ('for', 'IN'), ('dynamic', 'JJ'), ('distribution', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Furthermore', ',', 'modular', 'architecture', 'allows', 'different', 'configurations', 'dynamic', 'distribution', '.']

 TOTAL FILTERED TOKENS ==>  10

 ---- POST FOR FILTERED TOKENS ----

 [('Furthermore', 'RB'), (',', ','), ('modular', 'JJ'), ('architecture', 'NN'), ('allows', 'VBZ'), ('different', 'JJ'), ('configurations', 'NNS'), ('dynamic', 'JJ'), ('distribution', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Furthermore ,', ', modular', 'modular architecture', 'architecture allows', 'allows different', 'different configurations', 'configurations dynamic', 'dynamic distribution', 'distribution .'] 

 TOTAL BIGRAMS --> 9 



 ---- TRI-GRAMS ---- 

 ['Furthermore , modular', ', modular architecture', 'modular architecture allows', 'architecture allows different', 'allows different configurations', 'different configurations dynamic', 'configurations dynamic distribution', 'dynamic distribution .'] 

 TOTAL TRIGRAMS --> 8 



 ---- NOUN PHRASES ---- 

 ['modular architecture', 'dynamic distribution'] 

 TOTAL NOUN PHRASES --> 2 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['furthermor', ',', 'modular', 'architectur', 'allow', 'differ', 'configur', 'dynam', 'distribut', '.']

 TOTAL PORTER STEM WORDS ==> 10



 ---- SNOWBALL STEMMING ----

['furthermor', ',', 'modular', 'architectur', 'allow', 'differ', 'configur', 'dynam', 'distribut', '.']

 TOTAL SNOWBALL STEM WORDS ==> 10



 ---- LEMMATIZATION ----

['Furthermore', ',', 'modular', 'architecture', 'allows', 'different', 'configuration', 'dynamic', 'distribution', '.']

 TOTAL LEMMATIZE WORDS ==> 10

************************************************************************************************************************

43 --> Most of the work in Natural Language Processing is conducted by computer scientists while  various other professionals have also shown interest such as linguistics, psychologist and  philosophers etc. 


 ---- TOKENS ----

 ['Most', 'of', 'the', 'work', 'in', 'Natural', 'Language', 'Processing', 'is', 'conducted', 'by', 'computer', 'scientists', 'while', 'various', 'other', 'professionals', 'have', 'also', 'shown', 'interest', 'such', 'as', 'linguistics', ',', 'psychologist', 'and', 'philosophers', 'etc', '.'] 

 TOTAL TOKENS ==> 30

 ---- POST ----

 [('Most', 'JJS'), ('of', 'IN'), ('the', 'DT'), ('work', 'NN'), ('in', 'IN'), ('Natural', 'NNP'), ('Language', 'NNP'), ('Processing', 'NNP'), ('is', 'VBZ'), ('conducted', 'VBN'), ('by', 'IN'), ('computer', 'NN'), ('scientists', 'NNS'), ('while', 'IN'), ('various', 'JJ'), ('other', 'JJ'), ('professionals', 'NNS'), ('have', 'VBP'), ('also', 'RB'), ('shown', 'VBN'), ('interest', 'NN'), ('such', 'JJ'), ('as', 'IN'), ('linguistics', 'NNS'), (',', ','), ('psychologist', 'NN'), ('and', 'CC'), ('philosophers', 'NNS'), ('etc', 'VBP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['work', 'Natural', 'Language', 'Processing', 'conducted', 'computer', 'scientists', 'various', 'professionals', 'also', 'shown', 'interest', 'linguistics', ',', 'psychologist', 'philosophers', 'etc', '.']

 TOTAL FILTERED TOKENS ==>  18

 ---- POST FOR FILTERED TOKENS ----

 [('work', 'NN'), ('Natural', 'NNP'), ('Language', 'NNP'), ('Processing', 'NNP'), ('conducted', 'VBD'), ('computer', 'NN'), ('scientists', 'NNS'), ('various', 'JJ'), ('professionals', 'NNS'), ('also', 'RB'), ('shown', 'VBN'), ('interest', 'NN'), ('linguistics', 'NNS'), (',', ','), ('psychologist', 'NN'), ('philosophers', 'NNS'), ('etc', 'VBP'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['work Natural', 'Natural Language', 'Language Processing', 'Processing conducted', 'conducted computer', 'computer scientists', 'scientists various', 'various professionals', 'professionals also', 'also shown', 'shown interest', 'interest linguistics', 'linguistics ,', ', psychologist', 'psychologist philosophers', 'philosophers etc', 'etc .'] 

 TOTAL BIGRAMS --> 17 



 ---- TRI-GRAMS ---- 

 ['work Natural Language', 'Natural Language Processing', 'Language Processing conducted', 'Processing conducted computer', 'conducted computer scientists', 'computer scientists various', 'scientists various professionals', 'various professionals also', 'professionals also shown', 'also shown interest', 'shown interest linguistics', 'interest linguistics ,', 'linguistics , psychologist', ', psychologist philosophers', 'psychologist philosophers etc', 'philosophers etc .'] 

 TOTAL TRIGRAMS --> 16 



 ---- NOUN PHRASES ---- 

 ['work', 'computer', 'interest', 'psychologist'] 

 TOTAL NOUN PHRASES --> 4 



 ---- NER ----

 
 ORGANIZATION ---> ['Natural Language']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['work', 'natur', 'languag', 'process', 'conduct', 'comput', 'scientist', 'variou', 'profession', 'also', 'shown', 'interest', 'linguist', ',', 'psychologist', 'philosoph', 'etc', '.']

 TOTAL PORTER STEM WORDS ==> 18



 ---- SNOWBALL STEMMING ----

['work', 'natur', 'languag', 'process', 'conduct', 'comput', 'scientist', 'various', 'profession', 'also', 'shown', 'interest', 'linguist', ',', 'psychologist', 'philosoph', 'etc', '.']

 TOTAL SNOWBALL STEM WORDS ==> 18



 ---- LEMMATIZATION ----

['work', 'Natural', 'Language', 'Processing', 'conducted', 'computer', 'scientist', 'various', 'professional', 'also', 'shown', 'interest', 'linguistics', ',', 'psychologist', 'philosopher', 'etc', '.']

 TOTAL LEMMATIZE WORDS ==> 18

************************************************************************************************************************

44 --> One of the most ironical aspect of NLP is that it adds up to the knowledge  of human language. 


 ---- TOKENS ----

 ['One', 'of', 'the', 'most', 'ironical', 'aspect', 'of', 'NLP', 'is', 'that', 'it', 'adds', 'up', 'to', 'the', 'knowledge', 'of', 'human', 'language', '.'] 

 TOTAL TOKENS ==> 20

 ---- POST ----

 [('One', 'CD'), ('of', 'IN'), ('the', 'DT'), ('most', 'RBS'), ('ironical', 'JJ'), ('aspect', 'NN'), ('of', 'IN'), ('NLP', 'NNP'), ('is', 'VBZ'), ('that', 'IN'), ('it', 'PRP'), ('adds', 'VBZ'), ('up', 'RP'), ('to', 'TO'), ('the', 'DT'), ('knowledge', 'NN'), ('of', 'IN'), ('human', 'JJ'), ('language', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['One', 'ironical', 'aspect', 'NLP', 'adds', 'knowledge', 'human', 'language', '.']

 TOTAL FILTERED TOKENS ==>  9

 ---- POST FOR FILTERED TOKENS ----

 [('One', 'CD'), ('ironical', 'JJ'), ('aspect', 'NN'), ('NLP', 'NNP'), ('adds', 'VBZ'), ('knowledge', 'NN'), ('human', 'JJ'), ('language', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['One ironical', 'ironical aspect', 'aspect NLP', 'NLP adds', 'adds knowledge', 'knowledge human', 'human language', 'language .'] 

 TOTAL BIGRAMS --> 8 



 ---- TRI-GRAMS ---- 

 ['One ironical aspect', 'ironical aspect NLP', 'aspect NLP adds', 'NLP adds knowledge', 'adds knowledge human', 'knowledge human language', 'human language .'] 

 TOTAL TRIGRAMS --> 7 



 ---- NOUN PHRASES ---- 

 ['ironical aspect', 'knowledge', 'human language'] 

 TOTAL NOUN PHRASES --> 3 



 ---- NER ----

 
 ORGANIZATION ---> ['NLP']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['one', 'iron', 'aspect', 'nlp', 'add', 'knowledg', 'human', 'languag', '.']

 TOTAL PORTER STEM WORDS ==> 9



 ---- SNOWBALL STEMMING ----

['one', 'iron', 'aspect', 'nlp', 'add', 'knowledg', 'human', 'languag', '.']

 TOTAL SNOWBALL STEM WORDS ==> 9



 ---- LEMMATIZATION ----

['One', 'ironical', 'aspect', 'NLP', 'add', 'knowledge', 'human', 'language', '.']

 TOTAL LEMMATIZE WORDS ==> 9

************************************************************************************************************************

45 --> The field of Natural Language Processing is related with different  theories and techniques that deal with the problem of natural language of communicating  with the computers. 


 ---- TOKENS ----

 ['The', 'field', 'of', 'Natural', 'Language', 'Processing', 'is', 'related', 'with', 'different', 'theories', 'and', 'techniques', 'that', 'deal', 'with', 'the', 'problem', 'of', 'natural', 'language', 'of', 'communicating', 'with', 'the', 'computers', '.'] 

 TOTAL TOKENS ==> 27

 ---- POST ----

 [('The', 'DT'), ('field', 'NN'), ('of', 'IN'), ('Natural', 'NNP'), ('Language', 'NNP'), ('Processing', 'NNP'), ('is', 'VBZ'), ('related', 'VBN'), ('with', 'IN'), ('different', 'JJ'), ('theories', 'NNS'), ('and', 'CC'), ('techniques', 'NNS'), ('that', 'WDT'), ('deal', 'VBP'), ('with', 'IN'), ('the', 'DT'), ('problem', 'NN'), ('of', 'IN'), ('natural', 'JJ'), ('language', 'NN'), ('of', 'IN'), ('communicating', 'VBG'), ('with', 'IN'), ('the', 'DT'), ('computers', 'NNS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['field', 'Natural', 'Language', 'Processing', 'related', 'different', 'theories', 'techniques', 'deal', 'problem', 'natural', 'language', 'communicating', 'computers', '.']

 TOTAL FILTERED TOKENS ==>  15

 ---- POST FOR FILTERED TOKENS ----

 [('field', 'NN'), ('Natural', 'NNP'), ('Language', 'NNP'), ('Processing', 'NNP'), ('related', 'JJ'), ('different', 'JJ'), ('theories', 'NNS'), ('techniques', 'NNS'), ('deal', 'VBP'), ('problem', 'NN'), ('natural', 'JJ'), ('language', 'NN'), ('communicating', 'VBG'), ('computers', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['field Natural', 'Natural Language', 'Language Processing', 'Processing related', 'related different', 'different theories', 'theories techniques', 'techniques deal', 'deal problem', 'problem natural', 'natural language', 'language communicating', 'communicating computers', 'computers .'] 

 TOTAL BIGRAMS --> 14 



 ---- TRI-GRAMS ---- 

 ['field Natural Language', 'Natural Language Processing', 'Language Processing related', 'Processing related different', 'related different theories', 'different theories techniques', 'theories techniques deal', 'techniques deal problem', 'deal problem natural', 'problem natural language', 'natural language communicating', 'language communicating computers', 'communicating computers .'] 

 TOTAL TRIGRAMS --> 13 



 ---- NOUN PHRASES ---- 

 ['field', 'problem', 'natural language'] 

 TOTAL NOUN PHRASES --> 3 



 ---- NER ----

 
 ORGANIZATION ---> ['Natural Language']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['field', 'natur', 'languag', 'process', 'relat', 'differ', 'theori', 'techniqu', 'deal', 'problem', 'natur', 'languag', 'commun', 'comput', '.']

 TOTAL PORTER STEM WORDS ==> 15



 ---- SNOWBALL STEMMING ----

['field', 'natur', 'languag', 'process', 'relat', 'differ', 'theori', 'techniqu', 'deal', 'problem', 'natur', 'languag', 'communic', 'comput', '.']

 TOTAL SNOWBALL STEM WORDS ==> 15



 ---- LEMMATIZATION ----

['field', 'Natural', 'Language', 'Processing', 'related', 'different', 'theory', 'technique', 'deal', 'problem', 'natural', 'language', 'communicating', 'computer', '.']

 TOTAL LEMMATIZE WORDS ==> 15

************************************************************************************************************************

46 --> Ambiguity is one of the major problem of natural language which is  usually faced in syntactic level which has subtask as lexical and morphology which are  concerned with the study of words and word formation. 


 ---- TOKENS ----

 ['Ambiguity', 'is', 'one', 'of', 'the', 'major', 'problem', 'of', 'natural', 'language', 'which', 'is', 'usually', 'faced', 'in', 'syntactic', 'level', 'which', 'has', 'subtask', 'as', 'lexical', 'and', 'morphology', 'which', 'are', 'concerned', 'with', 'the', 'study', 'of', 'words', 'and', 'word', 'formation', '.'] 

 TOTAL TOKENS ==> 36

 ---- POST ----

 [('Ambiguity', 'NN'), ('is', 'VBZ'), ('one', 'CD'), ('of', 'IN'), ('the', 'DT'), ('major', 'JJ'), ('problem', 'NN'), ('of', 'IN'), ('natural', 'JJ'), ('language', 'NN'), ('which', 'WDT'), ('is', 'VBZ'), ('usually', 'RB'), ('faced', 'VBN'), ('in', 'IN'), ('syntactic', 'JJ'), ('level', 'NN'), ('which', 'WDT'), ('has', 'VBZ'), ('subtask', 'NN'), ('as', 'IN'), ('lexical', 'JJ'), ('and', 'CC'), ('morphology', 'NN'), ('which', 'WDT'), ('are', 'VBP'), ('concerned', 'VBN'), ('with', 'IN'), ('the', 'DT'), ('study', 'NN'), ('of', 'IN'), ('words', 'NNS'), ('and', 'CC'), ('word', 'NN'), ('formation', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Ambiguity', 'one', 'major', 'problem', 'natural', 'language', 'usually', 'faced', 'syntactic', 'level', 'subtask', 'lexical', 'morphology', 'concerned', 'study', 'words', 'word', 'formation', '.']

 TOTAL FILTERED TOKENS ==>  19

 ---- POST FOR FILTERED TOKENS ----

 [('Ambiguity', 'NNP'), ('one', 'CD'), ('major', 'JJ'), ('problem', 'NN'), ('natural', 'JJ'), ('language', 'NN'), ('usually', 'RB'), ('faced', 'VBN'), ('syntactic', 'JJ'), ('level', 'NN'), ('subtask', 'NN'), ('lexical', 'JJ'), ('morphology', 'NN'), ('concerned', 'VBN'), ('study', 'NN'), ('words', 'NNS'), ('word', 'NN'), ('formation', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Ambiguity one', 'one major', 'major problem', 'problem natural', 'natural language', 'language usually', 'usually faced', 'faced syntactic', 'syntactic level', 'level subtask', 'subtask lexical', 'lexical morphology', 'morphology concerned', 'concerned study', 'study words', 'words word', 'word formation', 'formation .'] 

 TOTAL BIGRAMS --> 18 



 ---- TRI-GRAMS ---- 

 ['Ambiguity one major', 'one major problem', 'major problem natural', 'problem natural language', 'natural language usually', 'language usually faced', 'usually faced syntactic', 'faced syntactic level', 'syntactic level subtask', 'level subtask lexical', 'subtask lexical morphology', 'lexical morphology concerned', 'morphology concerned study', 'concerned study words', 'study words word', 'words word formation', 'word formation .'] 

 TOTAL TRIGRAMS --> 17 



 ---- NOUN PHRASES ---- 

 ['major problem', 'natural language', 'syntactic level', 'subtask', 'lexical morphology', 'study', 'word', 'formation'] 

 TOTAL NOUN PHRASES --> 8 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['ambigu', 'one', 'major', 'problem', 'natur', 'languag', 'usual', 'face', 'syntact', 'level', 'subtask', 'lexic', 'morpholog', 'concern', 'studi', 'word', 'word', 'format', '.']

 TOTAL PORTER STEM WORDS ==> 19



 ---- SNOWBALL STEMMING ----

['ambigu', 'one', 'major', 'problem', 'natur', 'languag', 'usual', 'face', 'syntact', 'level', 'subtask', 'lexic', 'morpholog', 'concern', 'studi', 'word', 'word', 'format', '.']

 TOTAL SNOWBALL STEM WORDS ==> 19



 ---- LEMMATIZATION ----

['Ambiguity', 'one', 'major', 'problem', 'natural', 'language', 'usually', 'faced', 'syntactic', 'level', 'subtask', 'lexical', 'morphology', 'concerned', 'study', 'word', 'word', 'formation', '.']

 TOTAL LEMMATIZE WORDS ==> 19

************************************************************************************************************************

47 --> Each of these levels can produce  ambiguities that can be solved by the knowledge of the complete sentence. 


 ---- TOKENS ----

 ['Each', 'of', 'these', 'levels', 'can', 'produce', 'ambiguities', 'that', 'can', 'be', 'solved', 'by', 'the', 'knowledge', 'of', 'the', 'complete', 'sentence', '.'] 

 TOTAL TOKENS ==> 19

 ---- POST ----

 [('Each', 'DT'), ('of', 'IN'), ('these', 'DT'), ('levels', 'NNS'), ('can', 'MD'), ('produce', 'VB'), ('ambiguities', 'NNS'), ('that', 'WDT'), ('can', 'MD'), ('be', 'VB'), ('solved', 'VBN'), ('by', 'IN'), ('the', 'DT'), ('knowledge', 'NN'), ('of', 'IN'), ('the', 'DT'), ('complete', 'JJ'), ('sentence', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['levels', 'produce', 'ambiguities', 'solved', 'knowledge', 'complete', 'sentence', '.']

 TOTAL FILTERED TOKENS ==>  8

 ---- POST FOR FILTERED TOKENS ----

 [('levels', 'NNS'), ('produce', 'VBP'), ('ambiguities', 'NNS'), ('solved', 'VBD'), ('knowledge', 'NN'), ('complete', 'JJ'), ('sentence', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['levels produce', 'produce ambiguities', 'ambiguities solved', 'solved knowledge', 'knowledge complete', 'complete sentence', 'sentence .'] 

 TOTAL BIGRAMS --> 7 



 ---- TRI-GRAMS ---- 

 ['levels produce ambiguities', 'produce ambiguities solved', 'ambiguities solved knowledge', 'solved knowledge complete', 'knowledge complete sentence', 'complete sentence .'] 

 TOTAL TRIGRAMS --> 6 



 ---- NOUN PHRASES ---- 

 ['knowledge', 'complete sentence'] 

 TOTAL NOUN PHRASES --> 2 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['level', 'produc', 'ambigu', 'solv', 'knowledg', 'complet', 'sentenc', '.']

 TOTAL PORTER STEM WORDS ==> 8



 ---- SNOWBALL STEMMING ----

['level', 'produc', 'ambigu', 'solv', 'knowledg', 'complet', 'sentenc', '.']

 TOTAL SNOWBALL STEM WORDS ==> 8



 ---- LEMMATIZATION ----

['level', 'produce', 'ambiguity', 'solved', 'knowledge', 'complete', 'sentence', '.']

 TOTAL LEMMATIZE WORDS ==> 8

************************************************************************************************************************

48 --> The ambiguity  can be solved by various methods such as Minimising Ambiguity, Preserving Ambiguity,  Interactive Disambiguity and Weighting Ambiguity [3]. 


 ---- TOKENS ----

 ['The', 'ambiguity', 'can', 'be', 'solved', 'by', 'various', 'methods', 'such', 'as', 'Minimising', 'Ambiguity', ',', 'Preserving', 'Ambiguity', ',', 'Interactive', 'Disambiguity', 'and', 'Weighting', 'Ambiguity', '[', '3', ']', '.'] 

 TOTAL TOKENS ==> 25

 ---- POST ----

 [('The', 'DT'), ('ambiguity', 'NN'), ('can', 'MD'), ('be', 'VB'), ('solved', 'VBN'), ('by', 'IN'), ('various', 'JJ'), ('methods', 'NNS'), ('such', 'JJ'), ('as', 'IN'), ('Minimising', 'NNP'), ('Ambiguity', 'NNP'), (',', ','), ('Preserving', 'NNP'), ('Ambiguity', 'NNP'), (',', ','), ('Interactive', 'NNP'), ('Disambiguity', 'NNP'), ('and', 'CC'), ('Weighting', 'NNP'), ('Ambiguity', 'NNP'), ('[', 'NNP'), ('3', 'CD'), (']', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['ambiguity', 'solved', 'various', 'methods', 'Minimising', 'Ambiguity', ',', 'Preserving', 'Ambiguity', ',', 'Interactive', 'Disambiguity', 'Weighting', 'Ambiguity', '[', '3', ']', '.']

 TOTAL FILTERED TOKENS ==>  18

 ---- POST FOR FILTERED TOKENS ----

 [('ambiguity', 'NN'), ('solved', 'VBD'), ('various', 'JJ'), ('methods', 'NNS'), ('Minimising', 'VBG'), ('Ambiguity', 'NNP'), (',', ','), ('Preserving', 'NNP'), ('Ambiguity', 'NNP'), (',', ','), ('Interactive', 'NNP'), ('Disambiguity', 'NNP'), ('Weighting', 'NNP'), ('Ambiguity', 'NNP'), ('[', 'NNP'), ('3', 'CD'), (']', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['ambiguity solved', 'solved various', 'various methods', 'methods Minimising', 'Minimising Ambiguity', 'Ambiguity ,', ', Preserving', 'Preserving Ambiguity', 'Ambiguity ,', ', Interactive', 'Interactive Disambiguity', 'Disambiguity Weighting', 'Weighting Ambiguity', 'Ambiguity [', '[ 3', '3 ]', '] .'] 

 TOTAL BIGRAMS --> 17 



 ---- TRI-GRAMS ---- 

 ['ambiguity solved various', 'solved various methods', 'various methods Minimising', 'methods Minimising Ambiguity', 'Minimising Ambiguity ,', 'Ambiguity , Preserving', ', Preserving Ambiguity', 'Preserving Ambiguity ,', 'Ambiguity , Interactive', ', Interactive Disambiguity', 'Interactive Disambiguity Weighting', 'Disambiguity Weighting Ambiguity', 'Weighting Ambiguity [', 'Ambiguity [ 3', '[ 3 ]', '3 ] .'] 

 TOTAL TRIGRAMS --> 16 



 ---- NOUN PHRASES ---- 

 ['ambiguity', ']'] 

 TOTAL NOUN PHRASES --> 2 



 ---- NER ----

 
 ORGANIZATION ---> ['Ambiguity', 'Interactive Disambiguity']
 TOTAL ORGANIZATION ENTITY --> 2 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['ambigu', 'solv', 'variou', 'method', 'minimis', 'ambigu', ',', 'preserv', 'ambigu', ',', 'interact', 'disambigu', 'weight', 'ambigu', '[', '3', ']', '.']

 TOTAL PORTER STEM WORDS ==> 18



 ---- SNOWBALL STEMMING ----

['ambigu', 'solv', 'various', 'method', 'minimis', 'ambigu', ',', 'preserv', 'ambigu', ',', 'interact', 'disambigu', 'weight', 'ambigu', '[', '3', ']', '.']

 TOTAL SNOWBALL STEM WORDS ==> 18



 ---- LEMMATIZATION ----

['ambiguity', 'solved', 'various', 'method', 'Minimising', 'Ambiguity', ',', 'Preserving', 'Ambiguity', ',', 'Interactive', 'Disambiguity', 'Weighting', 'Ambiguity', '[', '3', ']', '.']

 TOTAL LEMMATIZE WORDS ==> 18

************************************************************************************************************************

49 --> Some of the methods proposed by  researchers to remove ambiguity is preserving ambiguity, e.g.- (Shemtov 1997; Emele &  Dorna 1998; Knight & Langkilde 2000) [3][4][5] Their objectives are closely in line with the  last of these: they cover a wide range of ambiguities and there is a statistical element implicit  in their approach. 


 ---- TOKENS ----

 ['Some', 'of', 'the', 'methods', 'proposed', 'by', 'researchers', 'to', 'remove', 'ambiguity', 'is', 'preserving', 'ambiguity', ',', 'e.g.-', '(', 'Shemtov', '1997', ';', 'Emele', '&', 'Dorna', '1998', ';', 'Knight', '&', 'Langkilde', '2000', ')', '[', '3', ']', '[', '4', ']', '[', '5', ']', 'Their', 'objectives', 'are', 'closely', 'in', 'line', 'with', 'the', 'last', 'of', 'these', ':', 'they', 'cover', 'a', 'wide', 'range', 'of', 'ambiguities', 'and', 'there', 'is', 'a', 'statistical', 'element', 'implicit', 'in', 'their', 'approach', '.'] 

 TOTAL TOKENS ==> 68

 ---- POST ----

 [('Some', 'DT'), ('of', 'IN'), ('the', 'DT'), ('methods', 'NNS'), ('proposed', 'VBN'), ('by', 'IN'), ('researchers', 'NNS'), ('to', 'TO'), ('remove', 'VB'), ('ambiguity', 'NN'), ('is', 'VBZ'), ('preserving', 'VBG'), ('ambiguity', 'NN'), (',', ','), ('e.g.-', 'JJ'), ('(', '('), ('Shemtov', 'JJ'), ('1997', 'CD'), (';', ':'), ('Emele', 'NNP'), ('&', 'CC'), ('Dorna', 'NNP'), ('1998', 'CD'), (';', ':'), ('Knight', 'NNP'), ('&', 'CC'), ('Langkilde', 'NNP'), ('2000', 'CD'), (')', ')'), ('[', 'VBD'), ('3', 'CD'), (']', 'NNP'), ('[', 'VBD'), ('4', 'CD'), (']', 'NNP'), ('[', 'VBD'), ('5', 'CD'), (']', 'NNP'), ('Their', 'NNP'), ('objectives', 'NNS'), ('are', 'VBP'), ('closely', 'RB'), ('in', 'IN'), ('line', 'NN'), ('with', 'IN'), ('the', 'DT'), ('last', 'JJ'), ('of', 'IN'), ('these', 'DT'), (':', ':'), ('they', 'PRP'), ('cover', 'VBP'), ('a', 'DT'), ('wide', 'JJ'), ('range', 'NN'), ('of', 'IN'), ('ambiguities', 'NNS'), ('and', 'CC'), ('there', 'EX'), ('is', 'VBZ'), ('a', 'DT'), ('statistical', 'JJ'), ('element', 'NN'), ('implicit', 'NN'), ('in', 'IN'), ('their', 'PRP$'), ('approach', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['methods', 'proposed', 'researchers', 'remove', 'ambiguity', 'preserving', 'ambiguity', ',', 'e.g.-', '(', 'Shemtov', '1997', ';', 'Emele', '&', 'Dorna', '1998', ';', 'Knight', '&', 'Langkilde', '2000', ')', '[', '3', ']', '[', '4', ']', '[', '5', ']', 'objectives', 'closely', 'line', 'last', ':', 'cover', 'wide', 'range', 'ambiguities', 'statistical', 'element', 'implicit', 'approach', '.']

 TOTAL FILTERED TOKENS ==>  46

 ---- POST FOR FILTERED TOKENS ----

 [('methods', 'NNS'), ('proposed', 'VBD'), ('researchers', 'NNS'), ('remove', 'VB'), ('ambiguity', 'NN'), ('preserving', 'VBG'), ('ambiguity', 'NN'), (',', ','), ('e.g.-', 'JJ'), ('(', '('), ('Shemtov', 'JJ'), ('1997', 'CD'), (';', ':'), ('Emele', 'NNP'), ('&', 'CC'), ('Dorna', 'NNP'), ('1998', 'CD'), (';', ':'), ('Knight', 'NNP'), ('&', 'CC'), ('Langkilde', 'NNP'), ('2000', 'CD'), (')', ')'), ('[', 'VBD'), ('3', 'CD'), (']', 'NNP'), ('[', 'VBD'), ('4', 'CD'), (']', 'NNP'), ('[', 'VBD'), ('5', 'CD'), (']', 'NN'), ('objectives', 'NNS'), ('closely', 'RB'), ('line', 'NN'), ('last', 'JJ'), (':', ':'), ('cover', 'NN'), ('wide', 'JJ'), ('range', 'NN'), ('ambiguities', 'NNS'), ('statistical', 'JJ'), ('element', 'NN'), ('implicit', 'JJ'), ('approach', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['methods proposed', 'proposed researchers', 'researchers remove', 'remove ambiguity', 'ambiguity preserving', 'preserving ambiguity', 'ambiguity ,', ', e.g.-', 'e.g.- (', '( Shemtov', 'Shemtov 1997', '1997 ;', '; Emele', 'Emele &', '& Dorna', 'Dorna 1998', '1998 ;', '; Knight', 'Knight &', '& Langkilde', 'Langkilde 2000', '2000 )', ') [', '[ 3', '3 ]', '] [', '[ 4', '4 ]', '] [', '[ 5', '5 ]', '] objectives', 'objectives closely', 'closely line', 'line last', 'last :', ': cover', 'cover wide', 'wide range', 'range ambiguities', 'ambiguities statistical', 'statistical element', 'element implicit', 'implicit approach', 'approach .'] 

 TOTAL BIGRAMS --> 45 



 ---- TRI-GRAMS ---- 

 ['methods proposed researchers', 'proposed researchers remove', 'researchers remove ambiguity', 'remove ambiguity preserving', 'ambiguity preserving ambiguity', 'preserving ambiguity ,', 'ambiguity , e.g.-', ', e.g.- (', 'e.g.- ( Shemtov', '( Shemtov 1997', 'Shemtov 1997 ;', '1997 ; Emele', '; Emele &', 'Emele & Dorna', '& Dorna 1998', 'Dorna 1998 ;', '1998 ; Knight', '; Knight &', 'Knight & Langkilde', '& Langkilde 2000', 'Langkilde 2000 )', '2000 ) [', ') [ 3', '[ 3 ]', '3 ] [', '] [ 4', '[ 4 ]', '4 ] [', '] [ 5', '[ 5 ]', '5 ] objectives', '] objectives closely', 'objectives closely line', 'closely line last', 'line last :', 'last : cover', ': cover wide', 'cover wide range', 'wide range ambiguities', 'range ambiguities statistical', 'ambiguities statistical element', 'statistical element implicit', 'element implicit approach', 'implicit approach .'] 

 TOTAL TRIGRAMS --> 44 



 ---- NOUN PHRASES ---- 

 ['ambiguity', 'ambiguity', ']', 'line', 'cover', 'wide range', 'statistical element', 'implicit approach'] 

 TOTAL NOUN PHRASES --> 8 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['method', 'propos', 'research', 'remov', 'ambigu', 'preserv', 'ambigu', ',', 'e.g.-', '(', 'shemtov', '1997', ';', 'emel', '&', 'dorna', '1998', ';', 'knight', '&', 'langkild', '2000', ')', '[', '3', ']', '[', '4', ']', '[', '5', ']', 'object', 'close', 'line', 'last', ':', 'cover', 'wide', 'rang', 'ambigu', 'statist', 'element', 'implicit', 'approach', '.']

 TOTAL PORTER STEM WORDS ==> 46



 ---- SNOWBALL STEMMING ----

['method', 'propos', 'research', 'remov', 'ambigu', 'preserv', 'ambigu', ',', 'e.g.-', '(', 'shemtov', '1997', ';', 'emel', '&', 'dorna', '1998', ';', 'knight', '&', 'langkild', '2000', ')', '[', '3', ']', '[', '4', ']', '[', '5', ']', 'object', 'close', 'line', 'last', ':', 'cover', 'wide', 'rang', 'ambigu', 'statist', 'element', 'implicit', 'approach', '.']

 TOTAL SNOWBALL STEM WORDS ==> 46



 ---- LEMMATIZATION ----

['method', 'proposed', 'researcher', 'remove', 'ambiguity', 'preserving', 'ambiguity', ',', 'e.g.-', '(', 'Shemtov', '1997', ';', 'Emele', '&', 'Dorna', '1998', ';', 'Knight', '&', 'Langkilde', '2000', ')', '[', '3', ']', '[', '4', ']', '[', '5', ']', 'objective', 'closely', 'line', 'last', ':', 'cover', 'wide', 'range', 'ambiguity', 'statistical', 'element', 'implicit', 'approach', '.']

 TOTAL LEMMATIZE WORDS ==> 46

************************************************************************************************************************

50 --> 2. 


 ---- TOKENS ----

 ['2', '.'] 

 TOTAL TOKENS ==> 2

 ---- POST ----

 [('2', 'CD'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['2', '.']

 TOTAL FILTERED TOKENS ==>  2

 ---- POST FOR FILTERED TOKENS ----

 [('2', 'CD'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['2 .'] 

 TOTAL BIGRAMS --> 1 



 ---- TRI-GRAMS ---- 

 [] 

 TOTAL TRIGRAMS --> 0 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['2', '.']

 TOTAL PORTER STEM WORDS ==> 2



 ---- SNOWBALL STEMMING ----

['2', '.']

 TOTAL SNOWBALL STEM WORDS ==> 2



 ---- LEMMATIZATION ----

['2', '.']

 TOTAL LEMMATIZE WORDS ==> 2

************************************************************************************************************************

51 --> Levels of NLP  The ‘levels of language’ are one of the most explanatory method for representing the Natural  Language processing which helps to generate the NLP text by realising Content Planning,  Sentence Planning and Surface Realization phases (Figure 2). 


 ---- TOKENS ----

 ['Levels', 'of', 'NLP', 'The', '‘', 'levels', 'of', 'language', '’', 'are', 'one', 'of', 'the', 'most', 'explanatory', 'method', 'for', 'representing', 'the', 'Natural', 'Language', 'processing', 'which', 'helps', 'to', 'generate', 'the', 'NLP', 'text', 'by', 'realising', 'Content', 'Planning', ',', 'Sentence', 'Planning', 'and', 'Surface', 'Realization', 'phases', '(', 'Figure', '2', ')', '.'] 

 TOTAL TOKENS ==> 45

 ---- POST ----

 [('Levels', 'NNS'), ('of', 'IN'), ('NLP', 'NNP'), ('The', 'DT'), ('‘', 'NNP'), ('levels', 'NNS'), ('of', 'IN'), ('language', 'NN'), ('’', 'NN'), ('are', 'VBP'), ('one', 'CD'), ('of', 'IN'), ('the', 'DT'), ('most', 'RBS'), ('explanatory', 'JJ'), ('method', 'NN'), ('for', 'IN'), ('representing', 'VBG'), ('the', 'DT'), ('Natural', 'NNP'), ('Language', 'NNP'), ('processing', 'NN'), ('which', 'WDT'), ('helps', 'VBZ'), ('to', 'TO'), ('generate', 'VB'), ('the', 'DT'), ('NLP', 'NNP'), ('text', 'NN'), ('by', 'IN'), ('realising', 'VBG'), ('Content', 'NNP'), ('Planning', 'NNP'), (',', ','), ('Sentence', 'NNP'), ('Planning', 'NNP'), ('and', 'CC'), ('Surface', 'NNP'), ('Realization', 'NNP'), ('phases', 'NNS'), ('(', '('), ('Figure', 'NNP'), ('2', 'CD'), (')', ')'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Levels', 'NLP', '‘', 'levels', 'language', '’', 'one', 'explanatory', 'method', 'representing', 'Natural', 'Language', 'processing', 'helps', 'generate', 'NLP', 'text', 'realising', 'Content', 'Planning', ',', 'Sentence', 'Planning', 'Surface', 'Realization', 'phases', '(', 'Figure', '2', ')', '.']

 TOTAL FILTERED TOKENS ==>  31

 ---- POST FOR FILTERED TOKENS ----

 [('Levels', 'NNP'), ('NLP', 'NNP'), ('‘', 'NNP'), ('levels', 'NNS'), ('language', 'NN'), ('’', 'VBP'), ('one', 'CD'), ('explanatory', 'NN'), ('method', 'NN'), ('representing', 'VBG'), ('Natural', 'NNP'), ('Language', 'NNP'), ('processing', 'NN'), ('helps', 'VBZ'), ('generate', 'VB'), ('NLP', 'NNP'), ('text', 'IN'), ('realising', 'VBG'), ('Content', 'NNP'), ('Planning', 'NNP'), (',', ','), ('Sentence', 'NNP'), ('Planning', 'NNP'), ('Surface', 'NNP'), ('Realization', 'NNP'), ('phases', 'NNS'), ('(', '('), ('Figure', 'NNP'), ('2', 'CD'), (')', ')'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Levels NLP', 'NLP ‘', '‘ levels', 'levels language', 'language ’', '’ one', 'one explanatory', 'explanatory method', 'method representing', 'representing Natural', 'Natural Language', 'Language processing', 'processing helps', 'helps generate', 'generate NLP', 'NLP text', 'text realising', 'realising Content', 'Content Planning', 'Planning ,', ', Sentence', 'Sentence Planning', 'Planning Surface', 'Surface Realization', 'Realization phases', 'phases (', '( Figure', 'Figure 2', '2 )', ') .'] 

 TOTAL BIGRAMS --> 30 



 ---- TRI-GRAMS ---- 

 ['Levels NLP ‘', 'NLP ‘ levels', '‘ levels language', 'levels language ’', 'language ’ one', '’ one explanatory', 'one explanatory method', 'explanatory method representing', 'method representing Natural', 'representing Natural Language', 'Natural Language processing', 'Language processing helps', 'processing helps generate', 'helps generate NLP', 'generate NLP text', 'NLP text realising', 'text realising Content', 'realising Content Planning', 'Content Planning ,', 'Planning , Sentence', ', Sentence Planning', 'Sentence Planning Surface', 'Planning Surface Realization', 'Surface Realization phases', 'Realization phases (', 'phases ( Figure', '( Figure 2', 'Figure 2 )', '2 ) .'] 

 TOTAL TRIGRAMS --> 29 



 ---- NOUN PHRASES ---- 

 ['language', 'explanatory', 'method', 'processing'] 

 TOTAL NOUN PHRASES --> 4 



 ---- NER ----

 
 ORGANIZATION ---> ['NLP', 'Natural Language', 'NLP', 'Content Planning', 'Sentence']
 TOTAL ORGANIZATION ENTITY --> 5 


 PERSON ---> ['Levels']
 TOTAL PERSON ENTITY --> 1 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['level', 'nlp', '‘', 'level', 'languag', '’', 'one', 'explanatori', 'method', 'repres', 'natur', 'languag', 'process', 'help', 'gener', 'nlp', 'text', 'realis', 'content', 'plan', ',', 'sentenc', 'plan', 'surfac', 'realiz', 'phase', '(', 'figur', '2', ')', '.']

 TOTAL PORTER STEM WORDS ==> 31



 ---- SNOWBALL STEMMING ----

['level', 'nlp', '‘', 'level', 'languag', '’', 'one', 'explanatori', 'method', 'repres', 'natur', 'languag', 'process', 'help', 'generat', 'nlp', 'text', 'realis', 'content', 'plan', ',', 'sentenc', 'plan', 'surfac', 'realize', 'phase', '(', 'figur', '2', ')', '.']

 TOTAL SNOWBALL STEM WORDS ==> 31



 ---- LEMMATIZATION ----

['Levels', 'NLP', '‘', 'level', 'language', '’', 'one', 'explanatory', 'method', 'representing', 'Natural', 'Language', 'processing', 'help', 'generate', 'NLP', 'text', 'realising', 'Content', 'Planning', ',', 'Sentence', 'Planning', 'Surface', 'Realization', 'phase', '(', 'Figure', '2', ')', '.']

 TOTAL LEMMATIZE WORDS ==> 31

************************************************************************************************************************

52 --> Figure 2. 


 ---- TOKENS ----

 ['Figure', '2', '.'] 

 TOTAL TOKENS ==> 3

 ---- POST ----

 [('Figure', 'NN'), ('2', 'CD'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Figure', '2', '.']

 TOTAL FILTERED TOKENS ==>  3

 ---- POST FOR FILTERED TOKENS ----

 [('Figure', 'NN'), ('2', 'CD'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Figure 2', '2 .'] 

 TOTAL BIGRAMS --> 2 



 ---- TRI-GRAMS ---- 

 ['Figure 2 .'] 

 TOTAL TRIGRAMS --> 1 



 ---- NOUN PHRASES ---- 

 ['Figure'] 

 TOTAL NOUN PHRASES --> 1 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['figur', '2', '.']

 TOTAL PORTER STEM WORDS ==> 3



 ---- SNOWBALL STEMMING ----

['figur', '2', '.']

 TOTAL SNOWBALL STEM WORDS ==> 3



 ---- LEMMATIZATION ----

['Figure', '2', '.']

 TOTAL LEMMATIZE WORDS ==> 3

************************************************************************************************************************

53 --> Phases of NLP architecture  Linguistic is the science which involves meaning of language, language context and various  forms of the language. 


 ---- TOKENS ----

 ['Phases', 'of', 'NLP', 'architecture', 'Linguistic', 'is', 'the', 'science', 'which', 'involves', 'meaning', 'of', 'language', ',', 'language', 'context', 'and', 'various', 'forms', 'of', 'the', 'language', '.'] 

 TOTAL TOKENS ==> 23

 ---- POST ----

 [('Phases', 'NNS'), ('of', 'IN'), ('NLP', 'NNP'), ('architecture', 'NN'), ('Linguistic', 'NNP'), ('is', 'VBZ'), ('the', 'DT'), ('science', 'NN'), ('which', 'WDT'), ('involves', 'VBZ'), ('meaning', 'NN'), ('of', 'IN'), ('language', 'NN'), (',', ','), ('language', 'NN'), ('context', 'NN'), ('and', 'CC'), ('various', 'JJ'), ('forms', 'NNS'), ('of', 'IN'), ('the', 'DT'), ('language', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Phases', 'NLP', 'architecture', 'Linguistic', 'science', 'involves', 'meaning', 'language', ',', 'language', 'context', 'various', 'forms', 'language', '.']

 TOTAL FILTERED TOKENS ==>  15

 ---- POST FOR FILTERED TOKENS ----

 [('Phases', 'NNS'), ('NLP', 'NNP'), ('architecture', 'NN'), ('Linguistic', 'NNP'), ('science', 'NN'), ('involves', 'VBZ'), ('meaning', 'VBG'), ('language', 'NN'), (',', ','), ('language', 'NN'), ('context', 'NN'), ('various', 'JJ'), ('forms', 'NNS'), ('language', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Phases NLP', 'NLP architecture', 'architecture Linguistic', 'Linguistic science', 'science involves', 'involves meaning', 'meaning language', 'language ,', ', language', 'language context', 'context various', 'various forms', 'forms language', 'language .'] 

 TOTAL BIGRAMS --> 14 



 ---- TRI-GRAMS ---- 

 ['Phases NLP architecture', 'NLP architecture Linguistic', 'architecture Linguistic science', 'Linguistic science involves', 'science involves meaning', 'involves meaning language', 'meaning language ,', 'language , language', ', language context', 'language context various', 'context various forms', 'various forms language', 'forms language .'] 

 TOTAL TRIGRAMS --> 13 



 ---- NOUN PHRASES ---- 

 ['architecture', 'science', 'language', 'language', 'context', 'language'] 

 TOTAL NOUN PHRASES --> 6 



 ---- NER ----

 
 ORGANIZATION ---> ['NLP']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> ['Linguistic']
 TOTAL PERSON ENTITY --> 1 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['phase', 'nlp', 'architectur', 'linguist', 'scienc', 'involv', 'mean', 'languag', ',', 'languag', 'context', 'variou', 'form', 'languag', '.']

 TOTAL PORTER STEM WORDS ==> 15



 ---- SNOWBALL STEMMING ----

['phase', 'nlp', 'architectur', 'linguist', 'scienc', 'involv', 'mean', 'languag', ',', 'languag', 'context', 'various', 'form', 'languag', '.']

 TOTAL SNOWBALL STEM WORDS ==> 15



 ---- LEMMATIZATION ----

['Phases', 'NLP', 'architecture', 'Linguistic', 'science', 'involves', 'meaning', 'language', ',', 'language', 'context', 'various', 'form', 'language', '.']

 TOTAL LEMMATIZE WORDS ==> 15

************************************************************************************************************************

54 --> The various important terminologies of Natural Language Processing  are: -  1. 


 ---- TOKENS ----

 ['The', 'various', 'important', 'terminologies', 'of', 'Natural', 'Language', 'Processing', 'are', ':', '-', '1', '.'] 

 TOTAL TOKENS ==> 13

 ---- POST ----

 [('The', 'DT'), ('various', 'JJ'), ('important', 'JJ'), ('terminologies', 'NNS'), ('of', 'IN'), ('Natural', 'NNP'), ('Language', 'NNP'), ('Processing', 'NNP'), ('are', 'VBP'), (':', ':'), ('-', ':'), ('1', 'CD'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['various', 'important', 'terminologies', 'Natural', 'Language', 'Processing', ':', '-', '1', '.']

 TOTAL FILTERED TOKENS ==>  10

 ---- POST FOR FILTERED TOKENS ----

 [('various', 'JJ'), ('important', 'JJ'), ('terminologies', 'NNS'), ('Natural', 'NNP'), ('Language', 'NNP'), ('Processing', 'NN'), (':', ':'), ('-', ':'), ('1', 'CD'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['various important', 'important terminologies', 'terminologies Natural', 'Natural Language', 'Language Processing', 'Processing :', ': -', '- 1', '1 .'] 

 TOTAL BIGRAMS --> 9 



 ---- TRI-GRAMS ---- 

 ['various important terminologies', 'important terminologies Natural', 'terminologies Natural Language', 'Natural Language Processing', 'Language Processing :', 'Processing : -', ': - 1', '- 1 .'] 

 TOTAL TRIGRAMS --> 8 



 ---- NOUN PHRASES ---- 

 ['Processing'] 

 TOTAL NOUN PHRASES --> 1 



 ---- NER ----

 
 ORGANIZATION ---> ['Natural Language']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['variou', 'import', 'terminolog', 'natur', 'languag', 'process', ':', '-', '1', '.']

 TOTAL PORTER STEM WORDS ==> 10



 ---- SNOWBALL STEMMING ----

['various', 'import', 'terminolog', 'natur', 'languag', 'process', ':', '-', '1', '.']

 TOTAL SNOWBALL STEM WORDS ==> 10



 ---- LEMMATIZATION ----

['various', 'important', 'terminology', 'Natural', 'Language', 'Processing', ':', '-', '1', '.']

 TOTAL LEMMATIZE WORDS ==> 10

************************************************************************************************************************

55 --> Phonology  Phonology is the part of Linguistics which refers to the systematic arrangement of sound. 


 ---- TOKENS ----

 ['Phonology', 'Phonology', 'is', 'the', 'part', 'of', 'Linguistics', 'which', 'refers', 'to', 'the', 'systematic', 'arrangement', 'of', 'sound', '.'] 

 TOTAL TOKENS ==> 16

 ---- POST ----

 [('Phonology', 'NNP'), ('Phonology', 'NNP'), ('is', 'VBZ'), ('the', 'DT'), ('part', 'NN'), ('of', 'IN'), ('Linguistics', 'NNP'), ('which', 'WDT'), ('refers', 'VBZ'), ('to', 'TO'), ('the', 'DT'), ('systematic', 'JJ'), ('arrangement', 'NN'), ('of', 'IN'), ('sound', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Phonology', 'Phonology', 'part', 'Linguistics', 'refers', 'systematic', 'arrangement', 'sound', '.']

 TOTAL FILTERED TOKENS ==>  9

 ---- POST FOR FILTERED TOKENS ----

 [('Phonology', 'NNP'), ('Phonology', 'NNP'), ('part', 'NN'), ('Linguistics', 'NNP'), ('refers', 'VBZ'), ('systematic', 'JJ'), ('arrangement', 'NN'), ('sound', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Phonology Phonology', 'Phonology part', 'part Linguistics', 'Linguistics refers', 'refers systematic', 'systematic arrangement', 'arrangement sound', 'sound .'] 

 TOTAL BIGRAMS --> 8 



 ---- TRI-GRAMS ---- 

 ['Phonology Phonology part', 'Phonology part Linguistics', 'part Linguistics refers', 'Linguistics refers systematic', 'refers systematic arrangement', 'systematic arrangement sound', 'arrangement sound .'] 

 TOTAL TRIGRAMS --> 7 



 ---- NOUN PHRASES ---- 

 ['part', 'systematic arrangement', 'sound'] 

 TOTAL NOUN PHRASES --> 3 



 ---- NER ----

 
 ORGANIZATION ---> ['Phonology']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> ['Phonology', 'Linguistics']
 TOTAL PERSON ENTITY --> 2 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['phonolog', 'phonolog', 'part', 'linguist', 'refer', 'systemat', 'arrang', 'sound', '.']

 TOTAL PORTER STEM WORDS ==> 9



 ---- SNOWBALL STEMMING ----

['phonolog', 'phonolog', 'part', 'linguist', 'refer', 'systemat', 'arrang', 'sound', '.']

 TOTAL SNOWBALL STEM WORDS ==> 9



 ---- LEMMATIZATION ----

['Phonology', 'Phonology', 'part', 'Linguistics', 'refers', 'systematic', 'arrangement', 'sound', '.']

 TOTAL LEMMATIZE WORDS ==> 9

************************************************************************************************************************

56 --> The  term phonology comes from Ancient Greek and the term phono- which means voice or  sound, and the suffix –logy refers to word or speech. 


 ---- TOKENS ----

 ['The', 'term', 'phonology', 'comes', 'from', 'Ancient', 'Greek', 'and', 'the', 'term', 'phono-', 'which', 'means', 'voice', 'or', 'sound', ',', 'and', 'the', 'suffix', '–logy', 'refers', 'to', 'word', 'or', 'speech', '.'] 

 TOTAL TOKENS ==> 27

 ---- POST ----

 [('The', 'DT'), ('term', 'NN'), ('phonology', 'NN'), ('comes', 'VBZ'), ('from', 'IN'), ('Ancient', 'NNP'), ('Greek', 'NNP'), ('and', 'CC'), ('the', 'DT'), ('term', 'NN'), ('phono-', 'NN'), ('which', 'WDT'), ('means', 'VBZ'), ('voice', 'NN'), ('or', 'CC'), ('sound', 'NN'), (',', ','), ('and', 'CC'), ('the', 'DT'), ('suffix', 'NN'), ('–logy', 'NN'), ('refers', 'NNS'), ('to', 'TO'), ('word', 'NN'), ('or', 'CC'), ('speech', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['term', 'phonology', 'comes', 'Ancient', 'Greek', 'term', 'phono-', 'means', 'voice', 'sound', ',', 'suffix', '–logy', 'refers', 'word', 'speech', '.']

 TOTAL FILTERED TOKENS ==>  17

 ---- POST FOR FILTERED TOKENS ----

 [('term', 'NN'), ('phonology', 'NN'), ('comes', 'VBZ'), ('Ancient', 'NNP'), ('Greek', 'JJ'), ('term', 'NN'), ('phono-', 'JJ'), ('means', 'VBZ'), ('voice', 'NN'), ('sound', 'NN'), (',', ','), ('suffix', 'JJ'), ('–logy', 'NN'), ('refers', 'NNS'), ('word', 'NN'), ('speech', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['term phonology', 'phonology comes', 'comes Ancient', 'Ancient Greek', 'Greek term', 'term phono-', 'phono- means', 'means voice', 'voice sound', 'sound ,', ', suffix', 'suffix –logy', '–logy refers', 'refers word', 'word speech', 'speech .'] 

 TOTAL BIGRAMS --> 16 



 ---- TRI-GRAMS ---- 

 ['term phonology comes', 'phonology comes Ancient', 'comes Ancient Greek', 'Ancient Greek term', 'Greek term phono-', 'term phono- means', 'phono- means voice', 'means voice sound', 'voice sound ,', 'sound , suffix', ', suffix –logy', 'suffix –logy refers', '–logy refers word', 'refers word speech', 'word speech .'] 

 TOTAL TRIGRAMS --> 15 



 ---- NOUN PHRASES ---- 

 ['term', 'phonology', 'Greek term', 'voice', 'sound', 'suffix –logy', 'word', 'speech'] 

 TOTAL NOUN PHRASES --> 8 



 ---- NER ----

 
 ORGANIZATION ---> ['Ancient Greek']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['term', 'phonolog', 'come', 'ancient', 'greek', 'term', 'phono-', 'mean', 'voic', 'sound', ',', 'suffix', '–logi', 'refer', 'word', 'speech', '.']

 TOTAL PORTER STEM WORDS ==> 17



 ---- SNOWBALL STEMMING ----

['term', 'phonolog', 'come', 'ancient', 'greek', 'term', 'phono-', 'mean', 'voic', 'sound', ',', 'suffix', '–logi', 'refer', 'word', 'speech', '.']

 TOTAL SNOWBALL STEM WORDS ==> 17



 ---- LEMMATIZATION ----

['term', 'phonology', 'come', 'Ancient', 'Greek', 'term', 'phono-', 'mean', 'voice', 'sound', ',', 'suffix', '–logy', 'refers', 'word', 'speech', '.']

 TOTAL LEMMATIZE WORDS ==> 17

************************************************************************************************************************

57 --> In 1993 Nikolai Trubetzkoy stated that  Phonology is “the study of sound pertaining to the system of language". 


 ---- TOKENS ----

 ['In', '1993', 'Nikolai', 'Trubetzkoy', 'stated', 'that', 'Phonology', 'is', '“', 'the', 'study', 'of', 'sound', 'pertaining', 'to', 'the', 'system', 'of', 'language', "''", '.'] 

 TOTAL TOKENS ==> 21

 ---- POST ----

 [('In', 'IN'), ('1993', 'CD'), ('Nikolai', 'NNP'), ('Trubetzkoy', 'NNP'), ('stated', 'VBD'), ('that', 'IN'), ('Phonology', 'NNP'), ('is', 'VBZ'), ('“', 'VBN'), ('the', 'DT'), ('study', 'NN'), ('of', 'IN'), ('sound', 'NN'), ('pertaining', 'VBG'), ('to', 'TO'), ('the', 'DT'), ('system', 'NN'), ('of', 'IN'), ('language', 'NN'), ("''", "''"), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['1993', 'Nikolai', 'Trubetzkoy', 'stated', 'Phonology', '“', 'study', 'sound', 'pertaining', 'system', 'language', "''", '.']

 TOTAL FILTERED TOKENS ==>  13

 ---- POST FOR FILTERED TOKENS ----

 [('1993', 'CD'), ('Nikolai', 'NNP'), ('Trubetzkoy', 'NNP'), ('stated', 'VBD'), ('Phonology', 'NNP'), ('“', 'NNP'), ('study', 'NN'), ('sound', 'VBD'), ('pertaining', 'VBG'), ('system', 'NN'), ('language', 'NN'), ("''", "''"), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['1993 Nikolai', 'Nikolai Trubetzkoy', 'Trubetzkoy stated', 'stated Phonology', 'Phonology “', '“ study', 'study sound', 'sound pertaining', 'pertaining system', 'system language', "language ''", "'' ."] 

 TOTAL BIGRAMS --> 12 



 ---- TRI-GRAMS ---- 

 ['1993 Nikolai Trubetzkoy', 'Nikolai Trubetzkoy stated', 'Trubetzkoy stated Phonology', 'stated Phonology “', 'Phonology “ study', '“ study sound', 'study sound pertaining', 'sound pertaining system', 'pertaining system language', "system language ''", "language '' ."] 

 TOTAL TRIGRAMS --> 11 



 ---- NOUN PHRASES ---- 

 ['study', 'system', 'language'] 

 TOTAL NOUN PHRASES --> 3 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> ['Nikolai Trubetzkoy', 'Phonology']
 TOTAL PERSON ENTITY --> 2 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['1993', 'nikolai', 'trubetzkoy', 'state', 'phonolog', '“', 'studi', 'sound', 'pertain', 'system', 'languag', "''", '.']

 TOTAL PORTER STEM WORDS ==> 13



 ---- SNOWBALL STEMMING ----

['1993', 'nikolai', 'trubetzkoy', 'state', 'phonolog', '“', 'studi', 'sound', 'pertain', 'system', 'languag', "''", '.']

 TOTAL SNOWBALL STEM WORDS ==> 13



 ---- LEMMATIZATION ----

['1993', 'Nikolai', 'Trubetzkoy', 'stated', 'Phonology', '“', 'study', 'sound', 'pertaining', 'system', 'language', "''", '.']

 TOTAL LEMMATIZE WORDS ==> 13

************************************************************************************************************************

58 --> Whereas Lass in  1998 wrote that phonology refers broadly with the sounds of language, concerned with the to  lathe sub discipline of linguistics, whereas it could be explained as, "phonology proper is  concerned with the function, behaviour and organization of sounds as linguistic items. 


 ---- TOKENS ----

 ['Whereas', 'Lass', 'in', '1998', 'wrote', 'that', 'phonology', 'refers', 'broadly', 'with', 'the', 'sounds', 'of', 'language', ',', 'concerned', 'with', 'the', 'to', 'lathe', 'sub', 'discipline', 'of', 'linguistics', ',', 'whereas', 'it', 'could', 'be', 'explained', 'as', ',', '``', 'phonology', 'proper', 'is', 'concerned', 'with', 'the', 'function', ',', 'behaviour', 'and', 'organization', 'of', 'sounds', 'as', 'linguistic', 'items', '.'] 

 TOTAL TOKENS ==> 50

 ---- POST ----

 [('Whereas', 'NNP'), ('Lass', 'NNP'), ('in', 'IN'), ('1998', 'CD'), ('wrote', 'VBD'), ('that', 'IN'), ('phonology', 'NN'), ('refers', 'NNS'), ('broadly', 'RB'), ('with', 'IN'), ('the', 'DT'), ('sounds', 'NNS'), ('of', 'IN'), ('language', 'NN'), (',', ','), ('concerned', 'VBN'), ('with', 'IN'), ('the', 'DT'), ('to', 'TO'), ('lathe', 'VB'), ('sub', 'JJ'), ('discipline', 'NN'), ('of', 'IN'), ('linguistics', 'NNS'), (',', ','), ('whereas', 'IN'), ('it', 'PRP'), ('could', 'MD'), ('be', 'VB'), ('explained', 'VBN'), ('as', 'IN'), (',', ','), ('``', '``'), ('phonology', 'NN'), ('proper', 'NN'), ('is', 'VBZ'), ('concerned', 'VBN'), ('with', 'IN'), ('the', 'DT'), ('function', 'NN'), (',', ','), ('behaviour', 'NN'), ('and', 'CC'), ('organization', 'NN'), ('of', 'IN'), ('sounds', 'NNS'), ('as', 'IN'), ('linguistic', 'JJ'), ('items', 'NNS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Whereas', 'Lass', '1998', 'wrote', 'phonology', 'refers', 'broadly', 'sounds', 'language', ',', 'concerned', 'lathe', 'sub', 'discipline', 'linguistics', ',', 'whereas', 'could', 'explained', ',', '``', 'phonology', 'proper', 'concerned', 'function', ',', 'behaviour', 'organization', 'sounds', 'linguistic', 'items', '.']

 TOTAL FILTERED TOKENS ==>  32

 ---- POST FOR FILTERED TOKENS ----

 [('Whereas', 'IN'), ('Lass', 'NNP'), ('1998', 'CD'), ('wrote', 'VBD'), ('phonology', 'NN'), ('refers', 'NNS'), ('broadly', 'RB'), ('sounds', 'VBZ'), ('language', 'NN'), (',', ','), ('concerned', 'VBN'), ('lathe', 'JJ'), ('sub', 'NN'), ('discipline', 'NN'), ('linguistics', 'NNS'), (',', ','), ('whereas', 'NNS'), ('could', 'MD'), ('explained', 'VB'), (',', ','), ('``', '``'), ('phonology', 'NN'), ('proper', 'JJ'), ('concerned', 'JJ'), ('function', 'NN'), (',', ','), ('behaviour', 'JJ'), ('organization', 'NN'), ('sounds', 'VBZ'), ('linguistic', 'JJ'), ('items', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Whereas Lass', 'Lass 1998', '1998 wrote', 'wrote phonology', 'phonology refers', 'refers broadly', 'broadly sounds', 'sounds language', 'language ,', ', concerned', 'concerned lathe', 'lathe sub', 'sub discipline', 'discipline linguistics', 'linguistics ,', ', whereas', 'whereas could', 'could explained', 'explained ,', ', ``', '`` phonology', 'phonology proper', 'proper concerned', 'concerned function', 'function ,', ', behaviour', 'behaviour organization', 'organization sounds', 'sounds linguistic', 'linguistic items', 'items .'] 

 TOTAL BIGRAMS --> 31 



 ---- TRI-GRAMS ---- 

 ['Whereas Lass 1998', 'Lass 1998 wrote', '1998 wrote phonology', 'wrote phonology refers', 'phonology refers broadly', 'refers broadly sounds', 'broadly sounds language', 'sounds language ,', 'language , concerned', ', concerned lathe', 'concerned lathe sub', 'lathe sub discipline', 'sub discipline linguistics', 'discipline linguistics ,', 'linguistics , whereas', ', whereas could', 'whereas could explained', 'could explained ,', 'explained , ``', ', `` phonology', '`` phonology proper', 'phonology proper concerned', 'proper concerned function', 'concerned function ,', 'function , behaviour', ', behaviour organization', 'behaviour organization sounds', 'organization sounds linguistic', 'sounds linguistic items', 'linguistic items .'] 

 TOTAL TRIGRAMS --> 30 



 ---- NOUN PHRASES ---- 

 ['phonology', 'language', 'lathe sub', 'discipline', 'phonology', 'proper concerned function', 'behaviour organization'] 

 TOTAL NOUN PHRASES --> 7 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['wherea', 'lass', '1998', 'wrote', 'phonolog', 'refer', 'broadli', 'sound', 'languag', ',', 'concern', 'lath', 'sub', 'disciplin', 'linguist', ',', 'wherea', 'could', 'explain', ',', '``', 'phonolog', 'proper', 'concern', 'function', ',', 'behaviour', 'organ', 'sound', 'linguist', 'item', '.']

 TOTAL PORTER STEM WORDS ==> 32



 ---- SNOWBALL STEMMING ----

['wherea', 'lass', '1998', 'wrote', 'phonolog', 'refer', 'broad', 'sound', 'languag', ',', 'concern', 'lath', 'sub', 'disciplin', 'linguist', ',', 'wherea', 'could', 'explain', ',', '``', 'phonolog', 'proper', 'concern', 'function', ',', 'behaviour', 'organ', 'sound', 'linguist', 'item', '.']

 TOTAL SNOWBALL STEM WORDS ==> 32



 ---- LEMMATIZATION ----

['Whereas', 'Lass', '1998', 'wrote', 'phonology', 'refers', 'broadly', 'sound', 'language', ',', 'concerned', 'lathe', 'sub', 'discipline', 'linguistics', ',', 'whereas', 'could', 'explained', ',', '``', 'phonology', 'proper', 'concerned', 'function', ',', 'behaviour', 'organization', 'sound', 'linguistic', 'item', '.']

 TOTAL LEMMATIZE WORDS ==> 32

************************************************************************************************************************

59 --> Phonology include semantic use of sound to encode meaning of any Human language. 


 ---- TOKENS ----

 ['Phonology', 'include', 'semantic', 'use', 'of', 'sound', 'to', 'encode', 'meaning', 'of', 'any', 'Human', 'language', '.'] 

 TOTAL TOKENS ==> 14

 ---- POST ----

 [('Phonology', 'NNP'), ('include', 'VBP'), ('semantic', 'JJ'), ('use', 'NN'), ('of', 'IN'), ('sound', 'NN'), ('to', 'TO'), ('encode', 'VB'), ('meaning', 'NN'), ('of', 'IN'), ('any', 'DT'), ('Human', 'NNP'), ('language', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Phonology', 'include', 'semantic', 'use', 'sound', 'encode', 'meaning', 'Human', 'language', '.']

 TOTAL FILTERED TOKENS ==>  10

 ---- POST FOR FILTERED TOKENS ----

 [('Phonology', 'NNP'), ('include', 'VBP'), ('semantic', 'JJ'), ('use', 'NN'), ('sound', 'JJ'), ('encode', 'NN'), ('meaning', 'VBG'), ('Human', 'NNP'), ('language', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Phonology include', 'include semantic', 'semantic use', 'use sound', 'sound encode', 'encode meaning', 'meaning Human', 'Human language', 'language .'] 

 TOTAL BIGRAMS --> 9 



 ---- TRI-GRAMS ---- 

 ['Phonology include semantic', 'include semantic use', 'semantic use sound', 'use sound encode', 'sound encode meaning', 'encode meaning Human', 'meaning Human language', 'Human language .'] 

 TOTAL TRIGRAMS --> 8 



 ---- NOUN PHRASES ---- 

 ['semantic use', 'sound encode', 'language'] 

 TOTAL NOUN PHRASES --> 3 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> ['Human']
 TOTAL PERSON ENTITY --> 1 


 GPE ---> ['Phonology']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['phonolog', 'includ', 'semant', 'use', 'sound', 'encod', 'mean', 'human', 'languag', '.']

 TOTAL PORTER STEM WORDS ==> 10



 ---- SNOWBALL STEMMING ----

['phonolog', 'includ', 'semant', 'use', 'sound', 'encod', 'mean', 'human', 'languag', '.']

 TOTAL SNOWBALL STEM WORDS ==> 10



 ---- LEMMATIZATION ----

['Phonology', 'include', 'semantic', 'use', 'sound', 'encode', 'meaning', 'Human', 'language', '.']

 TOTAL LEMMATIZE WORDS ==> 10

************************************************************************************************************************

60 --> (Clark et al.,2007) [6]. 


 ---- TOKENS ----

 ['(', 'Clark', 'et', 'al.,2007', ')', '[', '6', ']', '.'] 

 TOTAL TOKENS ==> 9

 ---- POST ----

 [('(', '('), ('Clark', 'NNP'), ('et', 'NNP'), ('al.,2007', 'NN'), (')', ')'), ('[', 'VBZ'), ('6', 'CD'), (']', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['(', 'Clark', 'et', 'al.,2007', ')', '[', '6', ']', '.']

 TOTAL FILTERED TOKENS ==>  9

 ---- POST FOR FILTERED TOKENS ----

 [('(', '('), ('Clark', 'NNP'), ('et', 'NNP'), ('al.,2007', 'NN'), (')', ')'), ('[', 'VBZ'), ('6', 'CD'), (']', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['( Clark', 'Clark et', 'et al.,2007', 'al.,2007 )', ') [', '[ 6', '6 ]', '] .'] 

 TOTAL BIGRAMS --> 8 



 ---- TRI-GRAMS ---- 

 ['( Clark et', 'Clark et al.,2007', 'et al.,2007 )', 'al.,2007 ) [', ') [ 6', '[ 6 ]', '6 ] .'] 

 TOTAL TRIGRAMS --> 7 



 ---- NOUN PHRASES ---- 

 [']'] 

 TOTAL NOUN PHRASES --> 1 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['(', 'clark', 'et', 'al.,2007', ')', '[', '6', ']', '.']

 TOTAL PORTER STEM WORDS ==> 9



 ---- SNOWBALL STEMMING ----

['(', 'clark', 'et', 'al.,2007', ')', '[', '6', ']', '.']

 TOTAL SNOWBALL STEM WORDS ==> 9



 ---- LEMMATIZATION ----

['(', 'Clark', 'et', 'al.,2007', ')', '[', '6', ']', '.']

 TOTAL LEMMATIZE WORDS ==> 9

************************************************************************************************************************

61 --> 2. 


 ---- TOKENS ----

 ['2', '.'] 

 TOTAL TOKENS ==> 2

 ---- POST ----

 [('2', 'CD'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['2', '.']

 TOTAL FILTERED TOKENS ==>  2

 ---- POST FOR FILTERED TOKENS ----

 [('2', 'CD'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['2 .'] 

 TOTAL BIGRAMS --> 1 



 ---- TRI-GRAMS ---- 

 [] 

 TOTAL TRIGRAMS --> 0 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['2', '.']

 TOTAL PORTER STEM WORDS ==> 2



 ---- SNOWBALL STEMMING ----

['2', '.']

 TOTAL SNOWBALL STEM WORDS ==> 2



 ---- LEMMATIZATION ----

['2', '.']

 TOTAL LEMMATIZE WORDS ==> 2

************************************************************************************************************************

62 --> Morphology  The different parts of the word represent the smallest units of meaning known as Morphemes. 


 ---- TOKENS ----

 ['Morphology', 'The', 'different', 'parts', 'of', 'the', 'word', 'represent', 'the', 'smallest', 'units', 'of', 'meaning', 'known', 'as', 'Morphemes', '.'] 

 TOTAL TOKENS ==> 17

 ---- POST ----

 [('Morphology', 'NNP'), ('The', 'DT'), ('different', 'JJ'), ('parts', 'NNS'), ('of', 'IN'), ('the', 'DT'), ('word', 'NN'), ('represent', 'NN'), ('the', 'DT'), ('smallest', 'JJS'), ('units', 'NNS'), ('of', 'IN'), ('meaning', 'VBG'), ('known', 'VBN'), ('as', 'IN'), ('Morphemes', 'NNP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Morphology', 'different', 'parts', 'word', 'represent', 'smallest', 'units', 'meaning', 'known', 'Morphemes', '.']

 TOTAL FILTERED TOKENS ==>  11

 ---- POST FOR FILTERED TOKENS ----

 [('Morphology', 'NNP'), ('different', 'JJ'), ('parts', 'NNS'), ('word', 'NN'), ('represent', 'NN'), ('smallest', 'JJS'), ('units', 'NNS'), ('meaning', 'VBG'), ('known', 'VBN'), ('Morphemes', 'NNP'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Morphology different', 'different parts', 'parts word', 'word represent', 'represent smallest', 'smallest units', 'units meaning', 'meaning known', 'known Morphemes', 'Morphemes .'] 

 TOTAL BIGRAMS --> 10 



 ---- TRI-GRAMS ---- 

 ['Morphology different parts', 'different parts word', 'parts word represent', 'word represent smallest', 'represent smallest units', 'smallest units meaning', 'units meaning known', 'meaning known Morphemes', 'known Morphemes .'] 

 TOTAL TRIGRAMS --> 9 



 ---- NOUN PHRASES ---- 

 ['word', 'represent'] 

 TOTAL NOUN PHRASES --> 2 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> ['Morphemes']
 TOTAL PERSON ENTITY --> 1 


 GPE ---> ['Morphology']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['morpholog', 'differ', 'part', 'word', 'repres', 'smallest', 'unit', 'mean', 'known', 'morphem', '.']

 TOTAL PORTER STEM WORDS ==> 11



 ---- SNOWBALL STEMMING ----

['morpholog', 'differ', 'part', 'word', 'repres', 'smallest', 'unit', 'mean', 'known', 'morphem', '.']

 TOTAL SNOWBALL STEM WORDS ==> 11



 ---- LEMMATIZATION ----

['Morphology', 'different', 'part', 'word', 'represent', 'smallest', 'unit', 'meaning', 'known', 'Morphemes', '.']

 TOTAL LEMMATIZE WORDS ==> 11

************************************************************************************************************************

63 --> Morphology which comprise of Nature of words, are initiated by morphemes. 


 ---- TOKENS ----

 ['Morphology', 'which', 'comprise', 'of', 'Nature', 'of', 'words', ',', 'are', 'initiated', 'by', 'morphemes', '.'] 

 TOTAL TOKENS ==> 13

 ---- POST ----

 [('Morphology', 'NNP'), ('which', 'WDT'), ('comprise', 'NN'), ('of', 'IN'), ('Nature', 'NN'), ('of', 'IN'), ('words', 'NNS'), (',', ','), ('are', 'VBP'), ('initiated', 'VBN'), ('by', 'IN'), ('morphemes', 'NNS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Morphology', 'comprise', 'Nature', 'words', ',', 'initiated', 'morphemes', '.']

 TOTAL FILTERED TOKENS ==>  8

 ---- POST FOR FILTERED TOKENS ----

 [('Morphology', 'NNP'), ('comprise', 'NN'), ('Nature', 'NN'), ('words', 'NNS'), (',', ','), ('initiated', 'VBN'), ('morphemes', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Morphology comprise', 'comprise Nature', 'Nature words', 'words ,', ', initiated', 'initiated morphemes', 'morphemes .'] 

 TOTAL BIGRAMS --> 7 



 ---- TRI-GRAMS ---- 

 ['Morphology comprise Nature', 'comprise Nature words', 'Nature words ,', 'words , initiated', ', initiated morphemes', 'initiated morphemes .'] 

 TOTAL TRIGRAMS --> 6 



 ---- NOUN PHRASES ---- 

 ['comprise', 'Nature'] 

 TOTAL NOUN PHRASES --> 2 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> ['Morphology']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['morpholog', 'compris', 'natur', 'word', ',', 'initi', 'morphem', '.']

 TOTAL PORTER STEM WORDS ==> 8



 ---- SNOWBALL STEMMING ----

['morpholog', 'compris', 'natur', 'word', ',', 'initi', 'morphem', '.']

 TOTAL SNOWBALL STEM WORDS ==> 8



 ---- LEMMATIZATION ----

['Morphology', 'comprise', 'Nature', 'word', ',', 'initiated', 'morpheme', '.']

 TOTAL LEMMATIZE WORDS ==> 8

************************************************************************************************************************

64 --> An example of  Morpheme could be, the word precancellation can be morphologically scrutinized into three  separate morphemes: the prefix pre, the root cancella, and the suffix -tion. 


 ---- TOKENS ----

 ['An', 'example', 'of', 'Morpheme', 'could', 'be', ',', 'the', 'word', 'precancellation', 'can', 'be', 'morphologically', 'scrutinized', 'into', 'three', 'separate', 'morphemes', ':', 'the', 'prefix', 'pre', ',', 'the', 'root', 'cancella', ',', 'and', 'the', 'suffix', '-tion', '.'] 

 TOTAL TOKENS ==> 32

 ---- POST ----

 [('An', 'DT'), ('example', 'NN'), ('of', 'IN'), ('Morpheme', 'NNP'), ('could', 'MD'), ('be', 'VB'), (',', ','), ('the', 'DT'), ('word', 'NN'), ('precancellation', 'NN'), ('can', 'MD'), ('be', 'VB'), ('morphologically', 'RB'), ('scrutinized', 'VBN'), ('into', 'IN'), ('three', 'CD'), ('separate', 'JJ'), ('morphemes', 'NNS'), (':', ':'), ('the', 'DT'), ('prefix', 'NN'), ('pre', 'NN'), (',', ','), ('the', 'DT'), ('root', 'NN'), ('cancella', 'NN'), (',', ','), ('and', 'CC'), ('the', 'DT'), ('suffix', 'JJ'), ('-tion', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['example', 'Morpheme', 'could', ',', 'word', 'precancellation', 'morphologically', 'scrutinized', 'three', 'separate', 'morphemes', ':', 'prefix', 'pre', ',', 'root', 'cancella', ',', 'suffix', '-tion', '.']

 TOTAL FILTERED TOKENS ==>  21

 ---- POST FOR FILTERED TOKENS ----

 [('example', 'NN'), ('Morpheme', 'NNP'), ('could', 'MD'), (',', ','), ('word', 'NN'), ('precancellation', 'NN'), ('morphologically', 'RB'), ('scrutinized', 'VBD'), ('three', 'CD'), ('separate', 'JJ'), ('morphemes', 'NNS'), (':', ':'), ('prefix', 'NN'), ('pre', 'NN'), (',', ','), ('root', 'NN'), ('cancella', 'NN'), (',', ','), ('suffix', 'JJ'), ('-tion', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['example Morpheme', 'Morpheme could', 'could ,', ', word', 'word precancellation', 'precancellation morphologically', 'morphologically scrutinized', 'scrutinized three', 'three separate', 'separate morphemes', 'morphemes :', ': prefix', 'prefix pre', 'pre ,', ', root', 'root cancella', 'cancella ,', ', suffix', 'suffix -tion', '-tion .'] 

 TOTAL BIGRAMS --> 20 



 ---- TRI-GRAMS ---- 

 ['example Morpheme could', 'Morpheme could ,', 'could , word', ', word precancellation', 'word precancellation morphologically', 'precancellation morphologically scrutinized', 'morphologically scrutinized three', 'scrutinized three separate', 'three separate morphemes', 'separate morphemes :', 'morphemes : prefix', ': prefix pre', 'prefix pre ,', 'pre , root', ', root cancella', 'root cancella ,', 'cancella , suffix', ', suffix -tion', 'suffix -tion .'] 

 TOTAL TRIGRAMS --> 19 



 ---- NOUN PHRASES ---- 

 ['example', 'word', 'precancellation', 'prefix', 'pre', 'root', 'cancella', 'suffix -tion'] 

 TOTAL NOUN PHRASES --> 8 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['exampl', 'morphem', 'could', ',', 'word', 'precancel', 'morpholog', 'scrutin', 'three', 'separ', 'morphem', ':', 'prefix', 'pre', ',', 'root', 'cancella', ',', 'suffix', '-tion', '.']

 TOTAL PORTER STEM WORDS ==> 21



 ---- SNOWBALL STEMMING ----

['exampl', 'morphem', 'could', ',', 'word', 'precancel', 'morpholog', 'scrutin', 'three', 'separ', 'morphem', ':', 'prefix', 'pre', ',', 'root', 'cancella', ',', 'suffix', '-tion', '.']

 TOTAL SNOWBALL STEM WORDS ==> 21



 ---- LEMMATIZATION ----

['example', 'Morpheme', 'could', ',', 'word', 'precancellation', 'morphologically', 'scrutinized', 'three', 'separate', 'morpheme', ':', 'prefix', 'pre', ',', 'root', 'cancella', ',', 'suffix', '-tion', '.']

 TOTAL LEMMATIZE WORDS ==> 21

************************************************************************************************************************

65 --> The interpretation  of morpheme stays same across all the words, just to understand the meaning humans can  break any unknown word into morphemes. 


 ---- TOKENS ----

 ['The', 'interpretation', 'of', 'morpheme', 'stays', 'same', 'across', 'all', 'the', 'words', ',', 'just', 'to', 'understand', 'the', 'meaning', 'humans', 'can', 'break', 'any', 'unknown', 'word', 'into', 'morphemes', '.'] 

 TOTAL TOKENS ==> 25

 ---- POST ----

 [('The', 'DT'), ('interpretation', 'NN'), ('of', 'IN'), ('morpheme', 'NN'), ('stays', 'NNS'), ('same', 'JJ'), ('across', 'IN'), ('all', 'PDT'), ('the', 'DT'), ('words', 'NNS'), (',', ','), ('just', 'RB'), ('to', 'TO'), ('understand', 'VB'), ('the', 'DT'), ('meaning', 'NN'), ('humans', 'NNS'), ('can', 'MD'), ('break', 'VB'), ('any', 'DT'), ('unknown', 'JJ'), ('word', 'NN'), ('into', 'IN'), ('morphemes', 'NNS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['interpretation', 'morpheme', 'stays', 'across', 'words', ',', 'understand', 'meaning', 'humans', 'break', 'unknown', 'word', 'morphemes', '.']

 TOTAL FILTERED TOKENS ==>  14

 ---- POST FOR FILTERED TOKENS ----

 [('interpretation', 'NN'), ('morpheme', 'NN'), ('stays', 'VBZ'), ('across', 'IN'), ('words', 'NNS'), (',', ','), ('understand', 'VBP'), ('meaning', 'VBG'), ('humans', 'NNS'), ('break', 'VBP'), ('unknown', 'JJ'), ('word', 'NN'), ('morphemes', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['interpretation morpheme', 'morpheme stays', 'stays across', 'across words', 'words ,', ', understand', 'understand meaning', 'meaning humans', 'humans break', 'break unknown', 'unknown word', 'word morphemes', 'morphemes .'] 

 TOTAL BIGRAMS --> 13 



 ---- TRI-GRAMS ---- 

 ['interpretation morpheme stays', 'morpheme stays across', 'stays across words', 'across words ,', 'words , understand', ', understand meaning', 'understand meaning humans', 'meaning humans break', 'humans break unknown', 'break unknown word', 'unknown word morphemes', 'word morphemes .'] 

 TOTAL TRIGRAMS --> 12 



 ---- NOUN PHRASES ---- 

 ['interpretation', 'morpheme', 'unknown word'] 

 TOTAL NOUN PHRASES --> 3 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['interpret', 'morphem', 'stay', 'across', 'word', ',', 'understand', 'mean', 'human', 'break', 'unknown', 'word', 'morphem', '.']

 TOTAL PORTER STEM WORDS ==> 14



 ---- SNOWBALL STEMMING ----

['interpret', 'morphem', 'stay', 'across', 'word', ',', 'understand', 'mean', 'human', 'break', 'unknown', 'word', 'morphem', '.']

 TOTAL SNOWBALL STEM WORDS ==> 14



 ---- LEMMATIZATION ----

['interpretation', 'morpheme', 'stay', 'across', 'word', ',', 'understand', 'meaning', 'human', 'break', 'unknown', 'word', 'morpheme', '.']

 TOTAL LEMMATIZE WORDS ==> 14

************************************************************************************************************************

66 --> For example, adding the suffix –ed to a verb,  conveys that the action of the verb took place in the past. 


 ---- TOKENS ----

 ['For', 'example', ',', 'adding', 'the', 'suffix', '–ed', 'to', 'a', 'verb', ',', 'conveys', 'that', 'the', 'action', 'of', 'the', 'verb', 'took', 'place', 'in', 'the', 'past', '.'] 

 TOTAL TOKENS ==> 24

 ---- POST ----

 [('For', 'IN'), ('example', 'NN'), (',', ','), ('adding', 'VBG'), ('the', 'DT'), ('suffix', 'NN'), ('–ed', 'NN'), ('to', 'TO'), ('a', 'DT'), ('verb', 'NN'), (',', ','), ('conveys', 'NNS'), ('that', 'IN'), ('the', 'DT'), ('action', 'NN'), ('of', 'IN'), ('the', 'DT'), ('verb', 'NN'), ('took', 'VBD'), ('place', 'NN'), ('in', 'IN'), ('the', 'DT'), ('past', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['example', ',', 'adding', 'suffix', '–ed', 'verb', ',', 'conveys', 'action', 'verb', 'took', 'place', 'past', '.']

 TOTAL FILTERED TOKENS ==>  14

 ---- POST FOR FILTERED TOKENS ----

 [('example', 'NN'), (',', ','), ('adding', 'VBG'), ('suffix', 'JJ'), ('–ed', 'NNP'), ('verb', 'NN'), (',', ','), ('conveys', 'JJ'), ('action', 'NN'), ('verb', 'NN'), ('took', 'VBD'), ('place', 'NN'), ('past', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['example ,', ', adding', 'adding suffix', 'suffix –ed', '–ed verb', 'verb ,', ', conveys', 'conveys action', 'action verb', 'verb took', 'took place', 'place past', 'past .'] 

 TOTAL BIGRAMS --> 13 



 ---- TRI-GRAMS ---- 

 ['example , adding', ', adding suffix', 'adding suffix –ed', 'suffix –ed verb', '–ed verb ,', 'verb , conveys', ', conveys action', 'conveys action verb', 'action verb took', 'verb took place', 'took place past', 'place past .'] 

 TOTAL TRIGRAMS --> 12 



 ---- NOUN PHRASES ---- 

 ['example', 'verb', 'conveys action', 'verb', 'place', 'past'] 

 TOTAL NOUN PHRASES --> 6 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['exampl', ',', 'ad', 'suffix', '–ed', 'verb', ',', 'convey', 'action', 'verb', 'took', 'place', 'past', '.']

 TOTAL PORTER STEM WORDS ==> 14



 ---- SNOWBALL STEMMING ----

['exampl', ',', 'ad', 'suffix', '–ed', 'verb', ',', 'convey', 'action', 'verb', 'took', 'place', 'past', '.']

 TOTAL SNOWBALL STEM WORDS ==> 14



 ---- LEMMATIZATION ----

['example', ',', 'adding', 'suffix', '–ed', 'verb', ',', 'conveys', 'action', 'verb', 'took', 'place', 'past', '.']

 TOTAL LEMMATIZE WORDS ==> 14

************************************************************************************************************************

67 --> The words that cannot be divided  and have meaning by themselves are called Lexical morpheme (e.g. 


 ---- TOKENS ----

 ['The', 'words', 'that', 'can', 'not', 'be', 'divided', 'and', 'have', 'meaning', 'by', 'themselves', 'are', 'called', 'Lexical', 'morpheme', '(', 'e.g', '.'] 

 TOTAL TOKENS ==> 19

 ---- POST ----

 [('The', 'DT'), ('words', 'NNS'), ('that', 'WDT'), ('can', 'MD'), ('not', 'RB'), ('be', 'VB'), ('divided', 'VBN'), ('and', 'CC'), ('have', 'VBP'), ('meaning', 'VBN'), ('by', 'IN'), ('themselves', 'PRP'), ('are', 'VBP'), ('called', 'VBN'), ('Lexical', 'JJ'), ('morpheme', 'NN'), ('(', '('), ('e.g', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['words', 'divided', 'meaning', 'called', 'Lexical', 'morpheme', '(', 'e.g', '.']

 TOTAL FILTERED TOKENS ==>  9

 ---- POST FOR FILTERED TOKENS ----

 [('words', 'NNS'), ('divided', 'VBD'), ('meaning', 'NN'), ('called', 'VBN'), ('Lexical', 'JJ'), ('morpheme', 'NN'), ('(', '('), ('e.g', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['words divided', 'divided meaning', 'meaning called', 'called Lexical', 'Lexical morpheme', 'morpheme (', '( e.g', 'e.g .'] 

 TOTAL BIGRAMS --> 8 



 ---- TRI-GRAMS ---- 

 ['words divided meaning', 'divided meaning called', 'meaning called Lexical', 'called Lexical morpheme', 'Lexical morpheme (', 'morpheme ( e.g', '( e.g .'] 

 TOTAL TRIGRAMS --> 7 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 []

 ---- PORTER STEMMING ----

['word', 'divid', 'mean', 'call', 'lexic', 'morphem', '(', 'e.g', '.']

 TOTAL PORTER STEM WORDS ==> 9



 ---- SNOWBALL STEMMING ----

['word', 'divid', 'mean', 'call', 'lexic', 'morphem', '(', 'e.g', '.']

 TOTAL SNOWBALL STEM WORDS ==> 9



 ---- LEMMATIZATION ----

['word', 'divided', 'meaning', 'called', 'Lexical', 'morpheme', '(', 'e.g', '.']

 TOTAL LEMMATIZE WORDS ==> 9

************************************************************************************************************************

68 --> : table, chair) The words  (e.g. 


 ---- TOKENS ----

 [':', 'table', ',', 'chair', ')', 'The', 'words', '(', 'e.g', '.'] 

 TOTAL TOKENS ==> 10

 ---- POST ----

 [(':', ':'), ('table', 'NN'), (',', ','), ('chair', 'NN'), (')', ')'), ('The', 'DT'), ('words', 'NNS'), ('(', '('), ('e.g', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 [':', 'table', ',', 'chair', ')', 'words', '(', 'e.g', '.']

 TOTAL FILTERED TOKENS ==>  9

 ---- POST FOR FILTERED TOKENS ----

 [(':', ':'), ('table', 'NN'), (',', ','), ('chair', 'NN'), (')', ')'), ('words', 'NNS'), ('(', '('), ('e.g', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 [': table', 'table ,', ', chair', 'chair )', ') words', 'words (', '( e.g', 'e.g .'] 

 TOTAL BIGRAMS --> 8 



 ---- TRI-GRAMS ---- 

 [': table ,', 'table , chair', ', chair )', 'chair ) words', ') words (', 'words ( e.g', '( e.g .'] 

 TOTAL TRIGRAMS --> 7 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 []

 ---- PORTER STEMMING ----

[':', 'tabl', ',', 'chair', ')', 'word', '(', 'e.g', '.']

 TOTAL PORTER STEM WORDS ==> 9



 ---- SNOWBALL STEMMING ----

[':', 'tabl', ',', 'chair', ')', 'word', '(', 'e.g', '.']

 TOTAL SNOWBALL STEM WORDS ==> 9



 ---- LEMMATIZATION ----

[':', 'table', ',', 'chair', ')', 'word', '(', 'e.g', '.']

 TOTAL LEMMATIZE WORDS ==> 9

************************************************************************************************************************

69 --> -ed, -ing, -est, -ly, -ful) that are combined with the lexical morpheme are known as  Grammatical morphemes (eg. 


 ---- TOKENS ----

 ['-ed', ',', '-ing', ',', '-est', ',', '-ly', ',', '-ful', ')', 'that', 'are', 'combined', 'with', 'the', 'lexical', 'morpheme', 'are', 'known', 'as', 'Grammatical', 'morphemes', '(', 'eg', '.'] 

 TOTAL TOKENS ==> 25

 ---- POST ----

 [('-ed', 'NN'), (',', ','), ('-ing', 'VBG'), (',', ','), ('-est', 'JJS'), (',', ','), ('-ly', 'NN'), (',', ','), ('-ful', 'JJ'), (')', ')'), ('that', 'WDT'), ('are', 'VBP'), ('combined', 'VBN'), ('with', 'IN'), ('the', 'DT'), ('lexical', 'JJ'), ('morpheme', 'NN'), ('are', 'VBP'), ('known', 'VBN'), ('as', 'IN'), ('Grammatical', 'JJ'), ('morphemes', 'NNS'), ('(', '('), ('eg', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['-ed', ',', '-ing', ',', '-est', ',', '-ly', ',', '-ful', ')', 'combined', 'lexical', 'morpheme', 'known', 'Grammatical', 'morphemes', '(', 'eg', '.']

 TOTAL FILTERED TOKENS ==>  19

 ---- POST FOR FILTERED TOKENS ----

 [('-ed', 'NN'), (',', ','), ('-ing', 'VBG'), (',', ','), ('-est', 'JJS'), (',', ','), ('-ly', 'NN'), (',', ','), ('-ful', 'JJ'), (')', ')'), ('combined', 'VBD'), ('lexical', 'JJ'), ('morpheme', 'NN'), ('known', 'VBN'), ('Grammatical', 'NNP'), ('morphemes', 'NNS'), ('(', '('), ('eg', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['-ed ,', ', -ing', '-ing ,', ', -est', '-est ,', ', -ly', '-ly ,', ', -ful', '-ful )', ') combined', 'combined lexical', 'lexical morpheme', 'morpheme known', 'known Grammatical', 'Grammatical morphemes', 'morphemes (', '( eg', 'eg .'] 

 TOTAL BIGRAMS --> 18 



 ---- TRI-GRAMS ---- 

 ['-ed , -ing', ', -ing ,', '-ing , -est', ', -est ,', '-est , -ly', ', -ly ,', '-ly , -ful', ', -ful )', '-ful ) combined', ') combined lexical', 'combined lexical morpheme', 'lexical morpheme known', 'morpheme known Grammatical', 'known Grammatical morphemes', 'Grammatical morphemes (', 'morphemes ( eg', '( eg .'] 

 TOTAL TRIGRAMS --> 17 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 []

 ---- PORTER STEMMING ----

['-ed', ',', '-ing', ',', '-est', ',', '-li', ',', '-ful', ')', 'combin', 'lexic', 'morphem', 'known', 'grammat', 'morphem', '(', 'eg', '.']

 TOTAL PORTER STEM WORDS ==> 19



 ---- SNOWBALL STEMMING ----

['-ed', ',', '-ing', ',', '-est', ',', '-li', ',', '-ful', ')', 'combin', 'lexic', 'morphem', 'known', 'grammat', 'morphem', '(', 'eg', '.']

 TOTAL SNOWBALL STEM WORDS ==> 19



 ---- LEMMATIZATION ----

['-ed', ',', '-ing', ',', '-est', ',', '-ly', ',', '-ful', ')', 'combined', 'lexical', 'morpheme', 'known', 'Grammatical', 'morpheme', '(', 'eg', '.']

 TOTAL LEMMATIZE WORDS ==> 19

************************************************************************************************************************

70 --> Worked, Consulting, Smallest, Likely, Use). 


 ---- TOKENS ----

 ['Worked', ',', 'Consulting', ',', 'Smallest', ',', 'Likely', ',', 'Use', ')', '.'] 

 TOTAL TOKENS ==> 11

 ---- POST ----

 [('Worked', 'VBN'), (',', ','), ('Consulting', 'NNP'), (',', ','), ('Smallest', 'NNP'), (',', ','), ('Likely', 'NNP'), (',', ','), ('Use', 'NNP'), (')', ')'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Worked', ',', 'Consulting', ',', 'Smallest', ',', 'Likely', ',', 'Use', ')', '.']

 TOTAL FILTERED TOKENS ==>  11

 ---- POST FOR FILTERED TOKENS ----

 [('Worked', 'VBN'), (',', ','), ('Consulting', 'NNP'), (',', ','), ('Smallest', 'NNP'), (',', ','), ('Likely', 'NNP'), (',', ','), ('Use', 'NNP'), (')', ')'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Worked ,', ', Consulting', 'Consulting ,', ', Smallest', 'Smallest ,', ', Likely', 'Likely ,', ', Use', 'Use )', ') .'] 

 TOTAL BIGRAMS --> 10 



 ---- TRI-GRAMS ---- 

 ['Worked , Consulting', ', Consulting ,', 'Consulting , Smallest', ', Smallest ,', 'Smallest , Likely', ', Likely ,', 'Likely , Use', ', Use )', 'Use ) .'] 

 TOTAL TRIGRAMS --> 9 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 []

 ---- PORTER STEMMING ----

['work', ',', 'consult', ',', 'smallest', ',', 'like', ',', 'use', ')', '.']

 TOTAL PORTER STEM WORDS ==> 11



 ---- SNOWBALL STEMMING ----

['work', ',', 'consult', ',', 'smallest', ',', 'like', ',', 'use', ')', '.']

 TOTAL SNOWBALL STEM WORDS ==> 11



 ---- LEMMATIZATION ----

['Worked', ',', 'Consulting', ',', 'Smallest', ',', 'Likely', ',', 'Use', ')', '.']

 TOTAL LEMMATIZE WORDS ==> 11

************************************************************************************************************************

71 --> Those  grammatical morphemes that occurs in combination called bound morphemes( eg. 


 ---- TOKENS ----

 ['Those', 'grammatical', 'morphemes', 'that', 'occurs', 'in', 'combination', 'called', 'bound', 'morphemes', '(', 'eg', '.'] 

 TOTAL TOKENS ==> 13

 ---- POST ----

 [('Those', 'DT'), ('grammatical', 'JJ'), ('morphemes', 'NNS'), ('that', 'WDT'), ('occurs', 'VBZ'), ('in', 'IN'), ('combination', 'NN'), ('called', 'VBN'), ('bound', 'NN'), ('morphemes', 'NNS'), ('(', '('), ('eg', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['grammatical', 'morphemes', 'occurs', 'combination', 'called', 'bound', 'morphemes', '(', 'eg', '.']

 TOTAL FILTERED TOKENS ==>  10

 ---- POST FOR FILTERED TOKENS ----

 [('grammatical', 'JJ'), ('morphemes', 'NNS'), ('occurs', 'VBZ'), ('combination', 'NN'), ('called', 'VBN'), ('bound', 'NN'), ('morphemes', 'NNS'), ('(', '('), ('eg', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['grammatical morphemes', 'morphemes occurs', 'occurs combination', 'combination called', 'called bound', 'bound morphemes', 'morphemes (', '( eg', 'eg .'] 

 TOTAL BIGRAMS --> 9 



 ---- TRI-GRAMS ---- 

 ['grammatical morphemes occurs', 'morphemes occurs combination', 'occurs combination called', 'combination called bound', 'called bound morphemes', 'bound morphemes (', 'morphemes ( eg', '( eg .'] 

 TOTAL TRIGRAMS --> 8 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 []

 ---- PORTER STEMMING ----

['grammat', 'morphem', 'occur', 'combin', 'call', 'bound', 'morphem', '(', 'eg', '.']

 TOTAL PORTER STEM WORDS ==> 10



 ---- SNOWBALL STEMMING ----

['grammat', 'morphem', 'occur', 'combin', 'call', 'bound', 'morphem', '(', 'eg', '.']

 TOTAL SNOWBALL STEM WORDS ==> 10



 ---- LEMMATIZATION ----

['grammatical', 'morpheme', 'occurs', 'combination', 'called', 'bound', 'morpheme', '(', 'eg', '.']

 TOTAL LEMMATIZE WORDS ==> 10

************************************************************************************************************************

72 --> -ed, -ing)  Grammatical morphemes can be divided into bound morphemes and derivational morphemes. 


 ---- TOKENS ----

 ['-ed', ',', '-ing', ')', 'Grammatical', 'morphemes', 'can', 'be', 'divided', 'into', 'bound', 'morphemes', 'and', 'derivational', 'morphemes', '.'] 

 TOTAL TOKENS ==> 16

 ---- POST ----

 [('-ed', 'NN'), (',', ','), ('-ing', 'VBG'), (')', ')'), ('Grammatical', 'NNP'), ('morphemes', 'NNS'), ('can', 'MD'), ('be', 'VB'), ('divided', 'VBN'), ('into', 'IN'), ('bound', 'NN'), ('morphemes', 'NNS'), ('and', 'CC'), ('derivational', 'JJ'), ('morphemes', 'NNS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['-ed', ',', '-ing', ')', 'Grammatical', 'morphemes', 'divided', 'bound', 'morphemes', 'derivational', 'morphemes', '.']

 TOTAL FILTERED TOKENS ==>  12

 ---- POST FOR FILTERED TOKENS ----

 [('-ed', 'NN'), (',', ','), ('-ing', 'VBG'), (')', ')'), ('Grammatical', 'NNP'), ('morphemes', 'NNS'), ('divided', 'VBD'), ('bound', 'NN'), ('morphemes', 'NNS'), ('derivational', 'JJ'), ('morphemes', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['-ed ,', ', -ing', '-ing )', ') Grammatical', 'Grammatical morphemes', 'morphemes divided', 'divided bound', 'bound morphemes', 'morphemes derivational', 'derivational morphemes', 'morphemes .'] 

 TOTAL BIGRAMS --> 11 



 ---- TRI-GRAMS ---- 

 ['-ed , -ing', ', -ing )', '-ing ) Grammatical', ') Grammatical morphemes', 'Grammatical morphemes divided', 'morphemes divided bound', 'divided bound morphemes', 'bound morphemes derivational', 'morphemes derivational morphemes', 'derivational morphemes .'] 

 TOTAL TRIGRAMS --> 10 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 []

 ---- PORTER STEMMING ----

['-ed', ',', '-ing', ')', 'grammat', 'morphem', 'divid', 'bound', 'morphem', 'deriv', 'morphem', '.']

 TOTAL PORTER STEM WORDS ==> 12



 ---- SNOWBALL STEMMING ----

['-ed', ',', '-ing', ')', 'grammat', 'morphem', 'divid', 'bound', 'morphem', 'deriv', 'morphem', '.']

 TOTAL SNOWBALL STEM WORDS ==> 12



 ---- LEMMATIZATION ----

['-ed', ',', '-ing', ')', 'Grammatical', 'morpheme', 'divided', 'bound', 'morpheme', 'derivational', 'morpheme', '.']

 TOTAL LEMMATIZE WORDS ==> 12

************************************************************************************************************************

73 --> https://en.wikipedia.org/wiki/Ancient_Greek https://en.wikipedia.org/wiki/Nikolai_Trubetzkoy  3. 


 ---- TOKENS ----

 ['https', ':', '//en.wikipedia.org/wiki/Ancient_Greek', 'https', ':', '//en.wikipedia.org/wiki/Nikolai_Trubetzkoy', '3', '.'] 

 TOTAL TOKENS ==> 8

 ---- POST ----

 [('https', 'NN'), (':', ':'), ('//en.wikipedia.org/wiki/Ancient_Greek', 'JJ'), ('https', 'NN'), (':', ':'), ('//en.wikipedia.org/wiki/Nikolai_Trubetzkoy', 'JJ'), ('3', 'CD'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['https', ':', '//en.wikipedia.org/wiki/Ancient_Greek', 'https', ':', '//en.wikipedia.org/wiki/Nikolai_Trubetzkoy', '3', '.']

 TOTAL FILTERED TOKENS ==>  8

 ---- POST FOR FILTERED TOKENS ----

 [('https', 'NN'), (':', ':'), ('//en.wikipedia.org/wiki/Ancient_Greek', 'JJ'), ('https', 'NN'), (':', ':'), ('//en.wikipedia.org/wiki/Nikolai_Trubetzkoy', 'JJ'), ('3', 'CD'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['https :', ': //en.wikipedia.org/wiki/Ancient_Greek', '//en.wikipedia.org/wiki/Ancient_Greek https', 'https :', ': //en.wikipedia.org/wiki/Nikolai_Trubetzkoy', '//en.wikipedia.org/wiki/Nikolai_Trubetzkoy 3', '3 .'] 

 TOTAL BIGRAMS --> 7 



 ---- TRI-GRAMS ---- 

 ['https : //en.wikipedia.org/wiki/Ancient_Greek', ': //en.wikipedia.org/wiki/Ancient_Greek https', '//en.wikipedia.org/wiki/Ancient_Greek https :', 'https : //en.wikipedia.org/wiki/Nikolai_Trubetzkoy', ': //en.wikipedia.org/wiki/Nikolai_Trubetzkoy 3', '//en.wikipedia.org/wiki/Nikolai_Trubetzkoy 3 .'] 

 TOTAL TRIGRAMS --> 6 



 ---- NOUN PHRASES ---- 

 ['https', ' https'] 

 TOTAL NOUN PHRASES --> 2 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['http', ':', '//en.wikipedia.org/wiki/ancient_greek', 'http', ':', '//en.wikipedia.org/wiki/nikolai_trubetzkoy', '3', '.']

 TOTAL PORTER STEM WORDS ==> 8



 ---- SNOWBALL STEMMING ----

['https', ':', '//en.wikipedia.org/wiki/ancient_greek', 'https', ':', '//en.wikipedia.org/wiki/nikolai_trubetzkoy', '3', '.']

 TOTAL SNOWBALL STEM WORDS ==> 8



 ---- LEMMATIZATION ----

['http', ':', '//en.wikipedia.org/wiki/Ancient_Greek', 'http', ':', '//en.wikipedia.org/wiki/Nikolai_Trubetzkoy', '3', '.']

 TOTAL LEMMATIZE WORDS ==> 8

************************************************************************************************************************

74 --> Lexical  In Lexical, humans, as well as NLP systems, interpret the meaning of individual words. 


 ---- TOKENS ----

 ['Lexical', 'In', 'Lexical', ',', 'humans', ',', 'as', 'well', 'as', 'NLP', 'systems', ',', 'interpret', 'the', 'meaning', 'of', 'individual', 'words', '.'] 

 TOTAL TOKENS ==> 19

 ---- POST ----

 [('Lexical', 'JJ'), ('In', 'IN'), ('Lexical', 'NNP'), (',', ','), ('humans', 'NNS'), (',', ','), ('as', 'RB'), ('well', 'RB'), ('as', 'IN'), ('NLP', 'NNP'), ('systems', 'NNS'), (',', ','), ('interpret', 'VB'), ('the', 'DT'), ('meaning', 'NN'), ('of', 'IN'), ('individual', 'JJ'), ('words', 'NNS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Lexical', 'Lexical', ',', 'humans', ',', 'well', 'NLP', 'systems', ',', 'interpret', 'meaning', 'individual', 'words', '.']

 TOTAL FILTERED TOKENS ==>  14

 ---- POST FOR FILTERED TOKENS ----

 [('Lexical', 'JJ'), ('Lexical', 'NNP'), (',', ','), ('humans', 'NNS'), (',', ','), ('well', 'RB'), ('NLP', 'NNP'), ('systems', 'NNS'), (',', ','), ('interpret', 'JJ'), ('meaning', 'VBG'), ('individual', 'JJ'), ('words', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Lexical Lexical', 'Lexical ,', ', humans', 'humans ,', ', well', 'well NLP', 'NLP systems', 'systems ,', ', interpret', 'interpret meaning', 'meaning individual', 'individual words', 'words .'] 

 TOTAL BIGRAMS --> 13 



 ---- TRI-GRAMS ---- 

 ['Lexical Lexical ,', 'Lexical , humans', ', humans ,', 'humans , well', ', well NLP', 'well NLP systems', 'NLP systems ,', 'systems , interpret', ', interpret meaning', 'interpret meaning individual', 'meaning individual words', 'individual words .'] 

 TOTAL TRIGRAMS --> 12 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> ['NLP']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> ['Lexical Lexical']
 TOTAL PERSON ENTITY --> 1 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['lexic', 'lexic', ',', 'human', ',', 'well', 'nlp', 'system', ',', 'interpret', 'mean', 'individu', 'word', '.']

 TOTAL PORTER STEM WORDS ==> 14



 ---- SNOWBALL STEMMING ----

['lexic', 'lexic', ',', 'human', ',', 'well', 'nlp', 'system', ',', 'interpret', 'mean', 'individu', 'word', '.']

 TOTAL SNOWBALL STEM WORDS ==> 14



 ---- LEMMATIZATION ----

['Lexical', 'Lexical', ',', 'human', ',', 'well', 'NLP', 'system', ',', 'interpret', 'meaning', 'individual', 'word', '.']

 TOTAL LEMMATIZE WORDS ==> 14

************************************************************************************************************************

75 --> Sundry types of processing bestow to word-level understanding – the first of these being a  part-of-speech tag to each word. 


 ---- TOKENS ----

 ['Sundry', 'types', 'of', 'processing', 'bestow', 'to', 'word-level', 'understanding', '–', 'the', 'first', 'of', 'these', 'being', 'a', 'part-of-speech', 'tag', 'to', 'each', 'word', '.'] 

 TOTAL TOKENS ==> 21

 ---- POST ----

 [('Sundry', 'NNP'), ('types', 'NNS'), ('of', 'IN'), ('processing', 'VBG'), ('bestow', 'NN'), ('to', 'TO'), ('word-level', 'JJ'), ('understanding', 'JJ'), ('–', 'PDT'), ('the', 'DT'), ('first', 'JJ'), ('of', 'IN'), ('these', 'DT'), ('being', 'VBG'), ('a', 'DT'), ('part-of-speech', 'JJ'), ('tag', 'NN'), ('to', 'TO'), ('each', 'DT'), ('word', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Sundry', 'types', 'processing', 'bestow', 'word-level', 'understanding', '–', 'first', 'part-of-speech', 'tag', 'word', '.']

 TOTAL FILTERED TOKENS ==>  12

 ---- POST FOR FILTERED TOKENS ----

 [('Sundry', 'NNP'), ('types', 'NNS'), ('processing', 'VBG'), ('bestow', 'NN'), ('word-level', 'JJ'), ('understanding', 'NN'), ('–', 'NN'), ('first', 'RB'), ('part-of-speech', 'JJ'), ('tag', 'NN'), ('word', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Sundry types', 'types processing', 'processing bestow', 'bestow word-level', 'word-level understanding', 'understanding –', '– first', 'first part-of-speech', 'part-of-speech tag', 'tag word', 'word .'] 

 TOTAL BIGRAMS --> 11 



 ---- TRI-GRAMS ---- 

 ['Sundry types processing', 'types processing bestow', 'processing bestow word-level', 'bestow word-level understanding', 'word-level understanding –', 'understanding – first', '– first part-of-speech', 'first part-of-speech tag', 'part-of-speech tag word', 'tag word .'] 

 TOTAL TRIGRAMS --> 10 



 ---- NOUN PHRASES ---- 

 ['bestow', 'word-level understanding', '–', 'part-of-speech tag', 'word'] 

 TOTAL NOUN PHRASES --> 5 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> ['Sundry']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['sundri', 'type', 'process', 'bestow', 'word-level', 'understand', '–', 'first', 'part-of-speech', 'tag', 'word', '.']

 TOTAL PORTER STEM WORDS ==> 12



 ---- SNOWBALL STEMMING ----

['sundri', 'type', 'process', 'bestow', 'word-level', 'understand', '–', 'first', 'part-of-speech', 'tag', 'word', '.']

 TOTAL SNOWBALL STEM WORDS ==> 12



 ---- LEMMATIZATION ----

['Sundry', 'type', 'processing', 'bestow', 'word-level', 'understanding', '–', 'first', 'part-of-speech', 'tag', 'word', '.']

 TOTAL LEMMATIZE WORDS ==> 12

************************************************************************************************************************

76 --> In this processing, words that can act as more than one part- of-speech are assigned the most probable part-of speech tag based on the context in which  they occur. 


 ---- TOKENS ----

 ['In', 'this', 'processing', ',', 'words', 'that', 'can', 'act', 'as', 'more', 'than', 'one', 'part-', 'of-speech', 'are', 'assigned', 'the', 'most', 'probable', 'part-of', 'speech', 'tag', 'based', 'on', 'the', 'context', 'in', 'which', 'they', 'occur', '.'] 

 TOTAL TOKENS ==> 31

 ---- POST ----

 [('In', 'IN'), ('this', 'DT'), ('processing', 'NN'), (',', ','), ('words', 'NNS'), ('that', 'WDT'), ('can', 'MD'), ('act', 'VB'), ('as', 'IN'), ('more', 'JJR'), ('than', 'IN'), ('one', 'CD'), ('part-', 'JJ'), ('of-speech', 'NN'), ('are', 'VBP'), ('assigned', 'VBN'), ('the', 'DT'), ('most', 'RBS'), ('probable', 'JJ'), ('part-of', 'JJ'), ('speech', 'NN'), ('tag', 'NN'), ('based', 'VBN'), ('on', 'IN'), ('the', 'DT'), ('context', 'NN'), ('in', 'IN'), ('which', 'WDT'), ('they', 'PRP'), ('occur', 'VBP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['processing', ',', 'words', 'act', 'one', 'part-', 'of-speech', 'assigned', 'probable', 'part-of', 'speech', 'tag', 'based', 'context', 'occur', '.']

 TOTAL FILTERED TOKENS ==>  16

 ---- POST FOR FILTERED TOKENS ----

 [('processing', 'NN'), (',', ','), ('words', 'NNS'), ('act', 'VBP'), ('one', 'CD'), ('part-', 'JJ'), ('of-speech', 'NN'), ('assigned', 'VBN'), ('probable', 'JJ'), ('part-of', 'JJ'), ('speech', 'NN'), ('tag', 'NN'), ('based', 'VBN'), ('context', 'JJ'), ('occur', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['processing ,', ', words', 'words act', 'act one', 'one part-', 'part- of-speech', 'of-speech assigned', 'assigned probable', 'probable part-of', 'part-of speech', 'speech tag', 'tag based', 'based context', 'context occur', 'occur .'] 

 TOTAL BIGRAMS --> 15 



 ---- TRI-GRAMS ---- 

 ['processing , words', ', words act', 'words act one', 'act one part-', 'one part- of-speech', 'part- of-speech assigned', 'of-speech assigned probable', 'assigned probable part-of', 'probable part-of speech', 'part-of speech tag', 'speech tag based', 'tag based context', 'based context occur', 'context occur .'] 

 TOTAL TRIGRAMS --> 14 



 ---- NOUN PHRASES ---- 

 ['processing', 'part- of-speech', 'probable part-of speech', 'tag', 'context occur'] 

 TOTAL NOUN PHRASES --> 5 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['process', ',', 'word', 'act', 'one', 'part-', 'of-speech', 'assign', 'probabl', 'part-of', 'speech', 'tag', 'base', 'context', 'occur', '.']

 TOTAL PORTER STEM WORDS ==> 16



 ---- SNOWBALL STEMMING ----

['process', ',', 'word', 'act', 'one', 'part-', 'of-speech', 'assign', 'probabl', 'part-of', 'speech', 'tag', 'base', 'context', 'occur', '.']

 TOTAL SNOWBALL STEM WORDS ==> 16



 ---- LEMMATIZATION ----

['processing', ',', 'word', 'act', 'one', 'part-', 'of-speech', 'assigned', 'probable', 'part-of', 'speech', 'tag', 'based', 'context', 'occur', '.']

 TOTAL LEMMATIZE WORDS ==> 16

************************************************************************************************************************

77 --> At the lexical level, Semantic representations can be replaced by the words that  have one meaning. 


 ---- TOKENS ----

 ['At', 'the', 'lexical', 'level', ',', 'Semantic', 'representations', 'can', 'be', 'replaced', 'by', 'the', 'words', 'that', 'have', 'one', 'meaning', '.'] 

 TOTAL TOKENS ==> 18

 ---- POST ----

 [('At', 'IN'), ('the', 'DT'), ('lexical', 'JJ'), ('level', 'NN'), (',', ','), ('Semantic', 'JJ'), ('representations', 'NNS'), ('can', 'MD'), ('be', 'VB'), ('replaced', 'VBN'), ('by', 'IN'), ('the', 'DT'), ('words', 'NNS'), ('that', 'WDT'), ('have', 'VBP'), ('one', 'CD'), ('meaning', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['lexical', 'level', ',', 'Semantic', 'representations', 'replaced', 'words', 'one', 'meaning', '.']

 TOTAL FILTERED TOKENS ==>  10

 ---- POST FOR FILTERED TOKENS ----

 [('lexical', 'JJ'), ('level', 'NN'), (',', ','), ('Semantic', 'JJ'), ('representations', 'NNS'), ('replaced', 'VBD'), ('words', 'NNS'), ('one', 'CD'), ('meaning', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['lexical level', 'level ,', ', Semantic', 'Semantic representations', 'representations replaced', 'replaced words', 'words one', 'one meaning', 'meaning .'] 

 TOTAL BIGRAMS --> 9 



 ---- TRI-GRAMS ---- 

 ['lexical level ,', 'level , Semantic', ', Semantic representations', 'Semantic representations replaced', 'representations replaced words', 'replaced words one', 'words one meaning', 'one meaning .'] 

 TOTAL TRIGRAMS --> 8 



 ---- NOUN PHRASES ---- 

 ['lexical level', 'meaning'] 

 TOTAL NOUN PHRASES --> 2 



 ---- NER ----

 
 ORGANIZATION ---> ['Semantic']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['lexic', 'level', ',', 'semant', 'represent', 'replac', 'word', 'one', 'mean', '.']

 TOTAL PORTER STEM WORDS ==> 10



 ---- SNOWBALL STEMMING ----

['lexic', 'level', ',', 'semant', 'represent', 'replac', 'word', 'one', 'mean', '.']

 TOTAL SNOWBALL STEM WORDS ==> 10



 ---- LEMMATIZATION ----

['lexical', 'level', ',', 'Semantic', 'representation', 'replaced', 'word', 'one', 'meaning', '.']

 TOTAL LEMMATIZE WORDS ==> 10

************************************************************************************************************************

78 --> In NLP system, the nature of the representation varies according to the  semantic theory deployed. 


 ---- TOKENS ----

 ['In', 'NLP', 'system', ',', 'the', 'nature', 'of', 'the', 'representation', 'varies', 'according', 'to', 'the', 'semantic', 'theory', 'deployed', '.'] 

 TOTAL TOKENS ==> 17

 ---- POST ----

 [('In', 'IN'), ('NLP', 'NNP'), ('system', 'NN'), (',', ','), ('the', 'DT'), ('nature', 'NN'), ('of', 'IN'), ('the', 'DT'), ('representation', 'NN'), ('varies', 'VBZ'), ('according', 'VBG'), ('to', 'TO'), ('the', 'DT'), ('semantic', 'JJ'), ('theory', 'NN'), ('deployed', 'VBN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['NLP', 'system', ',', 'nature', 'representation', 'varies', 'according', 'semantic', 'theory', 'deployed', '.']

 TOTAL FILTERED TOKENS ==>  11

 ---- POST FOR FILTERED TOKENS ----

 [('NLP', 'NNP'), ('system', 'NN'), (',', ','), ('nature', 'JJ'), ('representation', 'NN'), ('varies', 'NNS'), ('according', 'VBG'), ('semantic', 'JJ'), ('theory', 'NN'), ('deployed', 'VBN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['NLP system', 'system ,', ', nature', 'nature representation', 'representation varies', 'varies according', 'according semantic', 'semantic theory', 'theory deployed', 'deployed .'] 

 TOTAL BIGRAMS --> 10 



 ---- TRI-GRAMS ---- 

 ['NLP system ,', 'system , nature', ', nature representation', 'nature representation varies', 'representation varies according', 'varies according semantic', 'according semantic theory', 'semantic theory deployed', 'theory deployed .'] 

 TOTAL TRIGRAMS --> 9 



 ---- NOUN PHRASES ---- 

 ['system', 'nature representation', 'semantic theory'] 

 TOTAL NOUN PHRASES --> 3 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['nlp', 'system', ',', 'natur', 'represent', 'vari', 'accord', 'semant', 'theori', 'deploy', '.']

 TOTAL PORTER STEM WORDS ==> 11



 ---- SNOWBALL STEMMING ----

['nlp', 'system', ',', 'natur', 'represent', 'vari', 'accord', 'semant', 'theori', 'deploy', '.']

 TOTAL SNOWBALL STEM WORDS ==> 11



 ---- LEMMATIZATION ----

['NLP', 'system', ',', 'nature', 'representation', 'varies', 'according', 'semantic', 'theory', 'deployed', '.']

 TOTAL LEMMATIZE WORDS ==> 11

************************************************************************************************************************

79 --> 4. 


 ---- TOKENS ----

 ['4', '.'] 

 TOTAL TOKENS ==> 2

 ---- POST ----

 [('4', 'CD'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['4', '.']

 TOTAL FILTERED TOKENS ==>  2

 ---- POST FOR FILTERED TOKENS ----

 [('4', 'CD'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['4 .'] 

 TOTAL BIGRAMS --> 1 



 ---- TRI-GRAMS ---- 

 [] 

 TOTAL TRIGRAMS --> 0 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['4', '.']

 TOTAL PORTER STEM WORDS ==> 2



 ---- SNOWBALL STEMMING ----

['4', '.']

 TOTAL SNOWBALL STEM WORDS ==> 2



 ---- LEMMATIZATION ----

['4', '.']

 TOTAL LEMMATIZE WORDS ==> 2

************************************************************************************************************************

80 --> Syntactic  This level emphasis to scrutinize the words in a sentence so as to uncover the grammatical  structure of the sentence. 


 ---- TOKENS ----

 ['Syntactic', 'This', 'level', 'emphasis', 'to', 'scrutinize', 'the', 'words', 'in', 'a', 'sentence', 'so', 'as', 'to', 'uncover', 'the', 'grammatical', 'structure', 'of', 'the', 'sentence', '.'] 

 TOTAL TOKENS ==> 22

 ---- POST ----

 [('Syntactic', 'JJ'), ('This', 'DT'), ('level', 'JJ'), ('emphasis', 'NN'), ('to', 'TO'), ('scrutinize', 'VB'), ('the', 'DT'), ('words', 'NNS'), ('in', 'IN'), ('a', 'DT'), ('sentence', 'NN'), ('so', 'RB'), ('as', 'IN'), ('to', 'TO'), ('uncover', 'VB'), ('the', 'DT'), ('grammatical', 'JJ'), ('structure', 'NN'), ('of', 'IN'), ('the', 'DT'), ('sentence', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Syntactic', 'level', 'emphasis', 'scrutinize', 'words', 'sentence', 'uncover', 'grammatical', 'structure', 'sentence', '.']

 TOTAL FILTERED TOKENS ==>  11

 ---- POST FOR FILTERED TOKENS ----

 [('Syntactic', 'JJ'), ('level', 'NN'), ('emphasis', 'NN'), ('scrutinize', 'VB'), ('words', 'NNS'), ('sentence', 'NN'), ('uncover', 'RB'), ('grammatical', 'JJ'), ('structure', 'NN'), ('sentence', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Syntactic level', 'level emphasis', 'emphasis scrutinize', 'scrutinize words', 'words sentence', 'sentence uncover', 'uncover grammatical', 'grammatical structure', 'structure sentence', 'sentence .'] 

 TOTAL BIGRAMS --> 10 



 ---- TRI-GRAMS ---- 

 ['Syntactic level emphasis', 'level emphasis scrutinize', 'emphasis scrutinize words', 'scrutinize words sentence', 'words sentence uncover', 'sentence uncover grammatical', 'uncover grammatical structure', 'grammatical structure sentence', 'structure sentence .'] 

 TOTAL TRIGRAMS --> 9 



 ---- NOUN PHRASES ---- 

 ['Syntactic level', 'emphasis', 'sentence', 'grammatical structure', 'sentence'] 

 TOTAL NOUN PHRASES --> 5 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> ['Syntactic']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['syntact', 'level', 'emphasi', 'scrutin', 'word', 'sentenc', 'uncov', 'grammat', 'structur', 'sentenc', '.']

 TOTAL PORTER STEM WORDS ==> 11



 ---- SNOWBALL STEMMING ----

['syntact', 'level', 'emphasi', 'scrutin', 'word', 'sentenc', 'uncov', 'grammat', 'structur', 'sentenc', '.']

 TOTAL SNOWBALL STEM WORDS ==> 11



 ---- LEMMATIZATION ----

['Syntactic', 'level', 'emphasis', 'scrutinize', 'word', 'sentence', 'uncover', 'grammatical', 'structure', 'sentence', '.']

 TOTAL LEMMATIZE WORDS ==> 11

************************************************************************************************************************

81 --> Both grammar and parser are required in this level. 


 ---- TOKENS ----

 ['Both', 'grammar', 'and', 'parser', 'are', 'required', 'in', 'this', 'level', '.'] 

 TOTAL TOKENS ==> 10

 ---- POST ----

 [('Both', 'DT'), ('grammar', 'NN'), ('and', 'CC'), ('parser', 'NN'), ('are', 'VBP'), ('required', 'VBN'), ('in', 'IN'), ('this', 'DT'), ('level', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['grammar', 'parser', 'required', 'level', '.']

 TOTAL FILTERED TOKENS ==>  5

 ---- POST FOR FILTERED TOKENS ----

 [('grammar', 'NN'), ('parser', 'NN'), ('required', 'VBN'), ('level', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['grammar parser', 'parser required', 'required level', 'level .'] 

 TOTAL BIGRAMS --> 4 



 ---- TRI-GRAMS ---- 

 ['grammar parser required', 'parser required level', 'required level .'] 

 TOTAL TRIGRAMS --> 3 



 ---- NOUN PHRASES ---- 

 ['grammar', 'parser', 'level'] 

 TOTAL NOUN PHRASES --> 3 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['grammar', 'parser', 'requir', 'level', '.']

 TOTAL PORTER STEM WORDS ==> 5



 ---- SNOWBALL STEMMING ----

['grammar', 'parser', 'requir', 'level', '.']

 TOTAL SNOWBALL STEM WORDS ==> 5



 ---- LEMMATIZATION ----

['grammar', 'parser', 'required', 'level', '.']

 TOTAL LEMMATIZE WORDS ==> 5

************************************************************************************************************************

82 --> The output of  this level of processing is representation of the sentence that divulge the structural  dependency relationships between the words. 


 ---- TOKENS ----

 ['The', 'output', 'of', 'this', 'level', 'of', 'processing', 'is', 'representation', 'of', 'the', 'sentence', 'that', 'divulge', 'the', 'structural', 'dependency', 'relationships', 'between', 'the', 'words', '.'] 

 TOTAL TOKENS ==> 22

 ---- POST ----

 [('The', 'DT'), ('output', 'NN'), ('of', 'IN'), ('this', 'DT'), ('level', 'NN'), ('of', 'IN'), ('processing', 'NN'), ('is', 'VBZ'), ('representation', 'NN'), ('of', 'IN'), ('the', 'DT'), ('sentence', 'NN'), ('that', 'WDT'), ('divulge', 'VBZ'), ('the', 'DT'), ('structural', 'JJ'), ('dependency', 'NN'), ('relationships', 'NNS'), ('between', 'IN'), ('the', 'DT'), ('words', 'NNS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['output', 'level', 'processing', 'representation', 'sentence', 'divulge', 'structural', 'dependency', 'relationships', 'words', '.']

 TOTAL FILTERED TOKENS ==>  11

 ---- POST FOR FILTERED TOKENS ----

 [('output', 'NN'), ('level', 'NN'), ('processing', 'VBG'), ('representation', 'NN'), ('sentence', 'NN'), ('divulge', 'JJ'), ('structural', 'JJ'), ('dependency', 'NN'), ('relationships', 'NNS'), ('words', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['output level', 'level processing', 'processing representation', 'representation sentence', 'sentence divulge', 'divulge structural', 'structural dependency', 'dependency relationships', 'relationships words', 'words .'] 

 TOTAL BIGRAMS --> 10 



 ---- TRI-GRAMS ---- 

 ['output level processing', 'level processing representation', 'processing representation sentence', 'representation sentence divulge', 'sentence divulge structural', 'divulge structural dependency', 'structural dependency relationships', 'dependency relationships words', 'relationships words .'] 

 TOTAL TRIGRAMS --> 9 



 ---- NOUN PHRASES ---- 

 ['output', 'level', 'representation', 'sentence', 'divulge structural dependency'] 

 TOTAL NOUN PHRASES --> 5 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['output', 'level', 'process', 'represent', 'sentenc', 'divulg', 'structur', 'depend', 'relationship', 'word', '.']

 TOTAL PORTER STEM WORDS ==> 11



 ---- SNOWBALL STEMMING ----

['output', 'level', 'process', 'represent', 'sentenc', 'divulg', 'structur', 'depend', 'relationship', 'word', '.']

 TOTAL SNOWBALL STEM WORDS ==> 11



 ---- LEMMATIZATION ----

['output', 'level', 'processing', 'representation', 'sentence', 'divulge', 'structural', 'dependency', 'relationship', 'word', '.']

 TOTAL LEMMATIZE WORDS ==> 11

************************************************************************************************************************

83 --> There are various grammars that can be  impeded, and which in twirl, whack the option of a parser. 


 ---- TOKENS ----

 ['There', 'are', 'various', 'grammars', 'that', 'can', 'be', 'impeded', ',', 'and', 'which', 'in', 'twirl', ',', 'whack', 'the', 'option', 'of', 'a', 'parser', '.'] 

 TOTAL TOKENS ==> 21

 ---- POST ----

 [('There', 'EX'), ('are', 'VBP'), ('various', 'JJ'), ('grammars', 'NNS'), ('that', 'WDT'), ('can', 'MD'), ('be', 'VB'), ('impeded', 'VBN'), (',', ','), ('and', 'CC'), ('which', 'WDT'), ('in', 'IN'), ('twirl', 'NN'), (',', ','), ('whack', 'VBP'), ('the', 'DT'), ('option', 'NN'), ('of', 'IN'), ('a', 'DT'), ('parser', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['various', 'grammars', 'impeded', ',', 'twirl', ',', 'whack', 'option', 'parser', '.']

 TOTAL FILTERED TOKENS ==>  10

 ---- POST FOR FILTERED TOKENS ----

 [('various', 'JJ'), ('grammars', 'NNS'), ('impeded', 'VBD'), (',', ','), ('twirl', 'NN'), (',', ','), ('whack', 'NN'), ('option', 'NN'), ('parser', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['various grammars', 'grammars impeded', 'impeded ,', ', twirl', 'twirl ,', ', whack', 'whack option', 'option parser', 'parser .'] 

 TOTAL BIGRAMS --> 9 



 ---- TRI-GRAMS ---- 

 ['various grammars impeded', 'grammars impeded ,', 'impeded , twirl', ', twirl ,', 'twirl , whack', ', whack option', 'whack option parser', 'option parser .'] 

 TOTAL TRIGRAMS --> 8 



 ---- NOUN PHRASES ---- 

 ['twirl', 'whack', 'option', 'parser'] 

 TOTAL NOUN PHRASES --> 4 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['variou', 'grammar', 'imped', ',', 'twirl', ',', 'whack', 'option', 'parser', '.']

 TOTAL PORTER STEM WORDS ==> 10



 ---- SNOWBALL STEMMING ----

['various', 'grammar', 'imped', ',', 'twirl', ',', 'whack', 'option', 'parser', '.']

 TOTAL SNOWBALL STEM WORDS ==> 10



 ---- LEMMATIZATION ----

['various', 'grammar', 'impeded', ',', 'twirl', ',', 'whack', 'option', 'parser', '.']

 TOTAL LEMMATIZE WORDS ==> 10

************************************************************************************************************************

84 --> Not all NLP applications require a  full parse of sentences, therefore the abide challenges in parsing of prepositional phrase  attachment and conjunction audit no longer impede that plea for which phrasal and clausal  dependencies are adequate [7]. 


 ---- TOKENS ----

 ['Not', 'all', 'NLP', 'applications', 'require', 'a', 'full', 'parse', 'of', 'sentences', ',', 'therefore', 'the', 'abide', 'challenges', 'in', 'parsing', 'of', 'prepositional', 'phrase', 'attachment', 'and', 'conjunction', 'audit', 'no', 'longer', 'impede', 'that', 'plea', 'for', 'which', 'phrasal', 'and', 'clausal', 'dependencies', 'are', 'adequate', '[', '7', ']', '.'] 

 TOTAL TOKENS ==> 41

 ---- POST ----

 [('Not', 'RB'), ('all', 'DT'), ('NLP', 'NNP'), ('applications', 'NNS'), ('require', 'VBP'), ('a', 'DT'), ('full', 'JJ'), ('parse', 'NN'), ('of', 'IN'), ('sentences', 'NNS'), (',', ','), ('therefore', 'IN'), ('the', 'DT'), ('abide', 'NN'), ('challenges', 'VBZ'), ('in', 'IN'), ('parsing', 'NN'), ('of', 'IN'), ('prepositional', 'JJ'), ('phrase', 'NN'), ('attachment', 'NN'), ('and', 'CC'), ('conjunction', 'NN'), ('audit', 'NN'), ('no', 'RB'), ('longer', 'RBR'), ('impede', 'VB'), ('that', 'DT'), ('plea', 'NN'), ('for', 'IN'), ('which', 'WDT'), ('phrasal', 'NN'), ('and', 'CC'), ('clausal', 'NN'), ('dependencies', 'NNS'), ('are', 'VBP'), ('adequate', 'JJ'), ('[', 'NNS'), ('7', 'CD'), (']', 'NNS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['NLP', 'applications', 'require', 'full', 'parse', 'sentences', ',', 'therefore', 'abide', 'challenges', 'parsing', 'prepositional', 'phrase', 'attachment', 'conjunction', 'audit', 'longer', 'impede', 'plea', 'phrasal', 'clausal', 'dependencies', 'adequate', '[', '7', ']', '.']

 TOTAL FILTERED TOKENS ==>  27

 ---- POST FOR FILTERED TOKENS ----

 [('NLP', 'NNP'), ('applications', 'NNS'), ('require', 'VBP'), ('full', 'JJ'), ('parse', 'NN'), ('sentences', 'NNS'), (',', ','), ('therefore', 'RB'), ('abide', 'JJ'), ('challenges', 'NNS'), ('parsing', 'VBG'), ('prepositional', 'JJ'), ('phrase', 'NN'), ('attachment', 'NN'), ('conjunction', 'NN'), ('audit', 'NN'), ('longer', 'RBR'), ('impede', 'JJ'), ('plea', 'NN'), ('phrasal', 'NN'), ('clausal', 'NN'), ('dependencies', 'NNS'), ('adequate', 'VBP'), ('[', '$'), ('7', 'CD'), (']', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['NLP applications', 'applications require', 'require full', 'full parse', 'parse sentences', 'sentences ,', ', therefore', 'therefore abide', 'abide challenges', 'challenges parsing', 'parsing prepositional', 'prepositional phrase', 'phrase attachment', 'attachment conjunction', 'conjunction audit', 'audit longer', 'longer impede', 'impede plea', 'plea phrasal', 'phrasal clausal', 'clausal dependencies', 'dependencies adequate', 'adequate [', '[ 7', '7 ]', '] .'] 

 TOTAL BIGRAMS --> 26 



 ---- TRI-GRAMS ---- 

 ['NLP applications require', 'applications require full', 'require full parse', 'full parse sentences', 'parse sentences ,', 'sentences , therefore', ', therefore abide', 'therefore abide challenges', 'abide challenges parsing', 'challenges parsing prepositional', 'parsing prepositional phrase', 'prepositional phrase attachment', 'phrase attachment conjunction', 'attachment conjunction audit', 'conjunction audit longer', 'audit longer impede', 'longer impede plea', 'impede plea phrasal', 'plea phrasal clausal', 'phrasal clausal dependencies', 'clausal dependencies adequate', 'dependencies adequate [', 'adequate [ 7', '[ 7 ]', '7 ] .'] 

 TOTAL TRIGRAMS --> 25 



 ---- NOUN PHRASES ---- 

 ['full parse', 'prepositional phrase', 'attachment', 'conjunction', 'audit', 'impede plea', 'phrasal', 'clausal', ']'] 

 TOTAL NOUN PHRASES --> 9 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['nlp', 'applic', 'requir', 'full', 'pars', 'sentenc', ',', 'therefor', 'abid', 'challeng', 'pars', 'preposit', 'phrase', 'attach', 'conjunct', 'audit', 'longer', 'imped', 'plea', 'phrasal', 'clausal', 'depend', 'adequ', '[', '7', ']', '.']

 TOTAL PORTER STEM WORDS ==> 27



 ---- SNOWBALL STEMMING ----

['nlp', 'applic', 'requir', 'full', 'pars', 'sentenc', ',', 'therefor', 'abid', 'challeng', 'pars', 'preposit', 'phrase', 'attach', 'conjunct', 'audit', 'longer', 'imped', 'plea', 'phrasal', 'clausal', 'depend', 'adequ', '[', '7', ']', '.']

 TOTAL SNOWBALL STEM WORDS ==> 27



 ---- LEMMATIZATION ----

['NLP', 'application', 'require', 'full', 'parse', 'sentence', ',', 'therefore', 'abide', 'challenge', 'parsing', 'prepositional', 'phrase', 'attachment', 'conjunction', 'audit', 'longer', 'impede', 'plea', 'phrasal', 'clausal', 'dependency', 'adequate', '[', '7', ']', '.']

 TOTAL LEMMATIZE WORDS ==> 27

************************************************************************************************************************

85 --> Syntax conveys meaning in most languages because order and  dependency contribute to connotation. 


 ---- TOKENS ----

 ['Syntax', 'conveys', 'meaning', 'in', 'most', 'languages', 'because', 'order', 'and', 'dependency', 'contribute', 'to', 'connotation', '.'] 

 TOTAL TOKENS ==> 14

 ---- POST ----

 [('Syntax', 'NNP'), ('conveys', 'NNS'), ('meaning', 'VBG'), ('in', 'IN'), ('most', 'JJS'), ('languages', 'NNS'), ('because', 'IN'), ('order', 'NN'), ('and', 'CC'), ('dependency', 'NN'), ('contribute', 'NN'), ('to', 'TO'), ('connotation', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Syntax', 'conveys', 'meaning', 'languages', 'order', 'dependency', 'contribute', 'connotation', '.']

 TOTAL FILTERED TOKENS ==>  9

 ---- POST FOR FILTERED TOKENS ----

 [('Syntax', 'NNP'), ('conveys', 'NNS'), ('meaning', 'VBG'), ('languages', 'NNS'), ('order', 'NN'), ('dependency', 'NN'), ('contribute', 'JJ'), ('connotation', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Syntax conveys', 'conveys meaning', 'meaning languages', 'languages order', 'order dependency', 'dependency contribute', 'contribute connotation', 'connotation .'] 

 TOTAL BIGRAMS --> 8 



 ---- TRI-GRAMS ---- 

 ['Syntax conveys meaning', 'conveys meaning languages', 'meaning languages order', 'languages order dependency', 'order dependency contribute', 'dependency contribute connotation', 'contribute connotation .'] 

 TOTAL TRIGRAMS --> 7 



 ---- NOUN PHRASES ---- 

 ['order', 'dependency', 'contribute connotation'] 

 TOTAL NOUN PHRASES --> 3 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> ['Syntax']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['syntax', 'convey', 'mean', 'languag', 'order', 'depend', 'contribut', 'connot', '.']

 TOTAL PORTER STEM WORDS ==> 9



 ---- SNOWBALL STEMMING ----

['syntax', 'convey', 'mean', 'languag', 'order', 'depend', 'contribut', 'connot', '.']

 TOTAL SNOWBALL STEM WORDS ==> 9



 ---- LEMMATIZATION ----

['Syntax', 'conveys', 'meaning', 'language', 'order', 'dependency', 'contribute', 'connotation', '.']

 TOTAL LEMMATIZE WORDS ==> 9

************************************************************************************************************************

86 --> For example, the two sentences: ‘The cat chased the  mouse.’ and ‘The mouse chased the cat.’ differ only in terms of syntax, yet convey quite  different meanings. 


 ---- TOKENS ----

 ['For', 'example', ',', 'the', 'two', 'sentences', ':', '‘', 'The', 'cat', 'chased', 'the', 'mouse.', '’', 'and', '‘', 'The', 'mouse', 'chased', 'the', 'cat.', '’', 'differ', 'only', 'in', 'terms', 'of', 'syntax', ',', 'yet', 'convey', 'quite', 'different', 'meanings', '.'] 

 TOTAL TOKENS ==> 35

 ---- POST ----

 [('For', 'IN'), ('example', 'NN'), (',', ','), ('the', 'DT'), ('two', 'CD'), ('sentences', 'NNS'), (':', ':'), ('‘', 'VB'), ('The', 'DT'), ('cat', 'NN'), ('chased', 'VBD'), ('the', 'DT'), ('mouse.', 'NN'), ('’', 'NN'), ('and', 'CC'), ('‘', 'VB'), ('The', 'DT'), ('mouse', 'NN'), ('chased', 'VBD'), ('the', 'DT'), ('cat.', 'NN'), ('’', 'NN'), ('differ', 'NN'), ('only', 'RB'), ('in', 'IN'), ('terms', 'NNS'), ('of', 'IN'), ('syntax', 'NN'), (',', ','), ('yet', 'RB'), ('convey', 'JJ'), ('quite', 'RB'), ('different', 'JJ'), ('meanings', 'NNS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['example', ',', 'two', 'sentences', ':', '‘', 'cat', 'chased', 'mouse.', '’', '‘', 'mouse', 'chased', 'cat.', '’', 'differ', 'terms', 'syntax', ',', 'yet', 'convey', 'quite', 'different', 'meanings', '.']

 TOTAL FILTERED TOKENS ==>  25

 ---- POST FOR FILTERED TOKENS ----

 [('example', 'NN'), (',', ','), ('two', 'CD'), ('sentences', 'NNS'), (':', ':'), ('‘', 'JJ'), ('cat', 'NN'), ('chased', 'VBD'), ('mouse.', 'JJ'), ('’', 'NNP'), ('‘', 'NNP'), ('mouse', 'NN'), ('chased', 'VBD'), ('cat.', 'NN'), ('’', 'NNP'), ('differ', 'NN'), ('terms', 'NNS'), ('syntax', 'NN'), (',', ','), ('yet', 'RB'), ('convey', 'JJ'), ('quite', 'RB'), ('different', 'JJ'), ('meanings', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['example ,', ', two', 'two sentences', 'sentences :', ': ‘', '‘ cat', 'cat chased', 'chased mouse.', 'mouse. ’', '’ ‘', '‘ mouse', 'mouse chased', 'chased cat.', 'cat. ’', '’ differ', 'differ terms', 'terms syntax', 'syntax ,', ', yet', 'yet convey', 'convey quite', 'quite different', 'different meanings', 'meanings .'] 

 TOTAL BIGRAMS --> 24 



 ---- TRI-GRAMS ---- 

 ['example , two', ', two sentences', 'two sentences :', 'sentences : ‘', ': ‘ cat', '‘ cat chased', 'cat chased mouse.', 'chased mouse. ’', 'mouse. ’ ‘', '’ ‘ mouse', '‘ mouse chased', 'mouse chased cat.', 'chased cat. ’', 'cat. ’ differ', '’ differ terms', 'differ terms syntax', 'terms syntax ,', 'syntax , yet', ', yet convey', 'yet convey quite', 'convey quite different', 'quite different meanings', 'different meanings .'] 

 TOTAL TRIGRAMS --> 23 



 ---- NOUN PHRASES ---- 

 ['example', '‘ cat', 'mouse', 'cat.', 'differ', 'syntax'] 

 TOTAL NOUN PHRASES --> 6 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['exampl', ',', 'two', 'sentenc', ':', '‘', 'cat', 'chase', 'mouse.', '’', '‘', 'mous', 'chase', 'cat.', '’', 'differ', 'term', 'syntax', ',', 'yet', 'convey', 'quit', 'differ', 'mean', '.']

 TOTAL PORTER STEM WORDS ==> 25



 ---- SNOWBALL STEMMING ----

['exampl', ',', 'two', 'sentenc', ':', '‘', 'cat', 'chase', 'mouse.', '’', '‘', 'mous', 'chase', 'cat.', '’', 'differ', 'term', 'syntax', ',', 'yet', 'convey', 'quit', 'differ', 'mean', '.']

 TOTAL SNOWBALL STEM WORDS ==> 25



 ---- LEMMATIZATION ----

['example', ',', 'two', 'sentence', ':', '‘', 'cat', 'chased', 'mouse.', '’', '‘', 'mouse', 'chased', 'cat.', '’', 'differ', 'term', 'syntax', ',', 'yet', 'convey', 'quite', 'different', 'meaning', '.']

 TOTAL LEMMATIZE WORDS ==> 25

************************************************************************************************************************

87 --> 5. 


 ---- TOKENS ----

 ['5', '.'] 

 TOTAL TOKENS ==> 2

 ---- POST ----

 [('5', 'CD'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['5', '.']

 TOTAL FILTERED TOKENS ==>  2

 ---- POST FOR FILTERED TOKENS ----

 [('5', 'CD'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['5 .'] 

 TOTAL BIGRAMS --> 1 



 ---- TRI-GRAMS ---- 

 [] 

 TOTAL TRIGRAMS --> 0 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['5', '.']

 TOTAL PORTER STEM WORDS ==> 2



 ---- SNOWBALL STEMMING ----

['5', '.']

 TOTAL SNOWBALL STEM WORDS ==> 2



 ---- LEMMATIZATION ----

['5', '.']

 TOTAL LEMMATIZE WORDS ==> 2

************************************************************************************************************************

88 --> Semantic  In semantic most people think that meaning is determined, however, this is not it is all the  levels that bestow to meaning. 


 ---- TOKENS ----

 ['Semantic', 'In', 'semantic', 'most', 'people', 'think', 'that', 'meaning', 'is', 'determined', ',', 'however', ',', 'this', 'is', 'not', 'it', 'is', 'all', 'the', 'levels', 'that', 'bestow', 'to', 'meaning', '.'] 

 TOTAL TOKENS ==> 26

 ---- POST ----

 [('Semantic', 'JJ'), ('In', 'IN'), ('semantic', 'JJ'), ('most', 'JJS'), ('people', 'NNS'), ('think', 'VBP'), ('that', 'IN'), ('meaning', 'NN'), ('is', 'VBZ'), ('determined', 'VBN'), (',', ','), ('however', 'RB'), (',', ','), ('this', 'DT'), ('is', 'VBZ'), ('not', 'RB'), ('it', 'PRP'), ('is', 'VBZ'), ('all', 'PDT'), ('the', 'DT'), ('levels', 'NNS'), ('that', 'WDT'), ('bestow', 'VBP'), ('to', 'TO'), ('meaning', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Semantic', 'semantic', 'people', 'think', 'meaning', 'determined', ',', 'however', ',', 'levels', 'bestow', 'meaning', '.']

 TOTAL FILTERED TOKENS ==>  13

 ---- POST FOR FILTERED TOKENS ----

 [('Semantic', 'JJ'), ('semantic', 'JJ'), ('people', 'NNS'), ('think', 'VBP'), ('meaning', 'VBG'), ('determined', 'VBD'), (',', ','), ('however', 'RB'), (',', ','), ('levels', 'NNS'), ('bestow', 'VBP'), ('meaning', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Semantic semantic', 'semantic people', 'people think', 'think meaning', 'meaning determined', 'determined ,', ', however', 'however ,', ', levels', 'levels bestow', 'bestow meaning', 'meaning .'] 

 TOTAL BIGRAMS --> 12 



 ---- TRI-GRAMS ---- 

 ['Semantic semantic people', 'semantic people think', 'people think meaning', 'think meaning determined', 'meaning determined ,', 'determined , however', ', however ,', 'however , levels', ', levels bestow', 'levels bestow meaning', 'bestow meaning .'] 

 TOTAL TRIGRAMS --> 11 



 ---- NOUN PHRASES ---- 

 ['meaning'] 

 TOTAL NOUN PHRASES --> 1 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> ['Semantic']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['semant', 'semant', 'peopl', 'think', 'mean', 'determin', ',', 'howev', ',', 'level', 'bestow', 'mean', '.']

 TOTAL PORTER STEM WORDS ==> 13



 ---- SNOWBALL STEMMING ----

['semant', 'semant', 'peopl', 'think', 'mean', 'determin', ',', 'howev', ',', 'level', 'bestow', 'mean', '.']

 TOTAL SNOWBALL STEM WORDS ==> 13



 ---- LEMMATIZATION ----

['Semantic', 'semantic', 'people', 'think', 'meaning', 'determined', ',', 'however', ',', 'level', 'bestow', 'meaning', '.']

 TOTAL LEMMATIZE WORDS ==> 13

************************************************************************************************************************

89 --> Semantic processing determines the possible meanings of a  sentence by pivoting on the interactions among word-level meanings in the sentence. 


 ---- TOKENS ----

 ['Semantic', 'processing', 'determines', 'the', 'possible', 'meanings', 'of', 'a', 'sentence', 'by', 'pivoting', 'on', 'the', 'interactions', 'among', 'word-level', 'meanings', 'in', 'the', 'sentence', '.'] 

 TOTAL TOKENS ==> 21

 ---- POST ----

 [('Semantic', 'JJ'), ('processing', 'NN'), ('determines', 'VBZ'), ('the', 'DT'), ('possible', 'JJ'), ('meanings', 'NNS'), ('of', 'IN'), ('a', 'DT'), ('sentence', 'NN'), ('by', 'IN'), ('pivoting', 'VBG'), ('on', 'IN'), ('the', 'DT'), ('interactions', 'NNS'), ('among', 'IN'), ('word-level', 'JJ'), ('meanings', 'NNS'), ('in', 'IN'), ('the', 'DT'), ('sentence', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Semantic', 'processing', 'determines', 'possible', 'meanings', 'sentence', 'pivoting', 'interactions', 'among', 'word-level', 'meanings', 'sentence', '.']

 TOTAL FILTERED TOKENS ==>  13

 ---- POST FOR FILTERED TOKENS ----

 [('Semantic', 'JJ'), ('processing', 'NN'), ('determines', 'NNS'), ('possible', 'JJ'), ('meanings', 'NNS'), ('sentence', 'NN'), ('pivoting', 'VBG'), ('interactions', 'NNS'), ('among', 'IN'), ('word-level', 'JJ'), ('meanings', 'NNS'), ('sentence', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Semantic processing', 'processing determines', 'determines possible', 'possible meanings', 'meanings sentence', 'sentence pivoting', 'pivoting interactions', 'interactions among', 'among word-level', 'word-level meanings', 'meanings sentence', 'sentence .'] 

 TOTAL BIGRAMS --> 12 



 ---- TRI-GRAMS ---- 

 ['Semantic processing determines', 'processing determines possible', 'determines possible meanings', 'possible meanings sentence', 'meanings sentence pivoting', 'sentence pivoting interactions', 'pivoting interactions among', 'interactions among word-level', 'among word-level meanings', 'word-level meanings sentence', 'meanings sentence .'] 

 TOTAL TRIGRAMS --> 11 



 ---- NOUN PHRASES ---- 

 ['Semantic processing', 'sentence', 'sentence'] 

 TOTAL NOUN PHRASES --> 3 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> ['Semantic']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['semant', 'process', 'determin', 'possibl', 'mean', 'sentenc', 'pivot', 'interact', 'among', 'word-level', 'mean', 'sentenc', '.']

 TOTAL PORTER STEM WORDS ==> 13



 ---- SNOWBALL STEMMING ----

['semant', 'process', 'determin', 'possibl', 'mean', 'sentenc', 'pivot', 'interact', 'among', 'word-level', 'mean', 'sentenc', '.']

 TOTAL SNOWBALL STEM WORDS ==> 13



 ---- LEMMATIZATION ----

['Semantic', 'processing', 'determines', 'possible', 'meaning', 'sentence', 'pivoting', 'interaction', 'among', 'word-level', 'meaning', 'sentence', '.']

 TOTAL LEMMATIZE WORDS ==> 13

************************************************************************************************************************

90 --> This  level of processing can incorporate the semantic disambiguation of words with multiple  senses; in a cognate way to how syntactic disambiguation of words that can errand as  multiple parts-of-speech is adroit at the syntactic level. 


 ---- TOKENS ----

 ['This', 'level', 'of', 'processing', 'can', 'incorporate', 'the', 'semantic', 'disambiguation', 'of', 'words', 'with', 'multiple', 'senses', ';', 'in', 'a', 'cognate', 'way', 'to', 'how', 'syntactic', 'disambiguation', 'of', 'words', 'that', 'can', 'errand', 'as', 'multiple', 'parts-of-speech', 'is', 'adroit', 'at', 'the', 'syntactic', 'level', '.'] 

 TOTAL TOKENS ==> 38

 ---- POST ----

 [('This', 'DT'), ('level', 'NN'), ('of', 'IN'), ('processing', 'NN'), ('can', 'MD'), ('incorporate', 'VB'), ('the', 'DT'), ('semantic', 'JJ'), ('disambiguation', 'NN'), ('of', 'IN'), ('words', 'NNS'), ('with', 'IN'), ('multiple', 'JJ'), ('senses', 'NNS'), (';', ':'), ('in', 'IN'), ('a', 'DT'), ('cognate', 'JJ'), ('way', 'NN'), ('to', 'TO'), ('how', 'WRB'), ('syntactic', 'JJ'), ('disambiguation', 'NN'), ('of', 'IN'), ('words', 'NNS'), ('that', 'WDT'), ('can', 'MD'), ('errand', 'VB'), ('as', 'IN'), ('multiple', 'JJ'), ('parts-of-speech', 'NN'), ('is', 'VBZ'), ('adroit', 'VBN'), ('at', 'IN'), ('the', 'DT'), ('syntactic', 'JJ'), ('level', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['level', 'processing', 'incorporate', 'semantic', 'disambiguation', 'words', 'multiple', 'senses', ';', 'cognate', 'way', 'syntactic', 'disambiguation', 'words', 'errand', 'multiple', 'parts-of-speech', 'adroit', 'syntactic', 'level', '.']

 TOTAL FILTERED TOKENS ==>  21

 ---- POST FOR FILTERED TOKENS ----

 [('level', 'NN'), ('processing', 'NN'), ('incorporate', 'JJ'), ('semantic', 'JJ'), ('disambiguation', 'NN'), ('words', 'NNS'), ('multiple', 'JJ'), ('senses', 'NNS'), (';', ':'), ('cognate', 'VB'), ('way', 'NN'), ('syntactic', 'JJ'), ('disambiguation', 'NN'), ('words', 'NNS'), ('errand', 'VBP'), ('multiple', 'JJ'), ('parts-of-speech', 'JJ'), ('adroit', 'JJ'), ('syntactic', 'JJ'), ('level', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['level processing', 'processing incorporate', 'incorporate semantic', 'semantic disambiguation', 'disambiguation words', 'words multiple', 'multiple senses', 'senses ;', '; cognate', 'cognate way', 'way syntactic', 'syntactic disambiguation', 'disambiguation words', 'words errand', 'errand multiple', 'multiple parts-of-speech', 'parts-of-speech adroit', 'adroit syntactic', 'syntactic level', 'level .'] 

 TOTAL BIGRAMS --> 20 



 ---- TRI-GRAMS ---- 

 ['level processing incorporate', 'processing incorporate semantic', 'incorporate semantic disambiguation', 'semantic disambiguation words', 'disambiguation words multiple', 'words multiple senses', 'multiple senses ;', 'senses ; cognate', '; cognate way', 'cognate way syntactic', 'way syntactic disambiguation', 'syntactic disambiguation words', 'disambiguation words errand', 'words errand multiple', 'errand multiple parts-of-speech', 'multiple parts-of-speech adroit', 'parts-of-speech adroit syntactic', 'adroit syntactic level', 'syntactic level .'] 

 TOTAL TRIGRAMS --> 19 



 ---- NOUN PHRASES ---- 

 ['level', 'processing', 'incorporate semantic disambiguation', 'way', 'syntactic disambiguation', 'multiple parts-of-speech adroit syntactic level'] 

 TOTAL NOUN PHRASES --> 6 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['level', 'process', 'incorpor', 'semant', 'disambigu', 'word', 'multipl', 'sens', ';', 'cognat', 'way', 'syntact', 'disambigu', 'word', 'errand', 'multipl', 'parts-of-speech', 'adroit', 'syntact', 'level', '.']

 TOTAL PORTER STEM WORDS ==> 21



 ---- SNOWBALL STEMMING ----

['level', 'process', 'incorpor', 'semant', 'disambigu', 'word', 'multipl', 'sens', ';', 'cognat', 'way', 'syntact', 'disambigu', 'word', 'errand', 'multipl', 'parts-of-speech', 'adroit', 'syntact', 'level', '.']

 TOTAL SNOWBALL STEM WORDS ==> 21



 ---- LEMMATIZATION ----

['level', 'processing', 'incorporate', 'semantic', 'disambiguation', 'word', 'multiple', 'sens', ';', 'cognate', 'way', 'syntactic', 'disambiguation', 'word', 'errand', 'multiple', 'parts-of-speech', 'adroit', 'syntactic', 'level', '.']

 TOTAL LEMMATIZE WORDS ==> 21

************************************************************************************************************************

91 --> For example, amongst other  meanings, ‘file’ as a noun can mean either a binder for gathering papers, or a tool to form  one’s fingernails, or a line of individuals in a queue (Elizabeth D. Liddy,2001) [7]. 


 ---- TOKENS ----

 ['For', 'example', ',', 'amongst', 'other', 'meanings', ',', '‘', 'file', '’', 'as', 'a', 'noun', 'can', 'mean', 'either', 'a', 'binder', 'for', 'gathering', 'papers', ',', 'or', 'a', 'tool', 'to', 'form', 'one', '’', 's', 'fingernails', ',', 'or', 'a', 'line', 'of', 'individuals', 'in', 'a', 'queue', '(', 'Elizabeth', 'D.', 'Liddy,2001', ')', '[', '7', ']', '.'] 

 TOTAL TOKENS ==> 49

 ---- POST ----

 [('For', 'IN'), ('example', 'NN'), (',', ','), ('amongst', 'VBZ'), ('other', 'JJ'), ('meanings', 'NNS'), (',', ','), ('‘', 'NNP'), ('file', 'NN'), ('’', 'NN'), ('as', 'IN'), ('a', 'DT'), ('noun', 'NN'), ('can', 'MD'), ('mean', 'VB'), ('either', 'CC'), ('a', 'DT'), ('binder', 'NN'), ('for', 'IN'), ('gathering', 'VBG'), ('papers', 'NNS'), (',', ','), ('or', 'CC'), ('a', 'DT'), ('tool', 'NN'), ('to', 'TO'), ('form', 'VB'), ('one', 'CD'), ('’', 'NNP'), ('s', 'NN'), ('fingernails', 'NNS'), (',', ','), ('or', 'CC'), ('a', 'DT'), ('line', 'NN'), ('of', 'IN'), ('individuals', 'NNS'), ('in', 'IN'), ('a', 'DT'), ('queue', 'NN'), ('(', '('), ('Elizabeth', 'NNP'), ('D.', 'NNP'), ('Liddy,2001', 'NNP'), (')', ')'), ('[', 'VBD'), ('7', 'CD'), (']', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['example', ',', 'amongst', 'meanings', ',', '‘', 'file', '’', 'noun', 'mean', 'either', 'binder', 'gathering', 'papers', ',', 'tool', 'form', 'one', '’', 'fingernails', ',', 'line', 'individuals', 'queue', '(', 'Elizabeth', 'D.', 'Liddy,2001', ')', '[', '7', ']', '.']

 TOTAL FILTERED TOKENS ==>  33

 ---- POST FOR FILTERED TOKENS ----

 [('example', 'NN'), (',', ','), ('amongst', 'JJ'), ('meanings', 'NNS'), (',', ','), ('‘', 'NNP'), ('file', 'NN'), ('’', 'NNP'), ('noun', 'CC'), ('mean', 'VBP'), ('either', 'DT'), ('binder', 'NN'), ('gathering', 'NN'), ('papers', 'NNS'), (',', ','), ('tool', 'JJ'), ('form', 'NN'), ('one', 'CD'), ('’', 'NN'), ('fingernails', 'NNS'), (',', ','), ('line', 'NN'), ('individuals', 'NNS'), ('queue', 'VBP'), ('(', '('), ('Elizabeth', 'NNP'), ('D.', 'NNP'), ('Liddy,2001', 'NNP'), (')', ')'), ('[', 'VBD'), ('7', 'CD'), (']', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['example ,', ', amongst', 'amongst meanings', 'meanings ,', ', ‘', '‘ file', 'file ’', '’ noun', 'noun mean', 'mean either', 'either binder', 'binder gathering', 'gathering papers', 'papers ,', ', tool', 'tool form', 'form one', 'one ’', '’ fingernails', 'fingernails ,', ', line', 'line individuals', 'individuals queue', 'queue (', '( Elizabeth', 'Elizabeth D.', 'D. Liddy,2001', 'Liddy,2001 )', ') [', '[ 7', '7 ]', '] .'] 

 TOTAL BIGRAMS --> 32 



 ---- TRI-GRAMS ---- 

 ['example , amongst', ', amongst meanings', 'amongst meanings ,', 'meanings , ‘', ', ‘ file', '‘ file ’', 'file ’ noun', '’ noun mean', 'noun mean either', 'mean either binder', 'either binder gathering', 'binder gathering papers', 'gathering papers ,', 'papers , tool', ', tool form', 'tool form one', 'form one ’', 'one ’ fingernails', '’ fingernails ,', 'fingernails , line', ', line individuals', 'line individuals queue', 'individuals queue (', 'queue ( Elizabeth', '( Elizabeth D.', 'Elizabeth D. Liddy,2001', 'D. Liddy,2001 )', 'Liddy,2001 ) [', ') [ 7', '[ 7 ]', '7 ] .'] 

 TOTAL TRIGRAMS --> 31 



 ---- NOUN PHRASES ---- 

 ['example', 'file', 'either binder', 'gathering', 'tool form', '’', 'line', ']'] 

 TOTAL NOUN PHRASES --> 8 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['exampl', ',', 'amongst', 'mean', ',', '‘', 'file', '’', 'noun', 'mean', 'either', 'binder', 'gather', 'paper', ',', 'tool', 'form', 'one', '’', 'fingernail', ',', 'line', 'individu', 'queue', '(', 'elizabeth', 'd.', 'liddy,2001', ')', '[', '7', ']', '.']

 TOTAL PORTER STEM WORDS ==> 33



 ---- SNOWBALL STEMMING ----

['exampl', ',', 'amongst', 'mean', ',', '‘', 'file', '’', 'noun', 'mean', 'either', 'binder', 'gather', 'paper', ',', 'tool', 'form', 'one', '’', 'fingernail', ',', 'line', 'individu', 'queue', '(', 'elizabeth', 'd.', 'liddy,2001', ')', '[', '7', ']', '.']

 TOTAL SNOWBALL STEM WORDS ==> 33



 ---- LEMMATIZATION ----

['example', ',', 'amongst', 'meaning', ',', '‘', 'file', '’', 'noun', 'mean', 'either', 'binder', 'gathering', 'paper', ',', 'tool', 'form', 'one', '’', 'fingernail', ',', 'line', 'individual', 'queue', '(', 'Elizabeth', 'D.', 'Liddy,2001', ')', '[', '7', ']', '.']

 TOTAL LEMMATIZE WORDS ==> 33

************************************************************************************************************************

92 --> The  semantic level scrutinizes words for their dictionary elucidation, but also for the elucidation  they derive from the milieu of the sentence. 


 ---- TOKENS ----

 ['The', 'semantic', 'level', 'scrutinizes', 'words', 'for', 'their', 'dictionary', 'elucidation', ',', 'but', 'also', 'for', 'the', 'elucidation', 'they', 'derive', 'from', 'the', 'milieu', 'of', 'the', 'sentence', '.'] 

 TOTAL TOKENS ==> 24

 ---- POST ----

 [('The', 'DT'), ('semantic', 'JJ'), ('level', 'NN'), ('scrutinizes', 'NNS'), ('words', 'NNS'), ('for', 'IN'), ('their', 'PRP$'), ('dictionary', 'JJ'), ('elucidation', 'NN'), (',', ','), ('but', 'CC'), ('also', 'RB'), ('for', 'IN'), ('the', 'DT'), ('elucidation', 'NN'), ('they', 'PRP'), ('derive', 'VBP'), ('from', 'IN'), ('the', 'DT'), ('milieu', 'NN'), ('of', 'IN'), ('the', 'DT'), ('sentence', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['semantic', 'level', 'scrutinizes', 'words', 'dictionary', 'elucidation', ',', 'also', 'elucidation', 'derive', 'milieu', 'sentence', '.']

 TOTAL FILTERED TOKENS ==>  13

 ---- POST FOR FILTERED TOKENS ----

 [('semantic', 'JJ'), ('level', 'NN'), ('scrutinizes', 'NNS'), ('words', 'NNS'), ('dictionary', 'JJ'), ('elucidation', 'NN'), (',', ','), ('also', 'RB'), ('elucidation', 'NN'), ('derive', 'JJ'), ('milieu', 'NN'), ('sentence', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['semantic level', 'level scrutinizes', 'scrutinizes words', 'words dictionary', 'dictionary elucidation', 'elucidation ,', ', also', 'also elucidation', 'elucidation derive', 'derive milieu', 'milieu sentence', 'sentence .'] 

 TOTAL BIGRAMS --> 12 



 ---- TRI-GRAMS ---- 

 ['semantic level scrutinizes', 'level scrutinizes words', 'scrutinizes words dictionary', 'words dictionary elucidation', 'dictionary elucidation ,', 'elucidation , also', ', also elucidation', 'also elucidation derive', 'elucidation derive milieu', 'derive milieu sentence', 'milieu sentence .'] 

 TOTAL TRIGRAMS --> 11 



 ---- NOUN PHRASES ---- 

 ['semantic level', 'dictionary elucidation', 'elucidation', 'derive milieu', 'sentence'] 

 TOTAL NOUN PHRASES --> 5 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['semant', 'level', 'scrutin', 'word', 'dictionari', 'elucid', ',', 'also', 'elucid', 'deriv', 'milieu', 'sentenc', '.']

 TOTAL PORTER STEM WORDS ==> 13



 ---- SNOWBALL STEMMING ----

['semant', 'level', 'scrutin', 'word', 'dictionari', 'elucid', ',', 'also', 'elucid', 'deriv', 'milieu', 'sentenc', '.']

 TOTAL SNOWBALL STEM WORDS ==> 13



 ---- LEMMATIZATION ----

['semantic', 'level', 'scrutinizes', 'word', 'dictionary', 'elucidation', ',', 'also', 'elucidation', 'derive', 'milieu', 'sentence', '.']

 TOTAL LEMMATIZE WORDS ==> 13

************************************************************************************************************************

93 --> Semantics milieu that most words have more  than one elucidation but that we can spot the appropriate one by looking at the rest of the  sentence. 


 ---- TOKENS ----

 ['Semantics', 'milieu', 'that', 'most', 'words', 'have', 'more', 'than', 'one', 'elucidation', 'but', 'that', 'we', 'can', 'spot', 'the', 'appropriate', 'one', 'by', 'looking', 'at', 'the', 'rest', 'of', 'the', 'sentence', '.'] 

 TOTAL TOKENS ==> 27

 ---- POST ----

 [('Semantics', 'NNS'), ('milieu', 'VBP'), ('that', 'IN'), ('most', 'JJS'), ('words', 'NNS'), ('have', 'VBP'), ('more', 'JJR'), ('than', 'IN'), ('one', 'CD'), ('elucidation', 'NN'), ('but', 'CC'), ('that', 'IN'), ('we', 'PRP'), ('can', 'MD'), ('spot', 'VB'), ('the', 'DT'), ('appropriate', 'JJ'), ('one', 'CD'), ('by', 'IN'), ('looking', 'VBG'), ('at', 'IN'), ('the', 'DT'), ('rest', 'NN'), ('of', 'IN'), ('the', 'DT'), ('sentence', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Semantics', 'milieu', 'words', 'one', 'elucidation', 'spot', 'appropriate', 'one', 'looking', 'rest', 'sentence', '.']

 TOTAL FILTERED TOKENS ==>  12

 ---- POST FOR FILTERED TOKENS ----

 [('Semantics', 'NNS'), ('milieu', 'VBP'), ('words', 'NNS'), ('one', 'CD'), ('elucidation', 'NN'), ('spot', 'NN'), ('appropriate', 'VBP'), ('one', 'CD'), ('looking', 'VBG'), ('rest', 'NN'), ('sentence', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Semantics milieu', 'milieu words', 'words one', 'one elucidation', 'elucidation spot', 'spot appropriate', 'appropriate one', 'one looking', 'looking rest', 'rest sentence', 'sentence .'] 

 TOTAL BIGRAMS --> 11 



 ---- TRI-GRAMS ---- 

 ['Semantics milieu words', 'milieu words one', 'words one elucidation', 'one elucidation spot', 'elucidation spot appropriate', 'spot appropriate one', 'appropriate one looking', 'one looking rest', 'looking rest sentence', 'rest sentence .'] 

 TOTAL TRIGRAMS --> 10 



 ---- NOUN PHRASES ---- 

 ['elucidation', 'spot', 'rest', 'sentence'] 

 TOTAL NOUN PHRASES --> 4 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['semant', 'milieu', 'word', 'one', 'elucid', 'spot', 'appropri', 'one', 'look', 'rest', 'sentenc', '.']

 TOTAL PORTER STEM WORDS ==> 12



 ---- SNOWBALL STEMMING ----

['semant', 'milieu', 'word', 'one', 'elucid', 'spot', 'appropri', 'one', 'look', 'rest', 'sentenc', '.']

 TOTAL SNOWBALL STEM WORDS ==> 12



 ---- LEMMATIZATION ----

['Semantics', 'milieu', 'word', 'one', 'elucidation', 'spot', 'appropriate', 'one', 'looking', 'rest', 'sentence', '.']

 TOTAL LEMMATIZE WORDS ==> 12

************************************************************************************************************************

94 --> [8]  6. 


 ---- TOKENS ----

 ['[', '8', ']', '6', '.'] 

 TOTAL TOKENS ==> 5

 ---- POST ----

 [('[', 'RB'), ('8', 'CD'), (']', 'JJ'), ('6', 'CD'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['[', '8', ']', '6', '.']

 TOTAL FILTERED TOKENS ==>  5

 ---- POST FOR FILTERED TOKENS ----

 [('[', 'RB'), ('8', 'CD'), (']', 'JJ'), ('6', 'CD'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['[ 8', '8 ]', '] 6', '6 .'] 

 TOTAL BIGRAMS --> 4 



 ---- TRI-GRAMS ---- 

 ['[ 8 ]', '8 ] 6', '] 6 .'] 

 TOTAL TRIGRAMS --> 3 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['[', '8', ']', '6', '.']

 TOTAL PORTER STEM WORDS ==> 5



 ---- SNOWBALL STEMMING ----

['[', '8', ']', '6', '.']

 TOTAL SNOWBALL STEM WORDS ==> 5



 ---- LEMMATIZATION ----

['[', '8', ']', '6', '.']

 TOTAL LEMMATIZE WORDS ==> 5

************************************************************************************************************************

95 --> Discourse  While syntax and semantics travail with sentence-length units, the discourse level of NLP  travail with units of text longer than a sentence i.e, it does not interpret multi sentence texts as  just sequence sentences, apiece of which can be elucidated singly. 


 ---- TOKENS ----

 ['Discourse', 'While', 'syntax', 'and', 'semantics', 'travail', 'with', 'sentence-length', 'units', ',', 'the', 'discourse', 'level', 'of', 'NLP', 'travail', 'with', 'units', 'of', 'text', 'longer', 'than', 'a', 'sentence', 'i.e', ',', 'it', 'does', 'not', 'interpret', 'multi', 'sentence', 'texts', 'as', 'just', 'sequence', 'sentences', ',', 'apiece', 'of', 'which', 'can', 'be', 'elucidated', 'singly', '.'] 

 TOTAL TOKENS ==> 46

 ---- POST ----

 [('Discourse', 'NNP'), ('While', 'IN'), ('syntax', 'NN'), ('and', 'CC'), ('semantics', 'NNS'), ('travail', 'VBP'), ('with', 'IN'), ('sentence-length', 'JJ'), ('units', 'NNS'), (',', ','), ('the', 'DT'), ('discourse', 'JJ'), ('level', 'NN'), ('of', 'IN'), ('NLP', 'NNP'), ('travail', 'NN'), ('with', 'IN'), ('units', 'NNS'), ('of', 'IN'), ('text', 'NN'), ('longer', 'JJR'), ('than', 'IN'), ('a', 'DT'), ('sentence', 'NN'), ('i.e', 'NN'), (',', ','), ('it', 'PRP'), ('does', 'VBZ'), ('not', 'RB'), ('interpret', 'VB'), ('multi', 'JJ'), ('sentence', 'NN'), ('texts', 'NN'), ('as', 'IN'), ('just', 'RB'), ('sequence', 'NN'), ('sentences', 'NNS'), (',', ','), ('apiece', 'NN'), ('of', 'IN'), ('which', 'WDT'), ('can', 'MD'), ('be', 'VB'), ('elucidated', 'VBN'), ('singly', 'RB'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Discourse', 'syntax', 'semantics', 'travail', 'sentence-length', 'units', ',', 'discourse', 'level', 'NLP', 'travail', 'units', 'text', 'longer', 'sentence', 'i.e', ',', 'interpret', 'multi', 'sentence', 'texts', 'sequence', 'sentences', ',', 'apiece', 'elucidated', 'singly', '.']

 TOTAL FILTERED TOKENS ==>  28

 ---- POST FOR FILTERED TOKENS ----

 [('Discourse', 'NNP'), ('syntax', 'NN'), ('semantics', 'NNS'), ('travail', 'VBP'), ('sentence-length', 'JJ'), ('units', 'NNS'), (',', ','), ('discourse', 'JJ'), ('level', 'NN'), ('NLP', 'NNP'), ('travail', 'VBZ'), ('units', 'NNS'), ('text', 'RB'), ('longer', 'RBR'), ('sentence', 'NN'), ('i.e', 'NN'), (',', ','), ('interpret', 'JJ'), ('multi', 'NN'), ('sentence', 'NN'), ('texts', 'NN'), ('sequence', 'NN'), ('sentences', 'NNS'), (',', ','), ('apiece', 'RB'), ('elucidated', 'VBN'), ('singly', 'RB'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Discourse syntax', 'syntax semantics', 'semantics travail', 'travail sentence-length', 'sentence-length units', 'units ,', ', discourse', 'discourse level', 'level NLP', 'NLP travail', 'travail units', 'units text', 'text longer', 'longer sentence', 'sentence i.e', 'i.e ,', ', interpret', 'interpret multi', 'multi sentence', 'sentence texts', 'texts sequence', 'sequence sentences', 'sentences ,', ', apiece', 'apiece elucidated', 'elucidated singly', 'singly .'] 

 TOTAL BIGRAMS --> 27 



 ---- TRI-GRAMS ---- 

 ['Discourse syntax semantics', 'syntax semantics travail', 'semantics travail sentence-length', 'travail sentence-length units', 'sentence-length units ,', 'units , discourse', ', discourse level', 'discourse level NLP', 'level NLP travail', 'NLP travail units', 'travail units text', 'units text longer', 'text longer sentence', 'longer sentence i.e', 'sentence i.e ,', 'i.e , interpret', ', interpret multi', 'interpret multi sentence', 'multi sentence texts', 'sentence texts sequence', 'texts sequence sentences', 'sequence sentences ,', 'sentences , apiece', ', apiece elucidated', 'apiece elucidated singly', 'elucidated singly .'] 

 TOTAL TRIGRAMS --> 26 



 ---- NOUN PHRASES ---- 

 ['syntax', 'discourse level', 'sentence', 'i.e', 'interpret multi', 'sentence', 'texts', 'sequence'] 

 TOTAL NOUN PHRASES --> 8 



 ---- NER ----

 
 ORGANIZATION ---> ['NLP']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> ['Discourse']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['discours', 'syntax', 'semant', 'travail', 'sentence-length', 'unit', ',', 'discours', 'level', 'nlp', 'travail', 'unit', 'text', 'longer', 'sentenc', 'i.e', ',', 'interpret', 'multi', 'sentenc', 'text', 'sequenc', 'sentenc', ',', 'apiec', 'elucid', 'singli', '.']

 TOTAL PORTER STEM WORDS ==> 28



 ---- SNOWBALL STEMMING ----

['discours', 'syntax', 'semant', 'travail', 'sentence-length', 'unit', ',', 'discours', 'level', 'nlp', 'travail', 'unit', 'text', 'longer', 'sentenc', 'i.e', ',', 'interpret', 'multi', 'sentenc', 'text', 'sequenc', 'sentenc', ',', 'apiec', 'elucid', 'singl', '.']

 TOTAL SNOWBALL STEM WORDS ==> 28



 ---- LEMMATIZATION ----

['Discourse', 'syntax', 'semantics', 'travail', 'sentence-length', 'unit', ',', 'discourse', 'level', 'NLP', 'travail', 'unit', 'text', 'longer', 'sentence', 'i.e', ',', 'interpret', 'multi', 'sentence', 'text', 'sequence', 'sentence', ',', 'apiece', 'elucidated', 'singly', '.']

 TOTAL LEMMATIZE WORDS ==> 28

************************************************************************************************************************

96 --> Rather, discourse focuses  on the properties of the text as a whole that convey meaning by making connections between  component sentences (Elizabeth D. Liddy,2001) [7]. 


 ---- TOKENS ----

 ['Rather', ',', 'discourse', 'focuses', 'on', 'the', 'properties', 'of', 'the', 'text', 'as', 'a', 'whole', 'that', 'convey', 'meaning', 'by', 'making', 'connections', 'between', 'component', 'sentences', '(', 'Elizabeth', 'D.', 'Liddy,2001', ')', '[', '7', ']', '.'] 

 TOTAL TOKENS ==> 31

 ---- POST ----

 [('Rather', 'RB'), (',', ','), ('discourse', 'JJ'), ('focuses', 'NNS'), ('on', 'IN'), ('the', 'DT'), ('properties', 'NNS'), ('of', 'IN'), ('the', 'DT'), ('text', 'NN'), ('as', 'IN'), ('a', 'DT'), ('whole', 'NN'), ('that', 'WDT'), ('convey', 'VBZ'), ('meaning', 'NN'), ('by', 'IN'), ('making', 'VBG'), ('connections', 'NNS'), ('between', 'IN'), ('component', 'NN'), ('sentences', 'NNS'), ('(', '('), ('Elizabeth', 'NNP'), ('D.', 'NNP'), ('Liddy,2001', 'NNP'), (')', ')'), ('[', 'VBD'), ('7', 'CD'), (']', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Rather', ',', 'discourse', 'focuses', 'properties', 'text', 'whole', 'convey', 'meaning', 'making', 'connections', 'component', 'sentences', '(', 'Elizabeth', 'D.', 'Liddy,2001', ')', '[', '7', ']', '.']

 TOTAL FILTERED TOKENS ==>  22

 ---- POST FOR FILTERED TOKENS ----

 [('Rather', 'RB'), (',', ','), ('discourse', 'JJ'), ('focuses', 'NNS'), ('properties', 'NNS'), ('text', 'VBP'), ('whole', 'JJ'), ('convey', 'NN'), ('meaning', 'VBG'), ('making', 'VBG'), ('connections', 'NNS'), ('component', 'JJ'), ('sentences', 'NNS'), ('(', '('), ('Elizabeth', 'NNP'), ('D.', 'NNP'), ('Liddy,2001', 'NNP'), (')', ')'), ('[', 'VBD'), ('7', 'CD'), (']', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Rather ,', ', discourse', 'discourse focuses', 'focuses properties', 'properties text', 'text whole', 'whole convey', 'convey meaning', 'meaning making', 'making connections', 'connections component', 'component sentences', 'sentences (', '( Elizabeth', 'Elizabeth D.', 'D. Liddy,2001', 'Liddy,2001 )', ') [', '[ 7', '7 ]', '] .'] 

 TOTAL BIGRAMS --> 21 



 ---- TRI-GRAMS ---- 

 ['Rather , discourse', ', discourse focuses', 'discourse focuses properties', 'focuses properties text', 'properties text whole', 'text whole convey', 'whole convey meaning', 'convey meaning making', 'meaning making connections', 'making connections component', 'connections component sentences', 'component sentences (', 'sentences ( Elizabeth', '( Elizabeth D.', 'Elizabeth D. Liddy,2001', 'D. Liddy,2001 )', 'Liddy,2001 ) [', ') [ 7', '[ 7 ]', '7 ] .'] 

 TOTAL TRIGRAMS --> 20 



 ---- NOUN PHRASES ---- 

 ['whole convey', ']'] 

 TOTAL NOUN PHRASES --> 2 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['rather', ',', 'discours', 'focus', 'properti', 'text', 'whole', 'convey', 'mean', 'make', 'connect', 'compon', 'sentenc', '(', 'elizabeth', 'd.', 'liddy,2001', ')', '[', '7', ']', '.']

 TOTAL PORTER STEM WORDS ==> 22



 ---- SNOWBALL STEMMING ----

['rather', ',', 'discours', 'focus', 'properti', 'text', 'whole', 'convey', 'mean', 'make', 'connect', 'compon', 'sentenc', '(', 'elizabeth', 'd.', 'liddy,2001', ')', '[', '7', ']', '.']

 TOTAL SNOWBALL STEM WORDS ==> 22



 ---- LEMMATIZATION ----

['Rather', ',', 'discourse', 'focus', 'property', 'text', 'whole', 'convey', 'meaning', 'making', 'connection', 'component', 'sentence', '(', 'Elizabeth', 'D.', 'Liddy,2001', ')', '[', '7', ']', '.']

 TOTAL LEMMATIZE WORDS ==> 22

************************************************************************************************************************

97 --> The two of the most common levels are  Anaphora Resolution - Anaphora resolution is the replacing of words such as pronouns,  which are semantically stranded, with the pertinent entity to which they refer. 


 ---- TOKENS ----

 ['The', 'two', 'of', 'the', 'most', 'common', 'levels', 'are', 'Anaphora', 'Resolution', '-', 'Anaphora', 'resolution', 'is', 'the', 'replacing', 'of', 'words', 'such', 'as', 'pronouns', ',', 'which', 'are', 'semantically', 'stranded', ',', 'with', 'the', 'pertinent', 'entity', 'to', 'which', 'they', 'refer', '.'] 

 TOTAL TOKENS ==> 36

 ---- POST ----

 [('The', 'DT'), ('two', 'CD'), ('of', 'IN'), ('the', 'DT'), ('most', 'RBS'), ('common', 'JJ'), ('levels', 'NNS'), ('are', 'VBP'), ('Anaphora', 'NNP'), ('Resolution', 'NNP'), ('-', ':'), ('Anaphora', 'NNP'), ('resolution', 'NN'), ('is', 'VBZ'), ('the', 'DT'), ('replacing', 'NN'), ('of', 'IN'), ('words', 'NNS'), ('such', 'JJ'), ('as', 'IN'), ('pronouns', 'NNS'), (',', ','), ('which', 'WDT'), ('are', 'VBP'), ('semantically', 'RB'), ('stranded', 'VBN'), (',', ','), ('with', 'IN'), ('the', 'DT'), ('pertinent', 'JJ'), ('entity', 'NN'), ('to', 'TO'), ('which', 'WDT'), ('they', 'PRP'), ('refer', 'VBP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['two', 'common', 'levels', 'Anaphora', 'Resolution', '-', 'Anaphora', 'resolution', 'replacing', 'words', 'pronouns', ',', 'semantically', 'stranded', ',', 'pertinent', 'entity', 'refer', '.']

 TOTAL FILTERED TOKENS ==>  19

 ---- POST FOR FILTERED TOKENS ----

 [('two', 'CD'), ('common', 'JJ'), ('levels', 'NNS'), ('Anaphora', 'NNP'), ('Resolution', 'NNP'), ('-', ':'), ('Anaphora', 'NNP'), ('resolution', 'NN'), ('replacing', 'VBG'), ('words', 'NNS'), ('pronouns', 'NNS'), (',', ','), ('semantically', 'RB'), ('stranded', 'VBN'), (',', ','), ('pertinent', 'JJ'), ('entity', 'NN'), ('refer', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['two common', 'common levels', 'levels Anaphora', 'Anaphora Resolution', 'Resolution -', '- Anaphora', 'Anaphora resolution', 'resolution replacing', 'replacing words', 'words pronouns', 'pronouns ,', ', semantically', 'semantically stranded', 'stranded ,', ', pertinent', 'pertinent entity', 'entity refer', 'refer .'] 

 TOTAL BIGRAMS --> 18 



 ---- TRI-GRAMS ---- 

 ['two common levels', 'common levels Anaphora', 'levels Anaphora Resolution', 'Anaphora Resolution -', 'Resolution - Anaphora', '- Anaphora resolution', 'Anaphora resolution replacing', 'resolution replacing words', 'replacing words pronouns', 'words pronouns ,', 'pronouns , semantically', ', semantically stranded', 'semantically stranded ,', 'stranded , pertinent', ', pertinent entity', 'pertinent entity refer', 'entity refer .'] 

 TOTAL TRIGRAMS --> 17 



 ---- NOUN PHRASES ---- 

 ['resolution', 'pertinent entity', 'refer'] 

 TOTAL NOUN PHRASES --> 3 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> ['Anaphora Resolution']
 TOTAL PERSON ENTITY --> 1 


 GPE ---> ['Anaphora']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['two', 'common', 'level', 'anaphora', 'resolut', '-', 'anaphora', 'resolut', 'replac', 'word', 'pronoun', ',', 'semant', 'strand', ',', 'pertin', 'entiti', 'refer', '.']

 TOTAL PORTER STEM WORDS ==> 19



 ---- SNOWBALL STEMMING ----

['two', 'common', 'level', 'anaphora', 'resolut', '-', 'anaphora', 'resolut', 'replac', 'word', 'pronoun', ',', 'semant', 'strand', ',', 'pertin', 'entiti', 'refer', '.']

 TOTAL SNOWBALL STEM WORDS ==> 19



 ---- LEMMATIZATION ----

['two', 'common', 'level', 'Anaphora', 'Resolution', '-', 'Anaphora', 'resolution', 'replacing', 'word', 'pronoun', ',', 'semantically', 'stranded', ',', 'pertinent', 'entity', 'refer', '.']

 TOTAL LEMMATIZE WORDS ==> 19

************************************************************************************************************************

98 --> Discourse/Text  Structure Recognition - Discourse/text structure recognition sway the functions of sentences  in the text, which, in turn, adds to the meaningful representation of the text. 


 ---- TOKENS ----

 ['Discourse/Text', 'Structure', 'Recognition', '-', 'Discourse/text', 'structure', 'recognition', 'sway', 'the', 'functions', 'of', 'sentences', 'in', 'the', 'text', ',', 'which', ',', 'in', 'turn', ',', 'adds', 'to', 'the', 'meaningful', 'representation', 'of', 'the', 'text', '.'] 

 TOTAL TOKENS ==> 30

 ---- POST ----

 [('Discourse/Text', 'JJ'), ('Structure', 'NNP'), ('Recognition', 'NNP'), ('-', ':'), ('Discourse/text', 'JJ'), ('structure', 'NN'), ('recognition', 'NN'), ('sway', 'VBD'), ('the', 'DT'), ('functions', 'NNS'), ('of', 'IN'), ('sentences', 'NNS'), ('in', 'IN'), ('the', 'DT'), ('text', 'NN'), (',', ','), ('which', 'WDT'), (',', ','), ('in', 'IN'), ('turn', 'NN'), (',', ','), ('adds', 'VBZ'), ('to', 'TO'), ('the', 'DT'), ('meaningful', 'JJ'), ('representation', 'NN'), ('of', 'IN'), ('the', 'DT'), ('text', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Discourse/Text', 'Structure', 'Recognition', '-', 'Discourse/text', 'structure', 'recognition', 'sway', 'functions', 'sentences', 'text', ',', ',', 'turn', ',', 'adds', 'meaningful', 'representation', 'text', '.']

 TOTAL FILTERED TOKENS ==>  20

 ---- POST FOR FILTERED TOKENS ----

 [('Discourse/Text', 'JJ'), ('Structure', 'NNP'), ('Recognition', 'NNP'), ('-', ':'), ('Discourse/text', 'JJ'), ('structure', 'NN'), ('recognition', 'NN'), ('sway', 'NN'), ('functions', 'NNS'), ('sentences', 'NNS'), ('text', 'RB'), (',', ','), (',', ','), ('turn', 'NN'), (',', ','), ('adds', 'VBZ'), ('meaningful', 'JJ'), ('representation', 'NN'), ('text', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Discourse/Text Structure', 'Structure Recognition', 'Recognition -', '- Discourse/text', 'Discourse/text structure', 'structure recognition', 'recognition sway', 'sway functions', 'functions sentences', 'sentences text', 'text ,', ', ,', ', turn', 'turn ,', ', adds', 'adds meaningful', 'meaningful representation', 'representation text', 'text .'] 

 TOTAL BIGRAMS --> 19 



 ---- TRI-GRAMS ---- 

 ['Discourse/Text Structure Recognition', 'Structure Recognition -', 'Recognition - Discourse/text', '- Discourse/text structure', 'Discourse/text structure recognition', 'structure recognition sway', 'recognition sway functions', 'sway functions sentences', 'functions sentences text', 'sentences text ,', 'text , ,', ', , turn', ', turn ,', 'turn , adds', ', adds meaningful', 'adds meaningful representation', 'meaningful representation text', 'representation text .'] 

 TOTAL TRIGRAMS --> 18 



 ---- NOUN PHRASES ---- 

 ['Discourse structure', 'recognition', 'sway', 'turn', 'meaningful representation', 'text'] 

 TOTAL NOUN PHRASES --> 6 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> ['Structure Recognition']
 TOTAL PERSON ENTITY --> 1 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['discourse/text', 'structur', 'recognit', '-', 'discourse/text', 'structur', 'recognit', 'sway', 'function', 'sentenc', 'text', ',', ',', 'turn', ',', 'add', 'meaning', 'represent', 'text', '.']

 TOTAL PORTER STEM WORDS ==> 20



 ---- SNOWBALL STEMMING ----

['discourse/text', 'structur', 'recognit', '-', 'discourse/text', 'structur', 'recognit', 'sway', 'function', 'sentenc', 'text', ',', ',', 'turn', ',', 'add', 'meaning', 'represent', 'text', '.']

 TOTAL SNOWBALL STEM WORDS ==> 20



 ---- LEMMATIZATION ----

['Discourse/Text', 'Structure', 'Recognition', '-', 'Discourse/text', 'structure', 'recognition', 'sway', 'function', 'sentence', 'text', ',', ',', 'turn', ',', 'add', 'meaningful', 'representation', 'text', '.']

 TOTAL LEMMATIZE WORDS ==> 20

************************************************************************************************************************

99 --> 7. 


 ---- TOKENS ----

 ['7', '.'] 

 TOTAL TOKENS ==> 2

 ---- POST ----

 [('7', 'CD'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['7', '.']

 TOTAL FILTERED TOKENS ==>  2

 ---- POST FOR FILTERED TOKENS ----

 [('7', 'CD'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['7 .'] 

 TOTAL BIGRAMS --> 1 



 ---- TRI-GRAMS ---- 

 [] 

 TOTAL TRIGRAMS --> 0 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['7', '.']

 TOTAL PORTER STEM WORDS ==> 2



 ---- SNOWBALL STEMMING ----

['7', '.']

 TOTAL SNOWBALL STEM WORDS ==> 2



 ---- LEMMATIZATION ----

['7', '.']

 TOTAL LEMMATIZE WORDS ==> 2

************************************************************************************************************************

100 --> Pragmatic:   Pragmatic is concerned with the firm use of language in situations and utilizes nub over and  above the nub of the text for understanding the goal and to explain how extra meaning is read  into texts without literally being encoded in them. 


 ---- TOKENS ----

 ['Pragmatic', ':', 'Pragmatic', 'is', 'concerned', 'with', 'the', 'firm', 'use', 'of', 'language', 'in', 'situations', 'and', 'utilizes', 'nub', 'over', 'and', 'above', 'the', 'nub', 'of', 'the', 'text', 'for', 'understanding', 'the', 'goal', 'and', 'to', 'explain', 'how', 'extra', 'meaning', 'is', 'read', 'into', 'texts', 'without', 'literally', 'being', 'encoded', 'in', 'them', '.'] 

 TOTAL TOKENS ==> 45

 ---- POST ----

 [('Pragmatic', 'JJ'), (':', ':'), ('Pragmatic', 'JJ'), ('is', 'VBZ'), ('concerned', 'VBN'), ('with', 'IN'), ('the', 'DT'), ('firm', 'NN'), ('use', 'NN'), ('of', 'IN'), ('language', 'NN'), ('in', 'IN'), ('situations', 'NNS'), ('and', 'CC'), ('utilizes', 'JJ'), ('nub', 'NN'), ('over', 'IN'), ('and', 'CC'), ('above', 'IN'), ('the', 'DT'), ('nub', 'NN'), ('of', 'IN'), ('the', 'DT'), ('text', 'NN'), ('for', 'IN'), ('understanding', 'VBG'), ('the', 'DT'), ('goal', 'NN'), ('and', 'CC'), ('to', 'TO'), ('explain', 'VB'), ('how', 'WRB'), ('extra', 'JJ'), ('meaning', 'NN'), ('is', 'VBZ'), ('read', 'VBN'), ('into', 'IN'), ('texts', 'NNS'), ('without', 'IN'), ('literally', 'RB'), ('being', 'VBG'), ('encoded', 'VBN'), ('in', 'IN'), ('them', 'PRP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Pragmatic', ':', 'Pragmatic', 'concerned', 'firm', 'use', 'language', 'situations', 'utilizes', 'nub', 'nub', 'text', 'understanding', 'goal', 'explain', 'extra', 'meaning', 'read', 'texts', 'without', 'literally', 'encoded', '.']

 TOTAL FILTERED TOKENS ==>  23

 ---- POST FOR FILTERED TOKENS ----

 [('Pragmatic', 'JJ'), (':', ':'), ('Pragmatic', 'JJ'), ('concerned', 'JJ'), ('firm', 'NN'), ('use', 'NN'), ('language', 'NN'), ('situations', 'NNS'), ('utilizes', 'JJ'), ('nub', 'JJ'), ('nub', 'NN'), ('text', 'NN'), ('understanding', 'JJ'), ('goal', 'NN'), ('explain', 'VBP'), ('extra', 'JJ'), ('meaning', 'NN'), ('read', 'NN'), ('texts', 'NN'), ('without', 'IN'), ('literally', 'RB'), ('encoded', 'VBN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Pragmatic :', ': Pragmatic', 'Pragmatic concerned', 'concerned firm', 'firm use', 'use language', 'language situations', 'situations utilizes', 'utilizes nub', 'nub nub', 'nub text', 'text understanding', 'understanding goal', 'goal explain', 'explain extra', 'extra meaning', 'meaning read', 'read texts', 'texts without', 'without literally', 'literally encoded', 'encoded .'] 

 TOTAL BIGRAMS --> 22 



 ---- TRI-GRAMS ---- 

 ['Pragmatic : Pragmatic', ': Pragmatic concerned', 'Pragmatic concerned firm', 'concerned firm use', 'firm use language', 'use language situations', 'language situations utilizes', 'situations utilizes nub', 'utilizes nub nub', 'nub nub text', 'nub text understanding', 'text understanding goal', 'understanding goal explain', 'goal explain extra', 'explain extra meaning', 'extra meaning read', 'meaning read texts', 'read texts without', 'texts without literally', 'without literally encoded', 'literally encoded .'] 

 TOTAL TRIGRAMS --> 21 



 ---- NOUN PHRASES ---- 

 ['Pragmatic concerned firm', 'use', 'language', 'utilizes nub nub', 'text', 'understanding goal', 'extra meaning', 'read', 'texts'] 

 TOTAL NOUN PHRASES --> 9 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['pragmat', ':', 'pragmat', 'concern', 'firm', 'use', 'languag', 'situat', 'util', 'nub', 'nub', 'text', 'understand', 'goal', 'explain', 'extra', 'mean', 'read', 'text', 'without', 'liter', 'encod', '.']

 TOTAL PORTER STEM WORDS ==> 23



 ---- SNOWBALL STEMMING ----

['pragmat', ':', 'pragmat', 'concern', 'firm', 'use', 'languag', 'situat', 'util', 'nub', 'nub', 'text', 'understand', 'goal', 'explain', 'extra', 'mean', 'read', 'text', 'without', 'liter', 'encod', '.']

 TOTAL SNOWBALL STEM WORDS ==> 23



 ---- LEMMATIZATION ----

['Pragmatic', ':', 'Pragmatic', 'concerned', 'firm', 'use', 'language', 'situation', 'utilizes', 'nub', 'nub', 'text', 'understanding', 'goal', 'explain', 'extra', 'meaning', 'read', 'text', 'without', 'literally', 'encoded', '.']

 TOTAL LEMMATIZE WORDS ==> 23

************************************************************************************************************************

101 --> This requisite much world knowledge,  including the understanding of intentions, plans, and goals. 


 ---- TOKENS ----

 ['This', 'requisite', 'much', 'world', 'knowledge', ',', 'including', 'the', 'understanding', 'of', 'intentions', ',', 'plans', ',', 'and', 'goals', '.'] 

 TOTAL TOKENS ==> 17

 ---- POST ----

 [('This', 'DT'), ('requisite', 'JJ'), ('much', 'JJ'), ('world', 'NN'), ('knowledge', 'NN'), (',', ','), ('including', 'VBG'), ('the', 'DT'), ('understanding', 'NN'), ('of', 'IN'), ('intentions', 'NNS'), (',', ','), ('plans', 'NNS'), (',', ','), ('and', 'CC'), ('goals', 'NNS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['requisite', 'much', 'world', 'knowledge', ',', 'including', 'understanding', 'intentions', ',', 'plans', ',', 'goals', '.']

 TOTAL FILTERED TOKENS ==>  13

 ---- POST FOR FILTERED TOKENS ----

 [('requisite', 'RB'), ('much', 'JJ'), ('world', 'NN'), ('knowledge', 'NN'), (',', ','), ('including', 'VBG'), ('understanding', 'JJ'), ('intentions', 'NNS'), (',', ','), ('plans', 'NNS'), (',', ','), ('goals', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['requisite much', 'much world', 'world knowledge', 'knowledge ,', ', including', 'including understanding', 'understanding intentions', 'intentions ,', ', plans', 'plans ,', ', goals', 'goals .'] 

 TOTAL BIGRAMS --> 12 



 ---- TRI-GRAMS ---- 

 ['requisite much world', 'much world knowledge', 'world knowledge ,', 'knowledge , including', ', including understanding', 'including understanding intentions', 'understanding intentions ,', 'intentions , plans', ', plans ,', 'plans , goals', ', goals .'] 

 TOTAL TRIGRAMS --> 11 



 ---- NOUN PHRASES ---- 

 ['much world', 'knowledge'] 

 TOTAL NOUN PHRASES --> 2 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['requisit', 'much', 'world', 'knowledg', ',', 'includ', 'understand', 'intent', ',', 'plan', ',', 'goal', '.']

 TOTAL PORTER STEM WORDS ==> 13



 ---- SNOWBALL STEMMING ----

['requisit', 'much', 'world', 'knowledg', ',', 'includ', 'understand', 'intent', ',', 'plan', ',', 'goal', '.']

 TOTAL SNOWBALL STEM WORDS ==> 13



 ---- LEMMATIZATION ----

['requisite', 'much', 'world', 'knowledge', ',', 'including', 'understanding', 'intention', ',', 'plan', ',', 'goal', '.']

 TOTAL LEMMATIZE WORDS ==> 13

************************************************************************************************************************

102 --> For example, the following two  sentences need aspiration of the anaphoric term ‘they’, but this aspiration requires pragmatic  or world knowledge (Elizabeth D. Liddy,2001) [7]. 


 ---- TOKENS ----

 ['For', 'example', ',', 'the', 'following', 'two', 'sentences', 'need', 'aspiration', 'of', 'the', 'anaphoric', 'term', '‘', 'they', '’', ',', 'but', 'this', 'aspiration', 'requires', 'pragmatic', 'or', 'world', 'knowledge', '(', 'Elizabeth', 'D.', 'Liddy,2001', ')', '[', '7', ']', '.'] 

 TOTAL TOKENS ==> 34

 ---- POST ----

 [('For', 'IN'), ('example', 'NN'), (',', ','), ('the', 'DT'), ('following', 'VBG'), ('two', 'CD'), ('sentences', 'NNS'), ('need', 'VBP'), ('aspiration', 'NN'), ('of', 'IN'), ('the', 'DT'), ('anaphoric', 'JJ'), ('term', 'NN'), ('‘', 'IN'), ('they', 'PRP'), ('’', 'VBP'), (',', ','), ('but', 'CC'), ('this', 'DT'), ('aspiration', 'NN'), ('requires', 'VBZ'), ('pragmatic', 'JJ'), ('or', 'CC'), ('world', 'NN'), ('knowledge', 'NN'), ('(', '('), ('Elizabeth', 'NNP'), ('D.', 'NNP'), ('Liddy,2001', 'NNP'), (')', ')'), ('[', 'VBD'), ('7', 'CD'), (']', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['example', ',', 'following', 'two', 'sentences', 'need', 'aspiration', 'anaphoric', 'term', '‘', '’', ',', 'aspiration', 'requires', 'pragmatic', 'world', 'knowledge', '(', 'Elizabeth', 'D.', 'Liddy,2001', ')', '[', '7', ']', '.']

 TOTAL FILTERED TOKENS ==>  26

 ---- POST FOR FILTERED TOKENS ----

 [('example', 'NN'), (',', ','), ('following', 'VBG'), ('two', 'CD'), ('sentences', 'NNS'), ('need', 'VBP'), ('aspiration', 'NN'), ('anaphoric', 'JJ'), ('term', 'NN'), ('‘', 'NNP'), ('’', 'NNP'), (',', ','), ('aspiration', 'NN'), ('requires', 'VBZ'), ('pragmatic', 'JJ'), ('world', 'NN'), ('knowledge', 'NN'), ('(', '('), ('Elizabeth', 'NNP'), ('D.', 'NNP'), ('Liddy,2001', 'NNP'), (')', ')'), ('[', 'VBD'), ('7', 'CD'), (']', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['example ,', ', following', 'following two', 'two sentences', 'sentences need', 'need aspiration', 'aspiration anaphoric', 'anaphoric term', 'term ‘', '‘ ’', '’ ,', ', aspiration', 'aspiration requires', 'requires pragmatic', 'pragmatic world', 'world knowledge', 'knowledge (', '( Elizabeth', 'Elizabeth D.', 'D. Liddy,2001', 'Liddy,2001 )', ') [', '[ 7', '7 ]', '] .'] 

 TOTAL BIGRAMS --> 25 



 ---- TRI-GRAMS ---- 

 ['example , following', ', following two', 'following two sentences', 'two sentences need', 'sentences need aspiration', 'need aspiration anaphoric', 'aspiration anaphoric term', 'anaphoric term ‘', 'term ‘ ’', '‘ ’ ,', '’ , aspiration', ', aspiration requires', 'aspiration requires pragmatic', 'requires pragmatic world', 'pragmatic world knowledge', 'world knowledge (', 'knowledge ( Elizabeth', '( Elizabeth D.', 'Elizabeth D. Liddy,2001', 'D. Liddy,2001 )', 'Liddy,2001 ) [', ') [ 7', '[ 7 ]', '7 ] .'] 

 TOTAL TRIGRAMS --> 24 



 ---- NOUN PHRASES ---- 

 ['example', 'aspiration', 'anaphoric term', 'aspiration', 'pragmatic world', 'knowledge', ']'] 

 TOTAL NOUN PHRASES --> 7 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['exampl', ',', 'follow', 'two', 'sentenc', 'need', 'aspir', 'anaphor', 'term', '‘', '’', ',', 'aspir', 'requir', 'pragmat', 'world', 'knowledg', '(', 'elizabeth', 'd.', 'liddy,2001', ')', '[', '7', ']', '.']

 TOTAL PORTER STEM WORDS ==> 26



 ---- SNOWBALL STEMMING ----

['exampl', ',', 'follow', 'two', 'sentenc', 'need', 'aspir', 'anaphor', 'term', '‘', '’', ',', 'aspir', 'requir', 'pragmat', 'world', 'knowledg', '(', 'elizabeth', 'd.', 'liddy,2001', ')', '[', '7', ']', '.']

 TOTAL SNOWBALL STEM WORDS ==> 26



 ---- LEMMATIZATION ----

['example', ',', 'following', 'two', 'sentence', 'need', 'aspiration', 'anaphoric', 'term', '‘', '’', ',', 'aspiration', 'requires', 'pragmatic', 'world', 'knowledge', '(', 'Elizabeth', 'D.', 'Liddy,2001', ')', '[', '7', ']', '.']

 TOTAL LEMMATIZE WORDS ==> 26

************************************************************************************************************************

103 --> 3. 


 ---- TOKENS ----

 ['3', '.'] 

 TOTAL TOKENS ==> 2

 ---- POST ----

 [('3', 'CD'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['3', '.']

 TOTAL FILTERED TOKENS ==>  2

 ---- POST FOR FILTERED TOKENS ----

 [('3', 'CD'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['3 .'] 

 TOTAL BIGRAMS --> 1 



 ---- TRI-GRAMS ---- 

 [] 

 TOTAL TRIGRAMS --> 0 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['3', '.']

 TOTAL PORTER STEM WORDS ==> 2



 ---- SNOWBALL STEMMING ----

['3', '.']

 TOTAL SNOWBALL STEM WORDS ==> 2



 ---- LEMMATIZATION ----

['3', '.']

 TOTAL LEMMATIZE WORDS ==> 2

************************************************************************************************************************

104 --> Natural Language Generation  Natural Language Generation (NLG) is the process of producing phrases, sentences and  paragraphs that are meaningful from an internal representation. 


 ---- TOKENS ----

 ['Natural', 'Language', 'Generation', 'Natural', 'Language', 'Generation', '(', 'NLG', ')', 'is', 'the', 'process', 'of', 'producing', 'phrases', ',', 'sentences', 'and', 'paragraphs', 'that', 'are', 'meaningful', 'from', 'an', 'internal', 'representation', '.'] 

 TOTAL TOKENS ==> 27

 ---- POST ----

 [('Natural', 'JJ'), ('Language', 'NNP'), ('Generation', 'NNP'), ('Natural', 'NNP'), ('Language', 'NNP'), ('Generation', 'NNP'), ('(', '('), ('NLG', 'NNP'), (')', ')'), ('is', 'VBZ'), ('the', 'DT'), ('process', 'NN'), ('of', 'IN'), ('producing', 'VBG'), ('phrases', 'NNS'), (',', ','), ('sentences', 'NNS'), ('and', 'CC'), ('paragraphs', 'NN'), ('that', 'WDT'), ('are', 'VBP'), ('meaningful', 'JJ'), ('from', 'IN'), ('an', 'DT'), ('internal', 'JJ'), ('representation', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Natural', 'Language', 'Generation', 'Natural', 'Language', 'Generation', '(', 'NLG', ')', 'process', 'producing', 'phrases', ',', 'sentences', 'paragraphs', 'meaningful', 'internal', 'representation', '.']

 TOTAL FILTERED TOKENS ==>  19

 ---- POST FOR FILTERED TOKENS ----

 [('Natural', 'JJ'), ('Language', 'NNP'), ('Generation', 'NNP'), ('Natural', 'NNP'), ('Language', 'NNP'), ('Generation', 'NNP'), ('(', '('), ('NLG', 'NNP'), (')', ')'), ('process', 'NN'), ('producing', 'VBG'), ('phrases', 'NNS'), (',', ','), ('sentences', 'NNS'), ('paragraphs', 'VBP'), ('meaningful', 'JJ'), ('internal', 'JJ'), ('representation', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Natural Language', 'Language Generation', 'Generation Natural', 'Natural Language', 'Language Generation', 'Generation (', '( NLG', 'NLG )', ') process', 'process producing', 'producing phrases', 'phrases ,', ', sentences', 'sentences paragraphs', 'paragraphs meaningful', 'meaningful internal', 'internal representation', 'representation .'] 

 TOTAL BIGRAMS --> 18 



 ---- TRI-GRAMS ---- 

 ['Natural Language Generation', 'Language Generation Natural', 'Generation Natural Language', 'Natural Language Generation', 'Language Generation (', 'Generation ( NLG', '( NLG )', 'NLG ) process', ') process producing', 'process producing phrases', 'producing phrases ,', 'phrases , sentences', ', sentences paragraphs', 'sentences paragraphs meaningful', 'paragraphs meaningful internal', 'meaningful internal representation', 'internal representation .'] 

 TOTAL TRIGRAMS --> 17 



 ---- NOUN PHRASES ---- 

 ['process', 'meaningful internal representation'] 

 TOTAL NOUN PHRASES --> 2 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['natur', 'languag', 'gener', 'natur', 'languag', 'gener', '(', 'nlg', ')', 'process', 'produc', 'phrase', ',', 'sentenc', 'paragraph', 'meaning', 'intern', 'represent', '.']

 TOTAL PORTER STEM WORDS ==> 19



 ---- SNOWBALL STEMMING ----

['natur', 'languag', 'generat', 'natur', 'languag', 'generat', '(', 'nlg', ')', 'process', 'produc', 'phrase', ',', 'sentenc', 'paragraph', 'meaning', 'intern', 'represent', '.']

 TOTAL SNOWBALL STEM WORDS ==> 19



 ---- LEMMATIZATION ----

['Natural', 'Language', 'Generation', 'Natural', 'Language', 'Generation', '(', 'NLG', ')', 'process', 'producing', 'phrase', ',', 'sentence', 'paragraph', 'meaningful', 'internal', 'representation', '.']

 TOTAL LEMMATIZE WORDS ==> 19

************************************************************************************************************************

105 --> It is a part of Natural  Language Processing and happens in four phases: identifying the goals, planning on how  goals maybe achieved by evaluating the situation and available communicative sources and  realizing the plans as a text [Figure 3]. 


 ---- TOKENS ----

 ['It', 'is', 'a', 'part', 'of', 'Natural', 'Language', 'Processing', 'and', 'happens', 'in', 'four', 'phases', ':', 'identifying', 'the', 'goals', ',', 'planning', 'on', 'how', 'goals', 'maybe', 'achieved', 'by', 'evaluating', 'the', 'situation', 'and', 'available', 'communicative', 'sources', 'and', 'realizing', 'the', 'plans', 'as', 'a', 'text', '[', 'Figure', '3', ']', '.'] 

 TOTAL TOKENS ==> 44

 ---- POST ----

 [('It', 'PRP'), ('is', 'VBZ'), ('a', 'DT'), ('part', 'NN'), ('of', 'IN'), ('Natural', 'NNP'), ('Language', 'NNP'), ('Processing', 'NNP'), ('and', 'CC'), ('happens', 'VBZ'), ('in', 'IN'), ('four', 'CD'), ('phases', 'NNS'), (':', ':'), ('identifying', 'VBG'), ('the', 'DT'), ('goals', 'NNS'), (',', ','), ('planning', 'VBG'), ('on', 'IN'), ('how', 'WRB'), ('goals', 'NNS'), ('maybe', 'RB'), ('achieved', 'VBN'), ('by', 'IN'), ('evaluating', 'VBG'), ('the', 'DT'), ('situation', 'NN'), ('and', 'CC'), ('available', 'JJ'), ('communicative', 'JJ'), ('sources', 'NNS'), ('and', 'CC'), ('realizing', 'VBG'), ('the', 'DT'), ('plans', 'NNS'), ('as', 'IN'), ('a', 'DT'), ('text', 'NN'), ('[', 'JJ'), ('Figure', 'NNP'), ('3', 'CD'), (']', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['part', 'Natural', 'Language', 'Processing', 'happens', 'four', 'phases', ':', 'identifying', 'goals', ',', 'planning', 'goals', 'maybe', 'achieved', 'evaluating', 'situation', 'available', 'communicative', 'sources', 'realizing', 'plans', 'text', '[', 'Figure', '3', ']', '.']

 TOTAL FILTERED TOKENS ==>  28

 ---- POST FOR FILTERED TOKENS ----

 [('part', 'NN'), ('Natural', 'NNP'), ('Language', 'NNP'), ('Processing', 'NNP'), ('happens', 'VBZ'), ('four', 'CD'), ('phases', 'NNS'), (':', ':'), ('identifying', 'NN'), ('goals', 'NNS'), (',', ','), ('planning', 'VBG'), ('goals', 'NNS'), ('maybe', 'RB'), ('achieved', 'VBD'), ('evaluating', 'VBG'), ('situation', 'NN'), ('available', 'JJ'), ('communicative', 'JJ'), ('sources', 'NNS'), ('realizing', 'VBG'), ('plans', 'NNS'), ('text', 'JJ'), ('[', 'JJ'), ('Figure', 'NN'), ('3', 'CD'), (']', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['part Natural', 'Natural Language', 'Language Processing', 'Processing happens', 'happens four', 'four phases', 'phases :', ': identifying', 'identifying goals', 'goals ,', ', planning', 'planning goals', 'goals maybe', 'maybe achieved', 'achieved evaluating', 'evaluating situation', 'situation available', 'available communicative', 'communicative sources', 'sources realizing', 'realizing plans', 'plans text', 'text [', '[ Figure', 'Figure 3', '3 ]', '] .'] 

 TOTAL BIGRAMS --> 27 



 ---- TRI-GRAMS ---- 

 ['part Natural Language', 'Natural Language Processing', 'Language Processing happens', 'Processing happens four', 'happens four phases', 'four phases :', 'phases : identifying', ': identifying goals', 'identifying goals ,', 'goals , planning', ', planning goals', 'planning goals maybe', 'goals maybe achieved', 'maybe achieved evaluating', 'achieved evaluating situation', 'evaluating situation available', 'situation available communicative', 'available communicative sources', 'communicative sources realizing', 'sources realizing plans', 'realizing plans text', 'plans text [', 'text [ Figure', '[ Figure 3', 'Figure 3 ]', '3 ] .'] 

 TOTAL TRIGRAMS --> 26 



 ---- NOUN PHRASES ---- 

 ['part', 'identifying', 'situation', 'text [ Figure', ']'] 

 TOTAL NOUN PHRASES --> 5 



 ---- NER ----

 
 ORGANIZATION ---> ['Natural Language']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['part', 'natur', 'languag', 'process', 'happen', 'four', 'phase', ':', 'identifi', 'goal', ',', 'plan', 'goal', 'mayb', 'achiev', 'evalu', 'situat', 'avail', 'commun', 'sourc', 'realiz', 'plan', 'text', '[', 'figur', '3', ']', '.']

 TOTAL PORTER STEM WORDS ==> 28



 ---- SNOWBALL STEMMING ----

['part', 'natur', 'languag', 'process', 'happen', 'four', 'phase', ':', 'identifi', 'goal', ',', 'plan', 'goal', 'mayb', 'achiev', 'evalu', 'situat', 'avail', 'communic', 'sourc', 'realiz', 'plan', 'text', '[', 'figur', '3', ']', '.']

 TOTAL SNOWBALL STEM WORDS ==> 28



 ---- LEMMATIZATION ----

['part', 'Natural', 'Language', 'Processing', 'happens', 'four', 'phase', ':', 'identifying', 'goal', ',', 'planning', 'goal', 'maybe', 'achieved', 'evaluating', 'situation', 'available', 'communicative', 'source', 'realizing', 'plan', 'text', '[', 'Figure', '3', ']', '.']

 TOTAL LEMMATIZE WORDS ==> 28

************************************************************************************************************************

106 --> It is opposite to Understanding. 


 ---- TOKENS ----

 ['It', 'is', 'opposite', 'to', 'Understanding', '.'] 

 TOTAL TOKENS ==> 6

 ---- POST ----

 [('It', 'PRP'), ('is', 'VBZ'), ('opposite', 'JJ'), ('to', 'TO'), ('Understanding', 'NNP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['opposite', 'Understanding', '.']

 TOTAL FILTERED TOKENS ==>  3

 ---- POST FOR FILTERED TOKENS ----

 [('opposite', 'JJ'), ('Understanding', 'NNP'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['opposite Understanding', 'Understanding .'] 

 TOTAL BIGRAMS --> 2 



 ---- TRI-GRAMS ---- 

 ['opposite Understanding .'] 

 TOTAL TRIGRAMS --> 1 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['opposit', 'understand', '.']

 TOTAL PORTER STEM WORDS ==> 3



 ---- SNOWBALL STEMMING ----

['opposit', 'understand', '.']

 TOTAL SNOWBALL STEM WORDS ==> 3



 ---- LEMMATIZATION ----

['opposite', 'Understanding', '.']

 TOTAL LEMMATIZE WORDS ==> 3

************************************************************************************************************************

107 --> Figure 3. 


 ---- TOKENS ----

 ['Figure', '3', '.'] 

 TOTAL TOKENS ==> 3

 ---- POST ----

 [('Figure', 'NN'), ('3', 'CD'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Figure', '3', '.']

 TOTAL FILTERED TOKENS ==>  3

 ---- POST FOR FILTERED TOKENS ----

 [('Figure', 'NN'), ('3', 'CD'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Figure 3', '3 .'] 

 TOTAL BIGRAMS --> 2 



 ---- TRI-GRAMS ---- 

 ['Figure 3 .'] 

 TOTAL TRIGRAMS --> 1 



 ---- NOUN PHRASES ---- 

 ['Figure'] 

 TOTAL NOUN PHRASES --> 1 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['figur', '3', '.']

 TOTAL PORTER STEM WORDS ==> 3



 ---- SNOWBALL STEMMING ----

['figur', '3', '.']

 TOTAL SNOWBALL STEM WORDS ==> 3



 ---- LEMMATIZATION ----

['Figure', '3', '.']

 TOTAL LEMMATIZE WORDS ==> 3

************************************************************************************************************************

108 --> Components of NLG  Components of NLG are as follows:  Speaker and Generator – To generate a text we need to have a speaker or an application  and a generator or a program that renders the application’s intentions into fluent phrase  relevant to the situation. 


 ---- TOKENS ----

 ['Components', 'of', 'NLG', 'Components', 'of', 'NLG', 'are', 'as', 'follows', ':', 'Speaker', 'and', 'Generator', '–', 'To', 'generate', 'a', 'text', 'we', 'need', 'to', 'have', 'a', 'speaker', 'or', 'an', 'application', 'and', 'a', 'generator', 'or', 'a', 'program', 'that', 'renders', 'the', 'application', '’', 's', 'intentions', 'into', 'fluent', 'phrase', 'relevant', 'to', 'the', 'situation', '.'] 

 TOTAL TOKENS ==> 48

 ---- POST ----

 [('Components', 'NNS'), ('of', 'IN'), ('NLG', 'NNP'), ('Components', 'NNP'), ('of', 'IN'), ('NLG', 'NNP'), ('are', 'VBP'), ('as', 'IN'), ('follows', 'VBZ'), (':', ':'), ('Speaker', 'NN'), ('and', 'CC'), ('Generator', 'NNP'), ('–', 'NNP'), ('To', 'TO'), ('generate', 'VB'), ('a', 'DT'), ('text', 'NN'), ('we', 'PRP'), ('need', 'VBP'), ('to', 'TO'), ('have', 'VB'), ('a', 'DT'), ('speaker', 'NN'), ('or', 'CC'), ('an', 'DT'), ('application', 'NN'), ('and', 'CC'), ('a', 'DT'), ('generator', 'NN'), ('or', 'CC'), ('a', 'DT'), ('program', 'NN'), ('that', 'WDT'), ('renders', 'VBZ'), ('the', 'DT'), ('application', 'NN'), ('’', 'NNP'), ('s', 'NN'), ('intentions', 'NNS'), ('into', 'IN'), ('fluent', 'JJ'), ('phrase', 'NN'), ('relevant', 'NN'), ('to', 'TO'), ('the', 'DT'), ('situation', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Components', 'NLG', 'Components', 'NLG', 'follows', ':', 'Speaker', 'Generator', '–', 'generate', 'text', 'need', 'speaker', 'application', 'generator', 'program', 'renders', 'application', '’', 'intentions', 'fluent', 'phrase', 'relevant', 'situation', '.']

 TOTAL FILTERED TOKENS ==>  25

 ---- POST FOR FILTERED TOKENS ----

 [('Components', 'NNS'), ('NLG', 'NNP'), ('Components', 'NNP'), ('NLG', 'NNP'), ('follows', 'VBZ'), (':', ':'), ('Speaker', 'NNP'), ('Generator', 'NNP'), ('–', 'NNP'), ('generate', 'NN'), ('text', 'NN'), ('need', 'NN'), ('speaker', 'NN'), ('application', 'NN'), ('generator', 'NN'), ('program', 'NN'), ('renders', 'NNS'), ('application', 'VBP'), ('’', 'JJ'), ('intentions', 'NNS'), ('fluent', 'JJ'), ('phrase', 'NN'), ('relevant', 'JJ'), ('situation', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Components NLG', 'NLG Components', 'Components NLG', 'NLG follows', 'follows :', ': Speaker', 'Speaker Generator', 'Generator –', '– generate', 'generate text', 'text need', 'need speaker', 'speaker application', 'application generator', 'generator program', 'program renders', 'renders application', 'application ’', '’ intentions', 'intentions fluent', 'fluent phrase', 'phrase relevant', 'relevant situation', 'situation .'] 

 TOTAL BIGRAMS --> 24 



 ---- TRI-GRAMS ---- 

 ['Components NLG Components', 'NLG Components NLG', 'Components NLG follows', 'NLG follows :', 'follows : Speaker', ': Speaker Generator', 'Speaker Generator –', 'Generator – generate', '– generate text', 'generate text need', 'text need speaker', 'need speaker application', 'speaker application generator', 'application generator program', 'generator program renders', 'program renders application', 'renders application ’', 'application ’ intentions', '’ intentions fluent', 'intentions fluent phrase', 'fluent phrase relevant', 'phrase relevant situation', 'relevant situation .'] 

 TOTAL TRIGRAMS --> 23 



 ---- NOUN PHRASES ---- 

 ['generate', 'text', 'need', 'speaker', 'application', 'generator', 'program', 'fluent phrase', 'relevant situation'] 

 TOTAL NOUN PHRASES --> 9 



 ---- NER ----

 
 ORGANIZATION ---> ['NLG Components']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['compon', 'nlg', 'compon', 'nlg', 'follow', ':', 'speaker', 'gener', '–', 'gener', 'text', 'need', 'speaker', 'applic', 'gener', 'program', 'render', 'applic', '’', 'intent', 'fluent', 'phrase', 'relev', 'situat', '.']

 TOTAL PORTER STEM WORDS ==> 25



 ---- SNOWBALL STEMMING ----

['compon', 'nlg', 'compon', 'nlg', 'follow', ':', 'speaker', 'generat', '–', 'generat', 'text', 'need', 'speaker', 'applic', 'generat', 'program', 'render', 'applic', '’', 'intent', 'fluent', 'phrase', 'relev', 'situat', '.']

 TOTAL SNOWBALL STEM WORDS ==> 25



 ---- LEMMATIZATION ----

['Components', 'NLG', 'Components', 'NLG', 'follows', ':', 'Speaker', 'Generator', '–', 'generate', 'text', 'need', 'speaker', 'application', 'generator', 'program', 'render', 'application', '’', 'intention', 'fluent', 'phrase', 'relevant', 'situation', '.']

 TOTAL LEMMATIZE WORDS ==> 25

************************************************************************************************************************

109 --> Components and Levels of Representation -The process of language generation involves  the following interweaved tasks. 


 ---- TOKENS ----

 ['Components', 'and', 'Levels', 'of', 'Representation', '-The', 'process', 'of', 'language', 'generation', 'involves', 'the', 'following', 'interweaved', 'tasks', '.'] 

 TOTAL TOKENS ==> 16

 ---- POST ----

 [('Components', 'NNS'), ('and', 'CC'), ('Levels', 'NNP'), ('of', 'IN'), ('Representation', 'NNP'), ('-The', 'NNP'), ('process', 'NN'), ('of', 'IN'), ('language', 'NN'), ('generation', 'NN'), ('involves', 'VBZ'), ('the', 'DT'), ('following', 'VBG'), ('interweaved', 'JJ'), ('tasks', 'NNS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Components', 'Levels', 'Representation', '-The', 'process', 'language', 'generation', 'involves', 'following', 'interweaved', 'tasks', '.']

 TOTAL FILTERED TOKENS ==>  12

 ---- POST FOR FILTERED TOKENS ----

 [('Components', 'NNS'), ('Levels', 'NNP'), ('Representation', 'NNP'), ('-The', 'NNP'), ('process', 'NN'), ('language', 'NN'), ('generation', 'NN'), ('involves', 'VBZ'), ('following', 'VBG'), ('interweaved', 'JJ'), ('tasks', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Components Levels', 'Levels Representation', 'Representation -The', '-The process', 'process language', 'language generation', 'generation involves', 'involves following', 'following interweaved', 'interweaved tasks', 'tasks .'] 

 TOTAL BIGRAMS --> 11 



 ---- TRI-GRAMS ---- 

 ['Components Levels Representation', 'Levels Representation -The', 'Representation -The process', '-The process language', 'process language generation', 'language generation involves', 'generation involves following', 'involves following interweaved', 'following interweaved tasks', 'interweaved tasks .'] 

 TOTAL TRIGRAMS --> 10 



 ---- NOUN PHRASES ---- 

 ['process', 'language', 'generation'] 

 TOTAL NOUN PHRASES --> 3 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> ['Levels']
 TOTAL PERSON ENTITY --> 1 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['compon', 'level', 'represent', '-the', 'process', 'languag', 'gener', 'involv', 'follow', 'interweav', 'task', '.']

 TOTAL PORTER STEM WORDS ==> 12



 ---- SNOWBALL STEMMING ----

['compon', 'level', 'represent', '-the', 'process', 'languag', 'generat', 'involv', 'follow', 'interweav', 'task', '.']

 TOTAL SNOWBALL STEM WORDS ==> 12



 ---- LEMMATIZATION ----

['Components', 'Levels', 'Representation', '-The', 'process', 'language', 'generation', 'involves', 'following', 'interweaved', 'task', '.']

 TOTAL LEMMATIZE WORDS ==> 12

************************************************************************************************************************

110 --> Content selection: Information should be selected and  included in the set. 


 ---- TOKENS ----

 ['Content', 'selection', ':', 'Information', 'should', 'be', 'selected', 'and', 'included', 'in', 'the', 'set', '.'] 

 TOTAL TOKENS ==> 13

 ---- POST ----

 [('Content', 'JJ'), ('selection', 'NN'), (':', ':'), ('Information', 'NN'), ('should', 'MD'), ('be', 'VB'), ('selected', 'VBN'), ('and', 'CC'), ('included', 'VBN'), ('in', 'IN'), ('the', 'DT'), ('set', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Content', 'selection', ':', 'Information', 'selected', 'included', 'set', '.']

 TOTAL FILTERED TOKENS ==>  8

 ---- POST FOR FILTERED TOKENS ----

 [('Content', 'JJ'), ('selection', 'NN'), (':', ':'), ('Information', 'NN'), ('selected', 'VBD'), ('included', 'JJ'), ('set', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Content selection', 'selection :', ': Information', 'Information selected', 'selected included', 'included set', 'set .'] 

 TOTAL BIGRAMS --> 7 



 ---- TRI-GRAMS ---- 

 ['Content selection :', 'selection : Information', ': Information selected', 'Information selected included', 'selected included set', 'included set .'] 

 TOTAL TRIGRAMS --> 6 



 ---- NOUN PHRASES ---- 

 ['Content selection', 'Information', 'included set'] 

 TOTAL NOUN PHRASES --> 3 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['content', 'select', ':', 'inform', 'select', 'includ', 'set', '.']

 TOTAL PORTER STEM WORDS ==> 8



 ---- SNOWBALL STEMMING ----

['content', 'select', ':', 'inform', 'select', 'includ', 'set', '.']

 TOTAL SNOWBALL STEM WORDS ==> 8



 ---- LEMMATIZATION ----

['Content', 'selection', ':', 'Information', 'selected', 'included', 'set', '.']

 TOTAL LEMMATIZE WORDS ==> 8

************************************************************************************************************************

111 --> Depending on how this information is parsed into representational units,  parts of the units may have to be removed while some others may be added by default. 


 ---- TOKENS ----

 ['Depending', 'on', 'how', 'this', 'information', 'is', 'parsed', 'into', 'representational', 'units', ',', 'parts', 'of', 'the', 'units', 'may', 'have', 'to', 'be', 'removed', 'while', 'some', 'others', 'may', 'be', 'added', 'by', 'default', '.'] 

 TOTAL TOKENS ==> 29

 ---- POST ----

 [('Depending', 'VBG'), ('on', 'IN'), ('how', 'WRB'), ('this', 'DT'), ('information', 'NN'), ('is', 'VBZ'), ('parsed', 'VBN'), ('into', 'IN'), ('representational', 'JJ'), ('units', 'NNS'), (',', ','), ('parts', 'NNS'), ('of', 'IN'), ('the', 'DT'), ('units', 'NNS'), ('may', 'MD'), ('have', 'VB'), ('to', 'TO'), ('be', 'VB'), ('removed', 'VBN'), ('while', 'IN'), ('some', 'DT'), ('others', 'NNS'), ('may', 'MD'), ('be', 'VB'), ('added', 'VBN'), ('by', 'IN'), ('default', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Depending', 'information', 'parsed', 'representational', 'units', ',', 'parts', 'units', 'may', 'removed', 'others', 'may', 'added', 'default', '.']

 TOTAL FILTERED TOKENS ==>  15

 ---- POST FOR FILTERED TOKENS ----

 [('Depending', 'VBG'), ('information', 'NN'), ('parsed', 'VBD'), ('representational', 'JJ'), ('units', 'NNS'), (',', ','), ('parts', 'NNS'), ('units', 'NNS'), ('may', 'MD'), ('removed', 'VB'), ('others', 'NNS'), ('may', 'MD'), ('added', 'VB'), ('default', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Depending information', 'information parsed', 'parsed representational', 'representational units', 'units ,', ', parts', 'parts units', 'units may', 'may removed', 'removed others', 'others may', 'may added', 'added default', 'default .'] 

 TOTAL BIGRAMS --> 14 



 ---- TRI-GRAMS ---- 

 ['Depending information parsed', 'information parsed representational', 'parsed representational units', 'representational units ,', 'units , parts', ', parts units', 'parts units may', 'units may removed', 'may removed others', 'removed others may', 'others may added', 'may added default', 'added default .'] 

 TOTAL TRIGRAMS --> 13 



 ---- NOUN PHRASES ---- 

 ['information', 'default'] 

 TOTAL NOUN PHRASES --> 2 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['depend', 'inform', 'pars', 'represent', 'unit', ',', 'part', 'unit', 'may', 'remov', 'other', 'may', 'ad', 'default', '.']

 TOTAL PORTER STEM WORDS ==> 15



 ---- SNOWBALL STEMMING ----

['depend', 'inform', 'pars', 'represent', 'unit', ',', 'part', 'unit', 'may', 'remov', 'other', 'may', 'ad', 'default', '.']

 TOTAL SNOWBALL STEM WORDS ==> 15



 ---- LEMMATIZATION ----

['Depending', 'information', 'parsed', 'representational', 'unit', ',', 'part', 'unit', 'may', 'removed', 'others', 'may', 'added', 'default', '.']

 TOTAL LEMMATIZE WORDS ==> 15

************************************************************************************************************************

112 --> Textual Organization: The information must be textually organized according the grammar, it  must be ordered both sequentially and in terms of linguistic relations like modifications. 


 ---- TOKENS ----

 ['Textual', 'Organization', ':', 'The', 'information', 'must', 'be', 'textually', 'organized', 'according', 'the', 'grammar', ',', 'it', 'must', 'be', 'ordered', 'both', 'sequentially', 'and', 'in', 'terms', 'of', 'linguistic', 'relations', 'like', 'modifications', '.'] 

 TOTAL TOKENS ==> 28

 ---- POST ----

 [('Textual', 'JJ'), ('Organization', 'NN'), (':', ':'), ('The', 'DT'), ('information', 'NN'), ('must', 'MD'), ('be', 'VB'), ('textually', 'RB'), ('organized', 'VBN'), ('according', 'VBG'), ('the', 'DT'), ('grammar', 'NN'), (',', ','), ('it', 'PRP'), ('must', 'MD'), ('be', 'VB'), ('ordered', 'VBN'), ('both', 'DT'), ('sequentially', 'RB'), ('and', 'CC'), ('in', 'IN'), ('terms', 'NNS'), ('of', 'IN'), ('linguistic', 'JJ'), ('relations', 'NNS'), ('like', 'IN'), ('modifications', 'NNS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Textual', 'Organization', ':', 'information', 'must', 'textually', 'organized', 'according', 'grammar', ',', 'must', 'ordered', 'sequentially', 'terms', 'linguistic', 'relations', 'like', 'modifications', '.']

 TOTAL FILTERED TOKENS ==>  19

 ---- POST FOR FILTERED TOKENS ----

 [('Textual', 'JJ'), ('Organization', 'NNP'), (':', ':'), ('information', 'NN'), ('must', 'MD'), ('textually', 'RB'), ('organized', 'VBN'), ('according', 'VBG'), ('grammar', 'NN'), (',', ','), ('must', 'MD'), ('ordered', 'VBN'), ('sequentially', 'RB'), ('terms', 'NNS'), ('linguistic', 'JJ'), ('relations', 'NNS'), ('like', 'IN'), ('modifications', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Textual Organization', 'Organization :', ': information', 'information must', 'must textually', 'textually organized', 'organized according', 'according grammar', 'grammar ,', ', must', 'must ordered', 'ordered sequentially', 'sequentially terms', 'terms linguistic', 'linguistic relations', 'relations like', 'like modifications', 'modifications .'] 

 TOTAL BIGRAMS --> 18 



 ---- TRI-GRAMS ---- 

 ['Textual Organization :', 'Organization : information', ': information must', 'information must textually', 'must textually organized', 'textually organized according', 'organized according grammar', 'according grammar ,', 'grammar , must', ', must ordered', 'must ordered sequentially', 'ordered sequentially terms', 'sequentially terms linguistic', 'terms linguistic relations', 'linguistic relations like', 'relations like modifications', 'like modifications .'] 

 TOTAL TRIGRAMS --> 17 



 ---- NOUN PHRASES ---- 

 ['information', 'grammar'] 

 TOTAL NOUN PHRASES --> 2 



 ---- NER ----

 
 ORGANIZATION ---> ['Organization']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> ['Textual']
 TOTAL PERSON ENTITY --> 1 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['textual', 'organ', ':', 'inform', 'must', 'textual', 'organ', 'accord', 'grammar', ',', 'must', 'order', 'sequenti', 'term', 'linguist', 'relat', 'like', 'modif', '.']

 TOTAL PORTER STEM WORDS ==> 19



 ---- SNOWBALL STEMMING ----

['textual', 'organ', ':', 'inform', 'must', 'textual', 'organ', 'accord', 'grammar', ',', 'must', 'order', 'sequenti', 'term', 'linguist', 'relat', 'like', 'modif', '.']

 TOTAL SNOWBALL STEM WORDS ==> 19



 ---- LEMMATIZATION ----

['Textual', 'Organization', ':', 'information', 'must', 'textually', 'organized', 'according', 'grammar', ',', 'must', 'ordered', 'sequentially', 'term', 'linguistic', 'relation', 'like', 'modification', '.']

 TOTAL LEMMATIZE WORDS ==> 19

************************************************************************************************************************

113 --> Linguistic Resources: To support the information’s realization, linguistic resources must be   chosen. 


 ---- TOKENS ----

 ['Linguistic', 'Resources', ':', 'To', 'support', 'the', 'information', '’', 's', 'realization', ',', 'linguistic', 'resources', 'must', 'be', 'chosen', '.'] 

 TOTAL TOKENS ==> 17

 ---- POST ----

 [('Linguistic', 'JJ'), ('Resources', 'NNS'), (':', ':'), ('To', 'TO'), ('support', 'VB'), ('the', 'DT'), ('information', 'NN'), ('’', 'NNP'), ('s', 'NN'), ('realization', 'NN'), (',', ','), ('linguistic', 'JJ'), ('resources', 'NNS'), ('must', 'MD'), ('be', 'VB'), ('chosen', 'VBN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Linguistic', 'Resources', ':', 'support', 'information', '’', 'realization', ',', 'linguistic', 'resources', 'must', 'chosen', '.']

 TOTAL FILTERED TOKENS ==>  13

 ---- POST FOR FILTERED TOKENS ----

 [('Linguistic', 'JJ'), ('Resources', 'NNS'), (':', ':'), ('support', 'NN'), ('information', 'NN'), ('’', 'NNP'), ('realization', 'NN'), (',', ','), ('linguistic', 'JJ'), ('resources', 'NNS'), ('must', 'MD'), ('chosen', 'VB'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Linguistic Resources', 'Resources :', ': support', 'support information', 'information ’', '’ realization', 'realization ,', ', linguistic', 'linguistic resources', 'resources must', 'must chosen', 'chosen .'] 

 TOTAL BIGRAMS --> 12 



 ---- TRI-GRAMS ---- 

 ['Linguistic Resources :', 'Resources : support', ': support information', 'support information ’', 'information ’ realization', '’ realization ,', 'realization , linguistic', ', linguistic resources', 'linguistic resources must', 'resources must chosen', 'must chosen .'] 

 TOTAL TRIGRAMS --> 11 



 ---- NOUN PHRASES ---- 

 ['support', 'information', 'realization'] 

 TOTAL NOUN PHRASES --> 3 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> ['Linguistic']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['linguist', 'resourc', ':', 'support', 'inform', '’', 'realiz', ',', 'linguist', 'resourc', 'must', 'chosen', '.']

 TOTAL PORTER STEM WORDS ==> 13



 ---- SNOWBALL STEMMING ----

['linguist', 'resourc', ':', 'support', 'inform', '’', 'realize', ',', 'linguist', 'resourc', 'must', 'chosen', '.']

 TOTAL SNOWBALL STEM WORDS ==> 13



 ---- LEMMATIZATION ----

['Linguistic', 'Resources', ':', 'support', 'information', '’', 'realization', ',', 'linguistic', 'resource', 'must', 'chosen', '.']

 TOTAL LEMMATIZE WORDS ==> 13

************************************************************************************************************************

114 --> In the end these resources will come down to choices of particular words, idioms,  syntactic constructs etc. 


 ---- TOKENS ----

 ['In', 'the', 'end', 'these', 'resources', 'will', 'come', 'down', 'to', 'choices', 'of', 'particular', 'words', ',', 'idioms', ',', 'syntactic', 'constructs', 'etc', '.'] 

 TOTAL TOKENS ==> 20

 ---- POST ----

 [('In', 'IN'), ('the', 'DT'), ('end', 'NN'), ('these', 'DT'), ('resources', 'NNS'), ('will', 'MD'), ('come', 'VB'), ('down', 'RP'), ('to', 'TO'), ('choices', 'NNS'), ('of', 'IN'), ('particular', 'JJ'), ('words', 'NNS'), (',', ','), ('idioms', 'NNS'), (',', ','), ('syntactic', 'JJ'), ('constructs', 'NNS'), ('etc', 'FW'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['end', 'resources', 'come', 'choices', 'particular', 'words', ',', 'idioms', ',', 'syntactic', 'constructs', 'etc', '.']

 TOTAL FILTERED TOKENS ==>  13

 ---- POST FOR FILTERED TOKENS ----

 [('end', 'NN'), ('resources', 'NNS'), ('come', 'VBP'), ('choices', 'NNS'), ('particular', 'JJ'), ('words', 'NNS'), (',', ','), ('idioms', 'NNS'), (',', ','), ('syntactic', 'JJ'), ('constructs', 'NNS'), ('etc', 'FW'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['end resources', 'resources come', 'come choices', 'choices particular', 'particular words', 'words ,', ', idioms', 'idioms ,', ', syntactic', 'syntactic constructs', 'constructs etc', 'etc .'] 

 TOTAL BIGRAMS --> 12 



 ---- TRI-GRAMS ---- 

 ['end resources come', 'resources come choices', 'come choices particular', 'choices particular words', 'particular words ,', 'words , idioms', ', idioms ,', 'idioms , syntactic', ', syntactic constructs', 'syntactic constructs etc', 'constructs etc .'] 

 TOTAL TRIGRAMS --> 11 



 ---- NOUN PHRASES ---- 

 ['end'] 

 TOTAL NOUN PHRASES --> 1 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['end', 'resourc', 'come', 'choic', 'particular', 'word', ',', 'idiom', ',', 'syntact', 'construct', 'etc', '.']

 TOTAL PORTER STEM WORDS ==> 13



 ---- SNOWBALL STEMMING ----

['end', 'resourc', 'come', 'choic', 'particular', 'word', ',', 'idiom', ',', 'syntact', 'construct', 'etc', '.']

 TOTAL SNOWBALL STEM WORDS ==> 13



 ---- LEMMATIZATION ----

['end', 'resource', 'come', 'choice', 'particular', 'word', ',', 'idiom', ',', 'syntactic', 'construct', 'etc', '.']

 TOTAL LEMMATIZE WORDS ==> 13

************************************************************************************************************************

115 --> Realization: The selected and organized resources must be realized  as an actual text or voice output. 


 ---- TOKENS ----

 ['Realization', ':', 'The', 'selected', 'and', 'organized', 'resources', 'must', 'be', 'realized', 'as', 'an', 'actual', 'text', 'or', 'voice', 'output', '.'] 

 TOTAL TOKENS ==> 18

 ---- POST ----

 [('Realization', 'NN'), (':', ':'), ('The', 'DT'), ('selected', 'VBN'), ('and', 'CC'), ('organized', 'VBN'), ('resources', 'NNS'), ('must', 'MD'), ('be', 'VB'), ('realized', 'VBN'), ('as', 'IN'), ('an', 'DT'), ('actual', 'JJ'), ('text', 'NN'), ('or', 'CC'), ('voice', 'NN'), ('output', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Realization', ':', 'selected', 'organized', 'resources', 'must', 'realized', 'actual', 'text', 'voice', 'output', '.']

 TOTAL FILTERED TOKENS ==>  12

 ---- POST FOR FILTERED TOKENS ----

 [('Realization', 'NN'), (':', ':'), ('selected', 'VBN'), ('organized', 'JJ'), ('resources', 'NNS'), ('must', 'MD'), ('realized', 'VB'), ('actual', 'JJ'), ('text', 'NN'), ('voice', 'NN'), ('output', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Realization :', ': selected', 'selected organized', 'organized resources', 'resources must', 'must realized', 'realized actual', 'actual text', 'text voice', 'voice output', 'output .'] 

 TOTAL BIGRAMS --> 11 



 ---- TRI-GRAMS ---- 

 ['Realization : selected', ': selected organized', 'selected organized resources', 'organized resources must', 'resources must realized', 'must realized actual', 'realized actual text', 'actual text voice', 'text voice output', 'voice output .'] 

 TOTAL TRIGRAMS --> 10 



 ---- NOUN PHRASES ---- 

 ['Realization', 'actual text', 'voice', 'output'] 

 TOTAL NOUN PHRASES --> 4 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> ['Realization']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['realiz', ':', 'select', 'organ', 'resourc', 'must', 'realiz', 'actual', 'text', 'voic', 'output', '.']

 TOTAL PORTER STEM WORDS ==> 12



 ---- SNOWBALL STEMMING ----

['realize', ':', 'select', 'organ', 'resourc', 'must', 'realiz', 'actual', 'text', 'voic', 'output', '.']

 TOTAL SNOWBALL STEM WORDS ==> 12



 ---- LEMMATIZATION ----

['Realization', ':', 'selected', 'organized', 'resource', 'must', 'realized', 'actual', 'text', 'voice', 'output', '.']

 TOTAL LEMMATIZE WORDS ==> 12

************************************************************************************************************************

116 --> Application or Speaker –  This is only for maintaining the model of the situation. 


 ---- TOKENS ----

 ['Application', 'or', 'Speaker', '–', 'This', 'is', 'only', 'for', 'maintaining', 'the', 'model', 'of', 'the', 'situation', '.'] 

 TOTAL TOKENS ==> 15

 ---- POST ----

 [('Application', 'NN'), ('or', 'CC'), ('Speaker', 'NNP'), ('–', 'NNP'), ('This', 'DT'), ('is', 'VBZ'), ('only', 'RB'), ('for', 'IN'), ('maintaining', 'VBG'), ('the', 'DT'), ('model', 'NN'), ('of', 'IN'), ('the', 'DT'), ('situation', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Application', 'Speaker', '–', 'maintaining', 'model', 'situation', '.']

 TOTAL FILTERED TOKENS ==>  7

 ---- POST FOR FILTERED TOKENS ----

 [('Application', 'NN'), ('Speaker', 'NNP'), ('–', 'NNP'), ('maintaining', 'VBG'), ('model', 'NN'), ('situation', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Application Speaker', 'Speaker –', '– maintaining', 'maintaining model', 'model situation', 'situation .'] 

 TOTAL BIGRAMS --> 6 



 ---- TRI-GRAMS ---- 

 ['Application Speaker –', 'Speaker – maintaining', '– maintaining model', 'maintaining model situation', 'model situation .'] 

 TOTAL TRIGRAMS --> 5 



 ---- NOUN PHRASES ---- 

 ['Application', 'model', 'situation'] 

 TOTAL NOUN PHRASES --> 3 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> ['Speaker']
 TOTAL PERSON ENTITY --> 1 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['applic', 'speaker', '–', 'maintain', 'model', 'situat', '.']

 TOTAL PORTER STEM WORDS ==> 7



 ---- SNOWBALL STEMMING ----

['applic', 'speaker', '–', 'maintain', 'model', 'situat', '.']

 TOTAL SNOWBALL STEM WORDS ==> 7



 ---- LEMMATIZATION ----

['Application', 'Speaker', '–', 'maintaining', 'model', 'situation', '.']

 TOTAL LEMMATIZE WORDS ==> 7

************************************************************************************************************************

117 --> Here the  speaker just initiates the process doesn’t take part in the language generation. 


 ---- TOKENS ----

 ['Here', 'the', 'speaker', 'just', 'initiates', 'the', 'process', 'doesn', '’', 't', 'take', 'part', 'in', 'the', 'language', 'generation', '.'] 

 TOTAL TOKENS ==> 17

 ---- POST ----

 [('Here', 'RB'), ('the', 'DT'), ('speaker', 'NN'), ('just', 'RB'), ('initiates', 'VBZ'), ('the', 'DT'), ('process', 'NN'), ('doesn', 'NN'), ('’', 'NNP'), ('t', 'NNS'), ('take', 'VBP'), ('part', 'NN'), ('in', 'IN'), ('the', 'DT'), ('language', 'NN'), ('generation', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['speaker', 'initiates', 'process', '’', 'take', 'part', 'language', 'generation', '.']

 TOTAL FILTERED TOKENS ==>  9

 ---- POST FOR FILTERED TOKENS ----

 [('speaker', 'NN'), ('initiates', 'VBZ'), ('process', 'JJ'), ('’', 'NNS'), ('take', 'VBP'), ('part', 'NN'), ('language', 'NN'), ('generation', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['speaker initiates', 'initiates process', 'process ’', '’ take', 'take part', 'part language', 'language generation', 'generation .'] 

 TOTAL BIGRAMS --> 8 



 ---- TRI-GRAMS ---- 

 ['speaker initiates process', 'initiates process ’', 'process ’ take', '’ take part', 'take part language', 'part language generation', 'language generation .'] 

 TOTAL TRIGRAMS --> 7 



 ---- NOUN PHRASES ---- 

 ['speaker', 'part', 'language', 'generation'] 

 TOTAL NOUN PHRASES --> 4 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['speaker', 'initi', 'process', '’', 'take', 'part', 'languag', 'gener', '.']

 TOTAL PORTER STEM WORDS ==> 9



 ---- SNOWBALL STEMMING ----

['speaker', 'initi', 'process', '’', 'take', 'part', 'languag', 'generat', '.']

 TOTAL SNOWBALL STEM WORDS ==> 9



 ---- LEMMATIZATION ----

['speaker', 'initiate', 'process', '’', 'take', 'part', 'language', 'generation', '.']

 TOTAL LEMMATIZE WORDS ==> 9

************************************************************************************************************************

118 --> It stores the  history, structures the content that is potentially relevant and deploys a representation of what  it actually knows. 


 ---- TOKENS ----

 ['It', 'stores', 'the', 'history', ',', 'structures', 'the', 'content', 'that', 'is', 'potentially', 'relevant', 'and', 'deploys', 'a', 'representation', 'of', 'what', 'it', 'actually', 'knows', '.'] 

 TOTAL TOKENS ==> 22

 ---- POST ----

 [('It', 'PRP'), ('stores', 'NNS'), ('the', 'DT'), ('history', 'NN'), (',', ','), ('structures', 'VBZ'), ('the', 'DT'), ('content', 'NN'), ('that', 'WDT'), ('is', 'VBZ'), ('potentially', 'RB'), ('relevant', 'JJ'), ('and', 'CC'), ('deploys', 'VBZ'), ('a', 'DT'), ('representation', 'NN'), ('of', 'IN'), ('what', 'WP'), ('it', 'PRP'), ('actually', 'RB'), ('knows', 'VBZ'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['stores', 'history', ',', 'structures', 'content', 'potentially', 'relevant', 'deploys', 'representation', 'actually', 'knows', '.']

 TOTAL FILTERED TOKENS ==>  12

 ---- POST FOR FILTERED TOKENS ----

 [('stores', 'NNS'), ('history', 'NN'), (',', ','), ('structures', 'VBZ'), ('content', 'JJ'), ('potentially', 'RB'), ('relevant', 'JJ'), ('deploys', 'JJ'), ('representation', 'NN'), ('actually', 'RB'), ('knows', 'VBZ'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['stores history', 'history ,', ', structures', 'structures content', 'content potentially', 'potentially relevant', 'relevant deploys', 'deploys representation', 'representation actually', 'actually knows', 'knows .'] 

 TOTAL BIGRAMS --> 11 



 ---- TRI-GRAMS ---- 

 ['stores history ,', 'history , structures', ', structures content', 'structures content potentially', 'content potentially relevant', 'potentially relevant deploys', 'relevant deploys representation', 'deploys representation actually', 'representation actually knows', 'actually knows .'] 

 TOTAL TRIGRAMS --> 10 



 ---- NOUN PHRASES ---- 

 ['history', 'relevant deploys representation'] 

 TOTAL NOUN PHRASES --> 2 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['store', 'histori', ',', 'structur', 'content', 'potenti', 'relev', 'deploy', 'represent', 'actual', 'know', '.']

 TOTAL PORTER STEM WORDS ==> 12



 ---- SNOWBALL STEMMING ----

['store', 'histori', ',', 'structur', 'content', 'potenti', 'relev', 'deploy', 'represent', 'actual', 'know', '.']

 TOTAL SNOWBALL STEM WORDS ==> 12



 ---- LEMMATIZATION ----

['store', 'history', ',', 'structure', 'content', 'potentially', 'relevant', 'deploys', 'representation', 'actually', 'know', '.']

 TOTAL LEMMATIZE WORDS ==> 12

************************************************************************************************************************

119 --> All these form the situation, while selecting subset of propositions that  speaker has. 


 ---- TOKENS ----

 ['All', 'these', 'form', 'the', 'situation', ',', 'while', 'selecting', 'subset', 'of', 'propositions', 'that', 'speaker', 'has', '.'] 

 TOTAL TOKENS ==> 15

 ---- POST ----

 [('All', 'PDT'), ('these', 'DT'), ('form', 'VB'), ('the', 'DT'), ('situation', 'NN'), (',', ','), ('while', 'IN'), ('selecting', 'VBG'), ('subset', 'NN'), ('of', 'IN'), ('propositions', 'NNS'), ('that', 'WDT'), ('speaker', 'NN'), ('has', 'VBZ'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['form', 'situation', ',', 'selecting', 'subset', 'propositions', 'speaker', '.']

 TOTAL FILTERED TOKENS ==>  8

 ---- POST FOR FILTERED TOKENS ----

 [('form', 'NN'), ('situation', 'NN'), (',', ','), ('selecting', 'VBG'), ('subset', 'NN'), ('propositions', 'NNS'), ('speaker', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['form situation', 'situation ,', ', selecting', 'selecting subset', 'subset propositions', 'propositions speaker', 'speaker .'] 

 TOTAL BIGRAMS --> 7 



 ---- TRI-GRAMS ---- 

 ['form situation ,', 'situation , selecting', ', selecting subset', 'selecting subset propositions', 'subset propositions speaker', 'propositions speaker .'] 

 TOTAL TRIGRAMS --> 6 



 ---- NOUN PHRASES ---- 

 ['form', 'situation', 'subset', 'speaker'] 

 TOTAL NOUN PHRASES --> 4 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['form', 'situat', ',', 'select', 'subset', 'proposit', 'speaker', '.']

 TOTAL PORTER STEM WORDS ==> 8



 ---- SNOWBALL STEMMING ----

['form', 'situat', ',', 'select', 'subset', 'proposit', 'speaker', '.']

 TOTAL SNOWBALL STEM WORDS ==> 8



 ---- LEMMATIZATION ----

['form', 'situation', ',', 'selecting', 'subset', 'proposition', 'speaker', '.']

 TOTAL LEMMATIZE WORDS ==> 8

************************************************************************************************************************

120 --> The only requirement is the speaker has to make sense of the situation. 


 ---- TOKENS ----

 ['The', 'only', 'requirement', 'is', 'the', 'speaker', 'has', 'to', 'make', 'sense', 'of', 'the', 'situation', '.'] 

 TOTAL TOKENS ==> 14

 ---- POST ----

 [('The', 'DT'), ('only', 'JJ'), ('requirement', 'NN'), ('is', 'VBZ'), ('the', 'DT'), ('speaker', 'NN'), ('has', 'VBZ'), ('to', 'TO'), ('make', 'VB'), ('sense', 'NN'), ('of', 'IN'), ('the', 'DT'), ('situation', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['requirement', 'speaker', 'make', 'sense', 'situation', '.']

 TOTAL FILTERED TOKENS ==>  6

 ---- POST FOR FILTERED TOKENS ----

 [('requirement', 'NN'), ('speaker', 'NN'), ('make', 'VBP'), ('sense', 'NN'), ('situation', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['requirement speaker', 'speaker make', 'make sense', 'sense situation', 'situation .'] 

 TOTAL BIGRAMS --> 5 



 ---- TRI-GRAMS ---- 

 ['requirement speaker make', 'speaker make sense', 'make sense situation', 'sense situation .'] 

 TOTAL TRIGRAMS --> 4 



 ---- NOUN PHRASES ---- 

 ['requirement', 'speaker', 'sense', 'situation'] 

 TOTAL NOUN PHRASES --> 4 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['requir', 'speaker', 'make', 'sens', 'situat', '.']

 TOTAL PORTER STEM WORDS ==> 6



 ---- SNOWBALL STEMMING ----

['requir', 'speaker', 'make', 'sens', 'situat', '.']

 TOTAL SNOWBALL STEM WORDS ==> 6



 ---- LEMMATIZATION ----

['requirement', 'speaker', 'make', 'sense', 'situation', '.']

 TOTAL LEMMATIZE WORDS ==> 6

************************************************************************************************************************

121 --> [9]   4. 


 ---- TOKENS ----

 ['[', '9', ']', '4', '.'] 

 TOTAL TOKENS ==> 5

 ---- POST ----

 [('[', 'RB'), ('9', 'CD'), (']', 'JJ'), ('4', 'CD'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['[', '9', ']', '4', '.']

 TOTAL FILTERED TOKENS ==>  5

 ---- POST FOR FILTERED TOKENS ----

 [('[', 'RB'), ('9', 'CD'), (']', 'JJ'), ('4', 'CD'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['[ 9', '9 ]', '] 4', '4 .'] 

 TOTAL BIGRAMS --> 4 



 ---- TRI-GRAMS ---- 

 ['[ 9 ]', '9 ] 4', '] 4 .'] 

 TOTAL TRIGRAMS --> 3 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['[', '9', ']', '4', '.']

 TOTAL PORTER STEM WORDS ==> 5



 ---- SNOWBALL STEMMING ----

['[', '9', ']', '4', '.']

 TOTAL SNOWBALL STEM WORDS ==> 5



 ---- LEMMATIZATION ----

['[', '9', ']', '4', '.']

 TOTAL LEMMATIZE WORDS ==> 5

************************************************************************************************************************

122 --> History of NLP  In late 1940s the term wasn’t even in existence, but the work regarding machine translation  (MT) had started. 


 ---- TOKENS ----

 ['History', 'of', 'NLP', 'In', 'late', '1940s', 'the', 'term', 'wasn', '’', 't', 'even', 'in', 'existence', ',', 'but', 'the', 'work', 'regarding', 'machine', 'translation', '(', 'MT', ')', 'had', 'started', '.'] 

 TOTAL TOKENS ==> 27

 ---- POST ----

 [('History', 'NN'), ('of', 'IN'), ('NLP', 'NNP'), ('In', 'IN'), ('late', 'JJ'), ('1940s', 'CD'), ('the', 'DT'), ('term', 'NN'), ('wasn', 'NN'), ('’', 'NNP'), ('t', 'VBD'), ('even', 'RB'), ('in', 'IN'), ('existence', 'NN'), (',', ','), ('but', 'CC'), ('the', 'DT'), ('work', 'NN'), ('regarding', 'VBG'), ('machine', 'NN'), ('translation', 'NN'), ('(', '('), ('MT', 'NNP'), (')', ')'), ('had', 'VBD'), ('started', 'VBN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['History', 'NLP', 'late', '1940s', 'term', '’', 'even', 'existence', ',', 'work', 'regarding', 'machine', 'translation', '(', 'MT', ')', 'started', '.']

 TOTAL FILTERED TOKENS ==>  18

 ---- POST FOR FILTERED TOKENS ----

 [('History', 'NNP'), ('NLP', 'NNP'), ('late', 'RB'), ('1940s', 'CD'), ('term', 'NN'), ('’', 'NNP'), ('even', 'RB'), ('existence', 'NN'), (',', ','), ('work', 'NN'), ('regarding', 'VBG'), ('machine', 'NN'), ('translation', 'NN'), ('(', '('), ('MT', 'NNP'), (')', ')'), ('started', 'VBD'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['History NLP', 'NLP late', 'late 1940s', '1940s term', 'term ’', '’ even', 'even existence', 'existence ,', ', work', 'work regarding', 'regarding machine', 'machine translation', 'translation (', '( MT', 'MT )', ') started', 'started .'] 

 TOTAL BIGRAMS --> 17 



 ---- TRI-GRAMS ---- 

 ['History NLP late', 'NLP late 1940s', 'late 1940s term', '1940s term ’', 'term ’ even', '’ even existence', 'even existence ,', 'existence , work', ', work regarding', 'work regarding machine', 'regarding machine translation', 'machine translation (', 'translation ( MT', '( MT )', 'MT ) started', ') started .'] 

 TOTAL TRIGRAMS --> 16 



 ---- NOUN PHRASES ---- 

 ['term', 'existence', 'work', 'machine', 'translation'] 

 TOTAL NOUN PHRASES --> 5 



 ---- NER ----

 
 ORGANIZATION ---> ['NLP']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> ['History']
 TOTAL PERSON ENTITY --> 1 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['histori', 'nlp', 'late', '1940', 'term', '’', 'even', 'exist', ',', 'work', 'regard', 'machin', 'translat', '(', 'mt', ')', 'start', '.']

 TOTAL PORTER STEM WORDS ==> 18



 ---- SNOWBALL STEMMING ----

['histori', 'nlp', 'late', '1940s', 'term', '’', 'even', 'exist', ',', 'work', 'regard', 'machin', 'translat', '(', 'mt', ')', 'start', '.']

 TOTAL SNOWBALL STEM WORDS ==> 18



 ---- LEMMATIZATION ----

['History', 'NLP', 'late', '1940s', 'term', '’', 'even', 'existence', ',', 'work', 'regarding', 'machine', 'translation', '(', 'MT', ')', 'started', '.']

 TOTAL LEMMATIZE WORDS ==> 18

************************************************************************************************************************

123 --> Research in this period was not completely localised. 


 ---- TOKENS ----

 ['Research', 'in', 'this', 'period', 'was', 'not', 'completely', 'localised', '.'] 

 TOTAL TOKENS ==> 9

 ---- POST ----

 [('Research', 'NN'), ('in', 'IN'), ('this', 'DT'), ('period', 'NN'), ('was', 'VBD'), ('not', 'RB'), ('completely', 'RB'), ('localised', 'VBN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Research', 'period', 'completely', 'localised', '.']

 TOTAL FILTERED TOKENS ==>  5

 ---- POST FOR FILTERED TOKENS ----

 [('Research', 'NNP'), ('period', 'NN'), ('completely', 'RB'), ('localised', 'VBD'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Research period', 'period completely', 'completely localised', 'localised .'] 

 TOTAL BIGRAMS --> 4 



 ---- TRI-GRAMS ---- 

 ['Research period completely', 'period completely localised', 'completely localised .'] 

 TOTAL TRIGRAMS --> 3 



 ---- NOUN PHRASES ---- 

 ['period'] 

 TOTAL NOUN PHRASES --> 1 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> ['Research']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['research', 'period', 'complet', 'localis', '.']

 TOTAL PORTER STEM WORDS ==> 5



 ---- SNOWBALL STEMMING ----

['research', 'period', 'complet', 'localis', '.']

 TOTAL SNOWBALL STEM WORDS ==> 5



 ---- LEMMATIZATION ----

['Research', 'period', 'completely', 'localised', '.']

 TOTAL LEMMATIZE WORDS ==> 5

************************************************************************************************************************

124 --> Russian and English  were the dominant languages for MT, but others, like Chinese were used for MT (Booth  ,1967) [10]. 


 ---- TOKENS ----

 ['Russian', 'and', 'English', 'were', 'the', 'dominant', 'languages', 'for', 'MT', ',', 'but', 'others', ',', 'like', 'Chinese', 'were', 'used', 'for', 'MT', '(', 'Booth', ',1967', ')', '[', '10', ']', '.'] 

 TOTAL TOKENS ==> 27

 ---- POST ----

 [('Russian', 'JJ'), ('and', 'CC'), ('English', 'JJ'), ('were', 'VBD'), ('the', 'DT'), ('dominant', 'JJ'), ('languages', 'NNS'), ('for', 'IN'), ('MT', 'NNP'), (',', ','), ('but', 'CC'), ('others', 'NNS'), (',', ','), ('like', 'IN'), ('Chinese', 'NNP'), ('were', 'VBD'), ('used', 'VBN'), ('for', 'IN'), ('MT', 'NNP'), ('(', '('), ('Booth', 'NNP'), (',1967', 'NNP'), (')', ')'), ('[', 'VBD'), ('10', 'CD'), (']', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Russian', 'English', 'dominant', 'languages', 'MT', ',', 'others', ',', 'like', 'Chinese', 'used', 'MT', '(', 'Booth', ',1967', ')', '[', '10', ']', '.']

 TOTAL FILTERED TOKENS ==>  20

 ---- POST FOR FILTERED TOKENS ----

 [('Russian', 'JJ'), ('English', 'NNP'), ('dominant', 'NN'), ('languages', 'VBZ'), ('MT', 'NNP'), (',', ','), ('others', 'NNS'), (',', ','), ('like', 'IN'), ('Chinese', 'NNP'), ('used', 'VBD'), ('MT', 'NNP'), ('(', '('), ('Booth', 'NNP'), (',1967', 'NNP'), (')', ')'), ('[', 'VBD'), ('10', 'CD'), (']', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Russian English', 'English dominant', 'dominant languages', 'languages MT', 'MT ,', ', others', 'others ,', ', like', 'like Chinese', 'Chinese used', 'used MT', 'MT (', '( Booth', 'Booth ,1967', ',1967 )', ') [', '[ 10', '10 ]', '] .'] 

 TOTAL BIGRAMS --> 19 



 ---- TRI-GRAMS ---- 

 ['Russian English dominant', 'English dominant languages', 'dominant languages MT', 'languages MT ,', 'MT , others', ', others ,', 'others , like', ', like Chinese', 'like Chinese used', 'Chinese used MT', 'used MT (', 'MT ( Booth', '( Booth ,1967', 'Booth ,1967 )', ',1967 ) [', ') [ 10', '[ 10 ]', '10 ] .'] 

 TOTAL TRIGRAMS --> 18 



 ---- NOUN PHRASES ---- 

 ['dominant', ']'] 

 TOTAL NOUN PHRASES --> 2 



 ---- NER ----

 
 ORGANIZATION ---> ['MT']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> ['Russian', 'English', 'Chinese']
 TOTAL GPE ENTITY --> 3 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['russian', 'english', 'domin', 'languag', 'mt', ',', 'other', ',', 'like', 'chines', 'use', 'mt', '(', 'booth', ',1967', ')', '[', '10', ']', '.']

 TOTAL PORTER STEM WORDS ==> 20



 ---- SNOWBALL STEMMING ----

['russian', 'english', 'domin', 'languag', 'mt', ',', 'other', ',', 'like', 'chines', 'use', 'mt', '(', 'booth', ',1967', ')', '[', '10', ']', '.']

 TOTAL SNOWBALL STEM WORDS ==> 20



 ---- LEMMATIZATION ----

['Russian', 'English', 'dominant', 'language', 'MT', ',', 'others', ',', 'like', 'Chinese', 'used', 'MT', '(', 'Booth', ',1967', ')', '[', '10', ']', '.']

 TOTAL LEMMATIZE WORDS ==> 20

************************************************************************************************************************

125 --> MT/NLP research was almost died in 1966 according to ALPAC report, which  concluded that MT is going nowhere. 


 ---- TOKENS ----

 ['MT/NLP', 'research', 'was', 'almost', 'died', 'in', '1966', 'according', 'to', 'ALPAC', 'report', ',', 'which', 'concluded', 'that', 'MT', 'is', 'going', 'nowhere', '.'] 

 TOTAL TOKENS ==> 20

 ---- POST ----

 [('MT/NLP', 'NNP'), ('research', 'NN'), ('was', 'VBD'), ('almost', 'RB'), ('died', 'VBN'), ('in', 'IN'), ('1966', 'CD'), ('according', 'VBG'), ('to', 'TO'), ('ALPAC', 'NNP'), ('report', 'NN'), (',', ','), ('which', 'WDT'), ('concluded', 'VBD'), ('that', 'IN'), ('MT', 'NNP'), ('is', 'VBZ'), ('going', 'VBG'), ('nowhere', 'RB'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['MT/NLP', 'research', 'almost', 'died', '1966', 'according', 'ALPAC', 'report', ',', 'concluded', 'MT', 'going', 'nowhere', '.']

 TOTAL FILTERED TOKENS ==>  14

 ---- POST FOR FILTERED TOKENS ----

 [('MT/NLP', 'NNP'), ('research', 'NN'), ('almost', 'RB'), ('died', 'VBD'), ('1966', 'CD'), ('according', 'VBG'), ('ALPAC', 'NNP'), ('report', 'NN'), (',', ','), ('concluded', 'VBD'), ('MT', 'NNP'), ('going', 'VBG'), ('nowhere', 'RB'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['MT/NLP research', 'research almost', 'almost died', 'died 1966', '1966 according', 'according ALPAC', 'ALPAC report', 'report ,', ', concluded', 'concluded MT', 'MT going', 'going nowhere', 'nowhere .'] 

 TOTAL BIGRAMS --> 13 



 ---- TRI-GRAMS ---- 

 ['MT/NLP research almost', 'research almost died', 'almost died 1966', 'died 1966 according', '1966 according ALPAC', 'according ALPAC report', 'ALPAC report ,', 'report , concluded', ', concluded MT', 'concluded MT going', 'MT going nowhere', 'going nowhere .'] 

 TOTAL TRIGRAMS --> 12 



 ---- NOUN PHRASES ---- 

 ['research', 'report'] 

 TOTAL NOUN PHRASES --> 2 



 ---- NER ----

 
 ORGANIZATION ---> ['ALPAC']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['mt/nlp', 'research', 'almost', 'die', '1966', 'accord', 'alpac', 'report', ',', 'conclud', 'mt', 'go', 'nowher', '.']

 TOTAL PORTER STEM WORDS ==> 14



 ---- SNOWBALL STEMMING ----

['mt/nlp', 'research', 'almost', 'die', '1966', 'accord', 'alpac', 'report', ',', 'conclud', 'mt', 'go', 'nowher', '.']

 TOTAL SNOWBALL STEM WORDS ==> 14



 ---- LEMMATIZATION ----

['MT/NLP', 'research', 'almost', 'died', '1966', 'according', 'ALPAC', 'report', ',', 'concluded', 'MT', 'going', 'nowhere', '.']

 TOTAL LEMMATIZE WORDS ==> 14

************************************************************************************************************************

126 --> But later on some MT production systems were  providing output to their customers (Hutchins, 1986) [11]. 


 ---- TOKENS ----

 ['But', 'later', 'on', 'some', 'MT', 'production', 'systems', 'were', 'providing', 'output', 'to', 'their', 'customers', '(', 'Hutchins', ',', '1986', ')', '[', '11', ']', '.'] 

 TOTAL TOKENS ==> 22

 ---- POST ----

 [('But', 'CC'), ('later', 'RBR'), ('on', 'IN'), ('some', 'DT'), ('MT', 'NNP'), ('production', 'NN'), ('systems', 'NNS'), ('were', 'VBD'), ('providing', 'VBG'), ('output', 'NN'), ('to', 'TO'), ('their', 'PRP$'), ('customers', 'NNS'), ('(', '('), ('Hutchins', 'NNP'), (',', ','), ('1986', 'CD'), (')', ')'), ('[', 'VBD'), ('11', 'CD'), (']', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['later', 'MT', 'production', 'systems', 'providing', 'output', 'customers', '(', 'Hutchins', ',', '1986', ')', '[', '11', ']', '.']

 TOTAL FILTERED TOKENS ==>  16

 ---- POST FOR FILTERED TOKENS ----

 [('later', 'RB'), ('MT', 'NNP'), ('production', 'NN'), ('systems', 'NNS'), ('providing', 'VBG'), ('output', 'NN'), ('customers', 'NNS'), ('(', '('), ('Hutchins', 'NNP'), (',', ','), ('1986', 'CD'), (')', ')'), ('[', 'VBD'), ('11', 'CD'), (']', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['later MT', 'MT production', 'production systems', 'systems providing', 'providing output', 'output customers', 'customers (', '( Hutchins', 'Hutchins ,', ', 1986', '1986 )', ') [', '[ 11', '11 ]', '] .'] 

 TOTAL BIGRAMS --> 15 



 ---- TRI-GRAMS ---- 

 ['later MT production', 'MT production systems', 'production systems providing', 'systems providing output', 'providing output customers', 'output customers (', 'customers ( Hutchins', '( Hutchins ,', 'Hutchins , 1986', ', 1986 )', '1986 ) [', ') [ 11', '[ 11 ]', '11 ] .'] 

 TOTAL TRIGRAMS --> 14 



 ---- NOUN PHRASES ---- 

 ['production', 'output', ']'] 

 TOTAL NOUN PHRASES --> 3 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['later', 'mt', 'product', 'system', 'provid', 'output', 'custom', '(', 'hutchin', ',', '1986', ')', '[', '11', ']', '.']

 TOTAL PORTER STEM WORDS ==> 16



 ---- SNOWBALL STEMMING ----

['later', 'mt', 'product', 'system', 'provid', 'output', 'custom', '(', 'hutchin', ',', '1986', ')', '[', '11', ']', '.']

 TOTAL SNOWBALL STEM WORDS ==> 16



 ---- LEMMATIZATION ----

['later', 'MT', 'production', 'system', 'providing', 'output', 'customer', '(', 'Hutchins', ',', '1986', ')', '[', '11', ']', '.']

 TOTAL LEMMATIZE WORDS ==> 16

************************************************************************************************************************

127 --> By this time, work on the use of  computers for literary and linguistic studies had also started. 


 ---- TOKENS ----

 ['By', 'this', 'time', ',', 'work', 'on', 'the', 'use', 'of', 'computers', 'for', 'literary', 'and', 'linguistic', 'studies', 'had', 'also', 'started', '.'] 

 TOTAL TOKENS ==> 19

 ---- POST ----

 [('By', 'IN'), ('this', 'DT'), ('time', 'NN'), (',', ','), ('work', 'NN'), ('on', 'IN'), ('the', 'DT'), ('use', 'NN'), ('of', 'IN'), ('computers', 'NNS'), ('for', 'IN'), ('literary', 'JJ'), ('and', 'CC'), ('linguistic', 'JJ'), ('studies', 'NNS'), ('had', 'VBD'), ('also', 'RB'), ('started', 'VBN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['time', ',', 'work', 'use', 'computers', 'literary', 'linguistic', 'studies', 'also', 'started', '.']

 TOTAL FILTERED TOKENS ==>  11

 ---- POST FOR FILTERED TOKENS ----

 [('time', 'NN'), (',', ','), ('work', 'NN'), ('use', 'NN'), ('computers', 'NNS'), ('literary', 'JJ'), ('linguistic', 'JJ'), ('studies', 'NNS'), ('also', 'RB'), ('started', 'VBD'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['time ,', ', work', 'work use', 'use computers', 'computers literary', 'literary linguistic', 'linguistic studies', 'studies also', 'also started', 'started .'] 

 TOTAL BIGRAMS --> 10 



 ---- TRI-GRAMS ---- 

 ['time , work', ', work use', 'work use computers', 'use computers literary', 'computers literary linguistic', 'literary linguistic studies', 'linguistic studies also', 'studies also started', 'also started .'] 

 TOTAL TRIGRAMS --> 9 



 ---- NOUN PHRASES ---- 

 ['time', 'work', 'use'] 

 TOTAL NOUN PHRASES --> 3 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['time', ',', 'work', 'use', 'comput', 'literari', 'linguist', 'studi', 'also', 'start', '.']

 TOTAL PORTER STEM WORDS ==> 11



 ---- SNOWBALL STEMMING ----

['time', ',', 'work', 'use', 'comput', 'literari', 'linguist', 'studi', 'also', 'start', '.']

 TOTAL SNOWBALL STEM WORDS ==> 11



 ---- LEMMATIZATION ----

['time', ',', 'work', 'use', 'computer', 'literary', 'linguistic', 'study', 'also', 'started', '.']

 TOTAL LEMMATIZE WORDS ==> 11

************************************************************************************************************************

128 --> As early as 1960 signature work influenced by AI began, with the BASEBALL Q-A systems  (Green et al., 1961) [12]. 


 ---- TOKENS ----

 ['As', 'early', 'as', '1960', 'signature', 'work', 'influenced', 'by', 'AI', 'began', ',', 'with', 'the', 'BASEBALL', 'Q-A', 'systems', '(', 'Green', 'et', 'al.', ',', '1961', ')', '[', '12', ']', '.'] 

 TOTAL TOKENS ==> 27

 ---- POST ----

 [('As', 'RB'), ('early', 'RB'), ('as', 'IN'), ('1960', 'CD'), ('signature', 'NN'), ('work', 'NN'), ('influenced', 'VBN'), ('by', 'IN'), ('AI', 'NNP'), ('began', 'VBD'), (',', ','), ('with', 'IN'), ('the', 'DT'), ('BASEBALL', 'NNP'), ('Q-A', 'NNP'), ('systems', 'NNS'), ('(', '('), ('Green', 'JJ'), ('et', 'NN'), ('al.', 'NN'), (',', ','), ('1961', 'CD'), (')', ')'), ('[', 'VBD'), ('12', 'CD'), (']', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['early', '1960', 'signature', 'work', 'influenced', 'AI', 'began', ',', 'BASEBALL', 'Q-A', 'systems', '(', 'Green', 'et', 'al.', ',', '1961', ')', '[', '12', ']', '.']

 TOTAL FILTERED TOKENS ==>  22

 ---- POST FOR FILTERED TOKENS ----

 [('early', 'JJ'), ('1960', 'CD'), ('signature', 'NN'), ('work', 'NN'), ('influenced', 'VBD'), ('AI', 'NNP'), ('began', 'VBD'), (',', ','), ('BASEBALL', 'NNP'), ('Q-A', 'NNP'), ('systems', 'NNS'), ('(', '('), ('Green', 'JJ'), ('et', 'NN'), ('al.', 'NN'), (',', ','), ('1961', 'CD'), (')', ')'), ('[', 'VBD'), ('12', 'CD'), (']', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['early 1960', '1960 signature', 'signature work', 'work influenced', 'influenced AI', 'AI began', 'began ,', ', BASEBALL', 'BASEBALL Q-A', 'Q-A systems', 'systems (', '( Green', 'Green et', 'et al.', 'al. ,', ', 1961', '1961 )', ') [', '[ 12', '12 ]', '] .'] 

 TOTAL BIGRAMS --> 21 



 ---- TRI-GRAMS ---- 

 ['early 1960 signature', '1960 signature work', 'signature work influenced', 'work influenced AI', 'influenced AI began', 'AI began ,', 'began , BASEBALL', ', BASEBALL Q-A', 'BASEBALL Q-A systems', 'Q-A systems (', 'systems ( Green', '( Green et', 'Green et al.', 'et al. ,', 'al. , 1961', ', 1961 )', '1961 ) [', ') [ 12', '[ 12 ]', '12 ] .'] 

 TOTAL TRIGRAMS --> 20 



 ---- NOUN PHRASES ---- 

 ['signature', 'work', ']'] 

 TOTAL NOUN PHRASES --> 3 



 ---- NER ----

 
 ORGANIZATION ---> ['AI', 'BASEBALL']
 TOTAL ORGANIZATION ENTITY --> 2 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['earli', '1960', 'signatur', 'work', 'influenc', 'ai', 'began', ',', 'basebal', 'q-a', 'system', '(', 'green', 'et', 'al.', ',', '1961', ')', '[', '12', ']', '.']

 TOTAL PORTER STEM WORDS ==> 22



 ---- SNOWBALL STEMMING ----

['earli', '1960', 'signatur', 'work', 'influenc', 'ai', 'began', ',', 'basebal', 'q-a', 'system', '(', 'green', 'et', 'al.', ',', '1961', ')', '[', '12', ']', '.']

 TOTAL SNOWBALL STEM WORDS ==> 22



 ---- LEMMATIZATION ----

['early', '1960', 'signature', 'work', 'influenced', 'AI', 'began', ',', 'BASEBALL', 'Q-A', 'system', '(', 'Green', 'et', 'al.', ',', '1961', ')', '[', '12', ']', '.']

 TOTAL LEMMATIZE WORDS ==> 22

************************************************************************************************************************

129 --> LUNAR (Woods ,1978) [13] and Winograd SHRDLU were natural  successors of these systems but they were seen as stepped up sophistication, in terms of their  linguistic and their task processing capabilities. 


 ---- TOKENS ----

 ['LUNAR', '(', 'Woods', ',1978', ')', '[', '13', ']', 'and', 'Winograd', 'SHRDLU', 'were', 'natural', 'successors', 'of', 'these', 'systems', 'but', 'they', 'were', 'seen', 'as', 'stepped', 'up', 'sophistication', ',', 'in', 'terms', 'of', 'their', 'linguistic', 'and', 'their', 'task', 'processing', 'capabilities', '.'] 

 TOTAL TOKENS ==> 37

 ---- POST ----

 [('LUNAR', 'NNP'), ('(', '('), ('Woods', 'NNP'), (',1978', 'NNP'), (')', ')'), ('[', 'VBD'), ('13', 'CD'), (']', 'NN'), ('and', 'CC'), ('Winograd', 'NNP'), ('SHRDLU', 'NNP'), ('were', 'VBD'), ('natural', 'JJ'), ('successors', 'NNS'), ('of', 'IN'), ('these', 'DT'), ('systems', 'NNS'), ('but', 'CC'), ('they', 'PRP'), ('were', 'VBD'), ('seen', 'VBN'), ('as', 'IN'), ('stepped', 'VBN'), ('up', 'IN'), ('sophistication', 'NN'), (',', ','), ('in', 'IN'), ('terms', 'NNS'), ('of', 'IN'), ('their', 'PRP$'), ('linguistic', 'JJ'), ('and', 'CC'), ('their', 'PRP$'), ('task', 'NN'), ('processing', 'NN'), ('capabilities', 'NNS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['LUNAR', '(', 'Woods', ',1978', ')', '[', '13', ']', 'Winograd', 'SHRDLU', 'natural', 'successors', 'systems', 'seen', 'stepped', 'sophistication', ',', 'terms', 'linguistic', 'task', 'processing', 'capabilities', '.']

 TOTAL FILTERED TOKENS ==>  23

 ---- POST FOR FILTERED TOKENS ----

 [('LUNAR', 'NNP'), ('(', '('), ('Woods', 'NNP'), (',1978', 'NNP'), (')', ')'), ('[', 'VBD'), ('13', 'CD'), (']', 'NNP'), ('Winograd', 'NNP'), ('SHRDLU', 'NNP'), ('natural', 'JJ'), ('successors', 'NNS'), ('systems', 'NNS'), ('seen', 'VBN'), ('stepped', 'JJ'), ('sophistication', 'NN'), (',', ','), ('terms', 'NNS'), ('linguistic', 'JJ'), ('task', 'NN'), ('processing', 'NN'), ('capabilities', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['LUNAR (', '( Woods', 'Woods ,1978', ',1978 )', ') [', '[ 13', '13 ]', '] Winograd', 'Winograd SHRDLU', 'SHRDLU natural', 'natural successors', 'successors systems', 'systems seen', 'seen stepped', 'stepped sophistication', 'sophistication ,', ', terms', 'terms linguistic', 'linguistic task', 'task processing', 'processing capabilities', 'capabilities .'] 

 TOTAL BIGRAMS --> 22 



 ---- TRI-GRAMS ---- 

 ['LUNAR ( Woods', '( Woods ,1978', 'Woods ,1978 )', ',1978 ) [', ') [ 13', '[ 13 ]', '13 ] Winograd', '] Winograd SHRDLU', 'Winograd SHRDLU natural', 'SHRDLU natural successors', 'natural successors systems', 'successors systems seen', 'systems seen stepped', 'seen stepped sophistication', 'stepped sophistication ,', 'sophistication , terms', ', terms linguistic', 'terms linguistic task', 'linguistic task processing', 'task processing capabilities', 'processing capabilities .'] 

 TOTAL TRIGRAMS --> 21 



 ---- NOUN PHRASES ---- 

 ['stepped sophistication', 'linguistic task', 'processing'] 

 TOTAL NOUN PHRASES --> 3 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> ['LUNAR']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['lunar', '(', 'wood', ',1978', ')', '[', '13', ']', 'winograd', 'shrdlu', 'natur', 'successor', 'system', 'seen', 'step', 'sophist', ',', 'term', 'linguist', 'task', 'process', 'capabl', '.']

 TOTAL PORTER STEM WORDS ==> 23



 ---- SNOWBALL STEMMING ----

['lunar', '(', 'wood', ',1978', ')', '[', '13', ']', 'winograd', 'shrdlu', 'natur', 'successor', 'system', 'seen', 'step', 'sophist', ',', 'term', 'linguist', 'task', 'process', 'capabl', '.']

 TOTAL SNOWBALL STEM WORDS ==> 23



 ---- LEMMATIZATION ----

['LUNAR', '(', 'Woods', ',1978', ')', '[', '13', ']', 'Winograd', 'SHRDLU', 'natural', 'successor', 'system', 'seen', 'stepped', 'sophistication', ',', 'term', 'linguistic', 'task', 'processing', 'capability', '.']

 TOTAL LEMMATIZE WORDS ==> 23

************************************************************************************************************************

130 --> There was a widespread belief that progress  could only be made on the two sides, one is ARPA Speech Understanding Research (SUR)  project (Lea, 1980) and other in some major system developments projects building database  front ends. 


 ---- TOKENS ----

 ['There', 'was', 'a', 'widespread', 'belief', 'that', 'progress', 'could', 'only', 'be', 'made', 'on', 'the', 'two', 'sides', ',', 'one', 'is', 'ARPA', 'Speech', 'Understanding', 'Research', '(', 'SUR', ')', 'project', '(', 'Lea', ',', '1980', ')', 'and', 'other', 'in', 'some', 'major', 'system', 'developments', 'projects', 'building', 'database', 'front', 'ends', '.'] 

 TOTAL TOKENS ==> 44

 ---- POST ----

 [('There', 'EX'), ('was', 'VBD'), ('a', 'DT'), ('widespread', 'JJ'), ('belief', 'NN'), ('that', 'IN'), ('progress', 'NN'), ('could', 'MD'), ('only', 'RB'), ('be', 'VB'), ('made', 'VBN'), ('on', 'IN'), ('the', 'DT'), ('two', 'CD'), ('sides', 'NNS'), (',', ','), ('one', 'CD'), ('is', 'VBZ'), ('ARPA', 'NNP'), ('Speech', 'NNP'), ('Understanding', 'NNP'), ('Research', 'NNP'), ('(', '('), ('SUR', 'NNP'), (')', ')'), ('project', 'NN'), ('(', '('), ('Lea', 'NNP'), (',', ','), ('1980', 'CD'), (')', ')'), ('and', 'CC'), ('other', 'JJ'), ('in', 'IN'), ('some', 'DT'), ('major', 'JJ'), ('system', 'NN'), ('developments', 'NNS'), ('projects', 'NNS'), ('building', 'VBG'), ('database', 'NN'), ('front', 'NN'), ('ends', 'VBZ'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['widespread', 'belief', 'progress', 'could', 'made', 'two', 'sides', ',', 'one', 'ARPA', 'Speech', 'Understanding', 'Research', '(', 'SUR', ')', 'project', '(', 'Lea', ',', '1980', ')', 'major', 'system', 'developments', 'projects', 'building', 'database', 'front', 'ends', '.']

 TOTAL FILTERED TOKENS ==>  31

 ---- POST FOR FILTERED TOKENS ----

 [('widespread', 'JJ'), ('belief', 'NN'), ('progress', 'NN'), ('could', 'MD'), ('made', 'VB'), ('two', 'CD'), ('sides', 'NNS'), (',', ','), ('one', 'CD'), ('ARPA', 'NNP'), ('Speech', 'NNP'), ('Understanding', 'NNP'), ('Research', 'NNP'), ('(', '('), ('SUR', 'NNP'), (')', ')'), ('project', 'NN'), ('(', '('), ('Lea', 'NNP'), (',', ','), ('1980', 'CD'), (')', ')'), ('major', 'JJ'), ('system', 'NN'), ('developments', 'NNS'), ('projects', 'NNS'), ('building', 'VBG'), ('database', 'NN'), ('front', 'NN'), ('ends', 'VBZ'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['widespread belief', 'belief progress', 'progress could', 'could made', 'made two', 'two sides', 'sides ,', ', one', 'one ARPA', 'ARPA Speech', 'Speech Understanding', 'Understanding Research', 'Research (', '( SUR', 'SUR )', ') project', 'project (', '( Lea', 'Lea ,', ', 1980', '1980 )', ') major', 'major system', 'system developments', 'developments projects', 'projects building', 'building database', 'database front', 'front ends', 'ends .'] 

 TOTAL BIGRAMS --> 30 



 ---- TRI-GRAMS ---- 

 ['widespread belief progress', 'belief progress could', 'progress could made', 'could made two', 'made two sides', 'two sides ,', 'sides , one', ', one ARPA', 'one ARPA Speech', 'ARPA Speech Understanding', 'Speech Understanding Research', 'Understanding Research (', 'Research ( SUR', '( SUR )', 'SUR ) project', ') project (', 'project ( Lea', '( Lea ,', 'Lea , 1980', ', 1980 )', '1980 ) major', ') major system', 'major system developments', 'system developments projects', 'developments projects building', 'projects building database', 'building database front', 'database front ends', 'front ends .'] 

 TOTAL TRIGRAMS --> 29 



 ---- NOUN PHRASES ---- 

 ['widespread belief', 'progress', 'project', 'major system', 'database', 'front'] 

 TOTAL NOUN PHRASES --> 6 



 ---- NER ----

 
 ORGANIZATION ---> ['ARPA Speech']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['widespread', 'belief', 'progress', 'could', 'made', 'two', 'side', ',', 'one', 'arpa', 'speech', 'understand', 'research', '(', 'sur', ')', 'project', '(', 'lea', ',', '1980', ')', 'major', 'system', 'develop', 'project', 'build', 'databas', 'front', 'end', '.']

 TOTAL PORTER STEM WORDS ==> 31



 ---- SNOWBALL STEMMING ----

['widespread', 'belief', 'progress', 'could', 'made', 'two', 'side', ',', 'one', 'arpa', 'speech', 'understand', 'research', '(', 'sur', ')', 'project', '(', 'lea', ',', '1980', ')', 'major', 'system', 'develop', 'project', 'build', 'databas', 'front', 'end', '.']

 TOTAL SNOWBALL STEM WORDS ==> 31



 ---- LEMMATIZATION ----

['widespread', 'belief', 'progress', 'could', 'made', 'two', 'side', ',', 'one', 'ARPA', 'Speech', 'Understanding', 'Research', '(', 'SUR', ')', 'project', '(', 'Lea', ',', '1980', ')', 'major', 'system', 'development', 'project', 'building', 'database', 'front', 'end', '.']

 TOTAL LEMMATIZE WORDS ==> 31

************************************************************************************************************************

131 --> The front-end projects (Hendrix et al., 1978) [14] were intended to go beyond  LUNAR in interfacing the large databases. 


 ---- TOKENS ----

 ['The', 'front-end', 'projects', '(', 'Hendrix', 'et', 'al.', ',', '1978', ')', '[', '14', ']', 'were', 'intended', 'to', 'go', 'beyond', 'LUNAR', 'in', 'interfacing', 'the', 'large', 'databases', '.'] 

 TOTAL TOKENS ==> 25

 ---- POST ----

 [('The', 'DT'), ('front-end', 'JJ'), ('projects', 'NNS'), ('(', '('), ('Hendrix', 'NNP'), ('et', 'RB'), ('al.', 'RB'), (',', ','), ('1978', 'CD'), (')', ')'), ('[', 'VBD'), ('14', 'CD'), (']', 'NNS'), ('were', 'VBD'), ('intended', 'VBN'), ('to', 'TO'), ('go', 'VB'), ('beyond', 'IN'), ('LUNAR', 'NNP'), ('in', 'IN'), ('interfacing', 'VBG'), ('the', 'DT'), ('large', 'JJ'), ('databases', 'NNS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['front-end', 'projects', '(', 'Hendrix', 'et', 'al.', ',', '1978', ')', '[', '14', ']', 'intended', 'go', 'beyond', 'LUNAR', 'interfacing', 'large', 'databases', '.']

 TOTAL FILTERED TOKENS ==>  20

 ---- POST FOR FILTERED TOKENS ----

 [('front-end', 'JJ'), ('projects', 'NNS'), ('(', '('), ('Hendrix', 'NNP'), ('et', 'RB'), ('al.', 'RB'), (',', ','), ('1978', 'CD'), (')', ')'), ('[', 'VBD'), ('14', 'CD'), (']', 'NN'), ('intended', 'VBN'), ('go', 'VBP'), ('beyond', 'IN'), ('LUNAR', 'NNP'), ('interfacing', 'VBG'), ('large', 'JJ'), ('databases', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['front-end projects', 'projects (', '( Hendrix', 'Hendrix et', 'et al.', 'al. ,', ', 1978', '1978 )', ') [', '[ 14', '14 ]', '] intended', 'intended go', 'go beyond', 'beyond LUNAR', 'LUNAR interfacing', 'interfacing large', 'large databases', 'databases .'] 

 TOTAL BIGRAMS --> 19 



 ---- TRI-GRAMS ---- 

 ['front-end projects (', 'projects ( Hendrix', '( Hendrix et', 'Hendrix et al.', 'et al. ,', 'al. , 1978', ', 1978 )', '1978 ) [', ') [ 14', '[ 14 ]', '14 ] intended', '] intended go', 'intended go beyond', 'go beyond LUNAR', 'beyond LUNAR interfacing', 'LUNAR interfacing large', 'interfacing large databases', 'large databases .'] 

 TOTAL TRIGRAMS --> 18 



 ---- NOUN PHRASES ---- 

 [']'] 

 TOTAL NOUN PHRASES --> 1 



 ---- NER ----

 
 ORGANIZATION ---> ['LUNAR']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['front-end', 'project', '(', 'hendrix', 'et', 'al.', ',', '1978', ')', '[', '14', ']', 'intend', 'go', 'beyond', 'lunar', 'interfac', 'larg', 'databas', '.']

 TOTAL PORTER STEM WORDS ==> 20



 ---- SNOWBALL STEMMING ----

['front-end', 'project', '(', 'hendrix', 'et', 'al.', ',', '1978', ')', '[', '14', ']', 'intend', 'go', 'beyond', 'lunar', 'interfac', 'larg', 'databas', '.']

 TOTAL SNOWBALL STEM WORDS ==> 20



 ---- LEMMATIZATION ----

['front-end', 'project', '(', 'Hendrix', 'et', 'al.', ',', '1978', ')', '[', '14', ']', 'intended', 'go', 'beyond', 'LUNAR', 'interfacing', 'large', 'database', '.']

 TOTAL LEMMATIZE WORDS ==> 20

************************************************************************************************************************

132 --> In early 1980s computational grammar theory became a very active area of research linked  with logics for meaning and knowledge’s ability to deal with the user’s beliefs and intentions  and with functions like emphasis and themes. 


 ---- TOKENS ----

 ['In', 'early', '1980s', 'computational', 'grammar', 'theory', 'became', 'a', 'very', 'active', 'area', 'of', 'research', 'linked', 'with', 'logics', 'for', 'meaning', 'and', 'knowledge', '’', 's', 'ability', 'to', 'deal', 'with', 'the', 'user', '’', 's', 'beliefs', 'and', 'intentions', 'and', 'with', 'functions', 'like', 'emphasis', 'and', 'themes', '.'] 

 TOTAL TOKENS ==> 41

 ---- POST ----

 [('In', 'IN'), ('early', 'JJ'), ('1980s', 'CD'), ('computational', 'JJ'), ('grammar', 'NN'), ('theory', 'NN'), ('became', 'VBD'), ('a', 'DT'), ('very', 'RB'), ('active', 'JJ'), ('area', 'NN'), ('of', 'IN'), ('research', 'NN'), ('linked', 'VBN'), ('with', 'IN'), ('logics', 'NNS'), ('for', 'IN'), ('meaning', 'NN'), ('and', 'CC'), ('knowledge', 'NN'), ('’', 'NNP'), ('s', 'VBZ'), ('ability', 'NN'), ('to', 'TO'), ('deal', 'VB'), ('with', 'IN'), ('the', 'DT'), ('user', 'NN'), ('’', 'NNP'), ('s', 'NN'), ('beliefs', 'NNS'), ('and', 'CC'), ('intentions', 'NNS'), ('and', 'CC'), ('with', 'IN'), ('functions', 'NNS'), ('like', 'IN'), ('emphasis', 'NN'), ('and', 'CC'), ('themes', 'NNS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['early', '1980s', 'computational', 'grammar', 'theory', 'became', 'active', 'area', 'research', 'linked', 'logics', 'meaning', 'knowledge', '’', 'ability', 'deal', 'user', '’', 'beliefs', 'intentions', 'functions', 'like', 'emphasis', 'themes', '.']

 TOTAL FILTERED TOKENS ==>  25

 ---- POST FOR FILTERED TOKENS ----

 [('early', 'RB'), ('1980s', 'CD'), ('computational', 'JJ'), ('grammar', 'NN'), ('theory', 'NN'), ('became', 'VBD'), ('active', 'JJ'), ('area', 'NN'), ('research', 'NN'), ('linked', 'VBD'), ('logics', 'NNS'), ('meaning', 'VBG'), ('knowledge', 'NN'), ('’', 'NNP'), ('ability', 'NN'), ('deal', 'NN'), ('user', 'NN'), ('’', 'NNP'), ('beliefs', 'NN'), ('intentions', 'NNS'), ('functions', 'NNS'), ('like', 'IN'), ('emphasis', 'NN'), ('themes', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['early 1980s', '1980s computational', 'computational grammar', 'grammar theory', 'theory became', 'became active', 'active area', 'area research', 'research linked', 'linked logics', 'logics meaning', 'meaning knowledge', 'knowledge ’', '’ ability', 'ability deal', 'deal user', 'user ’', '’ beliefs', 'beliefs intentions', 'intentions functions', 'functions like', 'like emphasis', 'emphasis themes', 'themes .'] 

 TOTAL BIGRAMS --> 24 



 ---- TRI-GRAMS ---- 

 ['early 1980s computational', '1980s computational grammar', 'computational grammar theory', 'grammar theory became', 'theory became active', 'became active area', 'active area research', 'area research linked', 'research linked logics', 'linked logics meaning', 'logics meaning knowledge', 'meaning knowledge ’', 'knowledge ’ ability', '’ ability deal', 'ability deal user', 'deal user ’', 'user ’ beliefs', '’ beliefs intentions', 'beliefs intentions functions', 'intentions functions like', 'functions like emphasis', 'like emphasis themes', 'emphasis themes .'] 

 TOTAL TRIGRAMS --> 23 



 ---- NOUN PHRASES ---- 

 ['computational grammar', 'theory', 'active area', 'research', 'knowledge', 'ability', 'deal', 'user', 'beliefs', 'emphasis'] 

 TOTAL NOUN PHRASES --> 10 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['earli', '1980', 'comput', 'grammar', 'theori', 'becam', 'activ', 'area', 'research', 'link', 'logic', 'mean', 'knowledg', '’', 'abil', 'deal', 'user', '’', 'belief', 'intent', 'function', 'like', 'emphasi', 'theme', '.']

 TOTAL PORTER STEM WORDS ==> 25



 ---- SNOWBALL STEMMING ----

['earli', '1980s', 'comput', 'grammar', 'theori', 'becam', 'activ', 'area', 'research', 'link', 'logic', 'mean', 'knowledg', '’', 'abil', 'deal', 'user', '’', 'belief', 'intent', 'function', 'like', 'emphasi', 'theme', '.']

 TOTAL SNOWBALL STEM WORDS ==> 25



 ---- LEMMATIZATION ----

['early', '1980s', 'computational', 'grammar', 'theory', 'became', 'active', 'area', 'research', 'linked', 'logic', 'meaning', 'knowledge', '’', 'ability', 'deal', 'user', '’', 'belief', 'intention', 'function', 'like', 'emphasis', 'theme', '.']

 TOTAL LEMMATIZE WORDS ==> 25

************************************************************************************************************************

133 --> By the end of the decade the powerful general purpose sentence processors like SRI’s Core  Language Engine (Alshawi,1992) [15] and Discourse Representation Theory (Kamp and  Reyle,1993) [16] offered a means of tackling more extended discourse within the  grammatico-logical framework. 


 ---- TOKENS ----

 ['By', 'the', 'end', 'of', 'the', 'decade', 'the', 'powerful', 'general', 'purpose', 'sentence', 'processors', 'like', 'SRI', '’', 's', 'Core', 'Language', 'Engine', '(', 'Alshawi,1992', ')', '[', '15', ']', 'and', 'Discourse', 'Representation', 'Theory', '(', 'Kamp', 'and', 'Reyle,1993', ')', '[', '16', ']', 'offered', 'a', 'means', 'of', 'tackling', 'more', 'extended', 'discourse', 'within', 'the', 'grammatico-logical', 'framework', '.'] 

 TOTAL TOKENS ==> 50

 ---- POST ----

 [('By', 'IN'), ('the', 'DT'), ('end', 'NN'), ('of', 'IN'), ('the', 'DT'), ('decade', 'NN'), ('the', 'DT'), ('powerful', 'JJ'), ('general', 'JJ'), ('purpose', 'NN'), ('sentence', 'NN'), ('processors', 'NNS'), ('like', 'IN'), ('SRI', 'NNP'), ('’', 'NNP'), ('s', 'VBD'), ('Core', 'NNP'), ('Language', 'NNP'), ('Engine', 'NNP'), ('(', '('), ('Alshawi,1992', 'NNP'), (')', ')'), ('[', 'VBD'), ('15', 'CD'), (']', 'NN'), ('and', 'CC'), ('Discourse', 'NNP'), ('Representation', 'NNP'), ('Theory', 'NNP'), ('(', '('), ('Kamp', 'NNP'), ('and', 'CC'), ('Reyle,1993', 'NNP'), (')', ')'), ('[', 'VBD'), ('16', 'CD'), (']', 'NN'), ('offered', 'VBD'), ('a', 'DT'), ('means', 'NNS'), ('of', 'IN'), ('tackling', 'VBG'), ('more', 'RBR'), ('extended', 'JJ'), ('discourse', 'NN'), ('within', 'IN'), ('the', 'DT'), ('grammatico-logical', 'JJ'), ('framework', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['end', 'decade', 'powerful', 'general', 'purpose', 'sentence', 'processors', 'like', 'SRI', '’', 'Core', 'Language', 'Engine', '(', 'Alshawi,1992', ')', '[', '15', ']', 'Discourse', 'Representation', 'Theory', '(', 'Kamp', 'Reyle,1993', ')', '[', '16', ']', 'offered', 'means', 'tackling', 'extended', 'discourse', 'within', 'grammatico-logical', 'framework', '.']

 TOTAL FILTERED TOKENS ==>  38

 ---- POST FOR FILTERED TOKENS ----

 [('end', 'NN'), ('decade', 'NN'), ('powerful', 'JJ'), ('general', 'JJ'), ('purpose', 'NN'), ('sentence', 'NN'), ('processors', 'NNS'), ('like', 'IN'), ('SRI', 'NNP'), ('’', 'NNP'), ('Core', 'NNP'), ('Language', 'NNP'), ('Engine', 'NNP'), ('(', '('), ('Alshawi,1992', 'NNP'), (')', ')'), ('[', 'VBD'), ('15', 'CD'), (']', 'JJ'), ('Discourse', 'NNP'), ('Representation', 'NNP'), ('Theory', 'NNP'), ('(', '('), ('Kamp', 'NNP'), ('Reyle,1993', 'NNP'), (')', ')'), ('[', 'VBD'), ('16', 'CD'), (']', 'NN'), ('offered', 'VBN'), ('means', 'VBZ'), ('tackling', 'VBG'), ('extended', 'JJ'), ('discourse', 'NN'), ('within', 'IN'), ('grammatico-logical', 'JJ'), ('framework', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['end decade', 'decade powerful', 'powerful general', 'general purpose', 'purpose sentence', 'sentence processors', 'processors like', 'like SRI', 'SRI ’', '’ Core', 'Core Language', 'Language Engine', 'Engine (', '( Alshawi,1992', 'Alshawi,1992 )', ') [', '[ 15', '15 ]', '] Discourse', 'Discourse Representation', 'Representation Theory', 'Theory (', '( Kamp', 'Kamp Reyle,1993', 'Reyle,1993 )', ') [', '[ 16', '16 ]', '] offered', 'offered means', 'means tackling', 'tackling extended', 'extended discourse', 'discourse within', 'within grammatico-logical', 'grammatico-logical framework', 'framework .'] 

 TOTAL BIGRAMS --> 37 



 ---- TRI-GRAMS ---- 

 ['end decade powerful', 'decade powerful general', 'powerful general purpose', 'general purpose sentence', 'purpose sentence processors', 'sentence processors like', 'processors like SRI', 'like SRI ’', 'SRI ’ Core', '’ Core Language', 'Core Language Engine', 'Language Engine (', 'Engine ( Alshawi,1992', '( Alshawi,1992 )', 'Alshawi,1992 ) [', ') [ 15', '[ 15 ]', '15 ] Discourse', '] Discourse Representation', 'Discourse Representation Theory', 'Representation Theory (', 'Theory ( Kamp', '( Kamp Reyle,1993', 'Kamp Reyle,1993 )', 'Reyle,1993 ) [', ') [ 16', '[ 16 ]', '16 ] offered', '] offered means', 'offered means tackling', 'means tackling extended', 'tackling extended discourse', 'extended discourse within', 'discourse within grammatico-logical', 'within grammatico-logical framework', 'grammatico-logical framework .'] 

 TOTAL TRIGRAMS --> 36 



 ---- NOUN PHRASES ---- 

 ['end', 'decade', 'powerful general purpose', 'sentence', ']', 'extended discourse', 'grammatico-logical framework'] 

 TOTAL NOUN PHRASES --> 7 



 ---- NER ----

 
 ORGANIZATION ---> ['SRI', 'Discourse']
 TOTAL ORGANIZATION ENTITY --> 2 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['end', 'decad', 'power', 'gener', 'purpos', 'sentenc', 'processor', 'like', 'sri', '’', 'core', 'languag', 'engin', '(', 'alshawi,1992', ')', '[', '15', ']', 'discours', 'represent', 'theori', '(', 'kamp', 'reyle,1993', ')', '[', '16', ']', 'offer', 'mean', 'tackl', 'extend', 'discours', 'within', 'grammatico-log', 'framework', '.']

 TOTAL PORTER STEM WORDS ==> 38



 ---- SNOWBALL STEMMING ----

['end', 'decad', 'power', 'general', 'purpos', 'sentenc', 'processor', 'like', 'sri', '’', 'core', 'languag', 'engin', '(', 'alshawi,1992', ')', '[', '15', ']', 'discours', 'represent', 'theori', '(', 'kamp', 'reyle,1993', ')', '[', '16', ']', 'offer', 'mean', 'tackl', 'extend', 'discours', 'within', 'grammatico-log', 'framework', '.']

 TOTAL SNOWBALL STEM WORDS ==> 38



 ---- LEMMATIZATION ----

['end', 'decade', 'powerful', 'general', 'purpose', 'sentence', 'processor', 'like', 'SRI', '’', 'Core', 'Language', 'Engine', '(', 'Alshawi,1992', ')', '[', '15', ']', 'Discourse', 'Representation', 'Theory', '(', 'Kamp', 'Reyle,1993', ')', '[', '16', ']', 'offered', 'mean', 'tackling', 'extended', 'discourse', 'within', 'grammatico-logical', 'framework', '.']

 TOTAL LEMMATIZE WORDS ==> 38

************************************************************************************************************************

134 --> This period was one of the growing community. 


 ---- TOKENS ----

 ['This', 'period', 'was', 'one', 'of', 'the', 'growing', 'community', '.'] 

 TOTAL TOKENS ==> 9

 ---- POST ----

 [('This', 'DT'), ('period', 'NN'), ('was', 'VBD'), ('one', 'CD'), ('of', 'IN'), ('the', 'DT'), ('growing', 'VBG'), ('community', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['period', 'one', 'growing', 'community', '.']

 TOTAL FILTERED TOKENS ==>  5

 ---- POST FOR FILTERED TOKENS ----

 [('period', 'NN'), ('one', 'CD'), ('growing', 'VBG'), ('community', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['period one', 'one growing', 'growing community', 'community .'] 

 TOTAL BIGRAMS --> 4 



 ---- TRI-GRAMS ---- 

 ['period one growing', 'one growing community', 'growing community .'] 

 TOTAL TRIGRAMS --> 3 



 ---- NOUN PHRASES ---- 

 ['period', 'community'] 

 TOTAL NOUN PHRASES --> 2 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['period', 'one', 'grow', 'commun', '.']

 TOTAL PORTER STEM WORDS ==> 5



 ---- SNOWBALL STEMMING ----

['period', 'one', 'grow', 'communiti', '.']

 TOTAL SNOWBALL STEM WORDS ==> 5



 ---- LEMMATIZATION ----

['period', 'one', 'growing', 'community', '.']

 TOTAL LEMMATIZE WORDS ==> 5

************************************************************************************************************************

135 --> Practical  resources, grammars, and tools and parsers became available (e.g the Alvey Natural  Language Tools (Briscoe et al., 1987) [17]. 


 ---- TOKENS ----

 ['Practical', 'resources', ',', 'grammars', ',', 'and', 'tools', 'and', 'parsers', 'became', 'available', '(', 'e.g', 'the', 'Alvey', 'Natural', 'Language', 'Tools', '(', 'Briscoe', 'et', 'al.', ',', '1987', ')', '[', '17', ']', '.'] 

 TOTAL TOKENS ==> 29

 ---- POST ----

 [('Practical', 'JJ'), ('resources', 'NNS'), (',', ','), ('grammars', 'NNS'), (',', ','), ('and', 'CC'), ('tools', 'NNS'), ('and', 'CC'), ('parsers', 'NNS'), ('became', 'VBD'), ('available', 'JJ'), ('(', '('), ('e.g', 'IN'), ('the', 'DT'), ('Alvey', 'NNP'), ('Natural', 'NNP'), ('Language', 'NNP'), ('Tools', 'NNP'), ('(', '('), ('Briscoe', 'NNP'), ('et', 'RB'), ('al.', 'RB'), (',', ','), ('1987', 'CD'), (')', ')'), ('[', 'VBD'), ('17', 'CD'), (']', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Practical', 'resources', ',', 'grammars', ',', 'tools', 'parsers', 'became', 'available', '(', 'e.g', 'Alvey', 'Natural', 'Language', 'Tools', '(', 'Briscoe', 'et', 'al.', ',', '1987', ')', '[', '17', ']', '.']

 TOTAL FILTERED TOKENS ==>  26

 ---- POST FOR FILTERED TOKENS ----

 [('Practical', 'JJ'), ('resources', 'NNS'), (',', ','), ('grammars', 'NNS'), (',', ','), ('tools', 'NNS'), ('parsers', 'NNS'), ('became', 'VBD'), ('available', 'JJ'), ('(', '('), ('e.g', 'JJ'), ('Alvey', 'NNP'), ('Natural', 'NNP'), ('Language', 'NNP'), ('Tools', 'NNP'), ('(', '('), ('Briscoe', 'NNP'), ('et', 'RB'), ('al.', 'RB'), (',', ','), ('1987', 'CD'), (')', ')'), ('[', 'VBD'), ('17', 'CD'), (']', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Practical resources', 'resources ,', ', grammars', 'grammars ,', ', tools', 'tools parsers', 'parsers became', 'became available', 'available (', '( e.g', 'e.g Alvey', 'Alvey Natural', 'Natural Language', 'Language Tools', 'Tools (', '( Briscoe', 'Briscoe et', 'et al.', 'al. ,', ', 1987', '1987 )', ') [', '[ 17', '17 ]', '] .'] 

 TOTAL BIGRAMS --> 25 



 ---- TRI-GRAMS ---- 

 ['Practical resources ,', 'resources , grammars', ', grammars ,', 'grammars , tools', ', tools parsers', 'tools parsers became', 'parsers became available', 'became available (', 'available ( e.g', '( e.g Alvey', 'e.g Alvey Natural', 'Alvey Natural Language', 'Natural Language Tools', 'Language Tools (', 'Tools ( Briscoe', '( Briscoe et', 'Briscoe et al.', 'et al. ,', 'al. , 1987', ', 1987 )', '1987 ) [', ') [ 17', '[ 17 ]', '17 ] .'] 

 TOTAL TRIGRAMS --> 24 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 []

 ---- PORTER STEMMING ----

['practic', 'resourc', ',', 'grammar', ',', 'tool', 'parser', 'becam', 'avail', '(', 'e.g', 'alvey', 'natur', 'languag', 'tool', '(', 'brisco', 'et', 'al.', ',', '1987', ')', '[', '17', ']', '.']

 TOTAL PORTER STEM WORDS ==> 26



 ---- SNOWBALL STEMMING ----

['practic', 'resourc', ',', 'grammar', ',', 'tool', 'parser', 'becam', 'avail', '(', 'e.g', 'alvey', 'natur', 'languag', 'tool', '(', 'brisco', 'et', 'al.', ',', '1987', ')', '[', '17', ']', '.']

 TOTAL SNOWBALL STEM WORDS ==> 26



 ---- LEMMATIZATION ----

['Practical', 'resource', ',', 'grammar', ',', 'tool', 'parser', 'became', 'available', '(', 'e.g', 'Alvey', 'Natural', 'Language', 'Tools', '(', 'Briscoe', 'et', 'al.', ',', '1987', ')', '[', '17', ']', '.']

 TOTAL LEMMATIZE WORDS ==> 26

************************************************************************************************************************

136 --> The (D)ARPA speech recognition and message  understanding (information extraction) conferences were not only for the tasks they  addressed but for the emphasis on heavy evaluation, starting a trend that became a major  feature in 1990s (Young and Chase, 1998; Sundheim and Chinchor ,1993) [18][19]. 


 ---- TOKENS ----

 ['The', '(', 'D', ')', 'ARPA', 'speech', 'recognition', 'and', 'message', 'understanding', '(', 'information', 'extraction', ')', 'conferences', 'were', 'not', 'only', 'for', 'the', 'tasks', 'they', 'addressed', 'but', 'for', 'the', 'emphasis', 'on', 'heavy', 'evaluation', ',', 'starting', 'a', 'trend', 'that', 'became', 'a', 'major', 'feature', 'in', '1990s', '(', 'Young', 'and', 'Chase', ',', '1998', ';', 'Sundheim', 'and', 'Chinchor', ',1993', ')', '[', '18', ']', '[', '19', ']', '.'] 

 TOTAL TOKENS ==> 60

 ---- POST ----

 [('The', 'DT'), ('(', '('), ('D', 'NNP'), (')', ')'), ('ARPA', 'NNP'), ('speech', 'NN'), ('recognition', 'NN'), ('and', 'CC'), ('message', 'NN'), ('understanding', 'NN'), ('(', '('), ('information', 'NN'), ('extraction', 'NN'), (')', ')'), ('conferences', 'NNS'), ('were', 'VBD'), ('not', 'RB'), ('only', 'RB'), ('for', 'IN'), ('the', 'DT'), ('tasks', 'NNS'), ('they', 'PRP'), ('addressed', 'VBD'), ('but', 'CC'), ('for', 'IN'), ('the', 'DT'), ('emphasis', 'NN'), ('on', 'IN'), ('heavy', 'JJ'), ('evaluation', 'NN'), (',', ','), ('starting', 'VBG'), ('a', 'DT'), ('trend', 'NN'), ('that', 'WDT'), ('became', 'VBD'), ('a', 'DT'), ('major', 'JJ'), ('feature', 'NN'), ('in', 'IN'), ('1990s', 'CD'), ('(', '('), ('Young', 'NNP'), ('and', 'CC'), ('Chase', 'NNP'), (',', ','), ('1998', 'CD'), (';', ':'), ('Sundheim', 'NNP'), ('and', 'CC'), ('Chinchor', 'NNP'), (',1993', 'NNP'), (')', ')'), ('[', 'VBD'), ('18', 'CD'), (']', 'NNP'), ('[', 'VBD'), ('19', 'CD'), (']', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['(', ')', 'ARPA', 'speech', 'recognition', 'message', 'understanding', '(', 'information', 'extraction', ')', 'conferences', 'tasks', 'addressed', 'emphasis', 'heavy', 'evaluation', ',', 'starting', 'trend', 'became', 'major', 'feature', '1990s', '(', 'Young', 'Chase', ',', '1998', ';', 'Sundheim', 'Chinchor', ',1993', ')', '[', '18', ']', '[', '19', ']', '.']

 TOTAL FILTERED TOKENS ==>  41

 ---- POST FOR FILTERED TOKENS ----

 [('(', '('), (')', ')'), ('ARPA', 'NNP'), ('speech', 'NN'), ('recognition', 'NN'), ('message', 'NN'), ('understanding', 'NN'), ('(', '('), ('information', 'NN'), ('extraction', 'NN'), (')', ')'), ('conferences', 'VBZ'), ('tasks', 'NNS'), ('addressed', 'VBD'), ('emphasis', 'NN'), ('heavy', 'JJ'), ('evaluation', 'NN'), (',', ','), ('starting', 'VBG'), ('trend', 'NN'), ('became', 'VBD'), ('major', 'JJ'), ('feature', 'NN'), ('1990s', 'CD'), ('(', '('), ('Young', 'NNP'), ('Chase', 'NNP'), (',', ','), ('1998', 'CD'), (';', ':'), ('Sundheim', 'NNP'), ('Chinchor', 'NNP'), (',1993', 'NNP'), (')', ')'), ('[', 'VBD'), ('18', 'CD'), (']', 'NNP'), ('[', 'VBD'), ('19', 'CD'), (']', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['( )', ') ARPA', 'ARPA speech', 'speech recognition', 'recognition message', 'message understanding', 'understanding (', '( information', 'information extraction', 'extraction )', ') conferences', 'conferences tasks', 'tasks addressed', 'addressed emphasis', 'emphasis heavy', 'heavy evaluation', 'evaluation ,', ', starting', 'starting trend', 'trend became', 'became major', 'major feature', 'feature 1990s', '1990s (', '( Young', 'Young Chase', 'Chase ,', ', 1998', '1998 ;', '; Sundheim', 'Sundheim Chinchor', 'Chinchor ,1993', ',1993 )', ') [', '[ 18', '18 ]', '] [', '[ 19', '19 ]', '] .'] 

 TOTAL BIGRAMS --> 40 



 ---- TRI-GRAMS ---- 

 ['( ) ARPA', ') ARPA speech', 'ARPA speech recognition', 'speech recognition message', 'recognition message understanding', 'message understanding (', 'understanding ( information', '( information extraction', 'information extraction )', 'extraction ) conferences', ') conferences tasks', 'conferences tasks addressed', 'tasks addressed emphasis', 'addressed emphasis heavy', 'emphasis heavy evaluation', 'heavy evaluation ,', 'evaluation , starting', ', starting trend', 'starting trend became', 'trend became major', 'became major feature', 'major feature 1990s', 'feature 1990s (', '1990s ( Young', '( Young Chase', 'Young Chase ,', 'Chase , 1998', ', 1998 ;', '1998 ; Sundheim', '; Sundheim Chinchor', 'Sundheim Chinchor ,1993', 'Chinchor ,1993 )', ',1993 ) [', ') [ 18', '[ 18 ]', '18 ] [', '] [ 19', '[ 19 ]', '19 ] .'] 

 TOTAL TRIGRAMS --> 39 



 ---- NOUN PHRASES ---- 

 ['speech', 'recognition', 'message', 'understanding', 'emphasis', 'heavy evaluation', 'trend', 'major feature', ']'] 

 TOTAL NOUN PHRASES --> 9 



 ---- NER ----

 
 ORGANIZATION ---> ['ARPA']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['(', ')', 'arpa', 'speech', 'recognit', 'messag', 'understand', '(', 'inform', 'extract', ')', 'confer', 'task', 'address', 'emphasi', 'heavi', 'evalu', ',', 'start', 'trend', 'becam', 'major', 'featur', '1990', '(', 'young', 'chase', ',', '1998', ';', 'sundheim', 'chinchor', ',1993', ')', '[', '18', ']', '[', '19', ']', '.']

 TOTAL PORTER STEM WORDS ==> 41



 ---- SNOWBALL STEMMING ----

['(', ')', 'arpa', 'speech', 'recognit', 'messag', 'understand', '(', 'inform', 'extract', ')', 'confer', 'task', 'address', 'emphasi', 'heavi', 'evalu', ',', 'start', 'trend', 'becam', 'major', 'featur', '1990s', '(', 'young', 'chase', ',', '1998', ';', 'sundheim', 'chinchor', ',1993', ')', '[', '18', ']', '[', '19', ']', '.']

 TOTAL SNOWBALL STEM WORDS ==> 41



 ---- LEMMATIZATION ----

['(', ')', 'ARPA', 'speech', 'recognition', 'message', 'understanding', '(', 'information', 'extraction', ')', 'conference', 'task', 'addressed', 'emphasis', 'heavy', 'evaluation', ',', 'starting', 'trend', 'became', 'major', 'feature', '1990s', '(', 'Young', 'Chase', ',', '1998', ';', 'Sundheim', 'Chinchor', ',1993', ')', '[', '18', ']', '[', '19', ']', '.']

 TOTAL LEMMATIZE WORDS ==> 41

************************************************************************************************************************

137 --> Work on  user modelling (Kobsa and Wahlster , 1989) [20]  was one strand in research paper and on  discourse structure serving this (Cohen et al., 1990)  [21]. 


 ---- TOKENS ----

 ['Work', 'on', 'user', 'modelling', '(', 'Kobsa', 'and', 'Wahlster', ',', '1989', ')', '[', '20', ']', 'was', 'one', 'strand', 'in', 'research', 'paper', 'and', 'on', 'discourse', 'structure', 'serving', 'this', '(', 'Cohen', 'et', 'al.', ',', '1990', ')', '[', '21', ']', '.'] 

 TOTAL TOKENS ==> 37

 ---- POST ----

 [('Work', 'NN'), ('on', 'IN'), ('user', 'JJ'), ('modelling', 'NN'), ('(', '('), ('Kobsa', 'NNP'), ('and', 'CC'), ('Wahlster', 'NNP'), (',', ','), ('1989', 'CD'), (')', ')'), ('[', 'VBD'), ('20', 'CD'), (']', 'NN'), ('was', 'VBD'), ('one', 'CD'), ('strand', 'NN'), ('in', 'IN'), ('research', 'NN'), ('paper', 'NN'), ('and', 'CC'), ('on', 'IN'), ('discourse', 'NN'), ('structure', 'NN'), ('serving', 'VBG'), ('this', 'DT'), ('(', '('), ('Cohen', 'NNP'), ('et', 'VBZ'), ('al.', 'RB'), (',', ','), ('1990', 'CD'), (')', ')'), ('[', 'VBD'), ('21', 'CD'), (']', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Work', 'user', 'modelling', '(', 'Kobsa', 'Wahlster', ',', '1989', ')', '[', '20', ']', 'one', 'strand', 'research', 'paper', 'discourse', 'structure', 'serving', '(', 'Cohen', 'et', 'al.', ',', '1990', ')', '[', '21', ']', '.']

 TOTAL FILTERED TOKENS ==>  30

 ---- POST FOR FILTERED TOKENS ----

 [('Work', 'NNP'), ('user', 'RB'), ('modelling', 'VBG'), ('(', '('), ('Kobsa', 'NNP'), ('Wahlster', 'NNP'), (',', ','), ('1989', 'CD'), (')', ')'), ('[', 'VBD'), ('20', 'CD'), (']', 'NNP'), ('one', 'CD'), ('strand', 'NN'), ('research', 'NN'), ('paper', 'NN'), ('discourse', 'NN'), ('structure', 'NN'), ('serving', 'VBG'), ('(', '('), ('Cohen', 'NNP'), ('et', 'VBZ'), ('al.', 'RB'), (',', ','), ('1990', 'CD'), (')', ')'), ('[', 'VBD'), ('21', 'CD'), (']', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Work user', 'user modelling', 'modelling (', '( Kobsa', 'Kobsa Wahlster', 'Wahlster ,', ', 1989', '1989 )', ') [', '[ 20', '20 ]', '] one', 'one strand', 'strand research', 'research paper', 'paper discourse', 'discourse structure', 'structure serving', 'serving (', '( Cohen', 'Cohen et', 'et al.', 'al. ,', ', 1990', '1990 )', ') [', '[ 21', '21 ]', '] .'] 

 TOTAL BIGRAMS --> 29 



 ---- TRI-GRAMS ---- 

 ['Work user modelling', 'user modelling (', 'modelling ( Kobsa', '( Kobsa Wahlster', 'Kobsa Wahlster ,', 'Wahlster , 1989', ', 1989 )', '1989 ) [', ') [ 20', '[ 20 ]', '20 ] one', '] one strand', 'one strand research', 'strand research paper', 'research paper discourse', 'paper discourse structure', 'discourse structure serving', 'structure serving (', 'serving ( Cohen', '( Cohen et', 'Cohen et al.', 'et al. ,', 'al. , 1990', ', 1990 )', '1990 ) [', ') [ 21', '[ 21 ]', '21 ] .'] 

 TOTAL TRIGRAMS --> 28 



 ---- NOUN PHRASES ---- 

 ['strand', 'research', 'paper', 'discourse', 'structure', ']'] 

 TOTAL NOUN PHRASES --> 6 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> ['Work']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['work', 'user', 'model', '(', 'kobsa', 'wahlster', ',', '1989', ')', '[', '20', ']', 'one', 'strand', 'research', 'paper', 'discours', 'structur', 'serv', '(', 'cohen', 'et', 'al.', ',', '1990', ')', '[', '21', ']', '.']

 TOTAL PORTER STEM WORDS ==> 30



 ---- SNOWBALL STEMMING ----

['work', 'user', 'model', '(', 'kobsa', 'wahlster', ',', '1989', ')', '[', '20', ']', 'one', 'strand', 'research', 'paper', 'discours', 'structur', 'serv', '(', 'cohen', 'et', 'al.', ',', '1990', ')', '[', '21', ']', '.']

 TOTAL SNOWBALL STEM WORDS ==> 30



 ---- LEMMATIZATION ----

['Work', 'user', 'modelling', '(', 'Kobsa', 'Wahlster', ',', '1989', ')', '[', '20', ']', 'one', 'strand', 'research', 'paper', 'discourse', 'structure', 'serving', '(', 'Cohen', 'et', 'al.', ',', '1990', ')', '[', '21', ']', '.']

 TOTAL LEMMATIZE WORDS ==> 30

************************************************************************************************************************

138 --> At the same time, as McKeown  (1985) [22] showed, rhetorical schemas could be used for producing both linguistically  coherent and communicatively effective text. 


 ---- TOKENS ----

 ['At', 'the', 'same', 'time', ',', 'as', 'McKeown', '(', '1985', ')', '[', '22', ']', 'showed', ',', 'rhetorical', 'schemas', 'could', 'be', 'used', 'for', 'producing', 'both', 'linguistically', 'coherent', 'and', 'communicatively', 'effective', 'text', '.'] 

 TOTAL TOKENS ==> 30

 ---- POST ----

 [('At', 'IN'), ('the', 'DT'), ('same', 'JJ'), ('time', 'NN'), (',', ','), ('as', 'IN'), ('McKeown', 'NNP'), ('(', '('), ('1985', 'CD'), (')', ')'), ('[', 'VBD'), ('22', 'CD'), (']', 'NN'), ('showed', 'VBD'), (',', ','), ('rhetorical', 'JJ'), ('schemas', 'NN'), ('could', 'MD'), ('be', 'VB'), ('used', 'VBN'), ('for', 'IN'), ('producing', 'VBG'), ('both', 'DT'), ('linguistically', 'RB'), ('coherent', 'NN'), ('and', 'CC'), ('communicatively', 'RB'), ('effective', 'JJ'), ('text', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['time', ',', 'McKeown', '(', '1985', ')', '[', '22', ']', 'showed', ',', 'rhetorical', 'schemas', 'could', 'used', 'producing', 'linguistically', 'coherent', 'communicatively', 'effective', 'text', '.']

 TOTAL FILTERED TOKENS ==>  22

 ---- POST FOR FILTERED TOKENS ----

 [('time', 'NN'), (',', ','), ('McKeown', 'NNP'), ('(', '('), ('1985', 'CD'), (')', ')'), ('[', 'VBD'), ('22', 'CD'), (']', 'NN'), ('showed', 'VBD'), (',', ','), ('rhetorical', 'JJ'), ('schemas', 'NN'), ('could', 'MD'), ('used', 'VBN'), ('producing', 'VBG'), ('linguistically', 'RB'), ('coherent', 'JJ'), ('communicatively', 'RB'), ('effective', 'JJ'), ('text', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['time ,', ', McKeown', 'McKeown (', '( 1985', '1985 )', ') [', '[ 22', '22 ]', '] showed', 'showed ,', ', rhetorical', 'rhetorical schemas', 'schemas could', 'could used', 'used producing', 'producing linguistically', 'linguistically coherent', 'coherent communicatively', 'communicatively effective', 'effective text', 'text .'] 

 TOTAL BIGRAMS --> 21 



 ---- TRI-GRAMS ---- 

 ['time , McKeown', ', McKeown (', 'McKeown ( 1985', '( 1985 )', '1985 ) [', ') [ 22', '[ 22 ]', '22 ] showed', '] showed ,', 'showed , rhetorical', ', rhetorical schemas', 'rhetorical schemas could', 'schemas could used', 'could used producing', 'used producing linguistically', 'producing linguistically coherent', 'linguistically coherent communicatively', 'coherent communicatively effective', 'communicatively effective text', 'effective text .'] 

 TOTAL TRIGRAMS --> 20 



 ---- NOUN PHRASES ---- 

 ['time', ']', 'rhetorical schemas', 'effective text'] 

 TOTAL NOUN PHRASES --> 4 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> ['McKeown']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['time', ',', 'mckeown', '(', '1985', ')', '[', '22', ']', 'show', ',', 'rhetor', 'schema', 'could', 'use', 'produc', 'linguist', 'coher', 'commun', 'effect', 'text', '.']

 TOTAL PORTER STEM WORDS ==> 22



 ---- SNOWBALL STEMMING ----

['time', ',', 'mckeown', '(', '1985', ')', '[', '22', ']', 'show', ',', 'rhetor', 'schema', 'could', 'use', 'produc', 'linguist', 'coher', 'communic', 'effect', 'text', '.']

 TOTAL SNOWBALL STEM WORDS ==> 22



 ---- LEMMATIZATION ----

['time', ',', 'McKeown', '(', '1985', ')', '[', '22', ']', 'showed', ',', 'rhetorical', 'schema', 'could', 'used', 'producing', 'linguistically', 'coherent', 'communicatively', 'effective', 'text', '.']

 TOTAL LEMMATIZE WORDS ==> 22

************************************************************************************************************************

139 --> Some researches in NLP marked important  topics for future like word sense disambiguation (Small et al., 1988) [23] and probabilistic  networks, statistically coloured NLP, the work on the lexicon, also pointed in this direction. 


 ---- TOKENS ----

 ['Some', 'researches', 'in', 'NLP', 'marked', 'important', 'topics', 'for', 'future', 'like', 'word', 'sense', 'disambiguation', '(', 'Small', 'et', 'al.', ',', '1988', ')', '[', '23', ']', 'and', 'probabilistic', 'networks', ',', 'statistically', 'coloured', 'NLP', ',', 'the', 'work', 'on', 'the', 'lexicon', ',', 'also', 'pointed', 'in', 'this', 'direction', '.'] 

 TOTAL TOKENS ==> 43

 ---- POST ----

 [('Some', 'DT'), ('researches', 'NNS'), ('in', 'IN'), ('NLP', 'NNP'), ('marked', 'VBD'), ('important', 'JJ'), ('topics', 'NNS'), ('for', 'IN'), ('future', 'JJ'), ('like', 'IN'), ('word', 'NN'), ('sense', 'NN'), ('disambiguation', 'NN'), ('(', '('), ('Small', 'NNP'), ('et', 'RB'), ('al.', 'RB'), (',', ','), ('1988', 'CD'), (')', ')'), ('[', 'VBD'), ('23', 'CD'), (']', 'NN'), ('and', 'CC'), ('probabilistic', 'JJ'), ('networks', 'NNS'), (',', ','), ('statistically', 'RB'), ('coloured', 'VBN'), ('NLP', 'NNP'), (',', ','), ('the', 'DT'), ('work', 'NN'), ('on', 'IN'), ('the', 'DT'), ('lexicon', 'NN'), (',', ','), ('also', 'RB'), ('pointed', 'VBN'), ('in', 'IN'), ('this', 'DT'), ('direction', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['researches', 'NLP', 'marked', 'important', 'topics', 'future', 'like', 'word', 'sense', 'disambiguation', '(', 'Small', 'et', 'al.', ',', '1988', ')', '[', '23', ']', 'probabilistic', 'networks', ',', 'statistically', 'coloured', 'NLP', ',', 'work', 'lexicon', ',', 'also', 'pointed', 'direction', '.']

 TOTAL FILTERED TOKENS ==>  34

 ---- POST FOR FILTERED TOKENS ----

 [('researches', 'NNS'), ('NLP', 'NNP'), ('marked', 'VBD'), ('important', 'JJ'), ('topics', 'NNS'), ('future', 'VBP'), ('like', 'IN'), ('word', 'NN'), ('sense', 'NN'), ('disambiguation', 'NN'), ('(', '('), ('Small', 'NNP'), ('et', 'RB'), ('al.', 'RB'), (',', ','), ('1988', 'CD'), (')', ')'), ('[', 'VBD'), ('23', 'CD'), (']', 'NNP'), ('probabilistic', 'JJ'), ('networks', 'NNS'), (',', ','), ('statistically', 'RB'), ('coloured', 'VBN'), ('NLP', 'NNP'), (',', ','), ('work', 'NN'), ('lexicon', 'NN'), (',', ','), ('also', 'RB'), ('pointed', 'VBN'), ('direction', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['researches NLP', 'NLP marked', 'marked important', 'important topics', 'topics future', 'future like', 'like word', 'word sense', 'sense disambiguation', 'disambiguation (', '( Small', 'Small et', 'et al.', 'al. ,', ', 1988', '1988 )', ') [', '[ 23', '23 ]', '] probabilistic', 'probabilistic networks', 'networks ,', ', statistically', 'statistically coloured', 'coloured NLP', 'NLP ,', ', work', 'work lexicon', 'lexicon ,', ', also', 'also pointed', 'pointed direction', 'direction .'] 

 TOTAL BIGRAMS --> 33 



 ---- TRI-GRAMS ---- 

 ['researches NLP marked', 'NLP marked important', 'marked important topics', 'important topics future', 'topics future like', 'future like word', 'like word sense', 'word sense disambiguation', 'sense disambiguation (', 'disambiguation ( Small', '( Small et', 'Small et al.', 'et al. ,', 'al. , 1988', ', 1988 )', '1988 ) [', ') [ 23', '[ 23 ]', '23 ] probabilistic', '] probabilistic networks', 'probabilistic networks ,', 'networks , statistically', ', statistically coloured', 'statistically coloured NLP', 'coloured NLP ,', 'NLP , work', ', work lexicon', 'work lexicon ,', 'lexicon , also', ', also pointed', 'also pointed direction', 'pointed direction .'] 

 TOTAL TRIGRAMS --> 32 



 ---- NOUN PHRASES ---- 

 ['word', 'sense', 'disambiguation', 'work', 'lexicon', 'direction'] 

 TOTAL NOUN PHRASES --> 6 



 ---- NER ----

 
 ORGANIZATION ---> ['NLP', 'NLP']
 TOTAL ORGANIZATION ENTITY --> 2 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['research', 'nlp', 'mark', 'import', 'topic', 'futur', 'like', 'word', 'sens', 'disambigu', '(', 'small', 'et', 'al.', ',', '1988', ')', '[', '23', ']', 'probabilist', 'network', ',', 'statist', 'colour', 'nlp', ',', 'work', 'lexicon', ',', 'also', 'point', 'direct', '.']

 TOTAL PORTER STEM WORDS ==> 34



 ---- SNOWBALL STEMMING ----

['research', 'nlp', 'mark', 'import', 'topic', 'futur', 'like', 'word', 'sens', 'disambigu', '(', 'small', 'et', 'al.', ',', '1988', ')', '[', '23', ']', 'probabilist', 'network', ',', 'statist', 'colour', 'nlp', ',', 'work', 'lexicon', ',', 'also', 'point', 'direct', '.']

 TOTAL SNOWBALL STEM WORDS ==> 34



 ---- LEMMATIZATION ----

['research', 'NLP', 'marked', 'important', 'topic', 'future', 'like', 'word', 'sense', 'disambiguation', '(', 'Small', 'et', 'al.', ',', '1988', ')', '[', '23', ']', 'probabilistic', 'network', ',', 'statistically', 'coloured', 'NLP', ',', 'work', 'lexicon', ',', 'also', 'pointed', 'direction', '.']

 TOTAL LEMMATIZE WORDS ==> 34

************************************************************************************************************************

140 --> Statistical language processing was a major thing in 90s (Manning and Schuetze,1999) [24],  because this not only involves data analysts. 


 ---- TOKENS ----

 ['Statistical', 'language', 'processing', 'was', 'a', 'major', 'thing', 'in', '90s', '(', 'Manning', 'and', 'Schuetze,1999', ')', '[', '24', ']', ',', 'because', 'this', 'not', 'only', 'involves', 'data', 'analysts', '.'] 

 TOTAL TOKENS ==> 26

 ---- POST ----

 [('Statistical', 'JJ'), ('language', 'NN'), ('processing', 'NN'), ('was', 'VBD'), ('a', 'DT'), ('major', 'JJ'), ('thing', 'NN'), ('in', 'IN'), ('90s', 'CD'), ('(', '('), ('Manning', 'NNP'), ('and', 'CC'), ('Schuetze,1999', 'NNP'), (')', ')'), ('[', 'VBD'), ('24', 'CD'), (']', 'NN'), (',', ','), ('because', 'IN'), ('this', 'DT'), ('not', 'RB'), ('only', 'RB'), ('involves', 'VBZ'), ('data', 'NNS'), ('analysts', 'NNS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Statistical', 'language', 'processing', 'major', 'thing', '90s', '(', 'Manning', 'Schuetze,1999', ')', '[', '24', ']', ',', 'involves', 'data', 'analysts', '.']

 TOTAL FILTERED TOKENS ==>  18

 ---- POST FOR FILTERED TOKENS ----

 [('Statistical', 'JJ'), ('language', 'NN'), ('processing', 'NN'), ('major', 'JJ'), ('thing', 'NN'), ('90s', 'CD'), ('(', '('), ('Manning', 'NNP'), ('Schuetze,1999', 'NNP'), (')', ')'), ('[', 'VBD'), ('24', 'CD'), (']', 'NN'), (',', ','), ('involves', 'VBZ'), ('data', 'NNS'), ('analysts', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Statistical language', 'language processing', 'processing major', 'major thing', 'thing 90s', '90s (', '( Manning', 'Manning Schuetze,1999', 'Schuetze,1999 )', ') [', '[ 24', '24 ]', '] ,', ', involves', 'involves data', 'data analysts', 'analysts .'] 

 TOTAL BIGRAMS --> 17 



 ---- TRI-GRAMS ---- 

 ['Statistical language processing', 'language processing major', 'processing major thing', 'major thing 90s', 'thing 90s (', '90s ( Manning', '( Manning Schuetze,1999', 'Manning Schuetze,1999 )', 'Schuetze,1999 ) [', ') [ 24', '[ 24 ]', '24 ] ,', '] , involves', ', involves data', 'involves data analysts', 'data analysts .'] 

 TOTAL TRIGRAMS --> 16 



 ---- NOUN PHRASES ---- 

 ['Statistical language', 'processing', 'major thing', ']'] 

 TOTAL NOUN PHRASES --> 4 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['statist', 'languag', 'process', 'major', 'thing', '90', '(', 'man', 'schuetze,1999', ')', '[', '24', ']', ',', 'involv', 'data', 'analyst', '.']

 TOTAL PORTER STEM WORDS ==> 18



 ---- SNOWBALL STEMMING ----

['statist', 'languag', 'process', 'major', 'thing', '90s', '(', 'man', 'schuetze,1999', ')', '[', '24', ']', ',', 'involv', 'data', 'analyst', '.']

 TOTAL SNOWBALL STEM WORDS ==> 18



 ---- LEMMATIZATION ----

['Statistical', 'language', 'processing', 'major', 'thing', '90', '(', 'Manning', 'Schuetze,1999', ')', '[', '24', ']', ',', 'involves', 'data', 'analyst', '.']

 TOTAL LEMMATIZE WORDS ==> 18

************************************************************************************************************************

141 --> Information extraction and automatic  summarising (Mani and Maybury ,1999) [25] was also a point of focus. 


 ---- TOKENS ----

 ['Information', 'extraction', 'and', 'automatic', 'summarising', '(', 'Mani', 'and', 'Maybury', ',1999', ')', '[', '25', ']', 'was', 'also', 'a', 'point', 'of', 'focus', '.'] 

 TOTAL TOKENS ==> 21

 ---- POST ----

 [('Information', 'NN'), ('extraction', 'NN'), ('and', 'CC'), ('automatic', 'JJ'), ('summarising', 'NN'), ('(', '('), ('Mani', 'NNP'), ('and', 'CC'), ('Maybury', 'NNP'), (',1999', 'NNP'), (')', ')'), ('[', 'VBD'), ('25', 'CD'), (']', 'NN'), ('was', 'VBD'), ('also', 'RB'), ('a', 'DT'), ('point', 'NN'), ('of', 'IN'), ('focus', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Information', 'extraction', 'automatic', 'summarising', '(', 'Mani', 'Maybury', ',1999', ')', '[', '25', ']', 'also', 'point', 'focus', '.']

 TOTAL FILTERED TOKENS ==>  16

 ---- POST FOR FILTERED TOKENS ----

 [('Information', 'NN'), ('extraction', 'NN'), ('automatic', 'JJ'), ('summarising', 'NN'), ('(', '('), ('Mani', 'NNP'), ('Maybury', 'NNP'), (',1999', 'NNP'), (')', ')'), ('[', 'VBD'), ('25', 'CD'), (']', 'NNP'), ('also', 'RB'), ('point', 'NN'), ('focus', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Information extraction', 'extraction automatic', 'automatic summarising', 'summarising (', '( Mani', 'Mani Maybury', 'Maybury ,1999', ',1999 )', ') [', '[ 25', '25 ]', '] also', 'also point', 'point focus', 'focus .'] 

 TOTAL BIGRAMS --> 15 



 ---- TRI-GRAMS ---- 

 ['Information extraction automatic', 'extraction automatic summarising', 'automatic summarising (', 'summarising ( Mani', '( Mani Maybury', 'Mani Maybury ,1999', 'Maybury ,1999 )', ',1999 ) [', ') [ 25', '[ 25 ]', '25 ] also', '] also point', 'also point focus', 'point focus .'] 

 TOTAL TRIGRAMS --> 14 



 ---- NOUN PHRASES ---- 

 ['Information', 'extraction', 'automatic summarising', 'point', 'focus'] 

 TOTAL NOUN PHRASES --> 5 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> ['Information']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['inform', 'extract', 'automat', 'summaris', '(', 'mani', 'mayburi', ',1999', ')', '[', '25', ']', 'also', 'point', 'focu', '.']

 TOTAL PORTER STEM WORDS ==> 16



 ---- SNOWBALL STEMMING ----

['inform', 'extract', 'automat', 'summaris', '(', 'mani', 'mayburi', ',1999', ')', '[', '25', ']', 'also', 'point', 'focus', '.']

 TOTAL SNOWBALL STEM WORDS ==> 16



 ---- LEMMATIZATION ----

['Information', 'extraction', 'automatic', 'summarising', '(', 'Mani', 'Maybury', ',1999', ')', '[', '25', ']', 'also', 'point', 'focus', '.']

 TOTAL LEMMATIZE WORDS ==> 16

************************************************************************************************************************

142 --> Recent researches are mainly focused on unsupervised and semi-supervised learning  algorithms. 


 ---- TOKENS ----

 ['Recent', 'researches', 'are', 'mainly', 'focused', 'on', 'unsupervised', 'and', 'semi-supervised', 'learning', 'algorithms', '.'] 

 TOTAL TOKENS ==> 12

 ---- POST ----

 [('Recent', 'JJ'), ('researches', 'NNS'), ('are', 'VBP'), ('mainly', 'RB'), ('focused', 'VBN'), ('on', 'IN'), ('unsupervised', 'JJ'), ('and', 'CC'), ('semi-supervised', 'JJ'), ('learning', 'NN'), ('algorithms', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Recent', 'researches', 'mainly', 'focused', 'unsupervised', 'semi-supervised', 'learning', 'algorithms', '.']

 TOTAL FILTERED TOKENS ==>  9

 ---- POST FOR FILTERED TOKENS ----

 [('Recent', 'JJ'), ('researches', 'NNS'), ('mainly', 'RB'), ('focused', 'VBD'), ('unsupervised', 'JJ'), ('semi-supervised', 'JJ'), ('learning', 'NN'), ('algorithms', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Recent researches', 'researches mainly', 'mainly focused', 'focused unsupervised', 'unsupervised semi-supervised', 'semi-supervised learning', 'learning algorithms', 'algorithms .'] 

 TOTAL BIGRAMS --> 8 



 ---- TRI-GRAMS ---- 

 ['Recent researches mainly', 'researches mainly focused', 'mainly focused unsupervised', 'focused unsupervised semi-supervised', 'unsupervised semi-supervised learning', 'semi-supervised learning algorithms', 'learning algorithms .'] 

 TOTAL TRIGRAMS --> 7 



 ---- NOUN PHRASES ---- 

 ['unsupervised semi-supervised learning', 'algorithms'] 

 TOTAL NOUN PHRASES --> 2 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['recent', 'research', 'mainli', 'focus', 'unsupervis', 'semi-supervis', 'learn', 'algorithm', '.']

 TOTAL PORTER STEM WORDS ==> 9



 ---- SNOWBALL STEMMING ----

['recent', 'research', 'main', 'focus', 'unsupervis', 'semi-supervis', 'learn', 'algorithm', '.']

 TOTAL SNOWBALL STEM WORDS ==> 9



 ---- LEMMATIZATION ----

['Recent', 'research', 'mainly', 'focused', 'unsupervised', 'semi-supervised', 'learning', 'algorithm', '.']

 TOTAL LEMMATIZE WORDS ==> 9

************************************************************************************************************************

143 --> 5. 


 ---- TOKENS ----

 ['5', '.'] 

 TOTAL TOKENS ==> 2

 ---- POST ----

 [('5', 'CD'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['5', '.']

 TOTAL FILTERED TOKENS ==>  2

 ---- POST FOR FILTERED TOKENS ----

 [('5', 'CD'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['5 .'] 

 TOTAL BIGRAMS --> 1 



 ---- TRI-GRAMS ---- 

 [] 

 TOTAL TRIGRAMS --> 0 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['5', '.']

 TOTAL PORTER STEM WORDS ==> 2



 ---- SNOWBALL STEMMING ----

['5', '.']

 TOTAL SNOWBALL STEM WORDS ==> 2



 ---- LEMMATIZATION ----

['5', '.']

 TOTAL LEMMATIZE WORDS ==> 2

************************************************************************************************************************

144 --> Related Work  Many researchers worked on NLP, building tools and systems which makes NLP what it is  today. 


 ---- TOKENS ----

 ['Related', 'Work', 'Many', 'researchers', 'worked', 'on', 'NLP', ',', 'building', 'tools', 'and', 'systems', 'which', 'makes', 'NLP', 'what', 'it', 'is', 'today', '.'] 

 TOTAL TOKENS ==> 20

 ---- POST ----

 [('Related', 'JJ'), ('Work', 'NNP'), ('Many', 'NNP'), ('researchers', 'NNS'), ('worked', 'VBD'), ('on', 'IN'), ('NLP', 'NNP'), (',', ','), ('building', 'NN'), ('tools', 'NNS'), ('and', 'CC'), ('systems', 'NNS'), ('which', 'WDT'), ('makes', 'VBZ'), ('NLP', 'NNP'), ('what', 'WP'), ('it', 'PRP'), ('is', 'VBZ'), ('today', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Related', 'Work', 'Many', 'researchers', 'worked', 'NLP', ',', 'building', 'tools', 'systems', 'makes', 'NLP', 'today', '.']

 TOTAL FILTERED TOKENS ==>  14

 ---- POST FOR FILTERED TOKENS ----

 [('Related', 'JJ'), ('Work', 'NNP'), ('Many', 'NNP'), ('researchers', 'NNS'), ('worked', 'VBD'), ('NLP', 'NNP'), (',', ','), ('building', 'NN'), ('tools', 'NNS'), ('systems', 'NNS'), ('makes', 'VBZ'), ('NLP', 'NNP'), ('today', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Related Work', 'Work Many', 'Many researchers', 'researchers worked', 'worked NLP', 'NLP ,', ', building', 'building tools', 'tools systems', 'systems makes', 'makes NLP', 'NLP today', 'today .'] 

 TOTAL BIGRAMS --> 13 



 ---- TRI-GRAMS ---- 

 ['Related Work Many', 'Work Many researchers', 'Many researchers worked', 'researchers worked NLP', 'worked NLP ,', 'NLP , building', ', building tools', 'building tools systems', 'tools systems makes', 'systems makes NLP', 'makes NLP today', 'NLP today .'] 

 TOTAL TRIGRAMS --> 12 



 ---- NOUN PHRASES ---- 

 ['building', 'today'] 

 TOTAL NOUN PHRASES --> 2 



 ---- NER ----

 
 ORGANIZATION ---> ['NLP', 'NLP']
 TOTAL ORGANIZATION ENTITY --> 2 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['relat', 'work', 'mani', 'research', 'work', 'nlp', ',', 'build', 'tool', 'system', 'make', 'nlp', 'today', '.']

 TOTAL PORTER STEM WORDS ==> 14



 ---- SNOWBALL STEMMING ----

['relat', 'work', 'mani', 'research', 'work', 'nlp', ',', 'build', 'tool', 'system', 'make', 'nlp', 'today', '.']

 TOTAL SNOWBALL STEM WORDS ==> 14



 ---- LEMMATIZATION ----

['Related', 'Work', 'Many', 'researcher', 'worked', 'NLP', ',', 'building', 'tool', 'system', 'make', 'NLP', 'today', '.']

 TOTAL LEMMATIZE WORDS ==> 14

************************************************************************************************************************

145 --> Tools like Sentiment Analyser, Parts of Speech (POS)Taggers, Chunking, Named  Entity Recognitions (NER), Emotion detection, Semantic Role Labelling made NLP a good  topic for research. 


 ---- TOKENS ----

 ['Tools', 'like', 'Sentiment', 'Analyser', ',', 'Parts', 'of', 'Speech', '(', 'POS', ')', 'Taggers', ',', 'Chunking', ',', 'Named', 'Entity', 'Recognitions', '(', 'NER', ')', ',', 'Emotion', 'detection', ',', 'Semantic', 'Role', 'Labelling', 'made', 'NLP', 'a', 'good', 'topic', 'for', 'research', '.'] 

 TOTAL TOKENS ==> 36

 ---- POST ----

 [('Tools', 'NNS'), ('like', 'IN'), ('Sentiment', 'NNP'), ('Analyser', 'NNP'), (',', ','), ('Parts', 'NNP'), ('of', 'IN'), ('Speech', 'NNP'), ('(', '('), ('POS', 'NNP'), (')', ')'), ('Taggers', 'NNPS'), (',', ','), ('Chunking', 'NNP'), (',', ','), ('Named', 'NNP'), ('Entity', 'NNP'), ('Recognitions', 'NNP'), ('(', '('), ('NER', 'NNP'), (')', ')'), (',', ','), ('Emotion', 'NNP'), ('detection', 'NN'), (',', ','), ('Semantic', 'JJ'), ('Role', 'NNP'), ('Labelling', 'NNP'), ('made', 'VBD'), ('NLP', 'NNP'), ('a', 'DT'), ('good', 'JJ'), ('topic', 'NN'), ('for', 'IN'), ('research', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Tools', 'like', 'Sentiment', 'Analyser', ',', 'Parts', 'Speech', '(', 'POS', ')', 'Taggers', ',', 'Chunking', ',', 'Named', 'Entity', 'Recognitions', '(', 'NER', ')', ',', 'Emotion', 'detection', ',', 'Semantic', 'Role', 'Labelling', 'made', 'NLP', 'good', 'topic', 'research', '.']

 TOTAL FILTERED TOKENS ==>  33

 ---- POST FOR FILTERED TOKENS ----

 [('Tools', 'NNS'), ('like', 'IN'), ('Sentiment', 'NNP'), ('Analyser', 'NNP'), (',', ','), ('Parts', 'NNP'), ('Speech', 'NNP'), ('(', '('), ('POS', 'NNP'), (')', ')'), ('Taggers', 'NNPS'), (',', ','), ('Chunking', 'NNP'), (',', ','), ('Named', 'NNP'), ('Entity', 'NNP'), ('Recognitions', 'NNP'), ('(', '('), ('NER', 'NNP'), (')', ')'), (',', ','), ('Emotion', 'NNP'), ('detection', 'NN'), (',', ','), ('Semantic', 'JJ'), ('Role', 'NNP'), ('Labelling', 'NNP'), ('made', 'VBD'), ('NLP', 'NNP'), ('good', 'JJ'), ('topic', 'NN'), ('research', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Tools like', 'like Sentiment', 'Sentiment Analyser', 'Analyser ,', ', Parts', 'Parts Speech', 'Speech (', '( POS', 'POS )', ') Taggers', 'Taggers ,', ', Chunking', 'Chunking ,', ', Named', 'Named Entity', 'Entity Recognitions', 'Recognitions (', '( NER', 'NER )', ') ,', ', Emotion', 'Emotion detection', 'detection ,', ', Semantic', 'Semantic Role', 'Role Labelling', 'Labelling made', 'made NLP', 'NLP good', 'good topic', 'topic research', 'research .'] 

 TOTAL BIGRAMS --> 32 



 ---- TRI-GRAMS ---- 

 ['Tools like Sentiment', 'like Sentiment Analyser', 'Sentiment Analyser ,', 'Analyser , Parts', ', Parts Speech', 'Parts Speech (', 'Speech ( POS', '( POS )', 'POS ) Taggers', ') Taggers ,', 'Taggers , Chunking', ', Chunking ,', 'Chunking , Named', ', Named Entity', 'Named Entity Recognitions', 'Entity Recognitions (', 'Recognitions ( NER', '( NER )', 'NER ) ,', ') , Emotion', ', Emotion detection', 'Emotion detection ,', 'detection , Semantic', ', Semantic Role', 'Semantic Role Labelling', 'Role Labelling made', 'Labelling made NLP', 'made NLP good', 'NLP good topic', 'good topic research', 'topic research .'] 

 TOTAL TRIGRAMS --> 31 



 ---- NOUN PHRASES ---- 

 ['detection', 'good topic', 'research'] 

 TOTAL NOUN PHRASES --> 3 



 ---- NER ----

 
 ORGANIZATION ---> ['Sentiment Analyser', 'Semantic Role', 'NLP']
 TOTAL ORGANIZATION ENTITY --> 3 


 PERSON ---> ['Parts Speech', 'Named Entity Recognitions']
 TOTAL PERSON ENTITY --> 2 


 GPE ---> ['Chunking', 'Emotion']
 TOTAL GPE ENTITY --> 2 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['tool', 'like', 'sentiment', 'analys', ',', 'part', 'speech', '(', 'po', ')', 'tagger', ',', 'chunk', ',', 'name', 'entiti', 'recognit', '(', 'ner', ')', ',', 'emot', 'detect', ',', 'semant', 'role', 'label', 'made', 'nlp', 'good', 'topic', 'research', '.']

 TOTAL PORTER STEM WORDS ==> 33



 ---- SNOWBALL STEMMING ----

['tool', 'like', 'sentiment', 'analys', ',', 'part', 'speech', '(', 'pos', ')', 'tagger', ',', 'chunk', ',', 'name', 'entiti', 'recognit', '(', 'ner', ')', ',', 'emot', 'detect', ',', 'semant', 'role', 'label', 'made', 'nlp', 'good', 'topic', 'research', '.']

 TOTAL SNOWBALL STEM WORDS ==> 33



 ---- LEMMATIZATION ----

['Tools', 'like', 'Sentiment', 'Analyser', ',', 'Parts', 'Speech', '(', 'POS', ')', 'Taggers', ',', 'Chunking', ',', 'Named', 'Entity', 'Recognitions', '(', 'NER', ')', ',', 'Emotion', 'detection', ',', 'Semantic', 'Role', 'Labelling', 'made', 'NLP', 'good', 'topic', 'research', '.']

 TOTAL LEMMATIZE WORDS ==> 33

************************************************************************************************************************

146 --> Sentiment analyser (Jeonghee etal.,2003) [26] works by extracting sentiments about given  topic. 


 ---- TOKENS ----

 ['Sentiment', 'analyser', '(', 'Jeonghee', 'etal.,2003', ')', '[', '26', ']', 'works', 'by', 'extracting', 'sentiments', 'about', 'given', 'topic', '.'] 

 TOTAL TOKENS ==> 17

 ---- POST ----

 [('Sentiment', 'NNP'), ('analyser', 'NN'), ('(', '('), ('Jeonghee', 'NNP'), ('etal.,2003', 'RB'), (')', ')'), ('[', 'VBZ'), ('26', 'CD'), (']', 'NN'), ('works', 'NNS'), ('by', 'IN'), ('extracting', 'VBG'), ('sentiments', 'NNS'), ('about', 'IN'), ('given', 'VBN'), ('topic', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Sentiment', 'analyser', '(', 'Jeonghee', 'etal.,2003', ')', '[', '26', ']', 'works', 'extracting', 'sentiments', 'given', 'topic', '.']

 TOTAL FILTERED TOKENS ==>  15

 ---- POST FOR FILTERED TOKENS ----

 [('Sentiment', 'NNP'), ('analyser', 'NN'), ('(', '('), ('Jeonghee', 'NNP'), ('etal.,2003', 'RB'), (')', ')'), ('[', 'VBZ'), ('26', 'CD'), (']', 'NN'), ('works', 'NNS'), ('extracting', 'VBG'), ('sentiments', 'NNS'), ('given', 'VBN'), ('topic', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Sentiment analyser', 'analyser (', '( Jeonghee', 'Jeonghee etal.,2003', 'etal.,2003 )', ') [', '[ 26', '26 ]', '] works', 'works extracting', 'extracting sentiments', 'sentiments given', 'given topic', 'topic .'] 

 TOTAL BIGRAMS --> 14 



 ---- TRI-GRAMS ---- 

 ['Sentiment analyser (', 'analyser ( Jeonghee', '( Jeonghee etal.,2003', 'Jeonghee etal.,2003 )', 'etal.,2003 ) [', ') [ 26', '[ 26 ]', '26 ] works', '] works extracting', 'works extracting sentiments', 'extracting sentiments given', 'sentiments given topic', 'given topic .'] 

 TOTAL TRIGRAMS --> 13 



 ---- NOUN PHRASES ---- 

 ['analyser', ']'] 

 TOTAL NOUN PHRASES --> 2 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['sentiment', 'analys', '(', 'jeonghe', 'etal.,2003', ')', '[', '26', ']', 'work', 'extract', 'sentiment', 'given', 'topic', '.']

 TOTAL PORTER STEM WORDS ==> 15



 ---- SNOWBALL STEMMING ----

['sentiment', 'analys', '(', 'jeonghe', 'etal.,2003', ')', '[', '26', ']', 'work', 'extract', 'sentiment', 'given', 'topic', '.']

 TOTAL SNOWBALL STEM WORDS ==> 15



 ---- LEMMATIZATION ----

['Sentiment', 'analyser', '(', 'Jeonghee', 'etal.,2003', ')', '[', '26', ']', 'work', 'extracting', 'sentiment', 'given', 'topic', '.']

 TOTAL LEMMATIZE WORDS ==> 15

************************************************************************************************************************

147 --> Sentiment analysis consists of a topic specific feature term extraction, sentiment  extraction, and association by relationship analysis. 


 ---- TOKENS ----

 ['Sentiment', 'analysis', 'consists', 'of', 'a', 'topic', 'specific', 'feature', 'term', 'extraction', ',', 'sentiment', 'extraction', ',', 'and', 'association', 'by', 'relationship', 'analysis', '.'] 

 TOTAL TOKENS ==> 20

 ---- POST ----

 [('Sentiment', 'NN'), ('analysis', 'NN'), ('consists', 'VBZ'), ('of', 'IN'), ('a', 'DT'), ('topic', 'NN'), ('specific', 'JJ'), ('feature', 'JJ'), ('term', 'NN'), ('extraction', 'NN'), (',', ','), ('sentiment', 'NN'), ('extraction', 'NN'), (',', ','), ('and', 'CC'), ('association', 'NN'), ('by', 'IN'), ('relationship', 'NN'), ('analysis', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Sentiment', 'analysis', 'consists', 'topic', 'specific', 'feature', 'term', 'extraction', ',', 'sentiment', 'extraction', ',', 'association', 'relationship', 'analysis', '.']

 TOTAL FILTERED TOKENS ==>  16

 ---- POST FOR FILTERED TOKENS ----

 [('Sentiment', 'NN'), ('analysis', 'NN'), ('consists', 'VBZ'), ('topic', 'NN'), ('specific', 'JJ'), ('feature', 'JJ'), ('term', 'NN'), ('extraction', 'NN'), (',', ','), ('sentiment', 'NN'), ('extraction', 'NN'), (',', ','), ('association', 'NN'), ('relationship', 'NN'), ('analysis', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Sentiment analysis', 'analysis consists', 'consists topic', 'topic specific', 'specific feature', 'feature term', 'term extraction', 'extraction ,', ', sentiment', 'sentiment extraction', 'extraction ,', ', association', 'association relationship', 'relationship analysis', 'analysis .'] 

 TOTAL BIGRAMS --> 15 



 ---- TRI-GRAMS ---- 

 ['Sentiment analysis consists', 'analysis consists topic', 'consists topic specific', 'topic specific feature', 'specific feature term', 'feature term extraction', 'term extraction ,', 'extraction , sentiment', ', sentiment extraction', 'sentiment extraction ,', 'extraction , association', ', association relationship', 'association relationship analysis', 'relationship analysis .'] 

 TOTAL TRIGRAMS --> 14 



 ---- NOUN PHRASES ---- 

 ['Sentiment', 'analysis', 'topic', 'specific feature term', 'extraction', 'sentiment', 'extraction', 'association', 'relationship', 'analysis'] 

 TOTAL NOUN PHRASES --> 10 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['sentiment', 'analysi', 'consist', 'topic', 'specif', 'featur', 'term', 'extract', ',', 'sentiment', 'extract', ',', 'associ', 'relationship', 'analysi', '.']

 TOTAL PORTER STEM WORDS ==> 16



 ---- SNOWBALL STEMMING ----

['sentiment', 'analysi', 'consist', 'topic', 'specif', 'featur', 'term', 'extract', ',', 'sentiment', 'extract', ',', 'associ', 'relationship', 'analysi', '.']

 TOTAL SNOWBALL STEM WORDS ==> 16



 ---- LEMMATIZATION ----

['Sentiment', 'analysis', 'consists', 'topic', 'specific', 'feature', 'term', 'extraction', ',', 'sentiment', 'extraction', ',', 'association', 'relationship', 'analysis', '.']

 TOTAL LEMMATIZE WORDS ==> 16

************************************************************************************************************************

148 --> Sentiment Analysis utilizes two linguistic  resources for the analysis: the sentiment lexicon and the sentiment pattern database. 


 ---- TOKENS ----

 ['Sentiment', 'Analysis', 'utilizes', 'two', 'linguistic', 'resources', 'for', 'the', 'analysis', ':', 'the', 'sentiment', 'lexicon', 'and', 'the', 'sentiment', 'pattern', 'database', '.'] 

 TOTAL TOKENS ==> 19

 ---- POST ----

 [('Sentiment', 'NN'), ('Analysis', 'NN'), ('utilizes', 'JJ'), ('two', 'CD'), ('linguistic', 'JJ'), ('resources', 'NNS'), ('for', 'IN'), ('the', 'DT'), ('analysis', 'NN'), (':', ':'), ('the', 'DT'), ('sentiment', 'NN'), ('lexicon', 'NN'), ('and', 'CC'), ('the', 'DT'), ('sentiment', 'NN'), ('pattern', 'JJ'), ('database', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Sentiment', 'Analysis', 'utilizes', 'two', 'linguistic', 'resources', 'analysis', ':', 'sentiment', 'lexicon', 'sentiment', 'pattern', 'database', '.']

 TOTAL FILTERED TOKENS ==>  14

 ---- POST FOR FILTERED TOKENS ----

 [('Sentiment', 'NN'), ('Analysis', 'NN'), ('utilizes', 'JJ'), ('two', 'CD'), ('linguistic', 'JJ'), ('resources', 'NNS'), ('analysis', 'NN'), (':', ':'), ('sentiment', 'NN'), ('lexicon', 'NN'), ('sentiment', 'NN'), ('pattern', 'JJ'), ('database', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Sentiment Analysis', 'Analysis utilizes', 'utilizes two', 'two linguistic', 'linguistic resources', 'resources analysis', 'analysis :', ': sentiment', 'sentiment lexicon', 'lexicon sentiment', 'sentiment pattern', 'pattern database', 'database .'] 

 TOTAL BIGRAMS --> 13 



 ---- TRI-GRAMS ---- 

 ['Sentiment Analysis utilizes', 'Analysis utilizes two', 'utilizes two linguistic', 'two linguistic resources', 'linguistic resources analysis', 'resources analysis :', 'analysis : sentiment', ': sentiment lexicon', 'sentiment lexicon sentiment', 'lexicon sentiment pattern', 'sentiment pattern database', 'pattern database .'] 

 TOTAL TRIGRAMS --> 12 



 ---- NOUN PHRASES ---- 

 ['Sentiment', 'Analysis', 'analysis', 'sentiment', 'lexicon', 'sentiment', 'pattern database'] 

 TOTAL NOUN PHRASES --> 7 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['sentiment', 'analysi', 'util', 'two', 'linguist', 'resourc', 'analysi', ':', 'sentiment', 'lexicon', 'sentiment', 'pattern', 'databas', '.']

 TOTAL PORTER STEM WORDS ==> 14



 ---- SNOWBALL STEMMING ----

['sentiment', 'analysi', 'util', 'two', 'linguist', 'resourc', 'analysi', ':', 'sentiment', 'lexicon', 'sentiment', 'pattern', 'databas', '.']

 TOTAL SNOWBALL STEM WORDS ==> 14



 ---- LEMMATIZATION ----

['Sentiment', 'Analysis', 'utilizes', 'two', 'linguistic', 'resource', 'analysis', ':', 'sentiment', 'lexicon', 'sentiment', 'pattern', 'database', '.']

 TOTAL LEMMATIZE WORDS ==> 14

************************************************************************************************************************

149 --> It  analyses the documents for positive and negative words and try to give ratings on scale -5 to  +5. 


 ---- TOKENS ----

 ['It', 'analyses', 'the', 'documents', 'for', 'positive', 'and', 'negative', 'words', 'and', 'try', 'to', 'give', 'ratings', 'on', 'scale', '-5', 'to', '+5', '.'] 

 TOTAL TOKENS ==> 20

 ---- POST ----

 [('It', 'PRP'), ('analyses', 'VBZ'), ('the', 'DT'), ('documents', 'NNS'), ('for', 'IN'), ('positive', 'JJ'), ('and', 'CC'), ('negative', 'JJ'), ('words', 'NNS'), ('and', 'CC'), ('try', 'VB'), ('to', 'TO'), ('give', 'VB'), ('ratings', 'NNS'), ('on', 'IN'), ('scale', 'NN'), ('-5', 'NN'), ('to', 'TO'), ('+5', 'VB'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['analyses', 'documents', 'positive', 'negative', 'words', 'try', 'give', 'ratings', 'scale', '-5', '+5', '.']

 TOTAL FILTERED TOKENS ==>  12

 ---- POST FOR FILTERED TOKENS ----

 [('analyses', 'NNS'), ('documents', 'NNS'), ('positive', 'JJ'), ('negative', 'JJ'), ('words', 'NNS'), ('try', 'VBP'), ('give', 'JJ'), ('ratings', 'NNS'), ('scale', 'NN'), ('-5', 'NN'), ('+5', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['analyses documents', 'documents positive', 'positive negative', 'negative words', 'words try', 'try give', 'give ratings', 'ratings scale', 'scale -5', '-5 +5', '+5 .'] 

 TOTAL BIGRAMS --> 11 



 ---- TRI-GRAMS ---- 

 ['analyses documents positive', 'documents positive negative', 'positive negative words', 'negative words try', 'words try give', 'try give ratings', 'give ratings scale', 'ratings scale -5', 'scale -5 +5', '-5 +5 .'] 

 TOTAL TRIGRAMS --> 10 



 ---- NOUN PHRASES ---- 

 ['scale', '-5', '+5'] 

 TOTAL NOUN PHRASES --> 3 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['analys', 'document', 'posit', 'neg', 'word', 'tri', 'give', 'rate', 'scale', '-5', '+5', '.']

 TOTAL PORTER STEM WORDS ==> 12



 ---- SNOWBALL STEMMING ----

['analys', 'document', 'posit', 'negat', 'word', 'tri', 'give', 'rate', 'scale', '-5', '+5', '.']

 TOTAL SNOWBALL STEM WORDS ==> 12



 ---- LEMMATIZATION ----

['analysis', 'document', 'positive', 'negative', 'word', 'try', 'give', 'rating', 'scale', '-5', '+5', '.']

 TOTAL LEMMATIZE WORDS ==> 12

************************************************************************************************************************

150 --> Parts of speech taggers for the languages like European languages, research is being done on  making parts of speech taggers for other languages like Arabic, Sanskrit (Namrata Tapswi ,  Suresh Jain ., 2012) [27], Hindi (Pradipta Ranjan Ray et al., 2003 )[28] etc. 


 ---- TOKENS ----

 ['Parts', 'of', 'speech', 'taggers', 'for', 'the', 'languages', 'like', 'European', 'languages', ',', 'research', 'is', 'being', 'done', 'on', 'making', 'parts', 'of', 'speech', 'taggers', 'for', 'other', 'languages', 'like', 'Arabic', ',', 'Sanskrit', '(', 'Namrata', 'Tapswi', ',', 'Suresh', 'Jain', '.', ',', '2012', ')', '[', '27', ']', ',', 'Hindi', '(', 'Pradipta', 'Ranjan', 'Ray', 'et', 'al.', ',', '2003', ')', '[', '28', ']', 'etc', '.'] 

 TOTAL TOKENS ==> 57

 ---- POST ----

 [('Parts', 'NNS'), ('of', 'IN'), ('speech', 'NN'), ('taggers', 'NNS'), ('for', 'IN'), ('the', 'DT'), ('languages', 'NNS'), ('like', 'IN'), ('European', 'JJ'), ('languages', 'NNS'), (',', ','), ('research', 'NN'), ('is', 'VBZ'), ('being', 'VBG'), ('done', 'VBN'), ('on', 'IN'), ('making', 'VBG'), ('parts', 'NNS'), ('of', 'IN'), ('speech', 'NN'), ('taggers', 'NNS'), ('for', 'IN'), ('other', 'JJ'), ('languages', 'NNS'), ('like', 'IN'), ('Arabic', 'NNP'), (',', ','), ('Sanskrit', 'NNP'), ('(', '('), ('Namrata', 'NNP'), ('Tapswi', 'NNP'), (',', ','), ('Suresh', 'NNP'), ('Jain', 'NNP'), ('.', '.'), (',', ','), ('2012', 'CD'), (')', ')'), ('[', 'VBD'), ('27', 'CD'), (']', 'NN'), (',', ','), ('Hindi', 'NNP'), ('(', '('), ('Pradipta', 'NNP'), ('Ranjan', 'NNP'), ('Ray', 'NNP'), ('et', 'FW'), ('al.', 'NN'), (',', ','), ('2003', 'CD'), (')', ')'), ('[', 'VBD'), ('28', 'CD'), (']', 'JJ'), ('etc', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Parts', 'speech', 'taggers', 'languages', 'like', 'European', 'languages', ',', 'research', 'done', 'making', 'parts', 'speech', 'taggers', 'languages', 'like', 'Arabic', ',', 'Sanskrit', '(', 'Namrata', 'Tapswi', ',', 'Suresh', 'Jain', '.', ',', '2012', ')', '[', '27', ']', ',', 'Hindi', '(', 'Pradipta', 'Ranjan', 'Ray', 'et', 'al.', ',', '2003', ')', '[', '28', ']', 'etc', '.']

 TOTAL FILTERED TOKENS ==>  48

 ---- POST FOR FILTERED TOKENS ----

 [('Parts', 'NNS'), ('speech', 'NN'), ('taggers', 'NNS'), ('languages', 'VBP'), ('like', 'IN'), ('European', 'JJ'), ('languages', 'NNS'), (',', ','), ('research', 'NN'), ('done', 'VBN'), ('making', 'VBG'), ('parts', 'NNS'), ('speech', 'NN'), ('taggers', 'NNS'), ('languages', 'NNS'), ('like', 'IN'), ('Arabic', 'NNP'), (',', ','), ('Sanskrit', 'NNP'), ('(', '('), ('Namrata', 'NNP'), ('Tapswi', 'NNP'), (',', ','), ('Suresh', 'NNP'), ('Jain', 'NNP'), ('.', '.'), (',', ','), ('2012', 'CD'), (')', ')'), ('[', 'VBD'), ('27', 'CD'), (']', 'NN'), (',', ','), ('Hindi', 'NNP'), ('(', '('), ('Pradipta', 'NNP'), ('Ranjan', 'NNP'), ('Ray', 'NNP'), ('et', 'FW'), ('al.', 'NN'), (',', ','), ('2003', 'CD'), (')', ')'), ('[', 'VBD'), ('28', 'CD'), (']', 'JJ'), ('etc', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Parts speech', 'speech taggers', 'taggers languages', 'languages like', 'like European', 'European languages', 'languages ,', ', research', 'research done', 'done making', 'making parts', 'parts speech', 'speech taggers', 'taggers languages', 'languages like', 'like Arabic', 'Arabic ,', ', Sanskrit', 'Sanskrit (', '( Namrata', 'Namrata Tapswi', 'Tapswi ,', ', Suresh', 'Suresh Jain', 'Jain .', '. ,', ', 2012', '2012 )', ') [', '[ 27', '27 ]', '] ,', ', Hindi', 'Hindi (', '( Pradipta', 'Pradipta Ranjan', 'Ranjan Ray', 'Ray et', 'et al.', 'al. ,', ', 2003', '2003 )', ') [', '[ 28', '28 ]', '] etc', 'etc .'] 

 TOTAL BIGRAMS --> 47 



 ---- TRI-GRAMS ---- 

 ['Parts speech taggers', 'speech taggers languages', 'taggers languages like', 'languages like European', 'like European languages', 'European languages ,', 'languages , research', ', research done', 'research done making', 'done making parts', 'making parts speech', 'parts speech taggers', 'speech taggers languages', 'taggers languages like', 'languages like Arabic', 'like Arabic ,', 'Arabic , Sanskrit', ', Sanskrit (', 'Sanskrit ( Namrata', '( Namrata Tapswi', 'Namrata Tapswi ,', 'Tapswi , Suresh', ', Suresh Jain', 'Suresh Jain .', 'Jain . ,', '. , 2012', ', 2012 )', '2012 ) [', ') [ 27', '[ 27 ]', '27 ] ,', '] , Hindi', ', Hindi (', 'Hindi ( Pradipta', '( Pradipta Ranjan', 'Pradipta Ranjan Ray', 'Ranjan Ray et', 'Ray et al.', 'et al. ,', 'al. , 2003', ', 2003 )', '2003 ) [', ') [ 28', '[ 28 ]', '28 ] etc', '] etc .'] 

 TOTAL TRIGRAMS --> 46 



 ---- NOUN PHRASES ---- 

 ['speech', 'research', 'speech', ']', '] etc'] 

 TOTAL NOUN PHRASES --> 5 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> ['Arabic']
 TOTAL PERSON ENTITY --> 1 


 GPE ---> ['Parts', 'European', 'Sanskrit', 'Hindi']
 TOTAL GPE ENTITY --> 4 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['part', 'speech', 'tagger', 'languag', 'like', 'european', 'languag', ',', 'research', 'done', 'make', 'part', 'speech', 'tagger', 'languag', 'like', 'arab', ',', 'sanskrit', '(', 'namrata', 'tapswi', ',', 'suresh', 'jain', '.', ',', '2012', ')', '[', '27', ']', ',', 'hindi', '(', 'pradipta', 'ranjan', 'ray', 'et', 'al.', ',', '2003', ')', '[', '28', ']', 'etc', '.']

 TOTAL PORTER STEM WORDS ==> 48



 ---- SNOWBALL STEMMING ----

['part', 'speech', 'tagger', 'languag', 'like', 'european', 'languag', ',', 'research', 'done', 'make', 'part', 'speech', 'tagger', 'languag', 'like', 'arab', ',', 'sanskrit', '(', 'namrata', 'tapswi', ',', 'suresh', 'jain', '.', ',', '2012', ')', '[', '27', ']', ',', 'hindi', '(', 'pradipta', 'ranjan', 'ray', 'et', 'al.', ',', '2003', ')', '[', '28', ']', 'etc', '.']

 TOTAL SNOWBALL STEM WORDS ==> 48



 ---- LEMMATIZATION ----

['Parts', 'speech', 'tagger', 'language', 'like', 'European', 'language', ',', 'research', 'done', 'making', 'part', 'speech', 'tagger', 'language', 'like', 'Arabic', ',', 'Sanskrit', '(', 'Namrata', 'Tapswi', ',', 'Suresh', 'Jain', '.', ',', '2012', ')', '[', '27', ']', ',', 'Hindi', '(', 'Pradipta', 'Ranjan', 'Ray', 'et', 'al.', ',', '2003', ')', '[', '28', ']', 'etc', '.']

 TOTAL LEMMATIZE WORDS ==> 48

************************************************************************************************************************

151 --> It can efficiently  tag and classify words as nouns, adjectives, verbs etc. 


 ---- TOKENS ----

 ['It', 'can', 'efficiently', 'tag', 'and', 'classify', 'words', 'as', 'nouns', ',', 'adjectives', ',', 'verbs', 'etc', '.'] 

 TOTAL TOKENS ==> 15

 ---- POST ----

 [('It', 'PRP'), ('can', 'MD'), ('efficiently', 'RB'), ('tag', 'VB'), ('and', 'CC'), ('classify', 'VB'), ('words', 'NNS'), ('as', 'IN'), ('nouns', 'NNS'), (',', ','), ('adjectives', 'NNS'), (',', ','), ('verbs', 'JJ'), ('etc', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['efficiently', 'tag', 'classify', 'words', 'nouns', ',', 'adjectives', ',', 'verbs', 'etc', '.']

 TOTAL FILTERED TOKENS ==>  11

 ---- POST FOR FILTERED TOKENS ----

 [('efficiently', 'RB'), ('tag', 'JJ'), ('classify', 'NN'), ('words', 'NNS'), ('nouns', 'NNS'), (',', ','), ('adjectives', 'NNS'), (',', ','), ('verbs', 'JJ'), ('etc', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['efficiently tag', 'tag classify', 'classify words', 'words nouns', 'nouns ,', ', adjectives', 'adjectives ,', ', verbs', 'verbs etc', 'etc .'] 

 TOTAL BIGRAMS --> 10 



 ---- TRI-GRAMS ---- 

 ['efficiently tag classify', 'tag classify words', 'classify words nouns', 'words nouns ,', 'nouns , adjectives', ', adjectives ,', 'adjectives , verbs', ', verbs etc', 'verbs etc .'] 

 TOTAL TRIGRAMS --> 9 



 ---- NOUN PHRASES ---- 

 ['tag classify', 'verbs etc'] 

 TOTAL NOUN PHRASES --> 2 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['effici', 'tag', 'classifi', 'word', 'noun', ',', 'adject', ',', 'verb', 'etc', '.']

 TOTAL PORTER STEM WORDS ==> 11



 ---- SNOWBALL STEMMING ----

['effici', 'tag', 'classifi', 'word', 'noun', ',', 'adject', ',', 'verb', 'etc', '.']

 TOTAL SNOWBALL STEM WORDS ==> 11



 ---- LEMMATIZATION ----

['efficiently', 'tag', 'classify', 'word', 'noun', ',', 'adjective', ',', 'verb', 'etc', '.']

 TOTAL LEMMATIZE WORDS ==> 11

************************************************************************************************************************

152 --> The most procedures for part of speech  can work efficiently on European languages, but it won’t on Asian languages or middle  eastern languages. 


 ---- TOKENS ----

 ['The', 'most', 'procedures', 'for', 'part', 'of', 'speech', 'can', 'work', 'efficiently', 'on', 'European', 'languages', ',', 'but', 'it', 'won', '’', 't', 'on', 'Asian', 'languages', 'or', 'middle', 'eastern', 'languages', '.'] 

 TOTAL TOKENS ==> 27

 ---- POST ----

 [('The', 'DT'), ('most', 'RBS'), ('procedures', 'NNS'), ('for', 'IN'), ('part', 'NN'), ('of', 'IN'), ('speech', 'NN'), ('can', 'MD'), ('work', 'VB'), ('efficiently', 'RB'), ('on', 'IN'), ('European', 'JJ'), ('languages', 'NNS'), (',', ','), ('but', 'CC'), ('it', 'PRP'), ('won', 'VBD'), ('’', 'NNP'), ('t', 'NN'), ('on', 'IN'), ('Asian', 'JJ'), ('languages', 'NNS'), ('or', 'CC'), ('middle', 'JJ'), ('eastern', 'JJ'), ('languages', 'NNS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['procedures', 'part', 'speech', 'work', 'efficiently', 'European', 'languages', ',', '’', 'Asian', 'languages', 'middle', 'eastern', 'languages', '.']

 TOTAL FILTERED TOKENS ==>  15

 ---- POST FOR FILTERED TOKENS ----

 [('procedures', 'NNS'), ('part', 'NN'), ('speech', 'NN'), ('work', 'NN'), ('efficiently', 'RB'), ('European', 'JJ'), ('languages', 'NNS'), (',', ','), ('’', 'JJ'), ('Asian', 'JJ'), ('languages', 'NNS'), ('middle', 'VBP'), ('eastern', 'JJ'), ('languages', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['procedures part', 'part speech', 'speech work', 'work efficiently', 'efficiently European', 'European languages', 'languages ,', ', ’', '’ Asian', 'Asian languages', 'languages middle', 'middle eastern', 'eastern languages', 'languages .'] 

 TOTAL BIGRAMS --> 14 



 ---- TRI-GRAMS ---- 

 ['procedures part speech', 'part speech work', 'speech work efficiently', 'work efficiently European', 'efficiently European languages', 'European languages ,', 'languages , ’', ', ’ Asian', '’ Asian languages', 'Asian languages middle', 'languages middle eastern', 'middle eastern languages', 'eastern languages .'] 

 TOTAL TRIGRAMS --> 13 



 ---- NOUN PHRASES ---- 

 ['part', 'speech', 'work'] 

 TOTAL NOUN PHRASES --> 3 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> ['European', 'Asian']
 TOTAL GPE ENTITY --> 2 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['procedur', 'part', 'speech', 'work', 'effici', 'european', 'languag', ',', '’', 'asian', 'languag', 'middl', 'eastern', 'languag', '.']

 TOTAL PORTER STEM WORDS ==> 15



 ---- SNOWBALL STEMMING ----

['procedur', 'part', 'speech', 'work', 'effici', 'european', 'languag', ',', '’', 'asian', 'languag', 'middl', 'eastern', 'languag', '.']

 TOTAL SNOWBALL STEM WORDS ==> 15



 ---- LEMMATIZATION ----

['procedure', 'part', 'speech', 'work', 'efficiently', 'European', 'language', ',', '’', 'Asian', 'language', 'middle', 'eastern', 'language', '.']

 TOTAL LEMMATIZE WORDS ==> 15

************************************************************************************************************************

153 --> Sanskrit part of speech tagger is specifically uses treebank technique. 


 ---- TOKENS ----

 ['Sanskrit', 'part', 'of', 'speech', 'tagger', 'is', 'specifically', 'uses', 'treebank', 'technique', '.'] 

 TOTAL TOKENS ==> 11

 ---- POST ----

 [('Sanskrit', 'NNP'), ('part', 'NN'), ('of', 'IN'), ('speech', 'NN'), ('tagger', 'NN'), ('is', 'VBZ'), ('specifically', 'RB'), ('uses', 'JJ'), ('treebank', 'NN'), ('technique', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Sanskrit', 'part', 'speech', 'tagger', 'specifically', 'uses', 'treebank', 'technique', '.']

 TOTAL FILTERED TOKENS ==>  9

 ---- POST FOR FILTERED TOKENS ----

 [('Sanskrit', 'NNP'), ('part', 'NN'), ('speech', 'NN'), ('tagger', 'NN'), ('specifically', 'RB'), ('uses', 'VBZ'), ('treebank', 'NN'), ('technique', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Sanskrit part', 'part speech', 'speech tagger', 'tagger specifically', 'specifically uses', 'uses treebank', 'treebank technique', 'technique .'] 

 TOTAL BIGRAMS --> 8 



 ---- TRI-GRAMS ---- 

 ['Sanskrit part speech', 'part speech tagger', 'speech tagger specifically', 'tagger specifically uses', 'specifically uses treebank', 'uses treebank technique', 'treebank technique .'] 

 TOTAL TRIGRAMS --> 7 



 ---- NOUN PHRASES ---- 

 ['part', 'speech', 'tagger', 'treebank', 'technique'] 

 TOTAL NOUN PHRASES --> 5 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> ['Sanskrit']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['sanskrit', 'part', 'speech', 'tagger', 'specif', 'use', 'treebank', 'techniqu', '.']

 TOTAL PORTER STEM WORDS ==> 9



 ---- SNOWBALL STEMMING ----

['sanskrit', 'part', 'speech', 'tagger', 'specif', 'use', 'treebank', 'techniqu', '.']

 TOTAL SNOWBALL STEM WORDS ==> 9



 ---- LEMMATIZATION ----

['Sanskrit', 'part', 'speech', 'tagger', 'specifically', 'us', 'treebank', 'technique', '.']

 TOTAL LEMMATIZE WORDS ==> 9

************************************************************************************************************************

154 --> Arabic uses Support Vector Machine (SVM) (Mona Diab etal.,2004) [29] approach to  automatically tokenize, parts of speech tag and annotate base phrases in Arabic text. 


 ---- TOKENS ----

 ['Arabic', 'uses', 'Support', 'Vector', 'Machine', '(', 'SVM', ')', '(', 'Mona', 'Diab', 'etal.,2004', ')', '[', '29', ']', 'approach', 'to', 'automatically', 'tokenize', ',', 'parts', 'of', 'speech', 'tag', 'and', 'annotate', 'base', 'phrases', 'in', 'Arabic', 'text', '.'] 

 TOTAL TOKENS ==> 33

 ---- POST ----

 [('Arabic', 'JJ'), ('uses', 'VBZ'), ('Support', 'NNP'), ('Vector', 'NNP'), ('Machine', 'NNP'), ('(', '('), ('SVM', 'NNP'), (')', ')'), ('(', '('), ('Mona', 'NNP'), ('Diab', 'NNP'), ('etal.,2004', 'NN'), (')', ')'), ('[', 'VBZ'), ('29', 'CD'), (']', 'JJ'), ('approach', 'NN'), ('to', 'TO'), ('automatically', 'RB'), ('tokenize', 'VB'), (',', ','), ('parts', 'NNS'), ('of', 'IN'), ('speech', 'NN'), ('tag', 'NN'), ('and', 'CC'), ('annotate', 'JJ'), ('base', 'NN'), ('phrases', 'NNS'), ('in', 'IN'), ('Arabic', 'NNP'), ('text', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Arabic', 'uses', 'Support', 'Vector', 'Machine', '(', 'SVM', ')', '(', 'Mona', 'Diab', 'etal.,2004', ')', '[', '29', ']', 'approach', 'automatically', 'tokenize', ',', 'parts', 'speech', 'tag', 'annotate', 'base', 'phrases', 'Arabic', 'text', '.']

 TOTAL FILTERED TOKENS ==>  29

 ---- POST FOR FILTERED TOKENS ----

 [('Arabic', 'JJ'), ('uses', 'VBZ'), ('Support', 'NNP'), ('Vector', 'NNP'), ('Machine', 'NNP'), ('(', '('), ('SVM', 'NNP'), (')', ')'), ('(', '('), ('Mona', 'NNP'), ('Diab', 'NNP'), ('etal.,2004', 'NN'), (')', ')'), ('[', 'VBZ'), ('29', 'CD'), (']', 'NN'), ('approach', 'NN'), ('automatically', 'RB'), ('tokenize', 'VB'), (',', ','), ('parts', 'NNS'), ('speech', 'VBP'), ('tag', 'JJ'), ('annotate', 'NN'), ('base', 'NN'), ('phrases', 'VBZ'), ('Arabic', 'NNP'), ('text', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Arabic uses', 'uses Support', 'Support Vector', 'Vector Machine', 'Machine (', '( SVM', 'SVM )', ') (', '( Mona', 'Mona Diab', 'Diab etal.,2004', 'etal.,2004 )', ') [', '[ 29', '29 ]', '] approach', 'approach automatically', 'automatically tokenize', 'tokenize ,', ', parts', 'parts speech', 'speech tag', 'tag annotate', 'annotate base', 'base phrases', 'phrases Arabic', 'Arabic text', 'text .'] 

 TOTAL BIGRAMS --> 28 



 ---- TRI-GRAMS ---- 

 ['Arabic uses Support', 'uses Support Vector', 'Support Vector Machine', 'Vector Machine (', 'Machine ( SVM', '( SVM )', 'SVM ) (', ') ( Mona', '( Mona Diab', 'Mona Diab etal.,2004', 'Diab etal.,2004 )', 'etal.,2004 ) [', ') [ 29', '[ 29 ]', '29 ] approach', '] approach automatically', 'approach automatically tokenize', 'automatically tokenize ,', 'tokenize , parts', ', parts speech', 'parts speech tag', 'speech tag annotate', 'tag annotate base', 'annotate base phrases', 'base phrases Arabic', 'phrases Arabic text', 'Arabic text .'] 

 TOTAL TRIGRAMS --> 27 



 ---- NOUN PHRASES ---- 

 [']', 'approach', 'tag annotate', 'base', 'text'] 

 TOTAL NOUN PHRASES --> 5 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> ['Arabic', 'Support Vector Machine', 'Arabic']
 TOTAL PERSON ENTITY --> 3 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['arab', 'use', 'support', 'vector', 'machin', '(', 'svm', ')', '(', 'mona', 'diab', 'etal.,2004', ')', '[', '29', ']', 'approach', 'automat', 'token', ',', 'part', 'speech', 'tag', 'annot', 'base', 'phrase', 'arab', 'text', '.']

 TOTAL PORTER STEM WORDS ==> 29



 ---- SNOWBALL STEMMING ----

['arab', 'use', 'support', 'vector', 'machin', '(', 'svm', ')', '(', 'mona', 'diab', 'etal.,2004', ')', '[', '29', ']', 'approach', 'automat', 'token', ',', 'part', 'speech', 'tag', 'annot', 'base', 'phrase', 'arab', 'text', '.']

 TOTAL SNOWBALL STEM WORDS ==> 29



 ---- LEMMATIZATION ----

['Arabic', 'us', 'Support', 'Vector', 'Machine', '(', 'SVM', ')', '(', 'Mona', 'Diab', 'etal.,2004', ')', '[', '29', ']', 'approach', 'automatically', 'tokenize', ',', 'part', 'speech', 'tag', 'annotate', 'base', 'phrase', 'Arabic', 'text', '.']

 TOTAL LEMMATIZE WORDS ==> 29

************************************************************************************************************************

155 --> Chunking – it is also known as Shadow Parsing, it works by labelling segments of sentences  with syntactic correlated keywords like Noun Phrase and Verb Phrase (NP or VP). 


 ---- TOKENS ----

 ['Chunking', '–', 'it', 'is', 'also', 'known', 'as', 'Shadow', 'Parsing', ',', 'it', 'works', 'by', 'labelling', 'segments', 'of', 'sentences', 'with', 'syntactic', 'correlated', 'keywords', 'like', 'Noun', 'Phrase', 'and', 'Verb', 'Phrase', '(', 'NP', 'or', 'VP', ')', '.'] 

 TOTAL TOKENS ==> 33

 ---- POST ----

 [('Chunking', 'VBG'), ('–', 'NN'), ('it', 'PRP'), ('is', 'VBZ'), ('also', 'RB'), ('known', 'VBN'), ('as', 'IN'), ('Shadow', 'NNP'), ('Parsing', 'NNP'), (',', ','), ('it', 'PRP'), ('works', 'VBZ'), ('by', 'IN'), ('labelling', 'VBG'), ('segments', 'NNS'), ('of', 'IN'), ('sentences', 'NNS'), ('with', 'IN'), ('syntactic', 'JJ'), ('correlated', 'JJ'), ('keywords', 'NNS'), ('like', 'IN'), ('Noun', 'NNP'), ('Phrase', 'NNP'), ('and', 'CC'), ('Verb', 'NNP'), ('Phrase', 'NNP'), ('(', '('), ('NP', 'NNP'), ('or', 'CC'), ('VP', 'NNP'), (')', ')'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Chunking', '–', 'also', 'known', 'Shadow', 'Parsing', ',', 'works', 'labelling', 'segments', 'sentences', 'syntactic', 'correlated', 'keywords', 'like', 'Noun', 'Phrase', 'Verb', 'Phrase', '(', 'NP', 'VP', ')', '.']

 TOTAL FILTERED TOKENS ==>  24

 ---- POST FOR FILTERED TOKENS ----

 [('Chunking', 'VBG'), ('–', 'NNP'), ('also', 'RB'), ('known', 'VBN'), ('Shadow', 'NNP'), ('Parsing', 'NNP'), (',', ','), ('works', 'VBZ'), ('labelling', 'VBG'), ('segments', 'NNS'), ('sentences', 'NNS'), ('syntactic', 'JJ'), ('correlated', 'VBD'), ('keywords', 'NNS'), ('like', 'IN'), ('Noun', 'NNP'), ('Phrase', 'NNP'), ('Verb', 'NNP'), ('Phrase', 'NNP'), ('(', '('), ('NP', 'NNP'), ('VP', 'NNP'), (')', ')'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Chunking –', '– also', 'also known', 'known Shadow', 'Shadow Parsing', 'Parsing ,', ', works', 'works labelling', 'labelling segments', 'segments sentences', 'sentences syntactic', 'syntactic correlated', 'correlated keywords', 'keywords like', 'like Noun', 'Noun Phrase', 'Phrase Verb', 'Verb Phrase', 'Phrase (', '( NP', 'NP VP', 'VP )', ') .'] 

 TOTAL BIGRAMS --> 23 



 ---- TRI-GRAMS ---- 

 ['Chunking – also', '– also known', 'also known Shadow', 'known Shadow Parsing', 'Shadow Parsing ,', 'Parsing , works', ', works labelling', 'works labelling segments', 'labelling segments sentences', 'segments sentences syntactic', 'sentences syntactic correlated', 'syntactic correlated keywords', 'correlated keywords like', 'keywords like Noun', 'like Noun Phrase', 'Noun Phrase Verb', 'Phrase Verb Phrase', 'Verb Phrase (', 'Phrase ( NP', '( NP VP', 'NP VP )', 'VP ) .'] 

 TOTAL TRIGRAMS --> 22 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> ['Shadow Parsing', 'Noun Phrase Verb Phrase']
 TOTAL PERSON ENTITY --> 2 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['chunk', '–', 'also', 'known', 'shadow', 'pars', ',', 'work', 'label', 'segment', 'sentenc', 'syntact', 'correl', 'keyword', 'like', 'noun', 'phrase', 'verb', 'phrase', '(', 'np', 'vp', ')', '.']

 TOTAL PORTER STEM WORDS ==> 24



 ---- SNOWBALL STEMMING ----

['chunk', '–', 'also', 'known', 'shadow', 'pars', ',', 'work', 'label', 'segment', 'sentenc', 'syntact', 'correl', 'keyword', 'like', 'noun', 'phrase', 'verb', 'phrase', '(', 'np', 'vp', ')', '.']

 TOTAL SNOWBALL STEM WORDS ==> 24



 ---- LEMMATIZATION ----

['Chunking', '–', 'also', 'known', 'Shadow', 'Parsing', ',', 'work', 'labelling', 'segment', 'sentence', 'syntactic', 'correlated', 'keywords', 'like', 'Noun', 'Phrase', 'Verb', 'Phrase', '(', 'NP', 'VP', ')', '.']

 TOTAL LEMMATIZE WORDS ==> 24

************************************************************************************************************************

156 --> Every  word has a unique tag often marked as Begin Chunk (B-NP) tag or Inside Chunk (I-NP) tag. 


 ---- TOKENS ----

 ['Every', 'word', 'has', 'a', 'unique', 'tag', 'often', 'marked', 'as', 'Begin', 'Chunk', '(', 'B-NP', ')', 'tag', 'or', 'Inside', 'Chunk', '(', 'I-NP', ')', 'tag', '.'] 

 TOTAL TOKENS ==> 23

 ---- POST ----

 [('Every', 'DT'), ('word', 'NN'), ('has', 'VBZ'), ('a', 'DT'), ('unique', 'JJ'), ('tag', 'NN'), ('often', 'RB'), ('marked', 'VBD'), ('as', 'IN'), ('Begin', 'NNP'), ('Chunk', 'NNP'), ('(', '('), ('B-NP', 'NNP'), (')', ')'), ('tag', 'NN'), ('or', 'CC'), ('Inside', 'NNP'), ('Chunk', 'NNP'), ('(', '('), ('I-NP', 'NNP'), (')', ')'), ('tag', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Every', 'word', 'unique', 'tag', 'often', 'marked', 'Begin', 'Chunk', '(', 'B-NP', ')', 'tag', 'Inside', 'Chunk', '(', 'I-NP', ')', 'tag', '.']

 TOTAL FILTERED TOKENS ==>  19

 ---- POST FOR FILTERED TOKENS ----

 [('Every', 'DT'), ('word', 'NN'), ('unique', 'NN'), ('tag', 'NN'), ('often', 'RB'), ('marked', 'VBD'), ('Begin', 'NNP'), ('Chunk', 'NNP'), ('(', '('), ('B-NP', 'NNP'), (')', ')'), ('tag', 'NN'), ('Inside', 'NNP'), ('Chunk', 'NNP'), ('(', '('), ('I-NP', 'NNP'), (')', ')'), ('tag', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Every word', 'word unique', 'unique tag', 'tag often', 'often marked', 'marked Begin', 'Begin Chunk', 'Chunk (', '( B-NP', 'B-NP )', ') tag', 'tag Inside', 'Inside Chunk', 'Chunk (', '( I-NP', 'I-NP )', ') tag', 'tag .'] 

 TOTAL BIGRAMS --> 18 



 ---- TRI-GRAMS ---- 

 ['Every word unique', 'word unique tag', 'unique tag often', 'tag often marked', 'often marked Begin', 'marked Begin Chunk', 'Begin Chunk (', 'Chunk ( B-NP', '( B-NP )', 'B-NP ) tag', ') tag Inside', 'tag Inside Chunk', 'Inside Chunk (', 'Chunk ( I-NP', '( I-NP )', 'I-NP ) tag', ') tag .'] 

 TOTAL TRIGRAMS --> 17 



 ---- NOUN PHRASES ---- 

 ['Every word', 'unique', 'tag', 'tag', 'tag'] 

 TOTAL NOUN PHRASES --> 5 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> ['Begin Chunk', 'Inside Chunk']
 TOTAL PERSON ENTITY --> 2 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['everi', 'word', 'uniqu', 'tag', 'often', 'mark', 'begin', 'chunk', '(', 'b-np', ')', 'tag', 'insid', 'chunk', '(', 'i-np', ')', 'tag', '.']

 TOTAL PORTER STEM WORDS ==> 19



 ---- SNOWBALL STEMMING ----

['everi', 'word', 'uniqu', 'tag', 'often', 'mark', 'begin', 'chunk', '(', 'b-np', ')', 'tag', 'insid', 'chunk', '(', 'i-np', ')', 'tag', '.']

 TOTAL SNOWBALL STEM WORDS ==> 19



 ---- LEMMATIZATION ----

['Every', 'word', 'unique', 'tag', 'often', 'marked', 'Begin', 'Chunk', '(', 'B-NP', ')', 'tag', 'Inside', 'Chunk', '(', 'I-NP', ')', 'tag', '.']

 TOTAL LEMMATIZE WORDS ==> 19

************************************************************************************************************************

157 --> Chunking is often evaluated using the CoNLL 2000 shared task. 


 ---- TOKENS ----

 ['Chunking', 'is', 'often', 'evaluated', 'using', 'the', 'CoNLL', '2000', 'shared', 'task', '.'] 

 TOTAL TOKENS ==> 11

 ---- POST ----

 [('Chunking', 'NN'), ('is', 'VBZ'), ('often', 'RB'), ('evaluated', 'VBN'), ('using', 'VBG'), ('the', 'DT'), ('CoNLL', 'NNP'), ('2000', 'CD'), ('shared', 'VBD'), ('task', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Chunking', 'often', 'evaluated', 'using', 'CoNLL', '2000', 'shared', 'task', '.']

 TOTAL FILTERED TOKENS ==>  9

 ---- POST FOR FILTERED TOKENS ----

 [('Chunking', 'VBG'), ('often', 'RB'), ('evaluated', 'VBN'), ('using', 'VBG'), ('CoNLL', 'NNP'), ('2000', 'CD'), ('shared', 'VBD'), ('task', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Chunking often', 'often evaluated', 'evaluated using', 'using CoNLL', 'CoNLL 2000', '2000 shared', 'shared task', 'task .'] 

 TOTAL BIGRAMS --> 8 



 ---- TRI-GRAMS ---- 

 ['Chunking often evaluated', 'often evaluated using', 'evaluated using CoNLL', 'using CoNLL 2000', 'CoNLL 2000 shared', '2000 shared task', 'shared task .'] 

 TOTAL TRIGRAMS --> 7 



 ---- NOUN PHRASES ---- 

 ['task'] 

 TOTAL NOUN PHRASES --> 1 



 ---- NER ----

 
 ORGANIZATION ---> ['CoNLL']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['chunk', 'often', 'evalu', 'use', 'conll', '2000', 'share', 'task', '.']

 TOTAL PORTER STEM WORDS ==> 9



 ---- SNOWBALL STEMMING ----

['chunk', 'often', 'evalu', 'use', 'conll', '2000', 'share', 'task', '.']

 TOTAL SNOWBALL STEM WORDS ==> 9



 ---- LEMMATIZATION ----

['Chunking', 'often', 'evaluated', 'using', 'CoNLL', '2000', 'shared', 'task', '.']

 TOTAL LEMMATIZE WORDS ==> 9

************************************************************************************************************************

158 --> CoNLL 2000 provides test  data for Chunking. 


 ---- TOKENS ----

 ['CoNLL', '2000', 'provides', 'test', 'data', 'for', 'Chunking', '.'] 

 TOTAL TOKENS ==> 8

 ---- POST ----

 [('CoNLL', 'JJ'), ('2000', 'CD'), ('provides', 'VBZ'), ('test', 'NN'), ('data', 'NNS'), ('for', 'IN'), ('Chunking', 'NNP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['CoNLL', '2000', 'provides', 'test', 'data', 'Chunking', '.']

 TOTAL FILTERED TOKENS ==>  7

 ---- POST FOR FILTERED TOKENS ----

 [('CoNLL', 'JJ'), ('2000', 'CD'), ('provides', 'VBZ'), ('test', 'NN'), ('data', 'NNS'), ('Chunking', 'NNP'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['CoNLL 2000', '2000 provides', 'provides test', 'test data', 'data Chunking', 'Chunking .'] 

 TOTAL BIGRAMS --> 6 



 ---- TRI-GRAMS ---- 

 ['CoNLL 2000 provides', '2000 provides test', 'provides test data', 'test data Chunking', 'data Chunking .'] 

 TOTAL TRIGRAMS --> 5 



 ---- NOUN PHRASES ---- 

 ['test'] 

 TOTAL NOUN PHRASES --> 1 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['conll', '2000', 'provid', 'test', 'data', 'chunk', '.']

 TOTAL PORTER STEM WORDS ==> 7



 ---- SNOWBALL STEMMING ----

['conll', '2000', 'provid', 'test', 'data', 'chunk', '.']

 TOTAL SNOWBALL STEM WORDS ==> 7



 ---- LEMMATIZATION ----

['CoNLL', '2000', 'provides', 'test', 'data', 'Chunking', '.']

 TOTAL LEMMATIZE WORDS ==> 7

************************************************************************************************************************

159 --> Since then, a certain number of systems arised (Sha and Pereira, 2003;  McDonald et al., 2005; Sun et al., 2008) [30] [31] [32], all reporting around 94.3% F1 score. 


 ---- TOKENS ----

 ['Since', 'then', ',', 'a', 'certain', 'number', 'of', 'systems', 'arised', '(', 'Sha', 'and', 'Pereira', ',', '2003', ';', 'McDonald', 'et', 'al.', ',', '2005', ';', 'Sun', 'et', 'al.', ',', '2008', ')', '[', '30', ']', '[', '31', ']', '[', '32', ']', ',', 'all', 'reporting', 'around', '94.3', '%', 'F1', 'score', '.'] 

 TOTAL TOKENS ==> 46

 ---- POST ----

 [('Since', 'IN'), ('then', 'RB'), (',', ','), ('a', 'DT'), ('certain', 'JJ'), ('number', 'NN'), ('of', 'IN'), ('systems', 'NNS'), ('arised', 'VBN'), ('(', '('), ('Sha', 'NNP'), ('and', 'CC'), ('Pereira', 'NNP'), (',', ','), ('2003', 'CD'), (';', ':'), ('McDonald', 'NNP'), ('et', 'VBZ'), ('al.', 'RB'), (',', ','), ('2005', 'CD'), (';', ':'), ('Sun', 'NNP'), ('et', 'FW'), ('al.', 'NN'), (',', ','), ('2008', 'CD'), (')', ')'), ('[', 'VBD'), ('30', 'CD'), (']', 'NNP'), ('[', 'VBD'), ('31', 'CD'), (']', 'NNP'), ('[', 'VBD'), ('32', 'CD'), (']', 'NNS'), (',', ','), ('all', 'DT'), ('reporting', 'VBG'), ('around', 'RB'), ('94.3', 'CD'), ('%', 'NN'), ('F1', 'NNP'), ('score', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Since', ',', 'certain', 'number', 'systems', 'arised', '(', 'Sha', 'Pereira', ',', '2003', ';', 'McDonald', 'et', 'al.', ',', '2005', ';', 'Sun', 'et', 'al.', ',', '2008', ')', '[', '30', ']', '[', '31', ']', '[', '32', ']', ',', 'reporting', 'around', '94.3', '%', 'F1', 'score', '.']

 TOTAL FILTERED TOKENS ==>  41

 ---- POST FOR FILTERED TOKENS ----

 [('Since', 'IN'), (',', ','), ('certain', 'JJ'), ('number', 'NN'), ('systems', 'NNS'), ('arised', 'VBN'), ('(', '('), ('Sha', 'NNP'), ('Pereira', 'NNP'), (',', ','), ('2003', 'CD'), (';', ':'), ('McDonald', 'NNP'), ('et', 'VBZ'), ('al.', 'RB'), (',', ','), ('2005', 'CD'), (';', ':'), ('Sun', 'NNP'), ('et', 'FW'), ('al.', 'NN'), (',', ','), ('2008', 'CD'), (')', ')'), ('[', 'VBD'), ('30', 'CD'), (']', 'NNP'), ('[', 'VBD'), ('31', 'CD'), (']', 'NNP'), ('[', 'VBD'), ('32', 'CD'), (']', 'NN'), (',', ','), ('reporting', 'VBG'), ('around', 'RB'), ('94.3', 'CD'), ('%', 'NN'), ('F1', 'NNP'), ('score', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Since ,', ', certain', 'certain number', 'number systems', 'systems arised', 'arised (', '( Sha', 'Sha Pereira', 'Pereira ,', ', 2003', '2003 ;', '; McDonald', 'McDonald et', 'et al.', 'al. ,', ', 2005', '2005 ;', '; Sun', 'Sun et', 'et al.', 'al. ,', ', 2008', '2008 )', ') [', '[ 30', '30 ]', '] [', '[ 31', '31 ]', '] [', '[ 32', '32 ]', '] ,', ', reporting', 'reporting around', 'around 94.3', '94.3 %', '% F1', 'F1 score', 'score .'] 

 TOTAL BIGRAMS --> 40 



 ---- TRI-GRAMS ---- 

 ['Since , certain', ', certain number', 'certain number systems', 'number systems arised', 'systems arised (', 'arised ( Sha', '( Sha Pereira', 'Sha Pereira ,', 'Pereira , 2003', ', 2003 ;', '2003 ; McDonald', '; McDonald et', 'McDonald et al.', 'et al. ,', 'al. , 2005', ', 2005 ;', '2005 ; Sun', '; Sun et', 'Sun et al.', 'et al. ,', 'al. , 2008', ', 2008 )', '2008 ) [', ') [ 30', '[ 30 ]', '30 ] [', '] [ 31', '[ 31 ]', '31 ] [', '] [ 32', '[ 32 ]', '32 ] ,', '] , reporting', ', reporting around', 'reporting around 94.3', 'around 94.3 %', '94.3 % F1', '% F1 score', 'F1 score .'] 

 TOTAL TRIGRAMS --> 39 



 ---- NOUN PHRASES ---- 

 ['certain number', ']', '%', 'score'] 

 TOTAL NOUN PHRASES --> 4 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['sinc', ',', 'certain', 'number', 'system', 'aris', '(', 'sha', 'pereira', ',', '2003', ';', 'mcdonald', 'et', 'al.', ',', '2005', ';', 'sun', 'et', 'al.', ',', '2008', ')', '[', '30', ']', '[', '31', ']', '[', '32', ']', ',', 'report', 'around', '94.3', '%', 'f1', 'score', '.']

 TOTAL PORTER STEM WORDS ==> 41



 ---- SNOWBALL STEMMING ----

['sinc', ',', 'certain', 'number', 'system', 'aris', '(', 'sha', 'pereira', ',', '2003', ';', 'mcdonald', 'et', 'al.', ',', '2005', ';', 'sun', 'et', 'al.', ',', '2008', ')', '[', '30', ']', '[', '31', ']', '[', '32', ']', ',', 'report', 'around', '94.3', '%', 'f1', 'score', '.']

 TOTAL SNOWBALL STEM WORDS ==> 41



 ---- LEMMATIZATION ----

['Since', ',', 'certain', 'number', 'system', 'arised', '(', 'Sha', 'Pereira', ',', '2003', ';', 'McDonald', 'et', 'al.', ',', '2005', ';', 'Sun', 'et', 'al.', ',', '2008', ')', '[', '30', ']', '[', '31', ']', '[', '32', ']', ',', 'reporting', 'around', '94.3', '%', 'F1', 'score', '.']

 TOTAL LEMMATIZE WORDS ==> 41

************************************************************************************************************************

160 --> These systems use features composed of words, POS tags, and tags. 


 ---- TOKENS ----

 ['These', 'systems', 'use', 'features', 'composed', 'of', 'words', ',', 'POS', 'tags', ',', 'and', 'tags', '.'] 

 TOTAL TOKENS ==> 14

 ---- POST ----

 [('These', 'DT'), ('systems', 'NNS'), ('use', 'VBP'), ('features', 'NNS'), ('composed', 'VBN'), ('of', 'IN'), ('words', 'NNS'), (',', ','), ('POS', 'NNP'), ('tags', 'NN'), (',', ','), ('and', 'CC'), ('tags', 'NNS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['systems', 'use', 'features', 'composed', 'words', ',', 'POS', 'tags', ',', 'tags', '.']

 TOTAL FILTERED TOKENS ==>  11

 ---- POST FOR FILTERED TOKENS ----

 [('systems', 'NNS'), ('use', 'VBP'), ('features', 'NNS'), ('composed', 'VBD'), ('words', 'NNS'), (',', ','), ('POS', 'NNP'), ('tags', 'NN'), (',', ','), ('tags', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['systems use', 'use features', 'features composed', 'composed words', 'words ,', ', POS', 'POS tags', 'tags ,', ', tags', 'tags .'] 

 TOTAL BIGRAMS --> 10 



 ---- TRI-GRAMS ---- 

 ['systems use features', 'use features composed', 'features composed words', 'composed words ,', 'words , POS', ', POS tags', 'POS tags ,', 'tags , tags', ', tags .'] 

 TOTAL TRIGRAMS --> 9 



 ---- NOUN PHRASES ---- 

 ['tags'] 

 TOTAL NOUN PHRASES --> 1 



 ---- NER ----

 
 ORGANIZATION ---> ['POS']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['system', 'use', 'featur', 'compos', 'word', ',', 'po', 'tag', ',', 'tag', '.']

 TOTAL PORTER STEM WORDS ==> 11



 ---- SNOWBALL STEMMING ----

['system', 'use', 'featur', 'compos', 'word', ',', 'pos', 'tag', ',', 'tag', '.']

 TOTAL SNOWBALL STEM WORDS ==> 11



 ---- LEMMATIZATION ----

['system', 'use', 'feature', 'composed', 'word', ',', 'POS', 'tag', ',', 'tag', '.']

 TOTAL LEMMATIZE WORDS ==> 11

************************************************************************************************************************

161 --> Usage of Named Entity Recognition in places such as Internet is a problem as people don’t  use traditional or standard English. 


 ---- TOKENS ----

 ['Usage', 'of', 'Named', 'Entity', 'Recognition', 'in', 'places', 'such', 'as', 'Internet', 'is', 'a', 'problem', 'as', 'people', 'don', '’', 't', 'use', 'traditional', 'or', 'standard', 'English', '.'] 

 TOTAL TOKENS ==> 24

 ---- POST ----

 [('Usage', 'NN'), ('of', 'IN'), ('Named', 'NNP'), ('Entity', 'NNP'), ('Recognition', 'NNP'), ('in', 'IN'), ('places', 'NNS'), ('such', 'JJ'), ('as', 'IN'), ('Internet', 'NNP'), ('is', 'VBZ'), ('a', 'DT'), ('problem', 'NN'), ('as', 'IN'), ('people', 'NNS'), ('don', 'VBP'), ('’', 'JJ'), ('t', 'NN'), ('use', 'VBP'), ('traditional', 'JJ'), ('or', 'CC'), ('standard', 'JJ'), ('English', 'NNP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Usage', 'Named', 'Entity', 'Recognition', 'places', 'Internet', 'problem', 'people', '’', 'use', 'traditional', 'standard', 'English', '.']

 TOTAL FILTERED TOKENS ==>  14

 ---- POST FOR FILTERED TOKENS ----

 [('Usage', 'NN'), ('Named', 'VBN'), ('Entity', 'NNP'), ('Recognition', 'NNP'), ('places', 'NNS'), ('Internet', 'NNP'), ('problem', 'NN'), ('people', 'NNS'), ('’', 'VBP'), ('use', 'JJ'), ('traditional', 'JJ'), ('standard', 'NN'), ('English', 'NNP'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Usage Named', 'Named Entity', 'Entity Recognition', 'Recognition places', 'places Internet', 'Internet problem', 'problem people', 'people ’', '’ use', 'use traditional', 'traditional standard', 'standard English', 'English .'] 

 TOTAL BIGRAMS --> 13 



 ---- TRI-GRAMS ---- 

 ['Usage Named Entity', 'Named Entity Recognition', 'Entity Recognition places', 'Recognition places Internet', 'places Internet problem', 'Internet problem people', 'problem people ’', 'people ’ use', '’ use traditional', 'use traditional standard', 'traditional standard English', 'standard English .'] 

 TOTAL TRIGRAMS --> 12 



 ---- NOUN PHRASES ---- 

 ['Usage', 'problem', 'use traditional standard'] 

 TOTAL NOUN PHRASES --> 3 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> ['Named Entity']
 TOTAL PERSON ENTITY --> 1 


 GPE ---> ['Usage', 'English']
 TOTAL GPE ENTITY --> 2 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['usag', 'name', 'entiti', 'recognit', 'place', 'internet', 'problem', 'peopl', '’', 'use', 'tradit', 'standard', 'english', '.']

 TOTAL PORTER STEM WORDS ==> 14



 ---- SNOWBALL STEMMING ----

['usag', 'name', 'entiti', 'recognit', 'place', 'internet', 'problem', 'peopl', '’', 'use', 'tradit', 'standard', 'english', '.']

 TOTAL SNOWBALL STEM WORDS ==> 14



 ---- LEMMATIZATION ----

['Usage', 'Named', 'Entity', 'Recognition', 'place', 'Internet', 'problem', 'people', '’', 'use', 'traditional', 'standard', 'English', '.']

 TOTAL LEMMATIZE WORDS ==> 14

************************************************************************************************************************

162 --> This degrades the performance of standard natural  language processing tools substantially. 


 ---- TOKENS ----

 ['This', 'degrades', 'the', 'performance', 'of', 'standard', 'natural', 'language', 'processing', 'tools', 'substantially', '.'] 

 TOTAL TOKENS ==> 12

 ---- POST ----

 [('This', 'DT'), ('degrades', 'VBZ'), ('the', 'DT'), ('performance', 'NN'), ('of', 'IN'), ('standard', 'JJ'), ('natural', 'JJ'), ('language', 'NN'), ('processing', 'NN'), ('tools', 'NNS'), ('substantially', 'RB'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['degrades', 'performance', 'standard', 'natural', 'language', 'processing', 'tools', 'substantially', '.']

 TOTAL FILTERED TOKENS ==>  9

 ---- POST FOR FILTERED TOKENS ----

 [('degrades', 'NNS'), ('performance', 'NN'), ('standard', 'JJ'), ('natural', 'JJ'), ('language', 'NN'), ('processing', 'NN'), ('tools', 'NNS'), ('substantially', 'RB'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['degrades performance', 'performance standard', 'standard natural', 'natural language', 'language processing', 'processing tools', 'tools substantially', 'substantially .'] 

 TOTAL BIGRAMS --> 8 



 ---- TRI-GRAMS ---- 

 ['degrades performance standard', 'performance standard natural', 'standard natural language', 'natural language processing', 'language processing tools', 'processing tools substantially', 'tools substantially .'] 

 TOTAL TRIGRAMS --> 7 



 ---- NOUN PHRASES ---- 

 ['performance', 'standard natural language', 'processing'] 

 TOTAL NOUN PHRASES --> 3 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['degrad', 'perform', 'standard', 'natur', 'languag', 'process', 'tool', 'substanti', '.']

 TOTAL PORTER STEM WORDS ==> 9



 ---- SNOWBALL STEMMING ----

['degrad', 'perform', 'standard', 'natur', 'languag', 'process', 'tool', 'substanti', '.']

 TOTAL SNOWBALL STEM WORDS ==> 9



 ---- LEMMATIZATION ----

['degrades', 'performance', 'standard', 'natural', 'language', 'processing', 'tool', 'substantially', '.']

 TOTAL LEMMATIZE WORDS ==> 9

************************************************************************************************************************

163 --> By annotating the phrases or tweets and building  tools trained on unlabelled, in domain and out domain data (Alan Ritter., 2011) [33]. 


 ---- TOKENS ----

 ['By', 'annotating', 'the', 'phrases', 'or', 'tweets', 'and', 'building', 'tools', 'trained', 'on', 'unlabelled', ',', 'in', 'domain', 'and', 'out', 'domain', 'data', '(', 'Alan', 'Ritter.', ',', '2011', ')', '[', '33', ']', '.'] 

 TOTAL TOKENS ==> 29

 ---- POST ----

 [('By', 'IN'), ('annotating', 'VBG'), ('the', 'DT'), ('phrases', 'NNS'), ('or', 'CC'), ('tweets', 'NNS'), ('and', 'CC'), ('building', 'NN'), ('tools', 'NNS'), ('trained', 'VBD'), ('on', 'IN'), ('unlabelled', 'JJ'), (',', ','), ('in', 'IN'), ('domain', 'NN'), ('and', 'CC'), ('out', 'IN'), ('domain', 'NN'), ('data', 'NNS'), ('(', '('), ('Alan', 'NNP'), ('Ritter.', 'NNP'), (',', ','), ('2011', 'CD'), (')', ')'), ('[', 'VBD'), ('33', 'CD'), (']', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['annotating', 'phrases', 'tweets', 'building', 'tools', 'trained', 'unlabelled', ',', 'domain', 'domain', 'data', '(', 'Alan', 'Ritter.', ',', '2011', ')', '[', '33', ']', '.']

 TOTAL FILTERED TOKENS ==>  21

 ---- POST FOR FILTERED TOKENS ----

 [('annotating', 'VBG'), ('phrases', 'NNS'), ('tweets', 'NNS'), ('building', 'VBG'), ('tools', 'NNS'), ('trained', 'VBD'), ('unlabelled', 'JJ'), (',', ','), ('domain', 'VBP'), ('domain', 'NN'), ('data', 'NNS'), ('(', '('), ('Alan', 'NNP'), ('Ritter.', 'NNP'), (',', ','), ('2011', 'CD'), (')', ')'), ('[', 'VBD'), ('33', 'CD'), (']', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['annotating phrases', 'phrases tweets', 'tweets building', 'building tools', 'tools trained', 'trained unlabelled', 'unlabelled ,', ', domain', 'domain domain', 'domain data', 'data (', '( Alan', 'Alan Ritter.', 'Ritter. ,', ', 2011', '2011 )', ') [', '[ 33', '33 ]', '] .'] 

 TOTAL BIGRAMS --> 20 



 ---- TRI-GRAMS ---- 

 ['annotating phrases tweets', 'phrases tweets building', 'tweets building tools', 'building tools trained', 'tools trained unlabelled', 'trained unlabelled ,', 'unlabelled , domain', ', domain domain', 'domain domain data', 'domain data (', 'data ( Alan', '( Alan Ritter.', 'Alan Ritter. ,', 'Ritter. , 2011', ', 2011 )', '2011 ) [', ') [ 33', '[ 33 ]', '33 ] .'] 

 TOTAL TRIGRAMS --> 19 



 ---- NOUN PHRASES ---- 

 ['domain', ']'] 

 TOTAL NOUN PHRASES --> 2 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['annot', 'phrase', 'tweet', 'build', 'tool', 'train', 'unlabel', ',', 'domain', 'domain', 'data', '(', 'alan', 'ritter.', ',', '2011', ')', '[', '33', ']', '.']

 TOTAL PORTER STEM WORDS ==> 21



 ---- SNOWBALL STEMMING ----

['annot', 'phrase', 'tweet', 'build', 'tool', 'train', 'unlabel', ',', 'domain', 'domain', 'data', '(', 'alan', 'ritter.', ',', '2011', ')', '[', '33', ']', '.']

 TOTAL SNOWBALL STEM WORDS ==> 21



 ---- LEMMATIZATION ----

['annotating', 'phrase', 'tweet', 'building', 'tool', 'trained', 'unlabelled', ',', 'domain', 'domain', 'data', '(', 'Alan', 'Ritter.', ',', '2011', ')', '[', '33', ']', '.']

 TOTAL LEMMATIZE WORDS ==> 21

************************************************************************************************************************

164 --> It  improves the performance as compared to standard natural language processing tools. 


 ---- TOKENS ----

 ['It', 'improves', 'the', 'performance', 'as', 'compared', 'to', 'standard', 'natural', 'language', 'processing', 'tools', '.'] 

 TOTAL TOKENS ==> 13

 ---- POST ----

 [('It', 'PRP'), ('improves', 'VBZ'), ('the', 'DT'), ('performance', 'NN'), ('as', 'IN'), ('compared', 'VBN'), ('to', 'TO'), ('standard', 'VB'), ('natural', 'JJ'), ('language', 'NN'), ('processing', 'NN'), ('tools', 'NNS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['improves', 'performance', 'compared', 'standard', 'natural', 'language', 'processing', 'tools', '.']

 TOTAL FILTERED TOKENS ==>  9

 ---- POST FOR FILTERED TOKENS ----

 [('improves', 'NNS'), ('performance', 'NN'), ('compared', 'VBN'), ('standard', 'JJ'), ('natural', 'JJ'), ('language', 'NN'), ('processing', 'NN'), ('tools', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['improves performance', 'performance compared', 'compared standard', 'standard natural', 'natural language', 'language processing', 'processing tools', 'tools .'] 

 TOTAL BIGRAMS --> 8 



 ---- TRI-GRAMS ---- 

 ['improves performance compared', 'performance compared standard', 'compared standard natural', 'standard natural language', 'natural language processing', 'language processing tools', 'processing tools .'] 

 TOTAL TRIGRAMS --> 7 



 ---- NOUN PHRASES ---- 

 ['performance', 'standard natural language', 'processing'] 

 TOTAL NOUN PHRASES --> 3 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['improv', 'perform', 'compar', 'standard', 'natur', 'languag', 'process', 'tool', '.']

 TOTAL PORTER STEM WORDS ==> 9



 ---- SNOWBALL STEMMING ----

['improv', 'perform', 'compar', 'standard', 'natur', 'languag', 'process', 'tool', '.']

 TOTAL SNOWBALL STEM WORDS ==> 9



 ---- LEMMATIZATION ----

['improves', 'performance', 'compared', 'standard', 'natural', 'language', 'processing', 'tool', '.']

 TOTAL LEMMATIZE WORDS ==> 9

************************************************************************************************************************

165 --> Emotion Detection (Shashank Sharma, 2016) [34] is similar to sentiment analysis, but it  works on social media platforms on mixing of two languages (English + Any other Indian  Language). 


 ---- TOKENS ----

 ['Emotion', 'Detection', '(', 'Shashank', 'Sharma', ',', '2016', ')', '[', '34', ']', 'is', 'similar', 'to', 'sentiment', 'analysis', ',', 'but', 'it', 'works', 'on', 'social', 'media', 'platforms', 'on', 'mixing', 'of', 'two', 'languages', '(', 'English', '+', 'Any', 'other', 'Indian', 'Language', ')', '.'] 

 TOTAL TOKENS ==> 38

 ---- POST ----

 [('Emotion', 'NN'), ('Detection', 'NNP'), ('(', '('), ('Shashank', 'NNP'), ('Sharma', 'NNP'), (',', ','), ('2016', 'CD'), (')', ')'), ('[', 'VBD'), ('34', 'CD'), (']', 'NN'), ('is', 'VBZ'), ('similar', 'JJ'), ('to', 'TO'), ('sentiment', 'VB'), ('analysis', 'NN'), (',', ','), ('but', 'CC'), ('it', 'PRP'), ('works', 'VBZ'), ('on', 'IN'), ('social', 'JJ'), ('media', 'NNS'), ('platforms', 'NNS'), ('on', 'IN'), ('mixing', 'NN'), ('of', 'IN'), ('two', 'CD'), ('languages', 'NNS'), ('(', '('), ('English', 'JJ'), ('+', 'VBP'), ('Any', 'DT'), ('other', 'JJ'), ('Indian', 'JJ'), ('Language', 'NNP'), (')', ')'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Emotion', 'Detection', '(', 'Shashank', 'Sharma', ',', '2016', ')', '[', '34', ']', 'similar', 'sentiment', 'analysis', ',', 'works', 'social', 'media', 'platforms', 'mixing', 'two', 'languages', '(', 'English', '+', 'Indian', 'Language', ')', '.']

 TOTAL FILTERED TOKENS ==>  29

 ---- POST FOR FILTERED TOKENS ----

 [('Emotion', 'NN'), ('Detection', 'NNP'), ('(', '('), ('Shashank', 'NNP'), ('Sharma', 'NNP'), (',', ','), ('2016', 'CD'), (')', ')'), ('[', 'VBD'), ('34', 'CD'), (']', 'NNP'), ('similar', 'JJ'), ('sentiment', 'NN'), ('analysis', 'NN'), (',', ','), ('works', 'VBZ'), ('social', 'JJ'), ('media', 'NNS'), ('platforms', 'NNS'), ('mixing', 'VBG'), ('two', 'CD'), ('languages', 'NNS'), ('(', '('), ('English', 'JJ'), ('+', 'NNP'), ('Indian', 'NNP'), ('Language', 'NNP'), (')', ')'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Emotion Detection', 'Detection (', '( Shashank', 'Shashank Sharma', 'Sharma ,', ', 2016', '2016 )', ') [', '[ 34', '34 ]', '] similar', 'similar sentiment', 'sentiment analysis', 'analysis ,', ', works', 'works social', 'social media', 'media platforms', 'platforms mixing', 'mixing two', 'two languages', 'languages (', '( English', 'English +', '+ Indian', 'Indian Language', 'Language )', ') .'] 

 TOTAL BIGRAMS --> 28 



 ---- TRI-GRAMS ---- 

 ['Emotion Detection (', 'Detection ( Shashank', '( Shashank Sharma', 'Shashank Sharma ,', 'Sharma , 2016', ', 2016 )', '2016 ) [', ') [ 34', '[ 34 ]', '34 ] similar', '] similar sentiment', 'similar sentiment analysis', 'sentiment analysis ,', 'analysis , works', ', works social', 'works social media', 'social media platforms', 'media platforms mixing', 'platforms mixing two', 'mixing two languages', 'two languages (', 'languages ( English', '( English +', 'English + Indian', '+ Indian Language', 'Indian Language )', 'Language ) .'] 

 TOTAL TRIGRAMS --> 27 



 ---- NOUN PHRASES ---- 

 ['Emotion', 'similar sentiment', 'analysis'] 

 TOTAL NOUN PHRASES --> 3 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> ['Emotion Detection']
 TOTAL PERSON ENTITY --> 1 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['emot', 'detect', '(', 'shashank', 'sharma', ',', '2016', ')', '[', '34', ']', 'similar', 'sentiment', 'analysi', ',', 'work', 'social', 'media', 'platform', 'mix', 'two', 'languag', '(', 'english', '+', 'indian', 'languag', ')', '.']

 TOTAL PORTER STEM WORDS ==> 29



 ---- SNOWBALL STEMMING ----

['emot', 'detect', '(', 'shashank', 'sharma', ',', '2016', ')', '[', '34', ']', 'similar', 'sentiment', 'analysi', ',', 'work', 'social', 'media', 'platform', 'mix', 'two', 'languag', '(', 'english', '+', 'indian', 'languag', ')', '.']

 TOTAL SNOWBALL STEM WORDS ==> 29



 ---- LEMMATIZATION ----

['Emotion', 'Detection', '(', 'Shashank', 'Sharma', ',', '2016', ')', '[', '34', ']', 'similar', 'sentiment', 'analysis', ',', 'work', 'social', 'medium', 'platform', 'mixing', 'two', 'language', '(', 'English', '+', 'Indian', 'Language', ')', '.']

 TOTAL LEMMATIZE WORDS ==> 29

************************************************************************************************************************

166 --> It categorizes statements into six groups based on emotions. 


 ---- TOKENS ----

 ['It', 'categorizes', 'statements', 'into', 'six', 'groups', 'based', 'on', 'emotions', '.'] 

 TOTAL TOKENS ==> 10

 ---- POST ----

 [('It', 'PRP'), ('categorizes', 'VBZ'), ('statements', 'NNS'), ('into', 'IN'), ('six', 'CD'), ('groups', 'NNS'), ('based', 'VBN'), ('on', 'IN'), ('emotions', 'NNS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['categorizes', 'statements', 'six', 'groups', 'based', 'emotions', '.']

 TOTAL FILTERED TOKENS ==>  7

 ---- POST FOR FILTERED TOKENS ----

 [('categorizes', 'JJ'), ('statements', 'NNS'), ('six', 'CD'), ('groups', 'NNS'), ('based', 'VBN'), ('emotions', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['categorizes statements', 'statements six', 'six groups', 'groups based', 'based emotions', 'emotions .'] 

 TOTAL BIGRAMS --> 6 



 ---- TRI-GRAMS ---- 

 ['categorizes statements six', 'statements six groups', 'six groups based', 'groups based emotions', 'based emotions .'] 

 TOTAL TRIGRAMS --> 5 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['categor', 'statement', 'six', 'group', 'base', 'emot', '.']

 TOTAL PORTER STEM WORDS ==> 7



 ---- SNOWBALL STEMMING ----

['categor', 'statement', 'six', 'group', 'base', 'emot', '.']

 TOTAL SNOWBALL STEM WORDS ==> 7



 ---- LEMMATIZATION ----

['categorizes', 'statement', 'six', 'group', 'based', 'emotion', '.']

 TOTAL LEMMATIZE WORDS ==> 7

************************************************************************************************************************

167 --> During this process,  they were able to identify the language of ambiguous words which were common in Hindi  and English and tag lexical category or parts of speech in mixed script by identifying the base  language of the speaker. 


 ---- TOKENS ----

 ['During', 'this', 'process', ',', 'they', 'were', 'able', 'to', 'identify', 'the', 'language', 'of', 'ambiguous', 'words', 'which', 'were', 'common', 'in', 'Hindi', 'and', 'English', 'and', 'tag', 'lexical', 'category', 'or', 'parts', 'of', 'speech', 'in', 'mixed', 'script', 'by', 'identifying', 'the', 'base', 'language', 'of', 'the', 'speaker', '.'] 

 TOTAL TOKENS ==> 41

 ---- POST ----

 [('During', 'IN'), ('this', 'DT'), ('process', 'NN'), (',', ','), ('they', 'PRP'), ('were', 'VBD'), ('able', 'JJ'), ('to', 'TO'), ('identify', 'VB'), ('the', 'DT'), ('language', 'NN'), ('of', 'IN'), ('ambiguous', 'JJ'), ('words', 'NNS'), ('which', 'WDT'), ('were', 'VBD'), ('common', 'JJ'), ('in', 'IN'), ('Hindi', 'NNP'), ('and', 'CC'), ('English', 'NNP'), ('and', 'CC'), ('tag', 'VB'), ('lexical', 'JJ'), ('category', 'NN'), ('or', 'CC'), ('parts', 'NNS'), ('of', 'IN'), ('speech', 'NN'), ('in', 'IN'), ('mixed', 'JJ'), ('script', 'NN'), ('by', 'IN'), ('identifying', 'VBG'), ('the', 'DT'), ('base', 'JJ'), ('language', 'NN'), ('of', 'IN'), ('the', 'DT'), ('speaker', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['process', ',', 'able', 'identify', 'language', 'ambiguous', 'words', 'common', 'Hindi', 'English', 'tag', 'lexical', 'category', 'parts', 'speech', 'mixed', 'script', 'identifying', 'base', 'language', 'speaker', '.']

 TOTAL FILTERED TOKENS ==>  22

 ---- POST FOR FILTERED TOKENS ----

 [('process', 'NN'), (',', ','), ('able', 'JJ'), ('identify', 'NN'), ('language', 'NN'), ('ambiguous', 'JJ'), ('words', 'NNS'), ('common', 'JJ'), ('Hindi', 'NNP'), ('English', 'NNP'), ('tag', 'VBD'), ('lexical', 'JJ'), ('category', 'NN'), ('parts', 'NNS'), ('speech', 'VBD'), ('mixed', 'JJ'), ('script', 'NN'), ('identifying', 'VBG'), ('base', 'NN'), ('language', 'NN'), ('speaker', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['process ,', ', able', 'able identify', 'identify language', 'language ambiguous', 'ambiguous words', 'words common', 'common Hindi', 'Hindi English', 'English tag', 'tag lexical', 'lexical category', 'category parts', 'parts speech', 'speech mixed', 'mixed script', 'script identifying', 'identifying base', 'base language', 'language speaker', 'speaker .'] 

 TOTAL BIGRAMS --> 21 



 ---- TRI-GRAMS ---- 

 ['process , able', ', able identify', 'able identify language', 'identify language ambiguous', 'language ambiguous words', 'ambiguous words common', 'words common Hindi', 'common Hindi English', 'Hindi English tag', 'English tag lexical', 'tag lexical category', 'lexical category parts', 'category parts speech', 'parts speech mixed', 'speech mixed script', 'mixed script identifying', 'script identifying base', 'identifying base language', 'base language speaker', 'language speaker .'] 

 TOTAL TRIGRAMS --> 20 



 ---- NOUN PHRASES ---- 

 ['process', 'able identify', 'language', 'lexical category', 'mixed script', 'base', 'language', 'speaker'] 

 TOTAL NOUN PHRASES --> 8 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> ['Hindi English']
 TOTAL PERSON ENTITY --> 1 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['process', ',', 'abl', 'identifi', 'languag', 'ambigu', 'word', 'common', 'hindi', 'english', 'tag', 'lexic', 'categori', 'part', 'speech', 'mix', 'script', 'identifi', 'base', 'languag', 'speaker', '.']

 TOTAL PORTER STEM WORDS ==> 22



 ---- SNOWBALL STEMMING ----

['process', ',', 'abl', 'identifi', 'languag', 'ambigu', 'word', 'common', 'hindi', 'english', 'tag', 'lexic', 'categori', 'part', 'speech', 'mix', 'script', 'identifi', 'base', 'languag', 'speaker', '.']

 TOTAL SNOWBALL STEM WORDS ==> 22



 ---- LEMMATIZATION ----

['process', ',', 'able', 'identify', 'language', 'ambiguous', 'word', 'common', 'Hindi', 'English', 'tag', 'lexical', 'category', 'part', 'speech', 'mixed', 'script', 'identifying', 'base', 'language', 'speaker', '.']

 TOTAL LEMMATIZE WORDS ==> 22

************************************************************************************************************************

168 --> Sematic Role Labelling – SRL works by giving a semantic role to a sentence. 


 ---- TOKENS ----

 ['Sematic', 'Role', 'Labelling', '–', 'SRL', 'works', 'by', 'giving', 'a', 'semantic', 'role', 'to', 'a', 'sentence', '.'] 

 TOTAL TOKENS ==> 15

 ---- POST ----

 [('Sematic', 'JJ'), ('Role', 'NNP'), ('Labelling', 'NNP'), ('–', 'NNP'), ('SRL', 'NNP'), ('works', 'NNS'), ('by', 'IN'), ('giving', 'VBG'), ('a', 'DT'), ('semantic', 'JJ'), ('role', 'NN'), ('to', 'TO'), ('a', 'DT'), ('sentence', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Sematic', 'Role', 'Labelling', '–', 'SRL', 'works', 'giving', 'semantic', 'role', 'sentence', '.']

 TOTAL FILTERED TOKENS ==>  11

 ---- POST FOR FILTERED TOKENS ----

 [('Sematic', 'JJ'), ('Role', 'NNP'), ('Labelling', 'NNP'), ('–', 'NNP'), ('SRL', 'NNP'), ('works', 'VBZ'), ('giving', 'VBG'), ('semantic', 'JJ'), ('role', 'NN'), ('sentence', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Sematic Role', 'Role Labelling', 'Labelling –', '– SRL', 'SRL works', 'works giving', 'giving semantic', 'semantic role', 'role sentence', 'sentence .'] 

 TOTAL BIGRAMS --> 10 



 ---- TRI-GRAMS ---- 

 ['Sematic Role Labelling', 'Role Labelling –', 'Labelling – SRL', '– SRL works', 'SRL works giving', 'works giving semantic', 'giving semantic role', 'semantic role sentence', 'role sentence .'] 

 TOTAL TRIGRAMS --> 9 



 ---- NOUN PHRASES ---- 

 ['semantic role', 'sentence'] 

 TOTAL NOUN PHRASES --> 2 



 ---- NER ----

 
 ORGANIZATION ---> ['Role']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> ['Sematic']
 TOTAL PERSON ENTITY --> 1 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['semat', 'role', 'label', '–', 'srl', 'work', 'give', 'semant', 'role', 'sentenc', '.']

 TOTAL PORTER STEM WORDS ==> 11



 ---- SNOWBALL STEMMING ----

['semat', 'role', 'label', '–', 'srl', 'work', 'give', 'semant', 'role', 'sentenc', '.']

 TOTAL SNOWBALL STEM WORDS ==> 11



 ---- LEMMATIZATION ----

['Sematic', 'Role', 'Labelling', '–', 'SRL', 'work', 'giving', 'semantic', 'role', 'sentence', '.']

 TOTAL LEMMATIZE WORDS ==> 11

************************************************************************************************************************

169 --> For example in  the PropBank (Palmer et al., 2005) [35] formalism, one assigns roles to words that are  arguments of a verb in the sentence. 


 ---- TOKENS ----

 ['For', 'example', 'in', 'the', 'PropBank', '(', 'Palmer', 'et', 'al.', ',', '2005', ')', '[', '35', ']', 'formalism', ',', 'one', 'assigns', 'roles', 'to', 'words', 'that', 'are', 'arguments', 'of', 'a', 'verb', 'in', 'the', 'sentence', '.'] 

 TOTAL TOKENS ==> 32

 ---- POST ----

 [('For', 'IN'), ('example', 'NN'), ('in', 'IN'), ('the', 'DT'), ('PropBank', 'NNP'), ('(', '('), ('Palmer', 'NNP'), ('et', 'FW'), ('al.', 'NN'), (',', ','), ('2005', 'CD'), (')', ')'), ('[', 'VBD'), ('35', 'CD'), (']', 'JJ'), ('formalism', 'NN'), (',', ','), ('one', 'CD'), ('assigns', 'NN'), ('roles', 'VBZ'), ('to', 'TO'), ('words', 'NNS'), ('that', 'WDT'), ('are', 'VBP'), ('arguments', 'NNS'), ('of', 'IN'), ('a', 'DT'), ('verb', 'NN'), ('in', 'IN'), ('the', 'DT'), ('sentence', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['example', 'PropBank', '(', 'Palmer', 'et', 'al.', ',', '2005', ')', '[', '35', ']', 'formalism', ',', 'one', 'assigns', 'roles', 'words', 'arguments', 'verb', 'sentence', '.']

 TOTAL FILTERED TOKENS ==>  22

 ---- POST FOR FILTERED TOKENS ----

 [('example', 'NN'), ('PropBank', 'NNP'), ('(', '('), ('Palmer', 'NNP'), ('et', 'FW'), ('al.', 'NN'), (',', ','), ('2005', 'CD'), (')', ')'), ('[', 'VBD'), ('35', 'CD'), (']', 'JJ'), ('formalism', 'NN'), (',', ','), ('one', 'CD'), ('assigns', 'NN'), ('roles', 'VBZ'), ('words', 'NNS'), ('arguments', 'NNS'), ('verb', 'JJ'), ('sentence', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['example PropBank', 'PropBank (', '( Palmer', 'Palmer et', 'et al.', 'al. ,', ', 2005', '2005 )', ') [', '[ 35', '35 ]', '] formalism', 'formalism ,', ', one', 'one assigns', 'assigns roles', 'roles words', 'words arguments', 'arguments verb', 'verb sentence', 'sentence .'] 

 TOTAL BIGRAMS --> 21 



 ---- TRI-GRAMS ---- 

 ['example PropBank (', 'PropBank ( Palmer', '( Palmer et', 'Palmer et al.', 'et al. ,', 'al. , 2005', ', 2005 )', '2005 ) [', ') [ 35', '[ 35 ]', '35 ] formalism', '] formalism ,', 'formalism , one', ', one assigns', 'one assigns roles', 'assigns roles words', 'roles words arguments', 'words arguments verb', 'arguments verb sentence', 'verb sentence .'] 

 TOTAL TRIGRAMS --> 20 



 ---- NOUN PHRASES ---- 

 ['example', '] formalism', 'assigns', 'verb sentence'] 

 TOTAL NOUN PHRASES --> 4 



 ---- NER ----

 
 ORGANIZATION ---> ['PropBank']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['exampl', 'propbank', '(', 'palmer', 'et', 'al.', ',', '2005', ')', '[', '35', ']', 'formal', ',', 'one', 'assign', 'role', 'word', 'argument', 'verb', 'sentenc', '.']

 TOTAL PORTER STEM WORDS ==> 22



 ---- SNOWBALL STEMMING ----

['exampl', 'propbank', '(', 'palmer', 'et', 'al.', ',', '2005', ')', '[', '35', ']', 'formal', ',', 'one', 'assign', 'role', 'word', 'argument', 'verb', 'sentenc', '.']

 TOTAL SNOWBALL STEM WORDS ==> 22



 ---- LEMMATIZATION ----

['example', 'PropBank', '(', 'Palmer', 'et', 'al.', ',', '2005', ')', '[', '35', ']', 'formalism', ',', 'one', 'assigns', 'role', 'word', 'argument', 'verb', 'sentence', '.']

 TOTAL LEMMATIZE WORDS ==> 22

************************************************************************************************************************

170 --> The precise arguments depend on verb frame and if there  exists multiple verbs  in a sentence, it might have multiple tags. 


 ---- TOKENS ----

 ['The', 'precise', 'arguments', 'depend', 'on', 'verb', 'frame', 'and', 'if', 'there', 'exists', 'multiple', 'verbs', 'in', 'a', 'sentence', ',', 'it', 'might', 'have', 'multiple', 'tags', '.'] 

 TOTAL TOKENS ==> 23

 ---- POST ----

 [('The', 'DT'), ('precise', 'JJ'), ('arguments', 'NNS'), ('depend', 'VBP'), ('on', 'IN'), ('verb', 'NN'), ('frame', 'NN'), ('and', 'CC'), ('if', 'IN'), ('there', 'EX'), ('exists', 'VBZ'), ('multiple', 'JJ'), ('verbs', 'NNS'), ('in', 'IN'), ('a', 'DT'), ('sentence', 'NN'), (',', ','), ('it', 'PRP'), ('might', 'MD'), ('have', 'VB'), ('multiple', 'JJ'), ('tags', 'NNS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['precise', 'arguments', 'depend', 'verb', 'frame', 'exists', 'multiple', 'verbs', 'sentence', ',', 'might', 'multiple', 'tags', '.']

 TOTAL FILTERED TOKENS ==>  14

 ---- POST FOR FILTERED TOKENS ----

 [('precise', 'JJ'), ('arguments', 'NNS'), ('depend', 'VBP'), ('verb', 'NN'), ('frame', 'NN'), ('exists', 'VBZ'), ('multiple', 'JJ'), ('verbs', 'JJ'), ('sentence', 'NN'), (',', ','), ('might', 'MD'), ('multiple', 'VB'), ('tags', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['precise arguments', 'arguments depend', 'depend verb', 'verb frame', 'frame exists', 'exists multiple', 'multiple verbs', 'verbs sentence', 'sentence ,', ', might', 'might multiple', 'multiple tags', 'tags .'] 

 TOTAL BIGRAMS --> 13 



 ---- TRI-GRAMS ---- 

 ['precise arguments depend', 'arguments depend verb', 'depend verb frame', 'verb frame exists', 'frame exists multiple', 'exists multiple verbs', 'multiple verbs sentence', 'verbs sentence ,', 'sentence , might', ', might multiple', 'might multiple tags', 'multiple tags .'] 

 TOTAL TRIGRAMS --> 12 



 ---- NOUN PHRASES ---- 

 ['verb', 'frame', 'multiple verbs sentence'] 

 TOTAL NOUN PHRASES --> 3 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['precis', 'argument', 'depend', 'verb', 'frame', 'exist', 'multipl', 'verb', 'sentenc', ',', 'might', 'multipl', 'tag', '.']

 TOTAL PORTER STEM WORDS ==> 14



 ---- SNOWBALL STEMMING ----

['precis', 'argument', 'depend', 'verb', 'frame', 'exist', 'multipl', 'verb', 'sentenc', ',', 'might', 'multipl', 'tag', '.']

 TOTAL SNOWBALL STEM WORDS ==> 14



 ---- LEMMATIZATION ----

['precise', 'argument', 'depend', 'verb', 'frame', 'exists', 'multiple', 'verb', 'sentence', ',', 'might', 'multiple', 'tag', '.']

 TOTAL LEMMATIZE WORDS ==> 14

************************************************************************************************************************

171 --> State-of-the-art SRL systems  comprise of several stages: creating a parse tree, identifying which parse tree nodes represent  the arguments of a given verb, and finally classifying these nodes to compute the  corresponding SRL tags. 


 ---- TOKENS ----

 ['State-of-the-art', 'SRL', 'systems', 'comprise', 'of', 'several', 'stages', ':', 'creating', 'a', 'parse', 'tree', ',', 'identifying', 'which', 'parse', 'tree', 'nodes', 'represent', 'the', 'arguments', 'of', 'a', 'given', 'verb', ',', 'and', 'finally', 'classifying', 'these', 'nodes', 'to', 'compute', 'the', 'corresponding', 'SRL', 'tags', '.'] 

 TOTAL TOKENS ==> 38

 ---- POST ----

 [('State-of-the-art', 'JJ'), ('SRL', 'NNP'), ('systems', 'NNS'), ('comprise', 'NN'), ('of', 'IN'), ('several', 'JJ'), ('stages', 'NNS'), (':', ':'), ('creating', 'VBG'), ('a', 'DT'), ('parse', 'NN'), ('tree', 'NN'), (',', ','), ('identifying', 'VBG'), ('which', 'WDT'), ('parse', 'VBP'), ('tree', 'NN'), ('nodes', 'NNS'), ('represent', 'VBP'), ('the', 'DT'), ('arguments', 'NNS'), ('of', 'IN'), ('a', 'DT'), ('given', 'VBN'), ('verb', 'NN'), (',', ','), ('and', 'CC'), ('finally', 'RB'), ('classifying', 'VBG'), ('these', 'DT'), ('nodes', 'NNS'), ('to', 'TO'), ('compute', 'VB'), ('the', 'DT'), ('corresponding', 'JJ'), ('SRL', 'NNP'), ('tags', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['State-of-the-art', 'SRL', 'systems', 'comprise', 'several', 'stages', ':', 'creating', 'parse', 'tree', ',', 'identifying', 'parse', 'tree', 'nodes', 'represent', 'arguments', 'given', 'verb', ',', 'finally', 'classifying', 'nodes', 'compute', 'corresponding', 'SRL', 'tags', '.']

 TOTAL FILTERED TOKENS ==>  28

 ---- POST FOR FILTERED TOKENS ----

 [('State-of-the-art', 'JJ'), ('SRL', 'NNP'), ('systems', 'NNS'), ('comprise', 'VBD'), ('several', 'JJ'), ('stages', 'NNS'), (':', ':'), ('creating', 'VBG'), ('parse', 'NN'), ('tree', 'NN'), (',', ','), ('identifying', 'VBG'), ('parse', 'NN'), ('tree', 'NN'), ('nodes', 'NNS'), ('represent', 'VBP'), ('arguments', 'NNS'), ('given', 'VBN'), ('verb', 'NNS'), (',', ','), ('finally', 'RB'), ('classifying', 'VBG'), ('nodes', 'NNS'), ('compute', 'VBP'), ('corresponding', 'VBG'), ('SRL', 'NNP'), ('tags', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['State-of-the-art SRL', 'SRL systems', 'systems comprise', 'comprise several', 'several stages', 'stages :', ': creating', 'creating parse', 'parse tree', 'tree ,', ', identifying', 'identifying parse', 'parse tree', 'tree nodes', 'nodes represent', 'represent arguments', 'arguments given', 'given verb', 'verb ,', ', finally', 'finally classifying', 'classifying nodes', 'nodes compute', 'compute corresponding', 'corresponding SRL', 'SRL tags', 'tags .'] 

 TOTAL BIGRAMS --> 27 



 ---- TRI-GRAMS ---- 

 ['State-of-the-art SRL systems', 'SRL systems comprise', 'systems comprise several', 'comprise several stages', 'several stages :', 'stages : creating', ': creating parse', 'creating parse tree', 'parse tree ,', 'tree , identifying', ', identifying parse', 'identifying parse tree', 'parse tree nodes', 'tree nodes represent', 'nodes represent arguments', 'represent arguments given', 'arguments given verb', 'given verb ,', 'verb , finally', ', finally classifying', 'finally classifying nodes', 'classifying nodes compute', 'nodes compute corresponding', 'compute corresponding SRL', 'corresponding SRL tags', 'SRL tags .'] 

 TOTAL TRIGRAMS --> 26 



 ---- NOUN PHRASES ---- 

 ['parse', 'tree', 'parse', 'tree'] 

 TOTAL NOUN PHRASES --> 4 



 ---- NER ----

 
 ORGANIZATION ---> ['SRL', 'SRL']
 TOTAL ORGANIZATION ENTITY --> 2 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['state-of-the-art', 'srl', 'system', 'compris', 'sever', 'stage', ':', 'creat', 'pars', 'tree', ',', 'identifi', 'pars', 'tree', 'node', 'repres', 'argument', 'given', 'verb', ',', 'final', 'classifi', 'node', 'comput', 'correspond', 'srl', 'tag', '.']

 TOTAL PORTER STEM WORDS ==> 28



 ---- SNOWBALL STEMMING ----

['state-of-the-art', 'srl', 'system', 'compris', 'sever', 'stage', ':', 'creat', 'pars', 'tree', ',', 'identifi', 'pars', 'tree', 'node', 'repres', 'argument', 'given', 'verb', ',', 'final', 'classifi', 'node', 'comput', 'correspond', 'srl', 'tag', '.']

 TOTAL SNOWBALL STEM WORDS ==> 28



 ---- LEMMATIZATION ----

['State-of-the-art', 'SRL', 'system', 'comprise', 'several', 'stage', ':', 'creating', 'parse', 'tree', ',', 'identifying', 'parse', 'tree', 'node', 'represent', 'argument', 'given', 'verb', ',', 'finally', 'classifying', 'node', 'compute', 'corresponding', 'SRL', 'tag', '.']

 TOTAL LEMMATIZE WORDS ==> 28

************************************************************************************************************************

172 --> Event discovery in social media feeds (Edward Benson et al.,2011) [36], using a graphical  model to analyse any social media feeds to determine whether it contains name of a person or  name of a venue, place, time etc. 


 ---- TOKENS ----

 ['Event', 'discovery', 'in', 'social', 'media', 'feeds', '(', 'Edward', 'Benson', 'et', 'al.,2011', ')', '[', '36', ']', ',', 'using', 'a', 'graphical', 'model', 'to', 'analyse', 'any', 'social', 'media', 'feeds', 'to', 'determine', 'whether', 'it', 'contains', 'name', 'of', 'a', 'person', 'or', 'name', 'of', 'a', 'venue', ',', 'place', ',', 'time', 'etc', '.'] 

 TOTAL TOKENS ==> 46

 ---- POST ----

 [('Event', 'JJ'), ('discovery', 'NN'), ('in', 'IN'), ('social', 'JJ'), ('media', 'NNS'), ('feeds', 'NNS'), ('(', '('), ('Edward', 'NNP'), ('Benson', 'NNP'), ('et', 'FW'), ('al.,2011', 'NN'), (')', ')'), ('[', 'VBZ'), ('36', 'CD'), (']', 'NN'), (',', ','), ('using', 'VBG'), ('a', 'DT'), ('graphical', 'JJ'), ('model', 'NN'), ('to', 'TO'), ('analyse', 'VB'), ('any', 'DT'), ('social', 'JJ'), ('media', 'NNS'), ('feeds', 'NNS'), ('to', 'TO'), ('determine', 'VB'), ('whether', 'IN'), ('it', 'PRP'), ('contains', 'VBZ'), ('name', 'NN'), ('of', 'IN'), ('a', 'DT'), ('person', 'NN'), ('or', 'CC'), ('name', 'NN'), ('of', 'IN'), ('a', 'DT'), ('venue', 'NN'), (',', ','), ('place', 'NN'), (',', ','), ('time', 'NN'), ('etc', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Event', 'discovery', 'social', 'media', 'feeds', '(', 'Edward', 'Benson', 'et', 'al.,2011', ')', '[', '36', ']', ',', 'using', 'graphical', 'model', 'analyse', 'social', 'media', 'feeds', 'determine', 'whether', 'contains', 'name', 'person', 'name', 'venue', ',', 'place', ',', 'time', 'etc', '.']

 TOTAL FILTERED TOKENS ==>  35

 ---- POST FOR FILTERED TOKENS ----

 [('Event', 'JJ'), ('discovery', 'NN'), ('social', 'JJ'), ('media', 'NNS'), ('feeds', 'NNS'), ('(', '('), ('Edward', 'NNP'), ('Benson', 'NNP'), ('et', 'FW'), ('al.,2011', 'NN'), (')', ')'), ('[', 'VBZ'), ('36', 'CD'), (']', 'NN'), (',', ','), ('using', 'VBG'), ('graphical', 'JJ'), ('model', 'NN'), ('analyse', 'JJ'), ('social', 'JJ'), ('media', 'NNS'), ('feeds', 'NNS'), ('determine', 'VBP'), ('whether', 'IN'), ('contains', 'NNS'), ('name', 'JJ'), ('person', 'NN'), ('name', 'NN'), ('venue', 'NN'), (',', ','), ('place', 'NN'), (',', ','), ('time', 'NN'), ('etc', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Event discovery', 'discovery social', 'social media', 'media feeds', 'feeds (', '( Edward', 'Edward Benson', 'Benson et', 'et al.,2011', 'al.,2011 )', ') [', '[ 36', '36 ]', '] ,', ', using', 'using graphical', 'graphical model', 'model analyse', 'analyse social', 'social media', 'media feeds', 'feeds determine', 'determine whether', 'whether contains', 'contains name', 'name person', 'person name', 'name venue', 'venue ,', ', place', 'place ,', ', time', 'time etc', 'etc .'] 

 TOTAL BIGRAMS --> 34 



 ---- TRI-GRAMS ---- 

 ['Event discovery social', 'discovery social media', 'social media feeds', 'media feeds (', 'feeds ( Edward', '( Edward Benson', 'Edward Benson et', 'Benson et al.,2011', 'et al.,2011 )', 'al.,2011 ) [', ') [ 36', '[ 36 ]', '36 ] ,', '] , using', ', using graphical', 'using graphical model', 'graphical model analyse', 'model analyse social', 'analyse social media', 'social media feeds', 'media feeds determine', 'feeds determine whether', 'determine whether contains', 'whether contains name', 'contains name person', 'name person name', 'person name venue', 'name venue ,', 'venue , place', ', place ,', 'place , time', ', time etc', 'time etc .'] 

 TOTAL TRIGRAMS --> 33 



 ---- NOUN PHRASES ---- 

 ['Event discovery', ']', 'graphical model', 'name person', 'name', 'venue', 'place', 'time', 'etc'] 

 TOTAL NOUN PHRASES --> 9 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['event', 'discoveri', 'social', 'media', 'feed', '(', 'edward', 'benson', 'et', 'al.,2011', ')', '[', '36', ']', ',', 'use', 'graphic', 'model', 'analys', 'social', 'media', 'feed', 'determin', 'whether', 'contain', 'name', 'person', 'name', 'venu', ',', 'place', ',', 'time', 'etc', '.']

 TOTAL PORTER STEM WORDS ==> 35



 ---- SNOWBALL STEMMING ----

['event', 'discoveri', 'social', 'media', 'feed', '(', 'edward', 'benson', 'et', 'al.,2011', ')', '[', '36', ']', ',', 'use', 'graphic', 'model', 'analys', 'social', 'media', 'feed', 'determin', 'whether', 'contain', 'name', 'person', 'name', 'venu', ',', 'place', ',', 'time', 'etc', '.']

 TOTAL SNOWBALL STEM WORDS ==> 35



 ---- LEMMATIZATION ----

['Event', 'discovery', 'social', 'medium', 'feed', '(', 'Edward', 'Benson', 'et', 'al.,2011', ')', '[', '36', ']', ',', 'using', 'graphical', 'model', 'analyse', 'social', 'medium', 'feed', 'determine', 'whether', 'contains', 'name', 'person', 'name', 'venue', ',', 'place', ',', 'time', 'etc', '.']

 TOTAL LEMMATIZE WORDS ==> 35

************************************************************************************************************************

173 --> The model operates on noisy feeds of data to extract records  of events by aggregating multiple information across multiple messages, despite the noise of  irrelevant noisy messages and very irregular message language, this model was able to extract  records with high accuracy. 


 ---- TOKENS ----

 ['The', 'model', 'operates', 'on', 'noisy', 'feeds', 'of', 'data', 'to', 'extract', 'records', 'of', 'events', 'by', 'aggregating', 'multiple', 'information', 'across', 'multiple', 'messages', ',', 'despite', 'the', 'noise', 'of', 'irrelevant', 'noisy', 'messages', 'and', 'very', 'irregular', 'message', 'language', ',', 'this', 'model', 'was', 'able', 'to', 'extract', 'records', 'with', 'high', 'accuracy', '.'] 

 TOTAL TOKENS ==> 45

 ---- POST ----

 [('The', 'DT'), ('model', 'NN'), ('operates', 'VBZ'), ('on', 'IN'), ('noisy', 'JJ'), ('feeds', 'NNS'), ('of', 'IN'), ('data', 'NNS'), ('to', 'TO'), ('extract', 'VB'), ('records', 'NNS'), ('of', 'IN'), ('events', 'NNS'), ('by', 'IN'), ('aggregating', 'VBG'), ('multiple', 'JJ'), ('information', 'NN'), ('across', 'IN'), ('multiple', 'JJ'), ('messages', 'NNS'), (',', ','), ('despite', 'IN'), ('the', 'DT'), ('noise', 'NN'), ('of', 'IN'), ('irrelevant', 'JJ'), ('noisy', 'JJ'), ('messages', 'NNS'), ('and', 'CC'), ('very', 'RB'), ('irregular', 'JJ'), ('message', 'NN'), ('language', 'NN'), (',', ','), ('this', 'DT'), ('model', 'NN'), ('was', 'VBD'), ('able', 'JJ'), ('to', 'TO'), ('extract', 'VB'), ('records', 'NNS'), ('with', 'IN'), ('high', 'JJ'), ('accuracy', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['model', 'operates', 'noisy', 'feeds', 'data', 'extract', 'records', 'events', 'aggregating', 'multiple', 'information', 'across', 'multiple', 'messages', ',', 'despite', 'noise', 'irrelevant', 'noisy', 'messages', 'irregular', 'message', 'language', ',', 'model', 'able', 'extract', 'records', 'high', 'accuracy', '.']

 TOTAL FILTERED TOKENS ==>  31

 ---- POST FOR FILTERED TOKENS ----

 [('model', 'NN'), ('operates', 'VBZ'), ('noisy', 'JJ'), ('feeds', 'NNS'), ('data', 'NNS'), ('extract', 'NN'), ('records', 'NNS'), ('events', 'NNS'), ('aggregating', 'VBG'), ('multiple', 'JJ'), ('information', 'NN'), ('across', 'IN'), ('multiple', 'JJ'), ('messages', 'NNS'), (',', ','), ('despite', 'IN'), ('noise', 'NN'), ('irrelevant', 'JJ'), ('noisy', 'NN'), ('messages', 'NNS'), ('irregular', 'JJ'), ('message', 'NN'), ('language', 'NN'), (',', ','), ('model', 'NN'), ('able', 'JJ'), ('extract', 'NN'), ('records', 'NNS'), ('high', 'JJ'), ('accuracy', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['model operates', 'operates noisy', 'noisy feeds', 'feeds data', 'data extract', 'extract records', 'records events', 'events aggregating', 'aggregating multiple', 'multiple information', 'information across', 'across multiple', 'multiple messages', 'messages ,', ', despite', 'despite noise', 'noise irrelevant', 'irrelevant noisy', 'noisy messages', 'messages irregular', 'irregular message', 'message language', 'language ,', ', model', 'model able', 'able extract', 'extract records', 'records high', 'high accuracy', 'accuracy .'] 

 TOTAL BIGRAMS --> 30 



 ---- TRI-GRAMS ---- 

 ['model operates noisy', 'operates noisy feeds', 'noisy feeds data', 'feeds data extract', 'data extract records', 'extract records events', 'records events aggregating', 'events aggregating multiple', 'aggregating multiple information', 'multiple information across', 'information across multiple', 'across multiple messages', 'multiple messages ,', 'messages , despite', ', despite noise', 'despite noise irrelevant', 'noise irrelevant noisy', 'irrelevant noisy messages', 'noisy messages irregular', 'messages irregular message', 'irregular message language', 'message language ,', 'language , model', ', model able', 'model able extract', 'able extract records', 'extract records high', 'records high accuracy', 'high accuracy .'] 

 TOTAL TRIGRAMS --> 29 



 ---- NOUN PHRASES ---- 

 ['model', 'extract', 'multiple information', 'noise', 'irrelevant noisy', 'irregular message', 'language', 'model', 'able extract', 'high accuracy'] 

 TOTAL NOUN PHRASES --> 10 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['model', 'oper', 'noisi', 'feed', 'data', 'extract', 'record', 'event', 'aggreg', 'multipl', 'inform', 'across', 'multipl', 'messag', ',', 'despit', 'nois', 'irrelev', 'noisi', 'messag', 'irregular', 'messag', 'languag', ',', 'model', 'abl', 'extract', 'record', 'high', 'accuraci', '.']

 TOTAL PORTER STEM WORDS ==> 31



 ---- SNOWBALL STEMMING ----

['model', 'oper', 'noisi', 'feed', 'data', 'extract', 'record', 'event', 'aggreg', 'multipl', 'inform', 'across', 'multipl', 'messag', ',', 'despit', 'nois', 'irrelev', 'noisi', 'messag', 'irregular', 'messag', 'languag', ',', 'model', 'abl', 'extract', 'record', 'high', 'accuraci', '.']

 TOTAL SNOWBALL STEM WORDS ==> 31



 ---- LEMMATIZATION ----

['model', 'operates', 'noisy', 'feed', 'data', 'extract', 'record', 'event', 'aggregating', 'multiple', 'information', 'across', 'multiple', 'message', ',', 'despite', 'noise', 'irrelevant', 'noisy', 'message', 'irregular', 'message', 'language', ',', 'model', 'able', 'extract', 'record', 'high', 'accuracy', '.']

 TOTAL LEMMATIZE WORDS ==> 31

************************************************************************************************************************

174 --> However, there is some scope for improvement using broader  array of features on factors. 


 ---- TOKENS ----

 ['However', ',', 'there', 'is', 'some', 'scope', 'for', 'improvement', 'using', 'broader', 'array', 'of', 'features', 'on', 'factors', '.'] 

 TOTAL TOKENS ==> 16

 ---- POST ----

 [('However', 'RB'), (',', ','), ('there', 'EX'), ('is', 'VBZ'), ('some', 'DT'), ('scope', 'NN'), ('for', 'IN'), ('improvement', 'NN'), ('using', 'VBG'), ('broader', 'JJR'), ('array', 'NN'), ('of', 'IN'), ('features', 'NNS'), ('on', 'IN'), ('factors', 'NNS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['However', ',', 'scope', 'improvement', 'using', 'broader', 'array', 'features', 'factors', '.']

 TOTAL FILTERED TOKENS ==>  10

 ---- POST FOR FILTERED TOKENS ----

 [('However', 'RB'), (',', ','), ('scope', 'VBP'), ('improvement', 'NN'), ('using', 'VBG'), ('broader', 'JJR'), ('array', 'NN'), ('features', 'VBZ'), ('factors', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['However ,', ', scope', 'scope improvement', 'improvement using', 'using broader', 'broader array', 'array features', 'features factors', 'factors .'] 

 TOTAL BIGRAMS --> 9 



 ---- TRI-GRAMS ---- 

 ['However , scope', ', scope improvement', 'scope improvement using', 'improvement using broader', 'using broader array', 'broader array features', 'array features factors', 'features factors .'] 

 TOTAL TRIGRAMS --> 8 



 ---- NOUN PHRASES ---- 

 ['improvement', 'array'] 

 TOTAL NOUN PHRASES --> 2 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['howev', ',', 'scope', 'improv', 'use', 'broader', 'array', 'featur', 'factor', '.']

 TOTAL PORTER STEM WORDS ==> 10



 ---- SNOWBALL STEMMING ----

['howev', ',', 'scope', 'improv', 'use', 'broader', 'array', 'featur', 'factor', '.']

 TOTAL SNOWBALL STEM WORDS ==> 10



 ---- LEMMATIZATION ----

['However', ',', 'scope', 'improvement', 'using', 'broader', 'array', 'feature', 'factor', '.']

 TOTAL LEMMATIZE WORDS ==> 10

************************************************************************************************************************

175 --> 6. 


 ---- TOKENS ----

 ['6', '.'] 

 TOTAL TOKENS ==> 2

 ---- POST ----

 [('6', 'CD'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['6', '.']

 TOTAL FILTERED TOKENS ==>  2

 ---- POST FOR FILTERED TOKENS ----

 [('6', 'CD'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['6 .'] 

 TOTAL BIGRAMS --> 1 



 ---- TRI-GRAMS ---- 

 [] 

 TOTAL TRIGRAMS --> 0 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['6', '.']

 TOTAL PORTER STEM WORDS ==> 2



 ---- SNOWBALL STEMMING ----

['6', '.']

 TOTAL SNOWBALL STEM WORDS ==> 2



 ---- LEMMATIZATION ----

['6', '.']

 TOTAL LEMMATIZE WORDS ==> 2

************************************************************************************************************************

176 --> Applications of NLP  Natural Language Processing can be applied into various areas like Machine Translation,  Email Spam detection, Information Extraction, Summarization, Question Answering etc. 


 ---- TOKENS ----

 ['Applications', 'of', 'NLP', 'Natural', 'Language', 'Processing', 'can', 'be', 'applied', 'into', 'various', 'areas', 'like', 'Machine', 'Translation', ',', 'Email', 'Spam', 'detection', ',', 'Information', 'Extraction', ',', 'Summarization', ',', 'Question', 'Answering', 'etc', '.'] 

 TOTAL TOKENS ==> 29

 ---- POST ----

 [('Applications', 'NNS'), ('of', 'IN'), ('NLP', 'NNP'), ('Natural', 'NNP'), ('Language', 'NNP'), ('Processing', 'NNP'), ('can', 'MD'), ('be', 'VB'), ('applied', 'VBN'), ('into', 'IN'), ('various', 'JJ'), ('areas', 'NNS'), ('like', 'IN'), ('Machine', 'NNP'), ('Translation', 'NNP'), (',', ','), ('Email', 'NNP'), ('Spam', 'NNP'), ('detection', 'NN'), (',', ','), ('Information', 'NNP'), ('Extraction', 'NNP'), (',', ','), ('Summarization', 'NNP'), (',', ','), ('Question', 'NNP'), ('Answering', 'NNP'), ('etc', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Applications', 'NLP', 'Natural', 'Language', 'Processing', 'applied', 'various', 'areas', 'like', 'Machine', 'Translation', ',', 'Email', 'Spam', 'detection', ',', 'Information', 'Extraction', ',', 'Summarization', ',', 'Question', 'Answering', 'etc', '.']

 TOTAL FILTERED TOKENS ==>  25

 ---- POST FOR FILTERED TOKENS ----

 [('Applications', 'NNS'), ('NLP', 'NNP'), ('Natural', 'NNP'), ('Language', 'NNP'), ('Processing', 'NNP'), ('applied', 'VBD'), ('various', 'JJ'), ('areas', 'NNS'), ('like', 'IN'), ('Machine', 'NNP'), ('Translation', 'NNP'), (',', ','), ('Email', 'NNP'), ('Spam', 'NNP'), ('detection', 'NN'), (',', ','), ('Information', 'NNP'), ('Extraction', 'NNP'), (',', ','), ('Summarization', 'NNP'), (',', ','), ('Question', 'NNP'), ('Answering', 'NNP'), ('etc', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Applications NLP', 'NLP Natural', 'Natural Language', 'Language Processing', 'Processing applied', 'applied various', 'various areas', 'areas like', 'like Machine', 'Machine Translation', 'Translation ,', ', Email', 'Email Spam', 'Spam detection', 'detection ,', ', Information', 'Information Extraction', 'Extraction ,', ', Summarization', 'Summarization ,', ', Question', 'Question Answering', 'Answering etc', 'etc .'] 

 TOTAL BIGRAMS --> 24 



 ---- TRI-GRAMS ---- 

 ['Applications NLP Natural', 'NLP Natural Language', 'Natural Language Processing', 'Language Processing applied', 'Processing applied various', 'applied various areas', 'various areas like', 'areas like Machine', 'like Machine Translation', 'Machine Translation ,', 'Translation , Email', ', Email Spam', 'Email Spam detection', 'Spam detection ,', 'detection , Information', ', Information Extraction', 'Information Extraction ,', 'Extraction , Summarization', ', Summarization ,', 'Summarization , Question', ', Question Answering', 'Question Answering etc', 'Answering etc .'] 

 TOTAL TRIGRAMS --> 23 



 ---- NOUN PHRASES ---- 

 ['detection', 'etc'] 

 TOTAL NOUN PHRASES --> 2 



 ---- NER ----

 
 ORGANIZATION ---> ['NLP Natural Language', 'Information Extraction']
 TOTAL ORGANIZATION ENTITY --> 2 


 PERSON ---> ['Machine Translation', 'Email Spam', 'Question Answering']
 TOTAL PERSON ENTITY --> 3 


 GPE ---> ['Summarization']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['applic', 'nlp', 'natur', 'languag', 'process', 'appli', 'variou', 'area', 'like', 'machin', 'translat', ',', 'email', 'spam', 'detect', ',', 'inform', 'extract', ',', 'summar', ',', 'question', 'answer', 'etc', '.']

 TOTAL PORTER STEM WORDS ==> 25



 ---- SNOWBALL STEMMING ----

['applic', 'nlp', 'natur', 'languag', 'process', 'appli', 'various', 'area', 'like', 'machin', 'translat', ',', 'email', 'spam', 'detect', ',', 'inform', 'extract', ',', 'summar', ',', 'question', 'answer', 'etc', '.']

 TOTAL SNOWBALL STEM WORDS ==> 25



 ---- LEMMATIZATION ----

['Applications', 'NLP', 'Natural', 'Language', 'Processing', 'applied', 'various', 'area', 'like', 'Machine', 'Translation', ',', 'Email', 'Spam', 'detection', ',', 'Information', 'Extraction', ',', 'Summarization', ',', 'Question', 'Answering', 'etc', '.']

 TOTAL LEMMATIZE WORDS ==> 25

************************************************************************************************************************

177 --> 6.1 Machine Translation   As most of the world is online, the task of making data accessible and available to all is a  challenge. 


 ---- TOKENS ----

 ['6.1', 'Machine', 'Translation', 'As', 'most', 'of', 'the', 'world', 'is', 'online', ',', 'the', 'task', 'of', 'making', 'data', 'accessible', 'and', 'available', 'to', 'all', 'is', 'a', 'challenge', '.'] 

 TOTAL TOKENS ==> 25

 ---- POST ----

 [('6.1', 'CD'), ('Machine', 'NNP'), ('Translation', 'NNP'), ('As', 'IN'), ('most', 'JJS'), ('of', 'IN'), ('the', 'DT'), ('world', 'NN'), ('is', 'VBZ'), ('online', 'JJ'), (',', ','), ('the', 'DT'), ('task', 'NN'), ('of', 'IN'), ('making', 'VBG'), ('data', 'NNS'), ('accessible', 'JJ'), ('and', 'CC'), ('available', 'JJ'), ('to', 'TO'), ('all', 'DT'), ('is', 'VBZ'), ('a', 'DT'), ('challenge', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['6.1', 'Machine', 'Translation', 'world', 'online', ',', 'task', 'making', 'data', 'accessible', 'available', 'challenge', '.']

 TOTAL FILTERED TOKENS ==>  13

 ---- POST FOR FILTERED TOKENS ----

 [('6.1', 'CD'), ('Machine', 'NNP'), ('Translation', 'NNP'), ('world', 'NN'), ('online', 'NN'), (',', ','), ('task', 'NN'), ('making', 'VBG'), ('data', 'NNS'), ('accessible', 'JJ'), ('available', 'JJ'), ('challenge', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['6.1 Machine', 'Machine Translation', 'Translation world', 'world online', 'online ,', ', task', 'task making', 'making data', 'data accessible', 'accessible available', 'available challenge', 'challenge .'] 

 TOTAL BIGRAMS --> 12 



 ---- TRI-GRAMS ---- 

 ['6.1 Machine Translation', 'Machine Translation world', 'Translation world online', 'world online ,', 'online , task', ', task making', 'task making data', 'making data accessible', 'data accessible available', 'accessible available challenge', 'available challenge .'] 

 TOTAL TRIGRAMS --> 11 



 ---- NOUN PHRASES ---- 

 ['world', 'online', 'task', 'accessible available challenge'] 

 TOTAL NOUN PHRASES --> 4 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> ['Machine Translation']
 TOTAL PERSON ENTITY --> 1 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['6.1', 'machin', 'translat', 'world', 'onlin', ',', 'task', 'make', 'data', 'access', 'avail', 'challeng', '.']

 TOTAL PORTER STEM WORDS ==> 13



 ---- SNOWBALL STEMMING ----

['6.1', 'machin', 'translat', 'world', 'onlin', ',', 'task', 'make', 'data', 'access', 'avail', 'challeng', '.']

 TOTAL SNOWBALL STEM WORDS ==> 13



 ---- LEMMATIZATION ----

['6.1', 'Machine', 'Translation', 'world', 'online', ',', 'task', 'making', 'data', 'accessible', 'available', 'challenge', '.']

 TOTAL LEMMATIZE WORDS ==> 13

************************************************************************************************************************

178 --> Major challenge in making data accessible is the language barrier. 


 ---- TOKENS ----

 ['Major', 'challenge', 'in', 'making', 'data', 'accessible', 'is', 'the', 'language', 'barrier', '.'] 

 TOTAL TOKENS ==> 11

 ---- POST ----

 [('Major', 'JJ'), ('challenge', 'NN'), ('in', 'IN'), ('making', 'VBG'), ('data', 'NNS'), ('accessible', 'JJ'), ('is', 'VBZ'), ('the', 'DT'), ('language', 'NN'), ('barrier', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Major', 'challenge', 'making', 'data', 'accessible', 'language', 'barrier', '.']

 TOTAL FILTERED TOKENS ==>  8

 ---- POST FOR FILTERED TOKENS ----

 [('Major', 'JJ'), ('challenge', 'NN'), ('making', 'VBG'), ('data', 'NNS'), ('accessible', 'JJ'), ('language', 'NN'), ('barrier', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Major challenge', 'challenge making', 'making data', 'data accessible', 'accessible language', 'language barrier', 'barrier .'] 

 TOTAL BIGRAMS --> 7 



 ---- TRI-GRAMS ---- 

 ['Major challenge making', 'challenge making data', 'making data accessible', 'data accessible language', 'accessible language barrier', 'language barrier .'] 

 TOTAL TRIGRAMS --> 6 



 ---- NOUN PHRASES ---- 

 ['Major challenge', 'accessible language', 'barrier'] 

 TOTAL NOUN PHRASES --> 3 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['major', 'challeng', 'make', 'data', 'access', 'languag', 'barrier', '.']

 TOTAL PORTER STEM WORDS ==> 8



 ---- SNOWBALL STEMMING ----

['major', 'challeng', 'make', 'data', 'access', 'languag', 'barrier', '.']

 TOTAL SNOWBALL STEM WORDS ==> 8



 ---- LEMMATIZATION ----

['Major', 'challenge', 'making', 'data', 'accessible', 'language', 'barrier', '.']

 TOTAL LEMMATIZE WORDS ==> 8

************************************************************************************************************************

179 --> There are  multitude of languages with different sentence structure and grammar. 


 ---- TOKENS ----

 ['There', 'are', 'multitude', 'of', 'languages', 'with', 'different', 'sentence', 'structure', 'and', 'grammar', '.'] 

 TOTAL TOKENS ==> 12

 ---- POST ----

 [('There', 'EX'), ('are', 'VBP'), ('multitude', 'NN'), ('of', 'IN'), ('languages', 'NNS'), ('with', 'IN'), ('different', 'JJ'), ('sentence', 'NN'), ('structure', 'NN'), ('and', 'CC'), ('grammar', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['multitude', 'languages', 'different', 'sentence', 'structure', 'grammar', '.']

 TOTAL FILTERED TOKENS ==>  7

 ---- POST FOR FILTERED TOKENS ----

 [('multitude', 'NN'), ('languages', 'NNS'), ('different', 'JJ'), ('sentence', 'NN'), ('structure', 'NN'), ('grammar', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['multitude languages', 'languages different', 'different sentence', 'sentence structure', 'structure grammar', 'grammar .'] 

 TOTAL BIGRAMS --> 6 



 ---- TRI-GRAMS ---- 

 ['multitude languages different', 'languages different sentence', 'different sentence structure', 'sentence structure grammar', 'structure grammar .'] 

 TOTAL TRIGRAMS --> 5 



 ---- NOUN PHRASES ---- 

 ['multitude', 'different sentence', 'structure', 'grammar'] 

 TOTAL NOUN PHRASES --> 4 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['multitud', 'languag', 'differ', 'sentenc', 'structur', 'grammar', '.']

 TOTAL PORTER STEM WORDS ==> 7



 ---- SNOWBALL STEMMING ----

['multitud', 'languag', 'differ', 'sentenc', 'structur', 'grammar', '.']

 TOTAL SNOWBALL STEM WORDS ==> 7



 ---- LEMMATIZATION ----

['multitude', 'language', 'different', 'sentence', 'structure', 'grammar', '.']

 TOTAL LEMMATIZE WORDS ==> 7

************************************************************************************************************************

180 --> Machine Translation is  generally translating phrases from one language to another with the help of a statistical  engine like Google Translate. 


 ---- TOKENS ----

 ['Machine', 'Translation', 'is', 'generally', 'translating', 'phrases', 'from', 'one', 'language', 'to', 'another', 'with', 'the', 'help', 'of', 'a', 'statistical', 'engine', 'like', 'Google', 'Translate', '.'] 

 TOTAL TOKENS ==> 22

 ---- POST ----

 [('Machine', 'NN'), ('Translation', 'NN'), ('is', 'VBZ'), ('generally', 'RB'), ('translating', 'VBG'), ('phrases', 'NNS'), ('from', 'IN'), ('one', 'CD'), ('language', 'NN'), ('to', 'TO'), ('another', 'DT'), ('with', 'IN'), ('the', 'DT'), ('help', 'NN'), ('of', 'IN'), ('a', 'DT'), ('statistical', 'JJ'), ('engine', 'NN'), ('like', 'IN'), ('Google', 'NNP'), ('Translate', 'NNP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Machine', 'Translation', 'generally', 'translating', 'phrases', 'one', 'language', 'another', 'help', 'statistical', 'engine', 'like', 'Google', 'Translate', '.']

 TOTAL FILTERED TOKENS ==>  15

 ---- POST FOR FILTERED TOKENS ----

 [('Machine', 'NN'), ('Translation', 'NNP'), ('generally', 'RB'), ('translating', 'VBG'), ('phrases', 'NNS'), ('one', 'CD'), ('language', 'NN'), ('another', 'DT'), ('help', 'NN'), ('statistical', 'JJ'), ('engine', 'NN'), ('like', 'IN'), ('Google', 'NNP'), ('Translate', 'NNP'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Machine Translation', 'Translation generally', 'generally translating', 'translating phrases', 'phrases one', 'one language', 'language another', 'another help', 'help statistical', 'statistical engine', 'engine like', 'like Google', 'Google Translate', 'Translate .'] 

 TOTAL BIGRAMS --> 14 



 ---- TRI-GRAMS ---- 

 ['Machine Translation generally', 'Translation generally translating', 'generally translating phrases', 'translating phrases one', 'phrases one language', 'one language another', 'language another help', 'another help statistical', 'help statistical engine', 'statistical engine like', 'engine like Google', 'like Google Translate', 'Google Translate .'] 

 TOTAL TRIGRAMS --> 13 



 ---- NOUN PHRASES ---- 

 ['Machine', 'language', 'another help', 'statistical engine'] 

 TOTAL NOUN PHRASES --> 4 



 ---- NER ----

 
 ORGANIZATION ---> ['Translation']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> ['Machine', 'Google Translate']
 TOTAL PERSON ENTITY --> 2 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['machin', 'translat', 'gener', 'translat', 'phrase', 'one', 'languag', 'anoth', 'help', 'statist', 'engin', 'like', 'googl', 'translat', '.']

 TOTAL PORTER STEM WORDS ==> 15



 ---- SNOWBALL STEMMING ----

['machin', 'translat', 'general', 'translat', 'phrase', 'one', 'languag', 'anoth', 'help', 'statist', 'engin', 'like', 'googl', 'translat', '.']

 TOTAL SNOWBALL STEM WORDS ==> 15



 ---- LEMMATIZATION ----

['Machine', 'Translation', 'generally', 'translating', 'phrase', 'one', 'language', 'another', 'help', 'statistical', 'engine', 'like', 'Google', 'Translate', '.']

 TOTAL LEMMATIZE WORDS ==> 15

************************************************************************************************************************

181 --> The challenge with machine translation technologies is not  directly translating words but keeping the meaning of sentences intact along with grammar  and tenses. 


 ---- TOKENS ----

 ['The', 'challenge', 'with', 'machine', 'translation', 'technologies', 'is', 'not', 'directly', 'translating', 'words', 'but', 'keeping', 'the', 'meaning', 'of', 'sentences', 'intact', 'along', 'with', 'grammar', 'and', 'tenses', '.'] 

 TOTAL TOKENS ==> 24

 ---- POST ----

 [('The', 'DT'), ('challenge', 'NN'), ('with', 'IN'), ('machine', 'NN'), ('translation', 'NN'), ('technologies', 'NNS'), ('is', 'VBZ'), ('not', 'RB'), ('directly', 'RB'), ('translating', 'VBG'), ('words', 'NNS'), ('but', 'CC'), ('keeping', 'VBG'), ('the', 'DT'), ('meaning', 'NN'), ('of', 'IN'), ('sentences', 'NNS'), ('intact', 'JJ'), ('along', 'IN'), ('with', 'IN'), ('grammar', 'NN'), ('and', 'CC'), ('tenses', 'NNS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['challenge', 'machine', 'translation', 'technologies', 'directly', 'translating', 'words', 'keeping', 'meaning', 'sentences', 'intact', 'along', 'grammar', 'tenses', '.']

 TOTAL FILTERED TOKENS ==>  15

 ---- POST FOR FILTERED TOKENS ----

 [('challenge', 'NN'), ('machine', 'NN'), ('translation', 'NN'), ('technologies', 'NNS'), ('directly', 'RB'), ('translating', 'VBG'), ('words', 'NNS'), ('keeping', 'VBG'), ('meaning', 'JJ'), ('sentences', 'NNS'), ('intact', 'JJ'), ('along', 'IN'), ('grammar', 'NN'), ('tenses', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['challenge machine', 'machine translation', 'translation technologies', 'technologies directly', 'directly translating', 'translating words', 'words keeping', 'keeping meaning', 'meaning sentences', 'sentences intact', 'intact along', 'along grammar', 'grammar tenses', 'tenses .'] 

 TOTAL BIGRAMS --> 14 



 ---- TRI-GRAMS ---- 

 ['challenge machine translation', 'machine translation technologies', 'translation technologies directly', 'technologies directly translating', 'directly translating words', 'translating words keeping', 'words keeping meaning', 'keeping meaning sentences', 'meaning sentences intact', 'sentences intact along', 'intact along grammar', 'along grammar tenses', 'grammar tenses .'] 

 TOTAL TRIGRAMS --> 13 



 ---- NOUN PHRASES ---- 

 ['challenge', 'machine', 'translation', 'grammar'] 

 TOTAL NOUN PHRASES --> 4 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['challeng', 'machin', 'translat', 'technolog', 'directli', 'translat', 'word', 'keep', 'mean', 'sentenc', 'intact', 'along', 'grammar', 'tens', '.']

 TOTAL PORTER STEM WORDS ==> 15



 ---- SNOWBALL STEMMING ----

['challeng', 'machin', 'translat', 'technolog', 'direct', 'translat', 'word', 'keep', 'mean', 'sentenc', 'intact', 'along', 'grammar', 'tens', '.']

 TOTAL SNOWBALL STEM WORDS ==> 15



 ---- LEMMATIZATION ----

['challenge', 'machine', 'translation', 'technology', 'directly', 'translating', 'word', 'keeping', 'meaning', 'sentence', 'intact', 'along', 'grammar', 'tense', '.']

 TOTAL LEMMATIZE WORDS ==> 15

************************************************************************************************************************

182 --> The statistical machine learning gathers as many data as they can find that seems  to be parallel between two languages and they crunch their data to find the likelihood that  something in Language A corresponds to something in Language B. 


 ---- TOKENS ----

 ['The', 'statistical', 'machine', 'learning', 'gathers', 'as', 'many', 'data', 'as', 'they', 'can', 'find', 'that', 'seems', 'to', 'be', 'parallel', 'between', 'two', 'languages', 'and', 'they', 'crunch', 'their', 'data', 'to', 'find', 'the', 'likelihood', 'that', 'something', 'in', 'Language', 'A', 'corresponds', 'to', 'something', 'in', 'Language', 'B', '.'] 

 TOTAL TOKENS ==> 41

 ---- POST ----

 [('The', 'DT'), ('statistical', 'JJ'), ('machine', 'NN'), ('learning', 'VBG'), ('gathers', 'NNS'), ('as', 'IN'), ('many', 'JJ'), ('data', 'NNS'), ('as', 'IN'), ('they', 'PRP'), ('can', 'MD'), ('find', 'VB'), ('that', 'DT'), ('seems', 'VBZ'), ('to', 'TO'), ('be', 'VB'), ('parallel', 'JJ'), ('between', 'IN'), ('two', 'CD'), ('languages', 'NNS'), ('and', 'CC'), ('they', 'PRP'), ('crunch', 'VBP'), ('their', 'PRP$'), ('data', 'NNS'), ('to', 'TO'), ('find', 'VB'), ('the', 'DT'), ('likelihood', 'NN'), ('that', 'IN'), ('something', 'NN'), ('in', 'IN'), ('Language', 'NNP'), ('A', 'NNP'), ('corresponds', 'VBZ'), ('to', 'TO'), ('something', 'NN'), ('in', 'IN'), ('Language', 'NNP'), ('B', 'NNP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['statistical', 'machine', 'learning', 'gathers', 'many', 'data', 'find', 'seems', 'parallel', 'two', 'languages', 'crunch', 'data', 'find', 'likelihood', 'something', 'Language', 'corresponds', 'something', 'Language', 'B', '.']

 TOTAL FILTERED TOKENS ==>  22

 ---- POST FOR FILTERED TOKENS ----

 [('statistical', 'JJ'), ('machine', 'NN'), ('learning', 'VBG'), ('gathers', 'NNS'), ('many', 'JJ'), ('data', 'NNS'), ('find', 'VBP'), ('seems', 'VBZ'), ('parallel', 'JJ'), ('two', 'CD'), ('languages', 'NNS'), ('crunch', 'VBP'), ('data', 'NNS'), ('find', 'VBP'), ('likelihood', 'JJ'), ('something', 'NN'), ('Language', 'NNP'), ('corresponds', 'VBZ'), ('something', 'NN'), ('Language', 'NNP'), ('B', 'NNP'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['statistical machine', 'machine learning', 'learning gathers', 'gathers many', 'many data', 'data find', 'find seems', 'seems parallel', 'parallel two', 'two languages', 'languages crunch', 'crunch data', 'data find', 'find likelihood', 'likelihood something', 'something Language', 'Language corresponds', 'corresponds something', 'something Language', 'Language B', 'B .'] 

 TOTAL BIGRAMS --> 21 



 ---- TRI-GRAMS ---- 

 ['statistical machine learning', 'machine learning gathers', 'learning gathers many', 'gathers many data', 'many data find', 'data find seems', 'find seems parallel', 'seems parallel two', 'parallel two languages', 'two languages crunch', 'languages crunch data', 'crunch data find', 'data find likelihood', 'find likelihood something', 'likelihood something Language', 'something Language corresponds', 'Language corresponds something', 'corresponds something Language', 'something Language B', 'Language B .'] 

 TOTAL TRIGRAMS --> 20 



 ---- NOUN PHRASES ---- 

 ['statistical machine', 'likelihood something', 'something'] 

 TOTAL NOUN PHRASES --> 3 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> ['Language', 'Language B']
 TOTAL PERSON ENTITY --> 2 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['statist', 'machin', 'learn', 'gather', 'mani', 'data', 'find', 'seem', 'parallel', 'two', 'languag', 'crunch', 'data', 'find', 'likelihood', 'someth', 'languag', 'correspond', 'someth', 'languag', 'b', '.']

 TOTAL PORTER STEM WORDS ==> 22



 ---- SNOWBALL STEMMING ----

['statist', 'machin', 'learn', 'gather', 'mani', 'data', 'find', 'seem', 'parallel', 'two', 'languag', 'crunch', 'data', 'find', 'likelihood', 'someth', 'languag', 'correspond', 'someth', 'languag', 'b', '.']

 TOTAL SNOWBALL STEM WORDS ==> 22



 ---- LEMMATIZATION ----

['statistical', 'machine', 'learning', 'gather', 'many', 'data', 'find', 'seems', 'parallel', 'two', 'language', 'crunch', 'data', 'find', 'likelihood', 'something', 'Language', 'corresponds', 'something', 'Language', 'B', '.']

 TOTAL LEMMATIZE WORDS ==> 22

************************************************************************************************************************

183 --> As for Google, in  September 2016, announced a new machine translation system based on Artificial neural  networks and Deep learning . 


 ---- TOKENS ----

 ['As', 'for', 'Google', ',', 'in', 'September', '2016', ',', 'announced', 'a', 'new', 'machine', 'translation', 'system', 'based', 'on', 'Artificial', 'neural', 'networks', 'and', 'Deep', 'learning', '.'] 

 TOTAL TOKENS ==> 23

 ---- POST ----

 [('As', 'IN'), ('for', 'IN'), ('Google', 'NNP'), (',', ','), ('in', 'IN'), ('September', 'NNP'), ('2016', 'CD'), (',', ','), ('announced', 'VBD'), ('a', 'DT'), ('new', 'JJ'), ('machine', 'NN'), ('translation', 'NN'), ('system', 'NN'), ('based', 'VBN'), ('on', 'IN'), ('Artificial', 'NNP'), ('neural', 'JJ'), ('networks', 'NNS'), ('and', 'CC'), ('Deep', 'NNP'), ('learning', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Google', ',', 'September', '2016', ',', 'announced', 'new', 'machine', 'translation', 'system', 'based', 'Artificial', 'neural', 'networks', 'Deep', 'learning', '.']

 TOTAL FILTERED TOKENS ==>  17

 ---- POST FOR FILTERED TOKENS ----

 [('Google', 'NNP'), (',', ','), ('September', 'NNP'), ('2016', 'CD'), (',', ','), ('announced', 'VBD'), ('new', 'JJ'), ('machine', 'NN'), ('translation', 'NN'), ('system', 'NN'), ('based', 'VBN'), ('Artificial', 'NNP'), ('neural', 'JJ'), ('networks', 'NNS'), ('Deep', 'NNP'), ('learning', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Google ,', ', September', 'September 2016', '2016 ,', ', announced', 'announced new', 'new machine', 'machine translation', 'translation system', 'system based', 'based Artificial', 'Artificial neural', 'neural networks', 'networks Deep', 'Deep learning', 'learning .'] 

 TOTAL BIGRAMS --> 16 



 ---- TRI-GRAMS ---- 

 ['Google , September', ', September 2016', 'September 2016 ,', '2016 , announced', ', announced new', 'announced new machine', 'new machine translation', 'machine translation system', 'translation system based', 'system based Artificial', 'based Artificial neural', 'Artificial neural networks', 'neural networks Deep', 'networks Deep learning', 'Deep learning .'] 

 TOTAL TRIGRAMS --> 15 



 ---- NOUN PHRASES ---- 

 ['new machine', 'translation', 'system', 'learning'] 

 TOTAL NOUN PHRASES --> 4 



 ---- NER ----

 
 ORGANIZATION ---> ['Artificial']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> ['Google']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['googl', ',', 'septemb', '2016', ',', 'announc', 'new', 'machin', 'translat', 'system', 'base', 'artifici', 'neural', 'network', 'deep', 'learn', '.']

 TOTAL PORTER STEM WORDS ==> 17



 ---- SNOWBALL STEMMING ----

['googl', ',', 'septemb', '2016', ',', 'announc', 'new', 'machin', 'translat', 'system', 'base', 'artifici', 'neural', 'network', 'deep', 'learn', '.']

 TOTAL SNOWBALL STEM WORDS ==> 17



 ---- LEMMATIZATION ----

['Google', ',', 'September', '2016', ',', 'announced', 'new', 'machine', 'translation', 'system', 'based', 'Artificial', 'neural', 'network', 'Deep', 'learning', '.']

 TOTAL LEMMATIZE WORDS ==> 17

************************************************************************************************************************

184 --> In recent years, various methods have been proposed to  automatically evaluate machine translation quality by comparing hypothesis translations with  reference translations. 


 ---- TOKENS ----

 ['In', 'recent', 'years', ',', 'various', 'methods', 'have', 'been', 'proposed', 'to', 'automatically', 'evaluate', 'machine', 'translation', 'quality', 'by', 'comparing', 'hypothesis', 'translations', 'with', 'reference', 'translations', '.'] 

 TOTAL TOKENS ==> 23

 ---- POST ----

 [('In', 'IN'), ('recent', 'JJ'), ('years', 'NNS'), (',', ','), ('various', 'JJ'), ('methods', 'NNS'), ('have', 'VBP'), ('been', 'VBN'), ('proposed', 'VBN'), ('to', 'TO'), ('automatically', 'RB'), ('evaluate', 'VB'), ('machine', 'NN'), ('translation', 'NN'), ('quality', 'NN'), ('by', 'IN'), ('comparing', 'VBG'), ('hypothesis', 'NN'), ('translations', 'NNS'), ('with', 'IN'), ('reference', 'NN'), ('translations', 'NNS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['recent', 'years', ',', 'various', 'methods', 'proposed', 'automatically', 'evaluate', 'machine', 'translation', 'quality', 'comparing', 'hypothesis', 'translations', 'reference', 'translations', '.']

 TOTAL FILTERED TOKENS ==>  17

 ---- POST FOR FILTERED TOKENS ----

 [('recent', 'JJ'), ('years', 'NNS'), (',', ','), ('various', 'JJ'), ('methods', 'NNS'), ('proposed', 'VBN'), ('automatically', 'RB'), ('evaluate', 'JJ'), ('machine', 'NN'), ('translation', 'NN'), ('quality', 'NN'), ('comparing', 'VBG'), ('hypothesis', 'NN'), ('translations', 'NNS'), ('reference', 'NN'), ('translations', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['recent years', 'years ,', ', various', 'various methods', 'methods proposed', 'proposed automatically', 'automatically evaluate', 'evaluate machine', 'machine translation', 'translation quality', 'quality comparing', 'comparing hypothesis', 'hypothesis translations', 'translations reference', 'reference translations', 'translations .'] 

 TOTAL BIGRAMS --> 16 



 ---- TRI-GRAMS ---- 

 ['recent years ,', 'years , various', ', various methods', 'various methods proposed', 'methods proposed automatically', 'proposed automatically evaluate', 'automatically evaluate machine', 'evaluate machine translation', 'machine translation quality', 'translation quality comparing', 'quality comparing hypothesis', 'comparing hypothesis translations', 'hypothesis translations reference', 'translations reference translations', 'reference translations .'] 

 TOTAL TRIGRAMS --> 15 



 ---- NOUN PHRASES ---- 

 ['evaluate machine', 'translation', 'quality', 'hypothesis', 'reference'] 

 TOTAL NOUN PHRASES --> 5 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['recent', 'year', ',', 'variou', 'method', 'propos', 'automat', 'evalu', 'machin', 'translat', 'qualiti', 'compar', 'hypothesi', 'translat', 'refer', 'translat', '.']

 TOTAL PORTER STEM WORDS ==> 17



 ---- SNOWBALL STEMMING ----

['recent', 'year', ',', 'various', 'method', 'propos', 'automat', 'evalu', 'machin', 'translat', 'qualiti', 'compar', 'hypothesi', 'translat', 'refer', 'translat', '.']

 TOTAL SNOWBALL STEM WORDS ==> 17



 ---- LEMMATIZATION ----

['recent', 'year', ',', 'various', 'method', 'proposed', 'automatically', 'evaluate', 'machine', 'translation', 'quality', 'comparing', 'hypothesis', 'translation', 'reference', 'translation', '.']

 TOTAL LEMMATIZE WORDS ==> 17

************************************************************************************************************************

185 --> Examples of such methods are word error rate, position-independent  word error rate (Tillmann et al., 1997) [37], generation string accuracy (Bangalore et al.,  2000) [38], multi-reference word error rate (Nießen et al., 2000) [39], BLEU score (Papineni  et al., 2002) [40], NIST score (Doddington, 2002) [41]  All these criteria try to approximate  human assessment and often achieve an astonishing degree of correlation to human subjective  evaluation of fluency and adequacy (Papineni et al., 2001; Doddington, 2002) [42][43]. 


 ---- TOKENS ----

 ['Examples', 'of', 'such', 'methods', 'are', 'word', 'error', 'rate', ',', 'position-independent', 'word', 'error', 'rate', '(', 'Tillmann', 'et', 'al.', ',', '1997', ')', '[', '37', ']', ',', 'generation', 'string', 'accuracy', '(', 'Bangalore', 'et', 'al.', ',', '2000', ')', '[', '38', ']', ',', 'multi-reference', 'word', 'error', 'rate', '(', 'Nießen', 'et', 'al.', ',', '2000', ')', '[', '39', ']', ',', 'BLEU', 'score', '(', 'Papineni', 'et', 'al.', ',', '2002', ')', '[', '40', ']', ',', 'NIST', 'score', '(', 'Doddington', ',', '2002', ')', '[', '41', ']', 'All', 'these', 'criteria', 'try', 'to', 'approximate', 'human', 'assessment', 'and', 'often', 'achieve', 'an', 'astonishing', 'degree', 'of', 'correlation', 'to', 'human', 'subjective', 'evaluation', 'of', 'fluency', 'and', 'adequacy', '(', 'Papineni', 'et', 'al.', ',', '2001', ';', 'Doddington', ',', '2002', ')', '[', '42', ']', '[', '43', ']', '.'] 

 TOTAL TOKENS ==> 118

 ---- POST ----

 [('Examples', 'NNS'), ('of', 'IN'), ('such', 'JJ'), ('methods', 'NNS'), ('are', 'VBP'), ('word', 'NN'), ('error', 'NN'), ('rate', 'NN'), (',', ','), ('position-independent', 'JJ'), ('word', 'NN'), ('error', 'NN'), ('rate', 'NN'), ('(', '('), ('Tillmann', 'NNP'), ('et', 'RB'), ('al.', 'RB'), (',', ','), ('1997', 'CD'), (')', ')'), ('[', 'VBD'), ('37', 'CD'), (']', 'NN'), (',', ','), ('generation', 'NN'), ('string', 'VBG'), ('accuracy', 'NN'), ('(', '('), ('Bangalore', 'NNP'), ('et', 'FW'), ('al.', 'NN'), (',', ','), ('2000', 'CD'), (')', ')'), ('[', 'VBD'), ('38', 'CD'), (']', 'NN'), (',', ','), ('multi-reference', 'NN'), ('word', 'NN'), ('error', 'NN'), ('rate', 'NN'), ('(', '('), ('Nießen', 'NNP'), ('et', 'RB'), ('al.', 'RB'), (',', ','), ('2000', 'CD'), (')', ')'), ('[', 'VBD'), ('39', 'CD'), (']', 'NN'), (',', ','), ('BLEU', 'NNP'), ('score', 'NN'), ('(', '('), ('Papineni', 'NNP'), ('et', 'RB'), ('al.', 'RB'), (',', ','), ('2002', 'CD'), (')', ')'), ('[', 'VBD'), ('40', 'CD'), (']', 'NN'), (',', ','), ('NIST', 'NNP'), ('score', 'NN'), ('(', '('), ('Doddington', 'NNP'), (',', ','), ('2002', 'CD'), (')', ')'), ('[', 'VBD'), ('41', 'CD'), (']', 'IN'), ('All', 'PDT'), ('these', 'DT'), ('criteria', 'NNS'), ('try', 'VBP'), ('to', 'TO'), ('approximate', 'VB'), ('human', 'JJ'), ('assessment', 'NN'), ('and', 'CC'), ('often', 'RB'), ('achieve', 'VBP'), ('an', 'DT'), ('astonishing', 'JJ'), ('degree', 'NN'), ('of', 'IN'), ('correlation', 'NN'), ('to', 'TO'), ('human', 'JJ'), ('subjective', 'JJ'), ('evaluation', 'NN'), ('of', 'IN'), ('fluency', 'NN'), ('and', 'CC'), ('adequacy', 'NN'), ('(', '('), ('Papineni', 'NNP'), ('et', 'RB'), ('al.', 'RB'), (',', ','), ('2001', 'CD'), (';', ':'), ('Doddington', 'NNP'), (',', ','), ('2002', 'CD'), (')', ')'), ('[', 'VBD'), ('42', 'CD'), (']', 'NNP'), ('[', 'VBD'), ('43', 'CD'), (']', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Examples', 'methods', 'word', 'error', 'rate', ',', 'position-independent', 'word', 'error', 'rate', '(', 'Tillmann', 'et', 'al.', ',', '1997', ')', '[', '37', ']', ',', 'generation', 'string', 'accuracy', '(', 'Bangalore', 'et', 'al.', ',', '2000', ')', '[', '38', ']', ',', 'multi-reference', 'word', 'error', 'rate', '(', 'Nießen', 'et', 'al.', ',', '2000', ')', '[', '39', ']', ',', 'BLEU', 'score', '(', 'Papineni', 'et', 'al.', ',', '2002', ')', '[', '40', ']', ',', 'NIST', 'score', '(', 'Doddington', ',', '2002', ')', '[', '41', ']', 'criteria', 'try', 'approximate', 'human', 'assessment', 'often', 'achieve', 'astonishing', 'degree', 'correlation', 'human', 'subjective', 'evaluation', 'fluency', 'adequacy', '(', 'Papineni', 'et', 'al.', ',', '2001', ';', 'Doddington', ',', '2002', ')', '[', '42', ']', '[', '43', ']', '.']

 TOTAL FILTERED TOKENS ==>  106

 ---- POST FOR FILTERED TOKENS ----

 [('Examples', 'NNS'), ('methods', 'NNS'), ('word', 'NN'), ('error', 'NN'), ('rate', 'NN'), (',', ','), ('position-independent', 'JJ'), ('word', 'NN'), ('error', 'NN'), ('rate', 'NN'), ('(', '('), ('Tillmann', 'NNP'), ('et', 'RB'), ('al.', 'RB'), (',', ','), ('1997', 'CD'), (')', ')'), ('[', 'VBD'), ('37', 'CD'), (']', 'NN'), (',', ','), ('generation', 'NN'), ('string', 'VBG'), ('accuracy', 'NN'), ('(', '('), ('Bangalore', 'NNP'), ('et', 'FW'), ('al.', 'NN'), (',', ','), ('2000', 'CD'), (')', ')'), ('[', 'VBD'), ('38', 'CD'), (']', 'NN'), (',', ','), ('multi-reference', 'NN'), ('word', 'NN'), ('error', 'NN'), ('rate', 'NN'), ('(', '('), ('Nießen', 'NNP'), ('et', 'RB'), ('al.', 'RB'), (',', ','), ('2000', 'CD'), (')', ')'), ('[', 'VBD'), ('39', 'CD'), (']', 'NN'), (',', ','), ('BLEU', 'NNP'), ('score', 'NN'), ('(', '('), ('Papineni', 'NNP'), ('et', 'RB'), ('al.', 'RB'), (',', ','), ('2002', 'CD'), (')', ')'), ('[', 'VBD'), ('40', 'CD'), (']', 'NN'), (',', ','), ('NIST', 'NNP'), ('score', 'NN'), ('(', '('), ('Doddington', 'NNP'), (',', ','), ('2002', 'CD'), (')', ')'), ('[', 'VBD'), ('41', 'CD'), (']', 'NN'), ('criteria', 'NNS'), ('try', 'VBP'), ('approximate', 'JJ'), ('human', 'JJ'), ('assessment', 'NN'), ('often', 'RB'), ('achieve', 'VBP'), ('astonishing', 'VBG'), ('degree', 'JJ'), ('correlation', 'NN'), ('human', 'JJ'), ('subjective', 'JJ'), ('evaluation', 'NN'), ('fluency', 'NN'), ('adequacy', 'NN'), ('(', '('), ('Papineni', 'NNP'), ('et', 'RB'), ('al.', 'RB'), (',', ','), ('2001', 'CD'), (';', ':'), ('Doddington', 'NNP'), (',', ','), ('2002', 'CD'), (')', ')'), ('[', 'VBD'), ('42', 'CD'), (']', 'NNP'), ('[', 'VBD'), ('43', 'CD'), (']', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Examples methods', 'methods word', 'word error', 'error rate', 'rate ,', ', position-independent', 'position-independent word', 'word error', 'error rate', 'rate (', '( Tillmann', 'Tillmann et', 'et al.', 'al. ,', ', 1997', '1997 )', ') [', '[ 37', '37 ]', '] ,', ', generation', 'generation string', 'string accuracy', 'accuracy (', '( Bangalore', 'Bangalore et', 'et al.', 'al. ,', ', 2000', '2000 )', ') [', '[ 38', '38 ]', '] ,', ', multi-reference', 'multi-reference word', 'word error', 'error rate', 'rate (', '( Nießen', 'Nießen et', 'et al.', 'al. ,', ', 2000', '2000 )', ') [', '[ 39', '39 ]', '] ,', ', BLEU', 'BLEU score', 'score (', '( Papineni', 'Papineni et', 'et al.', 'al. ,', ', 2002', '2002 )', ') [', '[ 40', '40 ]', '] ,', ', NIST', 'NIST score', 'score (', '( Doddington', 'Doddington ,', ', 2002', '2002 )', ') [', '[ 41', '41 ]', '] criteria', 'criteria try', 'try approximate', 'approximate human', 'human assessment', 'assessment often', 'often achieve', 'achieve astonishing', 'astonishing degree', 'degree correlation', 'correlation human', 'human subjective', 'subjective evaluation', 'evaluation fluency', 'fluency adequacy', 'adequacy (', '( Papineni', 'Papineni et', 'et al.', 'al. ,', ', 2001', '2001 ;', '; Doddington', 'Doddington ,', ', 2002', '2002 )', ') [', '[ 42', '42 ]', '] [', '[ 43', '43 ]', '] .'] 

 TOTAL BIGRAMS --> 105 



 ---- TRI-GRAMS ---- 

 ['Examples methods word', 'methods word error', 'word error rate', 'error rate ,', 'rate , position-independent', ', position-independent word', 'position-independent word error', 'word error rate', 'error rate (', 'rate ( Tillmann', '( Tillmann et', 'Tillmann et al.', 'et al. ,', 'al. , 1997', ', 1997 )', '1997 ) [', ') [ 37', '[ 37 ]', '37 ] ,', '] , generation', ', generation string', 'generation string accuracy', 'string accuracy (', 'accuracy ( Bangalore', '( Bangalore et', 'Bangalore et al.', 'et al. ,', 'al. , 2000', ', 2000 )', '2000 ) [', ') [ 38', '[ 38 ]', '38 ] ,', '] , multi-reference', ', multi-reference word', 'multi-reference word error', 'word error rate', 'error rate (', 'rate ( Nießen', '( Nießen et', 'Nießen et al.', 'et al. ,', 'al. , 2000', ', 2000 )', '2000 ) [', ') [ 39', '[ 39 ]', '39 ] ,', '] , BLEU', ', BLEU score', 'BLEU score (', 'score ( Papineni', '( Papineni et', 'Papineni et al.', 'et al. ,', 'al. , 2002', ', 2002 )', '2002 ) [', ') [ 40', '[ 40 ]', '40 ] ,', '] , NIST', ', NIST score', 'NIST score (', 'score ( Doddington', '( Doddington ,', 'Doddington , 2002', ', 2002 )', '2002 ) [', ') [ 41', '[ 41 ]', '41 ] criteria', '] criteria try', 'criteria try approximate', 'try approximate human', 'approximate human assessment', 'human assessment often', 'assessment often achieve', 'often achieve astonishing', 'achieve astonishing degree', 'astonishing degree correlation', 'degree correlation human', 'correlation human subjective', 'human subjective evaluation', 'subjective evaluation fluency', 'evaluation fluency adequacy', 'fluency adequacy (', 'adequacy ( Papineni', '( Papineni et', 'Papineni et al.', 'et al. ,', 'al. , 2001', ', 2001 ;', '2001 ; Doddington', '; Doddington ,', 'Doddington , 2002', ', 2002 )', '2002 ) [', ') [ 42', '[ 42 ]', '42 ] [', '] [ 43', '[ 43 ]', '43 ] .'] 

 TOTAL TRIGRAMS --> 104 



 ---- NOUN PHRASES ---- 

 ['word', 'error', 'rate', 'position-independent word', 'error', 'rate', ']', 'generation', 'accuracy', ']', 'multi-reference', 'word', 'error', 'rate', ']', 'score', ']', 'score', ']', 'approximate human assessment', 'degree correlation', 'human subjective evaluation', 'fluency', 'adequacy', ']'] 

 TOTAL NOUN PHRASES --> 25 



 ---- NER ----

 
 ORGANIZATION ---> ['BLEU', 'NIST']
 TOTAL ORGANIZATION ENTITY --> 2 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['exampl', 'method', 'word', 'error', 'rate', ',', 'position-independ', 'word', 'error', 'rate', '(', 'tillmann', 'et', 'al.', ',', '1997', ')', '[', '37', ']', ',', 'gener', 'string', 'accuraci', '(', 'bangalor', 'et', 'al.', ',', '2000', ')', '[', '38', ']', ',', 'multi-refer', 'word', 'error', 'rate', '(', 'nießen', 'et', 'al.', ',', '2000', ')', '[', '39', ']', ',', 'bleu', 'score', '(', 'papineni', 'et', 'al.', ',', '2002', ')', '[', '40', ']', ',', 'nist', 'score', '(', 'doddington', ',', '2002', ')', '[', '41', ']', 'criteria', 'tri', 'approxim', 'human', 'assess', 'often', 'achiev', 'astonish', 'degre', 'correl', 'human', 'subject', 'evalu', 'fluenci', 'adequaci', '(', 'papineni', 'et', 'al.', ',', '2001', ';', 'doddington', ',', '2002', ')', '[', '42', ']', '[', '43', ']', '.']

 TOTAL PORTER STEM WORDS ==> 106



 ---- SNOWBALL STEMMING ----

['exampl', 'method', 'word', 'error', 'rate', ',', 'position-independ', 'word', 'error', 'rate', '(', 'tillmann', 'et', 'al.', ',', '1997', ')', '[', '37', ']', ',', 'generat', 'string', 'accuraci', '(', 'bangalor', 'et', 'al.', ',', '2000', ')', '[', '38', ']', ',', 'multi-refer', 'word', 'error', 'rate', '(', 'nießen', 'et', 'al.', ',', '2000', ')', '[', '39', ']', ',', 'bleu', 'score', '(', 'papineni', 'et', 'al.', ',', '2002', ')', '[', '40', ']', ',', 'nist', 'score', '(', 'doddington', ',', '2002', ')', '[', '41', ']', 'criteria', 'tri', 'approxim', 'human', 'assess', 'often', 'achiev', 'astonish', 'degre', 'correl', 'human', 'subject', 'evalu', 'fluenci', 'adequaci', '(', 'papineni', 'et', 'al.', ',', '2001', ';', 'doddington', ',', '2002', ')', '[', '42', ']', '[', '43', ']', '.']

 TOTAL SNOWBALL STEM WORDS ==> 106



 ---- LEMMATIZATION ----

['Examples', 'method', 'word', 'error', 'rate', ',', 'position-independent', 'word', 'error', 'rate', '(', 'Tillmann', 'et', 'al.', ',', '1997', ')', '[', '37', ']', ',', 'generation', 'string', 'accuracy', '(', 'Bangalore', 'et', 'al.', ',', '2000', ')', '[', '38', ']', ',', 'multi-reference', 'word', 'error', 'rate', '(', 'Nießen', 'et', 'al.', ',', '2000', ')', '[', '39', ']', ',', 'BLEU', 'score', '(', 'Papineni', 'et', 'al.', ',', '2002', ')', '[', '40', ']', ',', 'NIST', 'score', '(', 'Doddington', ',', '2002', ')', '[', '41', ']', 'criterion', 'try', 'approximate', 'human', 'assessment', 'often', 'achieve', 'astonishing', 'degree', 'correlation', 'human', 'subjective', 'evaluation', 'fluency', 'adequacy', '(', 'Papineni', 'et', 'al.', ',', '2001', ';', 'Doddington', ',', '2002', ')', '[', '42', ']', '[', '43', ']', '.']

 TOTAL LEMMATIZE WORDS ==> 106

************************************************************************************************************************

186 --> 6.2 Text Categorization   Categorization systems inputs a large flow of data like official documents, military casualty  reports, market data, newswires etc. 


 ---- TOKENS ----

 ['6.2', 'Text', 'Categorization', 'Categorization', 'systems', 'inputs', 'a', 'large', 'flow', 'of', 'data', 'like', 'official', 'documents', ',', 'military', 'casualty', 'reports', ',', 'market', 'data', ',', 'newswires', 'etc', '.'] 

 TOTAL TOKENS ==> 25

 ---- POST ----

 [('6.2', 'CD'), ('Text', 'NNP'), ('Categorization', 'NNP'), ('Categorization', 'NNP'), ('systems', 'NNS'), ('inputs', 'VBZ'), ('a', 'DT'), ('large', 'JJ'), ('flow', 'NN'), ('of', 'IN'), ('data', 'NNS'), ('like', 'IN'), ('official', 'JJ'), ('documents', 'NNS'), (',', ','), ('military', 'JJ'), ('casualty', 'NN'), ('reports', 'NNS'), (',', ','), ('market', 'NN'), ('data', 'NNS'), (',', ','), ('newswires', 'NNS'), ('etc', 'VBP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['6.2', 'Text', 'Categorization', 'Categorization', 'systems', 'inputs', 'large', 'flow', 'data', 'like', 'official', 'documents', ',', 'military', 'casualty', 'reports', ',', 'market', 'data', ',', 'newswires', 'etc', '.']

 TOTAL FILTERED TOKENS ==>  23

 ---- POST FOR FILTERED TOKENS ----

 [('6.2', 'CD'), ('Text', 'NNP'), ('Categorization', 'NNP'), ('Categorization', 'NNP'), ('systems', 'NNS'), ('inputs', 'VBZ'), ('large', 'JJ'), ('flow', 'JJ'), ('data', 'NNS'), ('like', 'IN'), ('official', 'JJ'), ('documents', 'NNS'), (',', ','), ('military', 'JJ'), ('casualty', 'NN'), ('reports', 'NNS'), (',', ','), ('market', 'NN'), ('data', 'NNS'), (',', ','), ('newswires', 'NNS'), ('etc', 'VBP'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['6.2 Text', 'Text Categorization', 'Categorization Categorization', 'Categorization systems', 'systems inputs', 'inputs large', 'large flow', 'flow data', 'data like', 'like official', 'official documents', 'documents ,', ', military', 'military casualty', 'casualty reports', 'reports ,', ', market', 'market data', 'data ,', ', newswires', 'newswires etc', 'etc .'] 

 TOTAL BIGRAMS --> 22 



 ---- TRI-GRAMS ---- 

 ['6.2 Text Categorization', 'Text Categorization Categorization', 'Categorization Categorization systems', 'Categorization systems inputs', 'systems inputs large', 'inputs large flow', 'large flow data', 'flow data like', 'data like official', 'like official documents', 'official documents ,', 'documents , military', ', military casualty', 'military casualty reports', 'casualty reports ,', 'reports , market', ', market data', 'market data ,', 'data , newswires', ', newswires etc', 'newswires etc .'] 

 TOTAL TRIGRAMS --> 21 



 ---- NOUN PHRASES ---- 

 ['military casualty', 'market'] 

 TOTAL NOUN PHRASES --> 2 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['6.2', 'text', 'categor', 'categor', 'system', 'input', 'larg', 'flow', 'data', 'like', 'offici', 'document', ',', 'militari', 'casualti', 'report', ',', 'market', 'data', ',', 'newswir', 'etc', '.']

 TOTAL PORTER STEM WORDS ==> 23



 ---- SNOWBALL STEMMING ----

['6.2', 'text', 'categor', 'categor', 'system', 'input', 'larg', 'flow', 'data', 'like', 'offici', 'document', ',', 'militari', 'casualti', 'report', ',', 'market', 'data', ',', 'newswir', 'etc', '.']

 TOTAL SNOWBALL STEM WORDS ==> 23



 ---- LEMMATIZATION ----

['6.2', 'Text', 'Categorization', 'Categorization', 'system', 'input', 'large', 'flow', 'data', 'like', 'official', 'document', ',', 'military', 'casualty', 'report', ',', 'market', 'data', ',', 'newswires', 'etc', '.']

 TOTAL LEMMATIZE WORDS ==> 23

************************************************************************************************************************

187 --> and assign them to predefined categories or indices. 


 ---- TOKENS ----

 ['and', 'assign', 'them', 'to', 'predefined', 'categories', 'or', 'indices', '.'] 

 TOTAL TOKENS ==> 9

 ---- POST ----

 [('and', 'CC'), ('assign', 'VB'), ('them', 'PRP'), ('to', 'TO'), ('predefined', 'VB'), ('categories', 'NNS'), ('or', 'CC'), ('indices', 'NNS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['assign', 'predefined', 'categories', 'indices', '.']

 TOTAL FILTERED TOKENS ==>  5

 ---- POST FOR FILTERED TOKENS ----

 [('assign', 'NN'), ('predefined', 'VBD'), ('categories', 'NNS'), ('indices', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['assign predefined', 'predefined categories', 'categories indices', 'indices .'] 

 TOTAL BIGRAMS --> 4 



 ---- TRI-GRAMS ---- 

 ['assign predefined categories', 'predefined categories indices', 'categories indices .'] 

 TOTAL TRIGRAMS --> 3 



 ---- NOUN PHRASES ---- 

 ['assign'] 

 TOTAL NOUN PHRASES --> 1 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['assign', 'predefin', 'categori', 'indic', '.']

 TOTAL PORTER STEM WORDS ==> 5



 ---- SNOWBALL STEMMING ----

['assign', 'predefin', 'categori', 'indic', '.']

 TOTAL SNOWBALL STEM WORDS ==> 5



 ---- LEMMATIZATION ----

['assign', 'predefined', 'category', 'index', '.']

 TOTAL LEMMATIZE WORDS ==> 5

************************************************************************************************************************

188 --> For  example, The Carnegie Group’s Construe system (Hayes PJ ,Westein ; 1991)[44] , inputs  Reuters articles and saves much time by doing the work that is to be done by staff or human   indexers. 


 ---- TOKENS ----

 ['For', 'example', ',', 'The', 'Carnegie', 'Group', '’', 's', 'Construe', 'system', '(', 'Hayes', 'PJ', ',', 'Westein', ';', '1991', ')', '[', '44', ']', ',', 'inputs', 'Reuters', 'articles', 'and', 'saves', 'much', 'time', 'by', 'doing', 'the', 'work', 'that', 'is', 'to', 'be', 'done', 'by', 'staff', 'or', 'human', 'indexers', '.'] 

 TOTAL TOKENS ==> 44

 ---- POST ----

 [('For', 'IN'), ('example', 'NN'), (',', ','), ('The', 'DT'), ('Carnegie', 'NNP'), ('Group', 'NNP'), ('’', 'NNP'), ('s', 'VBZ'), ('Construe', 'NNP'), ('system', 'NN'), ('(', '('), ('Hayes', 'NNP'), ('PJ', 'NNP'), (',', ','), ('Westein', 'NNP'), (';', ':'), ('1991', 'CD'), (')', ')'), ('[', 'VBD'), ('44', 'CD'), (']', 'NN'), (',', ','), ('inputs', 'VBZ'), ('Reuters', 'NNP'), ('articles', 'NNS'), ('and', 'CC'), ('saves', 'NNS'), ('much', 'JJ'), ('time', 'NN'), ('by', 'IN'), ('doing', 'VBG'), ('the', 'DT'), ('work', 'NN'), ('that', 'WDT'), ('is', 'VBZ'), ('to', 'TO'), ('be', 'VB'), ('done', 'VBN'), ('by', 'IN'), ('staff', 'NN'), ('or', 'CC'), ('human', 'JJ'), ('indexers', 'NNS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['example', ',', 'Carnegie', 'Group', '’', 'Construe', 'system', '(', 'Hayes', 'PJ', ',', 'Westein', ';', '1991', ')', '[', '44', ']', ',', 'inputs', 'Reuters', 'articles', 'saves', 'much', 'time', 'work', 'done', 'staff', 'human', 'indexers', '.']

 TOTAL FILTERED TOKENS ==>  31

 ---- POST FOR FILTERED TOKENS ----

 [('example', 'NN'), (',', ','), ('Carnegie', 'NNP'), ('Group', 'NNP'), ('’', 'NNP'), ('Construe', 'NNP'), ('system', 'NN'), ('(', '('), ('Hayes', 'NNP'), ('PJ', 'NNP'), (',', ','), ('Westein', 'NNP'), (';', ':'), ('1991', 'CD'), (')', ')'), ('[', 'VBD'), ('44', 'CD'), (']', 'NN'), (',', ','), ('inputs', 'VBZ'), ('Reuters', 'NNP'), ('articles', 'VBZ'), ('saves', 'NNS'), ('much', 'JJ'), ('time', 'NN'), ('work', 'NN'), ('done', 'VBN'), ('staff', 'NN'), ('human', 'JJ'), ('indexers', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['example ,', ', Carnegie', 'Carnegie Group', 'Group ’', '’ Construe', 'Construe system', 'system (', '( Hayes', 'Hayes PJ', 'PJ ,', ', Westein', 'Westein ;', '; 1991', '1991 )', ') [', '[ 44', '44 ]', '] ,', ', inputs', 'inputs Reuters', 'Reuters articles', 'articles saves', 'saves much', 'much time', 'time work', 'work done', 'done staff', 'staff human', 'human indexers', 'indexers .'] 

 TOTAL BIGRAMS --> 30 



 ---- TRI-GRAMS ---- 

 ['example , Carnegie', ', Carnegie Group', 'Carnegie Group ’', 'Group ’ Construe', '’ Construe system', 'Construe system (', 'system ( Hayes', '( Hayes PJ', 'Hayes PJ ,', 'PJ , Westein', ', Westein ;', 'Westein ; 1991', '; 1991 )', '1991 ) [', ') [ 44', '[ 44 ]', '44 ] ,', '] , inputs', ', inputs Reuters', 'inputs Reuters articles', 'Reuters articles saves', 'articles saves much', 'saves much time', 'much time work', 'time work done', 'work done staff', 'done staff human', 'staff human indexers', 'human indexers .'] 

 TOTAL TRIGRAMS --> 29 



 ---- NOUN PHRASES ---- 

 ['example', 'system', ']', 'much time', 'work', 'staff'] 

 TOTAL NOUN PHRASES --> 6 



 ---- NER ----

 
 ORGANIZATION ---> ['Reuters']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> ['Carnegie Group']
 TOTAL PERSON ENTITY --> 1 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['exampl', ',', 'carnegi', 'group', '’', 'constru', 'system', '(', 'hay', 'pj', ',', 'westein', ';', '1991', ')', '[', '44', ']', ',', 'input', 'reuter', 'articl', 'save', 'much', 'time', 'work', 'done', 'staff', 'human', 'index', '.']

 TOTAL PORTER STEM WORDS ==> 31



 ---- SNOWBALL STEMMING ----

['exampl', ',', 'carnegi', 'group', '’', 'constru', 'system', '(', 'hay', 'pj', ',', 'westein', ';', '1991', ')', '[', '44', ']', ',', 'input', 'reuter', 'articl', 'save', 'much', 'time', 'work', 'done', 'staff', 'human', 'index', '.']

 TOTAL SNOWBALL STEM WORDS ==> 31



 ---- LEMMATIZATION ----

['example', ',', 'Carnegie', 'Group', '’', 'Construe', 'system', '(', 'Hayes', 'PJ', ',', 'Westein', ';', '1991', ')', '[', '44', ']', ',', 'input', 'Reuters', 'article', 'save', 'much', 'time', 'work', 'done', 'staff', 'human', 'indexer', '.']

 TOTAL LEMMATIZE WORDS ==> 31

************************************************************************************************************************

189 --> Some companies have been using categorization systems to categorize trouble  tickets or complaint requests and routing to the appropriate desks. 


 ---- TOKENS ----

 ['Some', 'companies', 'have', 'been', 'using', 'categorization', 'systems', 'to', 'categorize', 'trouble', 'tickets', 'or', 'complaint', 'requests', 'and', 'routing', 'to', 'the', 'appropriate', 'desks', '.'] 

 TOTAL TOKENS ==> 21

 ---- POST ----

 [('Some', 'DT'), ('companies', 'NNS'), ('have', 'VBP'), ('been', 'VBN'), ('using', 'VBG'), ('categorization', 'NN'), ('systems', 'NNS'), ('to', 'TO'), ('categorize', 'VB'), ('trouble', 'NN'), ('tickets', 'NNS'), ('or', 'CC'), ('complaint', 'NN'), ('requests', 'NNS'), ('and', 'CC'), ('routing', 'VBG'), ('to', 'TO'), ('the', 'DT'), ('appropriate', 'JJ'), ('desks', 'NNS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['companies', 'using', 'categorization', 'systems', 'categorize', 'trouble', 'tickets', 'complaint', 'requests', 'routing', 'appropriate', 'desks', '.']

 TOTAL FILTERED TOKENS ==>  13

 ---- POST FOR FILTERED TOKENS ----

 [('companies', 'NNS'), ('using', 'VBG'), ('categorization', 'NN'), ('systems', 'NNS'), ('categorize', 'VBP'), ('trouble', 'NN'), ('tickets', 'NNS'), ('complaint', 'VBP'), ('requests', 'NNS'), ('routing', 'VBG'), ('appropriate', 'JJ'), ('desks', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['companies using', 'using categorization', 'categorization systems', 'systems categorize', 'categorize trouble', 'trouble tickets', 'tickets complaint', 'complaint requests', 'requests routing', 'routing appropriate', 'appropriate desks', 'desks .'] 

 TOTAL BIGRAMS --> 12 



 ---- TRI-GRAMS ---- 

 ['companies using categorization', 'using categorization systems', 'categorization systems categorize', 'systems categorize trouble', 'categorize trouble tickets', 'trouble tickets complaint', 'tickets complaint requests', 'complaint requests routing', 'requests routing appropriate', 'routing appropriate desks', 'appropriate desks .'] 

 TOTAL TRIGRAMS --> 11 



 ---- NOUN PHRASES ---- 

 ['categorization', 'trouble'] 

 TOTAL NOUN PHRASES --> 2 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['compani', 'use', 'categor', 'system', 'categor', 'troubl', 'ticket', 'complaint', 'request', 'rout', 'appropri', 'desk', '.']

 TOTAL PORTER STEM WORDS ==> 13



 ---- SNOWBALL STEMMING ----

['compani', 'use', 'categor', 'system', 'categor', 'troubl', 'ticket', 'complaint', 'request', 'rout', 'appropri', 'desk', '.']

 TOTAL SNOWBALL STEM WORDS ==> 13



 ---- LEMMATIZATION ----

['company', 'using', 'categorization', 'system', 'categorize', 'trouble', 'ticket', 'complaint', 'request', 'routing', 'appropriate', 'desk', '.']

 TOTAL LEMMATIZE WORDS ==> 13

************************************************************************************************************************

190 --> Another application of text  categorization is email spam filters. 


 ---- TOKENS ----

 ['Another', 'application', 'of', 'text', 'categorization', 'is', 'email', 'spam', 'filters', '.'] 

 TOTAL TOKENS ==> 10

 ---- POST ----

 [('Another', 'DT'), ('application', 'NN'), ('of', 'IN'), ('text', 'JJ'), ('categorization', 'NN'), ('is', 'VBZ'), ('email', 'JJ'), ('spam', 'NN'), ('filters', 'NNS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Another', 'application', 'text', 'categorization', 'email', 'spam', 'filters', '.']

 TOTAL FILTERED TOKENS ==>  8

 ---- POST FOR FILTERED TOKENS ----

 [('Another', 'DT'), ('application', 'NN'), ('text', 'NN'), ('categorization', 'NN'), ('email', 'NN'), ('spam', 'NN'), ('filters', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Another application', 'application text', 'text categorization', 'categorization email', 'email spam', 'spam filters', 'filters .'] 

 TOTAL BIGRAMS --> 7 



 ---- TRI-GRAMS ---- 

 ['Another application text', 'application text categorization', 'text categorization email', 'categorization email spam', 'email spam filters', 'spam filters .'] 

 TOTAL TRIGRAMS --> 6 



 ---- NOUN PHRASES ---- 

 ['Another application', 'text', 'categorization', 'email', 'spam'] 

 TOTAL NOUN PHRASES --> 5 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['anoth', 'applic', 'text', 'categor', 'email', 'spam', 'filter', '.']

 TOTAL PORTER STEM WORDS ==> 8



 ---- SNOWBALL STEMMING ----

['anoth', 'applic', 'text', 'categor', 'email', 'spam', 'filter', '.']

 TOTAL SNOWBALL STEM WORDS ==> 8



 ---- LEMMATIZATION ----

['Another', 'application', 'text', 'categorization', 'email', 'spam', 'filter', '.']

 TOTAL LEMMATIZE WORDS ==> 8

************************************************************************************************************************

191 --> Spam filters is becoming important as the first line of  defence against the unwanted emails. 


 ---- TOKENS ----

 ['Spam', 'filters', 'is', 'becoming', 'important', 'as', 'the', 'first', 'line', 'of', 'defence', 'against', 'the', 'unwanted', 'emails', '.'] 

 TOTAL TOKENS ==> 16

 ---- POST ----

 [('Spam', 'NNP'), ('filters', 'NNS'), ('is', 'VBZ'), ('becoming', 'VBG'), ('important', 'JJ'), ('as', 'IN'), ('the', 'DT'), ('first', 'JJ'), ('line', 'NN'), ('of', 'IN'), ('defence', 'NN'), ('against', 'IN'), ('the', 'DT'), ('unwanted', 'JJ'), ('emails', 'NNS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Spam', 'filters', 'becoming', 'important', 'first', 'line', 'defence', 'unwanted', 'emails', '.']

 TOTAL FILTERED TOKENS ==>  10

 ---- POST FOR FILTERED TOKENS ----

 [('Spam', 'NNP'), ('filters', 'NNS'), ('becoming', 'VBG'), ('important', 'JJ'), ('first', 'JJ'), ('line', 'NN'), ('defence', 'NN'), ('unwanted', 'VBD'), ('emails', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Spam filters', 'filters becoming', 'becoming important', 'important first', 'first line', 'line defence', 'defence unwanted', 'unwanted emails', 'emails .'] 

 TOTAL BIGRAMS --> 9 



 ---- TRI-GRAMS ---- 

 ['Spam filters becoming', 'filters becoming important', 'becoming important first', 'important first line', 'first line defence', 'line defence unwanted', 'defence unwanted emails', 'unwanted emails .'] 

 TOTAL TRIGRAMS --> 8 



 ---- NOUN PHRASES ---- 

 ['important first line', 'defence'] 

 TOTAL NOUN PHRASES --> 2 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> ['Spam']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['spam', 'filter', 'becom', 'import', 'first', 'line', 'defenc', 'unwant', 'email', '.']

 TOTAL PORTER STEM WORDS ==> 10



 ---- SNOWBALL STEMMING ----

['spam', 'filter', 'becom', 'import', 'first', 'line', 'defenc', 'unwant', 'email', '.']

 TOTAL SNOWBALL STEM WORDS ==> 10



 ---- LEMMATIZATION ----

['Spam', 'filter', 'becoming', 'important', 'first', 'line', 'defence', 'unwanted', 'email', '.']

 TOTAL LEMMATIZE WORDS ==> 10

************************************************************************************************************************

192 --> A false negative and false positive issues of spam filters  are at the heart of NLP technology, its brought down to the challenge of extracting meaning  from strings of text. 


 ---- TOKENS ----

 ['A', 'false', 'negative', 'and', 'false', 'positive', 'issues', 'of', 'spam', 'filters', 'are', 'at', 'the', 'heart', 'of', 'NLP', 'technology', ',', 'its', 'brought', 'down', 'to', 'the', 'challenge', 'of', 'extracting', 'meaning', 'from', 'strings', 'of', 'text', '.'] 

 TOTAL TOKENS ==> 32

 ---- POST ----

 [('A', 'DT'), ('false', 'JJ'), ('negative', 'JJ'), ('and', 'CC'), ('false', 'JJ'), ('positive', 'JJ'), ('issues', 'NNS'), ('of', 'IN'), ('spam', 'NN'), ('filters', 'NNS'), ('are', 'VBP'), ('at', 'IN'), ('the', 'DT'), ('heart', 'NN'), ('of', 'IN'), ('NLP', 'NNP'), ('technology', 'NN'), (',', ','), ('its', 'PRP$'), ('brought', 'VBD'), ('down', 'RB'), ('to', 'TO'), ('the', 'DT'), ('challenge', 'NN'), ('of', 'IN'), ('extracting', 'VBG'), ('meaning', 'VBG'), ('from', 'IN'), ('strings', 'NNS'), ('of', 'IN'), ('text', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['false', 'negative', 'false', 'positive', 'issues', 'spam', 'filters', 'heart', 'NLP', 'technology', ',', 'brought', 'challenge', 'extracting', 'meaning', 'strings', 'text', '.']

 TOTAL FILTERED TOKENS ==>  18

 ---- POST FOR FILTERED TOKENS ----

 [('false', 'JJ'), ('negative', 'JJ'), ('false', 'JJ'), ('positive', 'JJ'), ('issues', 'NNS'), ('spam', 'VBP'), ('filters', 'NNS'), ('heart', 'NN'), ('NLP', 'NNP'), ('technology', 'NN'), (',', ','), ('brought', 'VBD'), ('challenge', 'NN'), ('extracting', 'VBG'), ('meaning', 'NN'), ('strings', 'NNS'), ('text', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['false negative', 'negative false', 'false positive', 'positive issues', 'issues spam', 'spam filters', 'filters heart', 'heart NLP', 'NLP technology', 'technology ,', ', brought', 'brought challenge', 'challenge extracting', 'extracting meaning', 'meaning strings', 'strings text', 'text .'] 

 TOTAL BIGRAMS --> 17 



 ---- TRI-GRAMS ---- 

 ['false negative false', 'negative false positive', 'false positive issues', 'positive issues spam', 'issues spam filters', 'spam filters heart', 'filters heart NLP', 'heart NLP technology', 'NLP technology ,', 'technology , brought', ', brought challenge', 'brought challenge extracting', 'challenge extracting meaning', 'extracting meaning strings', 'meaning strings text', 'strings text .'] 

 TOTAL TRIGRAMS --> 16 



 ---- NOUN PHRASES ---- 

 ['heart', 'technology', 'challenge', 'meaning', 'text'] 

 TOTAL NOUN PHRASES --> 5 



 ---- NER ----

 
 ORGANIZATION ---> ['NLP']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['fals', 'neg', 'fals', 'posit', 'issu', 'spam', 'filter', 'heart', 'nlp', 'technolog', ',', 'brought', 'challeng', 'extract', 'mean', 'string', 'text', '.']

 TOTAL PORTER STEM WORDS ==> 18



 ---- SNOWBALL STEMMING ----

['fals', 'negat', 'fals', 'posit', 'issu', 'spam', 'filter', 'heart', 'nlp', 'technolog', ',', 'brought', 'challeng', 'extract', 'mean', 'string', 'text', '.']

 TOTAL SNOWBALL STEM WORDS ==> 18



 ---- LEMMATIZATION ----

['false', 'negative', 'false', 'positive', 'issue', 'spam', 'filter', 'heart', 'NLP', 'technology', ',', 'brought', 'challenge', 'extracting', 'meaning', 'string', 'text', '.']

 TOTAL LEMMATIZE WORDS ==> 18

************************************************************************************************************************

193 --> A filtering solution that is applied to an email system uses a set of  protocols to determine which of the incoming messages are spam and which are not. 


 ---- TOKENS ----

 ['A', 'filtering', 'solution', 'that', 'is', 'applied', 'to', 'an', 'email', 'system', 'uses', 'a', 'set', 'of', 'protocols', 'to', 'determine', 'which', 'of', 'the', 'incoming', 'messages', 'are', 'spam', 'and', 'which', 'are', 'not', '.'] 

 TOTAL TOKENS ==> 29

 ---- POST ----

 [('A', 'DT'), ('filtering', 'JJ'), ('solution', 'NN'), ('that', 'WDT'), ('is', 'VBZ'), ('applied', 'VBN'), ('to', 'TO'), ('an', 'DT'), ('email', 'NN'), ('system', 'NN'), ('uses', 'VBZ'), ('a', 'DT'), ('set', 'NN'), ('of', 'IN'), ('protocols', 'NNS'), ('to', 'TO'), ('determine', 'VB'), ('which', 'WDT'), ('of', 'IN'), ('the', 'DT'), ('incoming', 'NN'), ('messages', 'NNS'), ('are', 'VBP'), ('spam', 'JJ'), ('and', 'CC'), ('which', 'WDT'), ('are', 'VBP'), ('not', 'RB'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['filtering', 'solution', 'applied', 'email', 'system', 'uses', 'set', 'protocols', 'determine', 'incoming', 'messages', 'spam', '.']

 TOTAL FILTERED TOKENS ==>  13

 ---- POST FOR FILTERED TOKENS ----

 [('filtering', 'VBG'), ('solution', 'NN'), ('applied', 'VBN'), ('email', 'NN'), ('system', 'NN'), ('uses', 'VBZ'), ('set', 'VBN'), ('protocols', 'NNS'), ('determine', 'VBP'), ('incoming', 'VBG'), ('messages', 'NNS'), ('spam', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['filtering solution', 'solution applied', 'applied email', 'email system', 'system uses', 'uses set', 'set protocols', 'protocols determine', 'determine incoming', 'incoming messages', 'messages spam', 'spam .'] 

 TOTAL BIGRAMS --> 12 



 ---- TRI-GRAMS ---- 

 ['filtering solution applied', 'solution applied email', 'applied email system', 'email system uses', 'system uses set', 'uses set protocols', 'set protocols determine', 'protocols determine incoming', 'determine incoming messages', 'incoming messages spam', 'messages spam .'] 

 TOTAL TRIGRAMS --> 11 



 ---- NOUN PHRASES ---- 

 ['solution', 'email', 'system', 'spam'] 

 TOTAL NOUN PHRASES --> 4 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['filter', 'solut', 'appli', 'email', 'system', 'use', 'set', 'protocol', 'determin', 'incom', 'messag', 'spam', '.']

 TOTAL PORTER STEM WORDS ==> 13



 ---- SNOWBALL STEMMING ----

['filter', 'solut', 'appli', 'email', 'system', 'use', 'set', 'protocol', 'determin', 'incom', 'messag', 'spam', '.']

 TOTAL SNOWBALL STEM WORDS ==> 13



 ---- LEMMATIZATION ----

['filtering', 'solution', 'applied', 'email', 'system', 'us', 'set', 'protocol', 'determine', 'incoming', 'message', 'spam', '.']

 TOTAL LEMMATIZE WORDS ==> 13

************************************************************************************************************************

194 --> There  are several types of spam filters available. 


 ---- TOKENS ----

 ['There', 'are', 'several', 'types', 'of', 'spam', 'filters', 'available', '.'] 

 TOTAL TOKENS ==> 9

 ---- POST ----

 [('There', 'EX'), ('are', 'VBP'), ('several', 'JJ'), ('types', 'NNS'), ('of', 'IN'), ('spam', 'NN'), ('filters', 'NNS'), ('available', 'JJ'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['several', 'types', 'spam', 'filters', 'available', '.']

 TOTAL FILTERED TOKENS ==>  6

 ---- POST FOR FILTERED TOKENS ----

 [('several', 'JJ'), ('types', 'NNS'), ('spam', 'VBD'), ('filters', 'NNS'), ('available', 'JJ'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['several types', 'types spam', 'spam filters', 'filters available', 'available .'] 

 TOTAL BIGRAMS --> 5 



 ---- TRI-GRAMS ---- 

 ['several types spam', 'types spam filters', 'spam filters available', 'filters available .'] 

 TOTAL TRIGRAMS --> 4 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['sever', 'type', 'spam', 'filter', 'avail', '.']

 TOTAL PORTER STEM WORDS ==> 6



 ---- SNOWBALL STEMMING ----

['sever', 'type', 'spam', 'filter', 'avail', '.']

 TOTAL SNOWBALL STEM WORDS ==> 6



 ---- LEMMATIZATION ----

['several', 'type', 'spam', 'filter', 'available', '.']

 TOTAL LEMMATIZE WORDS ==> 6

************************************************************************************************************************

195 --> Content filters: Review the content within the  message to determine whether it is a spam or not. 


 ---- TOKENS ----

 ['Content', 'filters', ':', 'Review', 'the', 'content', 'within', 'the', 'message', 'to', 'determine', 'whether', 'it', 'is', 'a', 'spam', 'or', 'not', '.'] 

 TOTAL TOKENS ==> 19

 ---- POST ----

 [('Content', 'JJ'), ('filters', 'NNS'), (':', ':'), ('Review', 'VB'), ('the', 'DT'), ('content', 'NN'), ('within', 'IN'), ('the', 'DT'), ('message', 'NN'), ('to', 'TO'), ('determine', 'VB'), ('whether', 'IN'), ('it', 'PRP'), ('is', 'VBZ'), ('a', 'DT'), ('spam', 'NN'), ('or', 'CC'), ('not', 'RB'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Content', 'filters', ':', 'Review', 'content', 'within', 'message', 'determine', 'whether', 'spam', '.']

 TOTAL FILTERED TOKENS ==>  11

 ---- POST FOR FILTERED TOKENS ----

 [('Content', 'JJ'), ('filters', 'NNS'), (':', ':'), ('Review', 'NNP'), ('content', 'NN'), ('within', 'IN'), ('message', 'NN'), ('determine', 'NN'), ('whether', 'IN'), ('spam', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Content filters', 'filters :', ': Review', 'Review content', 'content within', 'within message', 'message determine', 'determine whether', 'whether spam', 'spam .'] 

 TOTAL BIGRAMS --> 10 



 ---- TRI-GRAMS ---- 

 ['Content filters :', 'filters : Review', ': Review content', 'Review content within', 'content within message', 'within message determine', 'message determine whether', 'determine whether spam', 'whether spam .'] 

 TOTAL TRIGRAMS --> 9 



 ---- NOUN PHRASES ---- 

 ['content', 'message', 'determine', 'spam'] 

 TOTAL NOUN PHRASES --> 4 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['content', 'filter', ':', 'review', 'content', 'within', 'messag', 'determin', 'whether', 'spam', '.']

 TOTAL PORTER STEM WORDS ==> 11



 ---- SNOWBALL STEMMING ----

['content', 'filter', ':', 'review', 'content', 'within', 'messag', 'determin', 'whether', 'spam', '.']

 TOTAL SNOWBALL STEM WORDS ==> 11



 ---- LEMMATIZATION ----

['Content', 'filter', ':', 'Review', 'content', 'within', 'message', 'determine', 'whether', 'spam', '.']

 TOTAL LEMMATIZE WORDS ==> 11

************************************************************************************************************************

196 --> Header filters: Review the email header  looking for fake information. 


 ---- TOKENS ----

 ['Header', 'filters', ':', 'Review', 'the', 'email', 'header', 'looking', 'for', 'fake', 'information', '.'] 

 TOTAL TOKENS ==> 12

 ---- POST ----

 [('Header', 'NN'), ('filters', 'NNS'), (':', ':'), ('Review', 'VB'), ('the', 'DT'), ('email', 'NN'), ('header', 'NN'), ('looking', 'VBG'), ('for', 'IN'), ('fake', 'JJ'), ('information', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Header', 'filters', ':', 'Review', 'email', 'header', 'looking', 'fake', 'information', '.']

 TOTAL FILTERED TOKENS ==>  10

 ---- POST FOR FILTERED TOKENS ----

 [('Header', 'NN'), ('filters', 'NNS'), (':', ':'), ('Review', 'NNP'), ('email', 'VBP'), ('header', 'NN'), ('looking', 'VBG'), ('fake', 'JJ'), ('information', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Header filters', 'filters :', ': Review', 'Review email', 'email header', 'header looking', 'looking fake', 'fake information', 'information .'] 

 TOTAL BIGRAMS --> 9 



 ---- TRI-GRAMS ---- 

 ['Header filters :', 'filters : Review', ': Review email', 'Review email header', 'email header looking', 'header looking fake', 'looking fake information', 'fake information .'] 

 TOTAL TRIGRAMS --> 8 



 ---- NOUN PHRASES ---- 

 ['Header', 'header', 'fake information'] 

 TOTAL NOUN PHRASES --> 3 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> ['Header']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['header', 'filter', ':', 'review', 'email', 'header', 'look', 'fake', 'inform', '.']

 TOTAL PORTER STEM WORDS ==> 10



 ---- SNOWBALL STEMMING ----

['header', 'filter', ':', 'review', 'email', 'header', 'look', 'fake', 'inform', '.']

 TOTAL SNOWBALL STEM WORDS ==> 10



 ---- LEMMATIZATION ----

['Header', 'filter', ':', 'Review', 'email', 'header', 'looking', 'fake', 'information', '.']

 TOTAL LEMMATIZE WORDS ==> 10

************************************************************************************************************************

197 --> General Blacklist filters: Stopes all emails from blacklisted  recipients. 


 ---- TOKENS ----

 ['General', 'Blacklist', 'filters', ':', 'Stopes', 'all', 'emails', 'from', 'blacklisted', 'recipients', '.'] 

 TOTAL TOKENS ==> 11

 ---- POST ----

 [('General', 'NNP'), ('Blacklist', 'NNP'), ('filters', 'NNS'), (':', ':'), ('Stopes', 'NNP'), ('all', 'DT'), ('emails', 'NNS'), ('from', 'IN'), ('blacklisted', 'VBN'), ('recipients', 'NNS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['General', 'Blacklist', 'filters', ':', 'Stopes', 'emails', 'blacklisted', 'recipients', '.']

 TOTAL FILTERED TOKENS ==>  9

 ---- POST FOR FILTERED TOKENS ----

 [('General', 'NNP'), ('Blacklist', 'NNP'), ('filters', 'NNS'), (':', ':'), ('Stopes', 'NNP'), ('emails', 'VBZ'), ('blacklisted', 'VBN'), ('recipients', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['General Blacklist', 'Blacklist filters', 'filters :', ': Stopes', 'Stopes emails', 'emails blacklisted', 'blacklisted recipients', 'recipients .'] 

 TOTAL BIGRAMS --> 8 



 ---- TRI-GRAMS ---- 

 ['General Blacklist filters', 'Blacklist filters :', 'filters : Stopes', ': Stopes emails', 'Stopes emails blacklisted', 'emails blacklisted recipients', 'blacklisted recipients .'] 

 TOTAL TRIGRAMS --> 7 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> ['Stopes']
 TOTAL PERSON ENTITY --> 1 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['gener', 'blacklist', 'filter', ':', 'stope', 'email', 'blacklist', 'recipi', '.']

 TOTAL PORTER STEM WORDS ==> 9



 ---- SNOWBALL STEMMING ----

['general', 'blacklist', 'filter', ':', 'stope', 'email', 'blacklist', 'recipi', '.']

 TOTAL SNOWBALL STEM WORDS ==> 9



 ---- LEMMATIZATION ----

['General', 'Blacklist', 'filter', ':', 'Stopes', 'email', 'blacklisted', 'recipient', '.']

 TOTAL LEMMATIZE WORDS ==> 9

************************************************************************************************************************

198 --> Rules Based Filters: It uses user-defined criteria. 


 ---- TOKENS ----

 ['Rules', 'Based', 'Filters', ':', 'It', 'uses', 'user-defined', 'criteria', '.'] 

 TOTAL TOKENS ==> 9

 ---- POST ----

 [('Rules', 'NNS'), ('Based', 'VBD'), ('Filters', 'NNS'), (':', ':'), ('It', 'PRP'), ('uses', 'VBZ'), ('user-defined', 'JJ'), ('criteria', 'NNS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Rules', 'Based', 'Filters', ':', 'uses', 'user-defined', 'criteria', '.']

 TOTAL FILTERED TOKENS ==>  8

 ---- POST FOR FILTERED TOKENS ----

 [('Rules', 'NNS'), ('Based', 'VBD'), ('Filters', 'NNS'), (':', ':'), ('uses', 'VBZ'), ('user-defined', 'JJ'), ('criteria', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Rules Based', 'Based Filters', 'Filters :', ': uses', 'uses user-defined', 'user-defined criteria', 'criteria .'] 

 TOTAL BIGRAMS --> 7 



 ---- TRI-GRAMS ---- 

 ['Rules Based Filters', 'Based Filters :', 'Filters : uses', ': uses user-defined', 'uses user-defined criteria', 'user-defined criteria .'] 

 TOTAL TRIGRAMS --> 6 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> ['Rules']
 TOTAL PERSON ENTITY --> 1 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['rule', 'base', 'filter', ':', 'use', 'user-defin', 'criteria', '.']

 TOTAL PORTER STEM WORDS ==> 8



 ---- SNOWBALL STEMMING ----

['rule', 'base', 'filter', ':', 'use', 'user-defin', 'criteria', '.']

 TOTAL SNOWBALL STEM WORDS ==> 8



 ---- LEMMATIZATION ----

['Rules', 'Based', 'Filters', ':', 'us', 'user-defined', 'criterion', '.']

 TOTAL LEMMATIZE WORDS ==> 8

************************************************************************************************************************

199 --> Such as stopping mails from  specific person or stopping mail including a specific word. 


 ---- TOKENS ----

 ['Such', 'as', 'stopping', 'mails', 'from', 'specific', 'person', 'or', 'stopping', 'mail', 'including', 'a', 'specific', 'word', '.'] 

 TOTAL TOKENS ==> 15

 ---- POST ----

 [('Such', 'JJ'), ('as', 'IN'), ('stopping', 'VBG'), ('mails', 'NNS'), ('from', 'IN'), ('specific', 'JJ'), ('person', 'NN'), ('or', 'CC'), ('stopping', 'VBG'), ('mail', 'NN'), ('including', 'VBG'), ('a', 'DT'), ('specific', 'JJ'), ('word', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['stopping', 'mails', 'specific', 'person', 'stopping', 'mail', 'including', 'specific', 'word', '.']

 TOTAL FILTERED TOKENS ==>  10

 ---- POST FOR FILTERED TOKENS ----

 [('stopping', 'VBG'), ('mails', 'NNS'), ('specific', 'JJ'), ('person', 'NN'), ('stopping', 'VBG'), ('mail', 'NN'), ('including', 'VBG'), ('specific', 'JJ'), ('word', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['stopping mails', 'mails specific', 'specific person', 'person stopping', 'stopping mail', 'mail including', 'including specific', 'specific word', 'word .'] 

 TOTAL BIGRAMS --> 9 



 ---- TRI-GRAMS ---- 

 ['stopping mails specific', 'mails specific person', 'specific person stopping', 'person stopping mail', 'stopping mail including', 'mail including specific', 'including specific word', 'specific word .'] 

 TOTAL TRIGRAMS --> 8 



 ---- NOUN PHRASES ---- 

 ['specific person', 'mail', 'specific word'] 

 TOTAL NOUN PHRASES --> 3 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['stop', 'mail', 'specif', 'person', 'stop', 'mail', 'includ', 'specif', 'word', '.']

 TOTAL PORTER STEM WORDS ==> 10



 ---- SNOWBALL STEMMING ----

['stop', 'mail', 'specif', 'person', 'stop', 'mail', 'includ', 'specif', 'word', '.']

 TOTAL SNOWBALL STEM WORDS ==> 10



 ---- LEMMATIZATION ----

['stopping', 'mail', 'specific', 'person', 'stopping', 'mail', 'including', 'specific', 'word', '.']

 TOTAL LEMMATIZE WORDS ==> 10

************************************************************************************************************************

200 --> Permission Filters: Require  anyone sending a message to be pre-approved by the recipient. 


 ---- TOKENS ----

 ['Permission', 'Filters', ':', 'Require', 'anyone', 'sending', 'a', 'message', 'to', 'be', 'pre-approved', 'by', 'the', 'recipient', '.'] 

 TOTAL TOKENS ==> 15

 ---- POST ----

 [('Permission', 'NN'), ('Filters', 'NNS'), (':', ':'), ('Require', 'NNP'), ('anyone', 'NN'), ('sending', 'VBG'), ('a', 'DT'), ('message', 'NN'), ('to', 'TO'), ('be', 'VB'), ('pre-approved', 'JJ'), ('by', 'IN'), ('the', 'DT'), ('recipient', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Permission', 'Filters', ':', 'Require', 'anyone', 'sending', 'message', 'pre-approved', 'recipient', '.']

 TOTAL FILTERED TOKENS ==>  10

 ---- POST FOR FILTERED TOKENS ----

 [('Permission', 'NN'), ('Filters', 'NNS'), (':', ':'), ('Require', 'NNP'), ('anyone', 'NN'), ('sending', 'VBG'), ('message', 'NN'), ('pre-approved', 'JJ'), ('recipient', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Permission Filters', 'Filters :', ': Require', 'Require anyone', 'anyone sending', 'sending message', 'message pre-approved', 'pre-approved recipient', 'recipient .'] 

 TOTAL BIGRAMS --> 9 



 ---- TRI-GRAMS ---- 

 ['Permission Filters :', 'Filters : Require', ': Require anyone', 'Require anyone sending', 'anyone sending message', 'sending message pre-approved', 'message pre-approved recipient', 'pre-approved recipient .'] 

 TOTAL TRIGRAMS --> 8 



 ---- NOUN PHRASES ---- 

 ['Permission', 'anyone', 'message', 'pre-approved recipient'] 

 TOTAL NOUN PHRASES --> 4 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> ['Permission']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['permiss', 'filter', ':', 'requir', 'anyon', 'send', 'messag', 'pre-approv', 'recipi', '.']

 TOTAL PORTER STEM WORDS ==> 10



 ---- SNOWBALL STEMMING ----

['permiss', 'filter', ':', 'requir', 'anyon', 'send', 'messag', 'pre-approv', 'recipi', '.']

 TOTAL SNOWBALL STEM WORDS ==> 10



 ---- LEMMATIZATION ----

['Permission', 'Filters', ':', 'Require', 'anyone', 'sending', 'message', 'pre-approved', 'recipient', '.']

 TOTAL LEMMATIZE WORDS ==> 10

************************************************************************************************************************

201 --> Challenge Response Filters:  Requires anyone sending a message to enter a code in order to gain permission to send email. 


 ---- TOKENS ----

 ['Challenge', 'Response', 'Filters', ':', 'Requires', 'anyone', 'sending', 'a', 'message', 'to', 'enter', 'a', 'code', 'in', 'order', 'to', 'gain', 'permission', 'to', 'send', 'email', '.'] 

 TOTAL TOKENS ==> 22

 ---- POST ----

 [('Challenge', 'NNP'), ('Response', 'NNP'), ('Filters', 'NNS'), (':', ':'), ('Requires', 'VBZ'), ('anyone', 'NN'), ('sending', 'VBG'), ('a', 'DT'), ('message', 'NN'), ('to', 'TO'), ('enter', 'VB'), ('a', 'DT'), ('code', 'NN'), ('in', 'IN'), ('order', 'NN'), ('to', 'TO'), ('gain', 'VB'), ('permission', 'NN'), ('to', 'TO'), ('send', 'VB'), ('email', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Challenge', 'Response', 'Filters', ':', 'Requires', 'anyone', 'sending', 'message', 'enter', 'code', 'order', 'gain', 'permission', 'send', 'email', '.']

 TOTAL FILTERED TOKENS ==>  16

 ---- POST FOR FILTERED TOKENS ----

 [('Challenge', 'NNP'), ('Response', 'NNP'), ('Filters', 'NNS'), (':', ':'), ('Requires', 'VBZ'), ('anyone', 'NN'), ('sending', 'VBG'), ('message', 'NN'), ('enter', 'NN'), ('code', 'NN'), ('order', 'NN'), ('gain', 'NN'), ('permission', 'NN'), ('send', 'NN'), ('email', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Challenge Response', 'Response Filters', 'Filters :', ': Requires', 'Requires anyone', 'anyone sending', 'sending message', 'message enter', 'enter code', 'code order', 'order gain', 'gain permission', 'permission send', 'send email', 'email .'] 

 TOTAL BIGRAMS --> 15 



 ---- TRI-GRAMS ---- 

 ['Challenge Response Filters', 'Response Filters :', 'Filters : Requires', ': Requires anyone', 'Requires anyone sending', 'anyone sending message', 'sending message enter', 'message enter code', 'enter code order', 'code order gain', 'order gain permission', 'gain permission send', 'permission send email', 'send email .'] 

 TOTAL TRIGRAMS --> 14 



 ---- NOUN PHRASES ---- 

 ['anyone', 'message', 'enter', 'code', 'order', 'gain', 'permission', 'send', 'email'] 

 TOTAL NOUN PHRASES --> 9 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> ['Challenge']
 TOTAL PERSON ENTITY --> 1 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['challeng', 'respons', 'filter', ':', 'requir', 'anyon', 'send', 'messag', 'enter', 'code', 'order', 'gain', 'permiss', 'send', 'email', '.']

 TOTAL PORTER STEM WORDS ==> 16



 ---- SNOWBALL STEMMING ----

['challeng', 'respons', 'filter', ':', 'requir', 'anyon', 'send', 'messag', 'enter', 'code', 'order', 'gain', 'permiss', 'send', 'email', '.']

 TOTAL SNOWBALL STEM WORDS ==> 16



 ---- LEMMATIZATION ----

['Challenge', 'Response', 'Filters', ':', 'Requires', 'anyone', 'sending', 'message', 'enter', 'code', 'order', 'gain', 'permission', 'send', 'email', '.']

 TOTAL LEMMATIZE WORDS ==> 16

************************************************************************************************************************

202 --> 6.3 Spam Filtering   It works using text categorization and in recent times, various machine learning techniques  have been applied to text categorization or Anti-Spam Filtering  like Rule Learning (Cohen  1996)[45], Naïve Bayes (Sahami et al., 1998 ;Androutsopoulos et al.,2000b ;Rennie  .,2000)[46][47][48],Memory based Learning (Androutsopoulos et al.,2000b)[47], Support  vector machines (Druker et al., 1999)[49], Decision Trees (Carreras and Marquez , 2001)[50]  Maximum Entropy Model (Berger et al. 


 ---- TOKENS ----

 ['6.3', 'Spam', 'Filtering', 'It', 'works', 'using', 'text', 'categorization', 'and', 'in', 'recent', 'times', ',', 'various', 'machine', 'learning', 'techniques', 'have', 'been', 'applied', 'to', 'text', 'categorization', 'or', 'Anti-Spam', 'Filtering', 'like', 'Rule', 'Learning', '(', 'Cohen', '1996', ')', '[', '45', ']', ',', 'Naïve', 'Bayes', '(', 'Sahami', 'et', 'al.', ',', '1998', ';', 'Androutsopoulos', 'et', 'al.,2000b', ';', 'Rennie', '.,2000', ')', '[', '46', ']', '[', '47', ']', '[', '48', ']', ',', 'Memory', 'based', 'Learning', '(', 'Androutsopoulos', 'et', 'al.,2000b', ')', '[', '47', ']', ',', 'Support', 'vector', 'machines', '(', 'Druker', 'et', 'al.', ',', '1999', ')', '[', '49', ']', ',', 'Decision', 'Trees', '(', 'Carreras', 'and', 'Marquez', ',', '2001', ')', '[', '50', ']', 'Maximum', 'Entropy', 'Model', '(', 'Berger', 'et', 'al', '.'] 

 TOTAL TOKENS ==> 109

 ---- POST ----

 [('6.3', 'CD'), ('Spam', 'NNP'), ('Filtering', 'VBG'), ('It', 'PRP'), ('works', 'VBZ'), ('using', 'VBG'), ('text', 'JJ'), ('categorization', 'NN'), ('and', 'CC'), ('in', 'IN'), ('recent', 'JJ'), ('times', 'NNS'), (',', ','), ('various', 'JJ'), ('machine', 'NN'), ('learning', 'VBG'), ('techniques', 'NNS'), ('have', 'VBP'), ('been', 'VBN'), ('applied', 'VBN'), ('to', 'TO'), ('text', 'VB'), ('categorization', 'NN'), ('or', 'CC'), ('Anti-Spam', 'JJ'), ('Filtering', 'NNP'), ('like', 'IN'), ('Rule', 'NNP'), ('Learning', 'NNP'), ('(', '('), ('Cohen', 'NNP'), ('1996', 'CD'), (')', ')'), ('[', 'VBD'), ('45', 'CD'), (']', 'NN'), (',', ','), ('Naïve', 'NNP'), ('Bayes', 'NNP'), ('(', '('), ('Sahami', 'NNP'), ('et', 'RB'), ('al.', 'RB'), (',', ','), ('1998', 'CD'), (';', ':'), ('Androutsopoulos', 'NNP'), ('et', 'CC'), ('al.,2000b', 'NN'), (';', ':'), ('Rennie', 'NNP'), ('.,2000', 'NNP'), (')', ')'), ('[', 'VBD'), ('46', 'CD'), (']', 'NNP'), ('[', 'VBD'), ('47', 'CD'), (']', 'NNP'), ('[', 'VBD'), ('48', 'CD'), (']', 'NN'), (',', ','), ('Memory', 'NNP'), ('based', 'VBN'), ('Learning', 'NNP'), ('(', '('), ('Androutsopoulos', 'NNP'), ('et', 'RB'), ('al.,2000b', 'RB'), (')', ')'), ('[', 'VBZ'), ('47', 'CD'), (']', 'NN'), (',', ','), ('Support', 'NNP'), ('vector', 'NN'), ('machines', 'NNS'), ('(', '('), ('Druker', 'NNP'), ('et', 'VBZ'), ('al.', 'RB'), (',', ','), ('1999', 'CD'), (')', ')'), ('[', 'VBD'), ('49', 'CD'), (']', 'NN'), (',', ','), ('Decision', 'NNP'), ('Trees', 'NNP'), ('(', '('), ('Carreras', 'NNP'), ('and', 'CC'), ('Marquez', 'NNP'), (',', ','), ('2001', 'CD'), (')', ')'), ('[', 'VBD'), ('50', 'CD'), (']', 'NNP'), ('Maximum', 'NNP'), ('Entropy', 'NNP'), ('Model', 'NNP'), ('(', '('), ('Berger', 'NNP'), ('et', 'VBZ'), ('al', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['6.3', 'Spam', 'Filtering', 'works', 'using', 'text', 'categorization', 'recent', 'times', ',', 'various', 'machine', 'learning', 'techniques', 'applied', 'text', 'categorization', 'Anti-Spam', 'Filtering', 'like', 'Rule', 'Learning', '(', 'Cohen', '1996', ')', '[', '45', ']', ',', 'Naïve', 'Bayes', '(', 'Sahami', 'et', 'al.', ',', '1998', ';', 'Androutsopoulos', 'et', 'al.,2000b', ';', 'Rennie', '.,2000', ')', '[', '46', ']', '[', '47', ']', '[', '48', ']', ',', 'Memory', 'based', 'Learning', '(', 'Androutsopoulos', 'et', 'al.,2000b', ')', '[', '47', ']', ',', 'Support', 'vector', 'machines', '(', 'Druker', 'et', 'al.', ',', '1999', ')', '[', '49', ']', ',', 'Decision', 'Trees', '(', 'Carreras', 'Marquez', ',', '2001', ')', '[', '50', ']', 'Maximum', 'Entropy', 'Model', '(', 'Berger', 'et', 'al', '.']

 TOTAL FILTERED TOKENS ==>  101

 ---- POST FOR FILTERED TOKENS ----

 [('6.3', 'CD'), ('Spam', 'NNP'), ('Filtering', 'NNP'), ('works', 'VBZ'), ('using', 'VBG'), ('text', 'JJ'), ('categorization', 'NN'), ('recent', 'JJ'), ('times', 'NNS'), (',', ','), ('various', 'JJ'), ('machine', 'NN'), ('learning', 'VBG'), ('techniques', 'NNS'), ('applied', 'VBN'), ('text', 'JJ'), ('categorization', 'NN'), ('Anti-Spam', 'NNP'), ('Filtering', 'NNP'), ('like', 'IN'), ('Rule', 'NNP'), ('Learning', 'NNP'), ('(', '('), ('Cohen', 'NNP'), ('1996', 'CD'), (')', ')'), ('[', 'VBD'), ('45', 'CD'), (']', 'NN'), (',', ','), ('Naïve', 'NNP'), ('Bayes', 'NNP'), ('(', '('), ('Sahami', 'NNP'), ('et', 'RB'), ('al.', 'RB'), (',', ','), ('1998', 'CD'), (';', ':'), ('Androutsopoulos', 'NNP'), ('et', 'CC'), ('al.,2000b', 'NN'), (';', ':'), ('Rennie', 'NNP'), ('.,2000', 'NNP'), (')', ')'), ('[', 'VBD'), ('46', 'CD'), (']', 'NNP'), ('[', 'VBD'), ('47', 'CD'), (']', 'NNP'), ('[', 'VBD'), ('48', 'CD'), (']', 'NN'), (',', ','), ('Memory', 'NNP'), ('based', 'VBN'), ('Learning', 'NNP'), ('(', '('), ('Androutsopoulos', 'NNP'), ('et', 'RB'), ('al.,2000b', 'RB'), (')', ')'), ('[', 'VBZ'), ('47', 'CD'), (']', 'NN'), (',', ','), ('Support', 'NNP'), ('vector', 'NN'), ('machines', 'NNS'), ('(', '('), ('Druker', 'NNP'), ('et', 'VBZ'), ('al.', 'RB'), (',', ','), ('1999', 'CD'), (')', ')'), ('[', 'VBD'), ('49', 'CD'), (']', 'NN'), (',', ','), ('Decision', 'NNP'), ('Trees', 'NNP'), ('(', '('), ('Carreras', 'NNP'), ('Marquez', 'NNP'), (',', ','), ('2001', 'CD'), (')', ')'), ('[', 'VBD'), ('50', 'CD'), (']', 'NNP'), ('Maximum', 'NNP'), ('Entropy', 'NNP'), ('Model', 'NNP'), ('(', '('), ('Berger', 'NNP'), ('et', 'VBZ'), ('al', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['6.3 Spam', 'Spam Filtering', 'Filtering works', 'works using', 'using text', 'text categorization', 'categorization recent', 'recent times', 'times ,', ', various', 'various machine', 'machine learning', 'learning techniques', 'techniques applied', 'applied text', 'text categorization', 'categorization Anti-Spam', 'Anti-Spam Filtering', 'Filtering like', 'like Rule', 'Rule Learning', 'Learning (', '( Cohen', 'Cohen 1996', '1996 )', ') [', '[ 45', '45 ]', '] ,', ', Naïve', 'Naïve Bayes', 'Bayes (', '( Sahami', 'Sahami et', 'et al.', 'al. ,', ', 1998', '1998 ;', '; Androutsopoulos', 'Androutsopoulos et', 'et al.,2000b', 'al.,2000b ;', '; Rennie', 'Rennie .,2000', '.,2000 )', ') [', '[ 46', '46 ]', '] [', '[ 47', '47 ]', '] [', '[ 48', '48 ]', '] ,', ', Memory', 'Memory based', 'based Learning', 'Learning (', '( Androutsopoulos', 'Androutsopoulos et', 'et al.,2000b', 'al.,2000b )', ') [', '[ 47', '47 ]', '] ,', ', Support', 'Support vector', 'vector machines', 'machines (', '( Druker', 'Druker et', 'et al.', 'al. ,', ', 1999', '1999 )', ') [', '[ 49', '49 ]', '] ,', ', Decision', 'Decision Trees', 'Trees (', '( Carreras', 'Carreras Marquez', 'Marquez ,', ', 2001', '2001 )', ') [', '[ 50', '50 ]', '] Maximum', 'Maximum Entropy', 'Entropy Model', 'Model (', '( Berger', 'Berger et', 'et al', 'al .'] 

 TOTAL BIGRAMS --> 100 



 ---- TRI-GRAMS ---- 

 ['6.3 Spam Filtering', 'Spam Filtering works', 'Filtering works using', 'works using text', 'using text categorization', 'text categorization recent', 'categorization recent times', 'recent times ,', 'times , various', ', various machine', 'various machine learning', 'machine learning techniques', 'learning techniques applied', 'techniques applied text', 'applied text categorization', 'text categorization Anti-Spam', 'categorization Anti-Spam Filtering', 'Anti-Spam Filtering like', 'Filtering like Rule', 'like Rule Learning', 'Rule Learning (', 'Learning ( Cohen', '( Cohen 1996', 'Cohen 1996 )', '1996 ) [', ') [ 45', '[ 45 ]', '45 ] ,', '] , Naïve', ', Naïve Bayes', 'Naïve Bayes (', 'Bayes ( Sahami', '( Sahami et', 'Sahami et al.', 'et al. ,', 'al. , 1998', ', 1998 ;', '1998 ; Androutsopoulos', '; Androutsopoulos et', 'Androutsopoulos et al.,2000b', 'et al.,2000b ;', 'al.,2000b ; Rennie', '; Rennie .,2000', 'Rennie .,2000 )', '.,2000 ) [', ') [ 46', '[ 46 ]', '46 ] [', '] [ 47', '[ 47 ]', '47 ] [', '] [ 48', '[ 48 ]', '48 ] ,', '] , Memory', ', Memory based', 'Memory based Learning', 'based Learning (', 'Learning ( Androutsopoulos', '( Androutsopoulos et', 'Androutsopoulos et al.,2000b', 'et al.,2000b )', 'al.,2000b ) [', ') [ 47', '[ 47 ]', '47 ] ,', '] , Support', ', Support vector', 'Support vector machines', 'vector machines (', 'machines ( Druker', '( Druker et', 'Druker et al.', 'et al. ,', 'al. , 1999', ', 1999 )', '1999 ) [', ') [ 49', '[ 49 ]', '49 ] ,', '] , Decision', ', Decision Trees', 'Decision Trees (', 'Trees ( Carreras', '( Carreras Marquez', 'Carreras Marquez ,', 'Marquez , 2001', ', 2001 )', '2001 ) [', ') [ 50', '[ 50 ]', '50 ] Maximum', '] Maximum Entropy', 'Maximum Entropy Model', 'Entropy Model (', 'Model ( Berger', '( Berger et', 'Berger et al', 'et al .'] 

 TOTAL TRIGRAMS --> 99 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 []

 ---- PORTER STEMMING ----

['6.3', 'spam', 'filter', 'work', 'use', 'text', 'categor', 'recent', 'time', ',', 'variou', 'machin', 'learn', 'techniqu', 'appli', 'text', 'categor', 'anti-spam', 'filter', 'like', 'rule', 'learn', '(', 'cohen', '1996', ')', '[', '45', ']', ',', 'naïv', 'bay', '(', 'sahami', 'et', 'al.', ',', '1998', ';', 'androutsopoulo', 'et', 'al.,2000b', ';', 'renni', '.,2000', ')', '[', '46', ']', '[', '47', ']', '[', '48', ']', ',', 'memori', 'base', 'learn', '(', 'androutsopoulo', 'et', 'al.,2000b', ')', '[', '47', ']', ',', 'support', 'vector', 'machin', '(', 'druker', 'et', 'al.', ',', '1999', ')', '[', '49', ']', ',', 'decis', 'tree', '(', 'carrera', 'marquez', ',', '2001', ')', '[', '50', ']', 'maximum', 'entropi', 'model', '(', 'berger', 'et', 'al', '.']

 TOTAL PORTER STEM WORDS ==> 101



 ---- SNOWBALL STEMMING ----

['6.3', 'spam', 'filter', 'work', 'use', 'text', 'categor', 'recent', 'time', ',', 'various', 'machin', 'learn', 'techniqu', 'appli', 'text', 'categor', 'anti-spam', 'filter', 'like', 'rule', 'learn', '(', 'cohen', '1996', ')', '[', '45', ']', ',', 'naïv', 'bay', '(', 'sahami', 'et', 'al.', ',', '1998', ';', 'androutsopoulo', 'et', 'al.,2000b', ';', 'renni', '.,2000', ')', '[', '46', ']', '[', '47', ']', '[', '48', ']', ',', 'memori', 'base', 'learn', '(', 'androutsopoulo', 'et', 'al.,2000b', ')', '[', '47', ']', ',', 'support', 'vector', 'machin', '(', 'druker', 'et', 'al.', ',', '1999', ')', '[', '49', ']', ',', 'decis', 'tree', '(', 'carrera', 'marquez', ',', '2001', ')', '[', '50', ']', 'maximum', 'entropi', 'model', '(', 'berger', 'et', 'al', '.']

 TOTAL SNOWBALL STEM WORDS ==> 101



 ---- LEMMATIZATION ----

['6.3', 'Spam', 'Filtering', 'work', 'using', 'text', 'categorization', 'recent', 'time', ',', 'various', 'machine', 'learning', 'technique', 'applied', 'text', 'categorization', 'Anti-Spam', 'Filtering', 'like', 'Rule', 'Learning', '(', 'Cohen', '1996', ')', '[', '45', ']', ',', 'Naïve', 'Bayes', '(', 'Sahami', 'et', 'al.', ',', '1998', ';', 'Androutsopoulos', 'et', 'al.,2000b', ';', 'Rennie', '.,2000', ')', '[', '46', ']', '[', '47', ']', '[', '48', ']', ',', 'Memory', 'based', 'Learning', '(', 'Androutsopoulos', 'et', 'al.,2000b', ')', '[', '47', ']', ',', 'Support', 'vector', 'machine', '(', 'Druker', 'et', 'al.', ',', '1999', ')', '[', '49', ']', ',', 'Decision', 'Trees', '(', 'Carreras', 'Marquez', ',', '2001', ')', '[', '50', ']', 'Maximum', 'Entropy', 'Model', '(', 'Berger', 'et', 'al', '.']

 TOTAL LEMMATIZE WORDS ==> 101

************************************************************************************************************************

203 --> 1996)[51]. 


 ---- TOKENS ----

 ['1996', ')', '[', '51', ']', '.'] 

 TOTAL TOKENS ==> 6

 ---- POST ----

 [('1996', 'CD'), (')', ')'), ('[', 'VBD'), ('51', 'CD'), (']', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['1996', ')', '[', '51', ']', '.']

 TOTAL FILTERED TOKENS ==>  6

 ---- POST FOR FILTERED TOKENS ----

 [('1996', 'CD'), (')', ')'), ('[', 'VBD'), ('51', 'CD'), (']', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['1996 )', ') [', '[ 51', '51 ]', '] .'] 

 TOTAL BIGRAMS --> 5 



 ---- TRI-GRAMS ---- 

 ['1996 ) [', ') [ 51', '[ 51 ]', '51 ] .'] 

 TOTAL TRIGRAMS --> 4 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 []

 ---- PORTER STEMMING ----

['1996', ')', '[', '51', ']', '.']

 TOTAL PORTER STEM WORDS ==> 6



 ---- SNOWBALL STEMMING ----

['1996', ')', '[', '51', ']', '.']

 TOTAL SNOWBALL STEM WORDS ==> 6



 ---- LEMMATIZATION ----

['1996', ')', '[', '51', ']', '.']

 TOTAL LEMMATIZE WORDS ==> 6

************************************************************************************************************************

204 --> Sometimes combining different learners  (Sakkis et al., 2001) [52]. 


 ---- TOKENS ----

 ['Sometimes', 'combining', 'different', 'learners', '(', 'Sakkis', 'et', 'al.', ',', '2001', ')', '[', '52', ']', '.'] 

 TOTAL TOKENS ==> 15

 ---- POST ----

 [('Sometimes', 'RB'), ('combining', 'VBG'), ('different', 'JJ'), ('learners', 'NNS'), ('(', '('), ('Sakkis', 'NNP'), ('et', 'RB'), ('al.', 'RB'), (',', ','), ('2001', 'CD'), (')', ')'), ('[', 'VBD'), ('52', 'CD'), (']', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Sometimes', 'combining', 'different', 'learners', '(', 'Sakkis', 'et', 'al.', ',', '2001', ')', '[', '52', ']', '.']

 TOTAL FILTERED TOKENS ==>  15

 ---- POST FOR FILTERED TOKENS ----

 [('Sometimes', 'RB'), ('combining', 'VBG'), ('different', 'JJ'), ('learners', 'NNS'), ('(', '('), ('Sakkis', 'NNP'), ('et', 'RB'), ('al.', 'RB'), (',', ','), ('2001', 'CD'), (')', ')'), ('[', 'VBD'), ('52', 'CD'), (']', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Sometimes combining', 'combining different', 'different learners', 'learners (', '( Sakkis', 'Sakkis et', 'et al.', 'al. ,', ', 2001', '2001 )', ') [', '[ 52', '52 ]', '] .'] 

 TOTAL BIGRAMS --> 14 



 ---- TRI-GRAMS ---- 

 ['Sometimes combining different', 'combining different learners', 'different learners (', 'learners ( Sakkis', '( Sakkis et', 'Sakkis et al.', 'et al. ,', 'al. , 2001', ', 2001 )', '2001 ) [', ') [ 52', '[ 52 ]', '52 ] .'] 

 TOTAL TRIGRAMS --> 13 



 ---- NOUN PHRASES ---- 

 [']'] 

 TOTAL NOUN PHRASES --> 1 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['sometim', 'combin', 'differ', 'learner', '(', 'sakki', 'et', 'al.', ',', '2001', ')', '[', '52', ']', '.']

 TOTAL PORTER STEM WORDS ==> 15



 ---- SNOWBALL STEMMING ----

['sometim', 'combin', 'differ', 'learner', '(', 'sakki', 'et', 'al.', ',', '2001', ')', '[', '52', ']', '.']

 TOTAL SNOWBALL STEM WORDS ==> 15



 ---- LEMMATIZATION ----

['Sometimes', 'combining', 'different', 'learner', '(', 'Sakkis', 'et', 'al.', ',', '2001', ')', '[', '52', ']', '.']

 TOTAL LEMMATIZE WORDS ==> 15

************************************************************************************************************************

205 --> Using these approaches is better as classifier is learned from  training data rather than making by hand. 


 ---- TOKENS ----

 ['Using', 'these', 'approaches', 'is', 'better', 'as', 'classifier', 'is', 'learned', 'from', 'training', 'data', 'rather', 'than', 'making', 'by', 'hand', '.'] 

 TOTAL TOKENS ==> 18

 ---- POST ----

 [('Using', 'VBG'), ('these', 'DT'), ('approaches', 'NNS'), ('is', 'VBZ'), ('better', 'RB'), ('as', 'IN'), ('classifier', 'NN'), ('is', 'VBZ'), ('learned', 'VBN'), ('from', 'IN'), ('training', 'VBG'), ('data', 'NNS'), ('rather', 'RB'), ('than', 'IN'), ('making', 'VBG'), ('by', 'IN'), ('hand', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Using', 'approaches', 'better', 'classifier', 'learned', 'training', 'data', 'rather', 'making', 'hand', '.']

 TOTAL FILTERED TOKENS ==>  11

 ---- POST FOR FILTERED TOKENS ----

 [('Using', 'VBG'), ('approaches', 'NNS'), ('better', 'RBR'), ('classifier', 'NN'), ('learned', 'VBD'), ('training', 'VBG'), ('data', 'NNS'), ('rather', 'RB'), ('making', 'VBG'), ('hand', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Using approaches', 'approaches better', 'better classifier', 'classifier learned', 'learned training', 'training data', 'data rather', 'rather making', 'making hand', 'hand .'] 

 TOTAL BIGRAMS --> 10 



 ---- TRI-GRAMS ---- 

 ['Using approaches better', 'approaches better classifier', 'better classifier learned', 'classifier learned training', 'learned training data', 'training data rather', 'data rather making', 'rather making hand', 'making hand .'] 

 TOTAL TRIGRAMS --> 9 



 ---- NOUN PHRASES ---- 

 ['classifier', 'hand'] 

 TOTAL NOUN PHRASES --> 2 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['use', 'approach', 'better', 'classifi', 'learn', 'train', 'data', 'rather', 'make', 'hand', '.']

 TOTAL PORTER STEM WORDS ==> 11



 ---- SNOWBALL STEMMING ----

['use', 'approach', 'better', 'classifi', 'learn', 'train', 'data', 'rather', 'make', 'hand', '.']

 TOTAL SNOWBALL STEM WORDS ==> 11



 ---- LEMMATIZATION ----

['Using', 'approach', 'better', 'classifier', 'learned', 'training', 'data', 'rather', 'making', 'hand', '.']

 TOTAL LEMMATIZE WORDS ==> 11

************************************************************************************************************************

206 --> The naïve bayes is preferred because of its  performance despite its simplicity (Lewis, 1998) [53] In Text Categorization two types of  models have been used (McCallum and Nigam, 1998) [54]. 


 ---- TOKENS ----

 ['The', 'naïve', 'bayes', 'is', 'preferred', 'because', 'of', 'its', 'performance', 'despite', 'its', 'simplicity', '(', 'Lewis', ',', '1998', ')', '[', '53', ']', 'In', 'Text', 'Categorization', 'two', 'types', 'of', 'models', 'have', 'been', 'used', '(', 'McCallum', 'and', 'Nigam', ',', '1998', ')', '[', '54', ']', '.'] 

 TOTAL TOKENS ==> 41

 ---- POST ----

 [('The', 'DT'), ('naïve', 'JJ'), ('bayes', 'NN'), ('is', 'VBZ'), ('preferred', 'VBN'), ('because', 'IN'), ('of', 'IN'), ('its', 'PRP$'), ('performance', 'NN'), ('despite', 'IN'), ('its', 'PRP$'), ('simplicity', 'NN'), ('(', '('), ('Lewis', 'NNP'), (',', ','), ('1998', 'CD'), (')', ')'), ('[', 'VBD'), ('53', 'CD'), (']', 'NN'), ('In', 'IN'), ('Text', 'NNP'), ('Categorization', 'NNP'), ('two', 'CD'), ('types', 'NNS'), ('of', 'IN'), ('models', 'NNS'), ('have', 'VBP'), ('been', 'VBN'), ('used', 'VBN'), ('(', '('), ('McCallum', 'NNP'), ('and', 'CC'), ('Nigam', 'NNP'), (',', ','), ('1998', 'CD'), (')', ')'), ('[', 'VBD'), ('54', 'CD'), (']', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['naïve', 'bayes', 'preferred', 'performance', 'despite', 'simplicity', '(', 'Lewis', ',', '1998', ')', '[', '53', ']', 'Text', 'Categorization', 'two', 'types', 'models', 'used', '(', 'McCallum', 'Nigam', ',', '1998', ')', '[', '54', ']', '.']

 TOTAL FILTERED TOKENS ==>  30

 ---- POST FOR FILTERED TOKENS ----

 [('naïve', 'JJ'), ('bayes', 'NNS'), ('preferred', 'VBD'), ('performance', 'NN'), ('despite', 'IN'), ('simplicity', 'NN'), ('(', '('), ('Lewis', 'NNP'), (',', ','), ('1998', 'CD'), (')', ')'), ('[', 'VBD'), ('53', 'CD'), (']', 'NNP'), ('Text', 'NNP'), ('Categorization', 'NNP'), ('two', 'CD'), ('types', 'NNS'), ('models', 'NNS'), ('used', 'VBN'), ('(', '('), ('McCallum', 'NNP'), ('Nigam', 'NNP'), (',', ','), ('1998', 'CD'), (')', ')'), ('[', 'VBD'), ('54', 'CD'), (']', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['naïve bayes', 'bayes preferred', 'preferred performance', 'performance despite', 'despite simplicity', 'simplicity (', '( Lewis', 'Lewis ,', ', 1998', '1998 )', ') [', '[ 53', '53 ]', '] Text', 'Text Categorization', 'Categorization two', 'two types', 'types models', 'models used', 'used (', '( McCallum', 'McCallum Nigam', 'Nigam ,', ', 1998', '1998 )', ') [', '[ 54', '54 ]', '] .'] 

 TOTAL BIGRAMS --> 29 



 ---- TRI-GRAMS ---- 

 ['naïve bayes preferred', 'bayes preferred performance', 'preferred performance despite', 'performance despite simplicity', 'despite simplicity (', 'simplicity ( Lewis', '( Lewis ,', 'Lewis , 1998', ', 1998 )', '1998 ) [', ') [ 53', '[ 53 ]', '53 ] Text', '] Text Categorization', 'Text Categorization two', 'Categorization two types', 'two types models', 'types models used', 'models used (', 'used ( McCallum', '( McCallum Nigam', 'McCallum Nigam ,', 'Nigam , 1998', ', 1998 )', '1998 ) [', ') [ 54', '[ 54 ]', '54 ] .'] 

 TOTAL TRIGRAMS --> 28 



 ---- NOUN PHRASES ---- 

 ['performance', 'simplicity', ']'] 

 TOTAL NOUN PHRASES --> 3 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['naïv', 'bay', 'prefer', 'perform', 'despit', 'simplic', '(', 'lewi', ',', '1998', ')', '[', '53', ']', 'text', 'categor', 'two', 'type', 'model', 'use', '(', 'mccallum', 'nigam', ',', '1998', ')', '[', '54', ']', '.']

 TOTAL PORTER STEM WORDS ==> 30



 ---- SNOWBALL STEMMING ----

['naïv', 'bay', 'prefer', 'perform', 'despit', 'simplic', '(', 'lewi', ',', '1998', ')', '[', '53', ']', 'text', 'categor', 'two', 'type', 'model', 'use', '(', 'mccallum', 'nigam', ',', '1998', ')', '[', '54', ']', '.']

 TOTAL SNOWBALL STEM WORDS ==> 30



 ---- LEMMATIZATION ----

['naïve', 'bayes', 'preferred', 'performance', 'despite', 'simplicity', '(', 'Lewis', ',', '1998', ')', '[', '53', ']', 'Text', 'Categorization', 'two', 'type', 'model', 'used', '(', 'McCallum', 'Nigam', ',', '1998', ')', '[', '54', ']', '.']

 TOTAL LEMMATIZE WORDS ==> 30

************************************************************************************************************************

207 --> Both modules assume that a fixed  vocabulary is present. 


 ---- TOKENS ----

 ['Both', 'modules', 'assume', 'that', 'a', 'fixed', 'vocabulary', 'is', 'present', '.'] 

 TOTAL TOKENS ==> 10

 ---- POST ----

 [('Both', 'DT'), ('modules', 'NNS'), ('assume', 'VBP'), ('that', 'IN'), ('a', 'DT'), ('fixed', 'JJ'), ('vocabulary', 'NN'), ('is', 'VBZ'), ('present', 'JJ'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['modules', 'assume', 'fixed', 'vocabulary', 'present', '.']

 TOTAL FILTERED TOKENS ==>  6

 ---- POST FOR FILTERED TOKENS ----

 [('modules', 'NNS'), ('assume', 'VBP'), ('fixed', 'VBN'), ('vocabulary', 'JJ'), ('present', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['modules assume', 'assume fixed', 'fixed vocabulary', 'vocabulary present', 'present .'] 

 TOTAL BIGRAMS --> 5 



 ---- TRI-GRAMS ---- 

 ['modules assume fixed', 'assume fixed vocabulary', 'fixed vocabulary present', 'vocabulary present .'] 

 TOTAL TRIGRAMS --> 4 



 ---- NOUN PHRASES ---- 

 ['vocabulary present'] 

 TOTAL NOUN PHRASES --> 1 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['modul', 'assum', 'fix', 'vocabulari', 'present', '.']

 TOTAL PORTER STEM WORDS ==> 6



 ---- SNOWBALL STEMMING ----

['modul', 'assum', 'fix', 'vocabulari', 'present', '.']

 TOTAL SNOWBALL STEM WORDS ==> 6



 ---- LEMMATIZATION ----

['module', 'assume', 'fixed', 'vocabulary', 'present', '.']

 TOTAL LEMMATIZE WORDS ==> 6

************************************************************************************************************************

208 --> But in first model a document is generated by first choosing a subset of  vocabulary and then using the selected words any number of times, at least once irrespective  of order. 


 ---- TOKENS ----

 ['But', 'in', 'first', 'model', 'a', 'document', 'is', 'generated', 'by', 'first', 'choosing', 'a', 'subset', 'of', 'vocabulary', 'and', 'then', 'using', 'the', 'selected', 'words', 'any', 'number', 'of', 'times', ',', 'at', 'least', 'once', 'irrespective', 'of', 'order', '.'] 

 TOTAL TOKENS ==> 33

 ---- POST ----

 [('But', 'CC'), ('in', 'IN'), ('first', 'JJ'), ('model', 'NN'), ('a', 'DT'), ('document', 'NN'), ('is', 'VBZ'), ('generated', 'VBN'), ('by', 'IN'), ('first', 'JJ'), ('choosing', 'VBG'), ('a', 'DT'), ('subset', 'NN'), ('of', 'IN'), ('vocabulary', 'JJ'), ('and', 'CC'), ('then', 'RB'), ('using', 'VBG'), ('the', 'DT'), ('selected', 'VBN'), ('words', 'NNS'), ('any', 'DT'), ('number', 'NN'), ('of', 'IN'), ('times', 'NNS'), (',', ','), ('at', 'IN'), ('least', 'JJS'), ('once', 'RB'), ('irrespective', 'JJ'), ('of', 'IN'), ('order', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['first', 'model', 'document', 'generated', 'first', 'choosing', 'subset', 'vocabulary', 'using', 'selected', 'words', 'number', 'times', ',', 'least', 'irrespective', 'order', '.']

 TOTAL FILTERED TOKENS ==>  18

 ---- POST FOR FILTERED TOKENS ----

 [('first', 'RB'), ('model', 'NN'), ('document', 'NN'), ('generated', 'VBD'), ('first', 'JJ'), ('choosing', 'VBG'), ('subset', 'NN'), ('vocabulary', 'JJ'), ('using', 'VBG'), ('selected', 'VBN'), ('words', 'NNS'), ('number', 'NN'), ('times', 'NNS'), (',', ','), ('least', 'JJS'), ('irrespective', 'JJ'), ('order', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['first model', 'model document', 'document generated', 'generated first', 'first choosing', 'choosing subset', 'subset vocabulary', 'vocabulary using', 'using selected', 'selected words', 'words number', 'number times', 'times ,', ', least', 'least irrespective', 'irrespective order', 'order .'] 

 TOTAL BIGRAMS --> 17 



 ---- TRI-GRAMS ---- 

 ['first model document', 'model document generated', 'document generated first', 'generated first choosing', 'first choosing subset', 'choosing subset vocabulary', 'subset vocabulary using', 'vocabulary using selected', 'using selected words', 'selected words number', 'words number times', 'number times ,', 'times , least', ', least irrespective', 'least irrespective order', 'irrespective order .'] 

 TOTAL TRIGRAMS --> 16 



 ---- NOUN PHRASES ---- 

 ['model', 'document', 'subset', 'number', 'irrespective order'] 

 TOTAL NOUN PHRASES --> 5 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['first', 'model', 'document', 'gener', 'first', 'choos', 'subset', 'vocabulari', 'use', 'select', 'word', 'number', 'time', ',', 'least', 'irrespect', 'order', '.']

 TOTAL PORTER STEM WORDS ==> 18



 ---- SNOWBALL STEMMING ----

['first', 'model', 'document', 'generat', 'first', 'choos', 'subset', 'vocabulari', 'use', 'select', 'word', 'number', 'time', ',', 'least', 'irrespect', 'order', '.']

 TOTAL SNOWBALL STEM WORDS ==> 18



 ---- LEMMATIZATION ----

['first', 'model', 'document', 'generated', 'first', 'choosing', 'subset', 'vocabulary', 'using', 'selected', 'word', 'number', 'time', ',', 'least', 'irrespective', 'order', '.']

 TOTAL LEMMATIZE WORDS ==> 18

************************************************************************************************************************

209 --> This is called Multi-variate Bernoulli model. 


 ---- TOKENS ----

 ['This', 'is', 'called', 'Multi-variate', 'Bernoulli', 'model', '.'] 

 TOTAL TOKENS ==> 7

 ---- POST ----

 [('This', 'DT'), ('is', 'VBZ'), ('called', 'VBN'), ('Multi-variate', 'NNP'), ('Bernoulli', 'NNP'), ('model', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['called', 'Multi-variate', 'Bernoulli', 'model', '.']

 TOTAL FILTERED TOKENS ==>  5

 ---- POST FOR FILTERED TOKENS ----

 [('called', 'VBN'), ('Multi-variate', 'NNP'), ('Bernoulli', 'NNP'), ('model', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['called Multi-variate', 'Multi-variate Bernoulli', 'Bernoulli model', 'model .'] 

 TOTAL BIGRAMS --> 4 



 ---- TRI-GRAMS ---- 

 ['called Multi-variate Bernoulli', 'Multi-variate Bernoulli model', 'Bernoulli model .'] 

 TOTAL TRIGRAMS --> 3 



 ---- NOUN PHRASES ---- 

 ['model'] 

 TOTAL NOUN PHRASES --> 1 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['call', 'multi-vari', 'bernoulli', 'model', '.']

 TOTAL PORTER STEM WORDS ==> 5



 ---- SNOWBALL STEMMING ----

['call', 'multi-vari', 'bernoulli', 'model', '.']

 TOTAL SNOWBALL STEM WORDS ==> 5



 ---- LEMMATIZATION ----

['called', 'Multi-variate', 'Bernoulli', 'model', '.']

 TOTAL LEMMATIZE WORDS ==> 5

************************************************************************************************************************

210 --> It takes the information of which  words are used in a document irrespective of number of words and order. 


 ---- TOKENS ----

 ['It', 'takes', 'the', 'information', 'of', 'which', 'words', 'are', 'used', 'in', 'a', 'document', 'irrespective', 'of', 'number', 'of', 'words', 'and', 'order', '.'] 

 TOTAL TOKENS ==> 20

 ---- POST ----

 [('It', 'PRP'), ('takes', 'VBZ'), ('the', 'DT'), ('information', 'NN'), ('of', 'IN'), ('which', 'WDT'), ('words', 'NNS'), ('are', 'VBP'), ('used', 'VBN'), ('in', 'IN'), ('a', 'DT'), ('document', 'NN'), ('irrespective', 'NN'), ('of', 'IN'), ('number', 'NN'), ('of', 'IN'), ('words', 'NNS'), ('and', 'CC'), ('order', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['takes', 'information', 'words', 'used', 'document', 'irrespective', 'number', 'words', 'order', '.']

 TOTAL FILTERED TOKENS ==>  10

 ---- POST FOR FILTERED TOKENS ----

 [('takes', 'VBZ'), ('information', 'NN'), ('words', 'NNS'), ('used', 'VBN'), ('document', 'NN'), ('irrespective', 'JJ'), ('number', 'NN'), ('words', 'NNS'), ('order', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['takes information', 'information words', 'words used', 'used document', 'document irrespective', 'irrespective number', 'number words', 'words order', 'order .'] 

 TOTAL BIGRAMS --> 9 



 ---- TRI-GRAMS ---- 

 ['takes information words', 'information words used', 'words used document', 'used document irrespective', 'document irrespective number', 'irrespective number words', 'number words order', 'words order .'] 

 TOTAL TRIGRAMS --> 8 



 ---- NOUN PHRASES ---- 

 ['information', 'document', 'irrespective number', 'order'] 

 TOTAL NOUN PHRASES --> 4 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['take', 'inform', 'word', 'use', 'document', 'irrespect', 'number', 'word', 'order', '.']

 TOTAL PORTER STEM WORDS ==> 10



 ---- SNOWBALL STEMMING ----

['take', 'inform', 'word', 'use', 'document', 'irrespect', 'number', 'word', 'order', '.']

 TOTAL SNOWBALL STEM WORDS ==> 10



 ---- LEMMATIZATION ----

['take', 'information', 'word', 'used', 'document', 'irrespective', 'number', 'word', 'order', '.']

 TOTAL LEMMATIZE WORDS ==> 10

************************************************************************************************************************

211 --> In second model, a  document is generated by choosing a set of word occurrences and arranging them in any  order. 


 ---- TOKENS ----

 ['In', 'second', 'model', ',', 'a', 'document', 'is', 'generated', 'by', 'choosing', 'a', 'set', 'of', 'word', 'occurrences', 'and', 'arranging', 'them', 'in', 'any', 'order', '.'] 

 TOTAL TOKENS ==> 22

 ---- POST ----

 [('In', 'IN'), ('second', 'JJ'), ('model', 'NN'), (',', ','), ('a', 'DT'), ('document', 'NN'), ('is', 'VBZ'), ('generated', 'VBN'), ('by', 'IN'), ('choosing', 'VBG'), ('a', 'DT'), ('set', 'NN'), ('of', 'IN'), ('word', 'NN'), ('occurrences', 'NNS'), ('and', 'CC'), ('arranging', 'VBG'), ('them', 'PRP'), ('in', 'IN'), ('any', 'DT'), ('order', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['second', 'model', ',', 'document', 'generated', 'choosing', 'set', 'word', 'occurrences', 'arranging', 'order', '.']

 TOTAL FILTERED TOKENS ==>  12

 ---- POST FOR FILTERED TOKENS ----

 [('second', 'JJ'), ('model', 'NN'), (',', ','), ('document', 'NN'), ('generated', 'VBD'), ('choosing', 'VBG'), ('set', 'VBN'), ('word', 'NN'), ('occurrences', 'NNS'), ('arranging', 'VBG'), ('order', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['second model', 'model ,', ', document', 'document generated', 'generated choosing', 'choosing set', 'set word', 'word occurrences', 'occurrences arranging', 'arranging order', 'order .'] 

 TOTAL BIGRAMS --> 11 



 ---- TRI-GRAMS ---- 

 ['second model ,', 'model , document', ', document generated', 'document generated choosing', 'generated choosing set', 'choosing set word', 'set word occurrences', 'word occurrences arranging', 'occurrences arranging order', 'arranging order .'] 

 TOTAL TRIGRAMS --> 10 



 ---- NOUN PHRASES ---- 

 ['second model', 'document', 'word', 'order'] 

 TOTAL NOUN PHRASES --> 4 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['second', 'model', ',', 'document', 'gener', 'choos', 'set', 'word', 'occurr', 'arrang', 'order', '.']

 TOTAL PORTER STEM WORDS ==> 12



 ---- SNOWBALL STEMMING ----

['second', 'model', ',', 'document', 'generat', 'choos', 'set', 'word', 'occurr', 'arrang', 'order', '.']

 TOTAL SNOWBALL STEM WORDS ==> 12



 ---- LEMMATIZATION ----

['second', 'model', ',', 'document', 'generated', 'choosing', 'set', 'word', 'occurrence', 'arranging', 'order', '.']

 TOTAL LEMMATIZE WORDS ==> 12

************************************************************************************************************************

212 --> this model is called multi-nomial model, in addition to the Multi-variate Bernoulli  model, it also captures information on how many times a word is used in a document. 


 ---- TOKENS ----

 ['this', 'model', 'is', 'called', 'multi-nomial', 'model', ',', 'in', 'addition', 'to', 'the', 'Multi-variate', 'Bernoulli', 'model', ',', 'it', 'also', 'captures', 'information', 'on', 'how', 'many', 'times', 'a', 'word', 'is', 'used', 'in', 'a', 'document', '.'] 

 TOTAL TOKENS ==> 31

 ---- POST ----

 [('this', 'DT'), ('model', 'NN'), ('is', 'VBZ'), ('called', 'VBN'), ('multi-nomial', 'JJ'), ('model', 'NN'), (',', ','), ('in', 'IN'), ('addition', 'NN'), ('to', 'TO'), ('the', 'DT'), ('Multi-variate', 'NNP'), ('Bernoulli', 'NNP'), ('model', 'NN'), (',', ','), ('it', 'PRP'), ('also', 'RB'), ('captures', 'VBZ'), ('information', 'NN'), ('on', 'IN'), ('how', 'WRB'), ('many', 'JJ'), ('times', 'NNS'), ('a', 'DT'), ('word', 'NN'), ('is', 'VBZ'), ('used', 'VBN'), ('in', 'IN'), ('a', 'DT'), ('document', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['model', 'called', 'multi-nomial', 'model', ',', 'addition', 'Multi-variate', 'Bernoulli', 'model', ',', 'also', 'captures', 'information', 'many', 'times', 'word', 'used', 'document', '.']

 TOTAL FILTERED TOKENS ==>  19

 ---- POST FOR FILTERED TOKENS ----

 [('model', 'NN'), ('called', 'VBN'), ('multi-nomial', 'JJ'), ('model', 'NN'), (',', ','), ('addition', 'NN'), ('Multi-variate', 'NNP'), ('Bernoulli', 'NNP'), ('model', 'NN'), (',', ','), ('also', 'RB'), ('captures', 'VBZ'), ('information', 'NN'), ('many', 'JJ'), ('times', 'NNS'), ('word', 'NN'), ('used', 'VBN'), ('document', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['model called', 'called multi-nomial', 'multi-nomial model', 'model ,', ', addition', 'addition Multi-variate', 'Multi-variate Bernoulli', 'Bernoulli model', 'model ,', ', also', 'also captures', 'captures information', 'information many', 'many times', 'times word', 'word used', 'used document', 'document .'] 

 TOTAL BIGRAMS --> 18 



 ---- TRI-GRAMS ---- 

 ['model called multi-nomial', 'called multi-nomial model', 'multi-nomial model ,', 'model , addition', ', addition Multi-variate', 'addition Multi-variate Bernoulli', 'Multi-variate Bernoulli model', 'Bernoulli model ,', 'model , also', ', also captures', 'also captures information', 'captures information many', 'information many times', 'many times word', 'times word used', 'word used document', 'used document .'] 

 TOTAL TRIGRAMS --> 17 



 ---- NOUN PHRASES ---- 

 ['model', 'multi-nomial model', 'addition', 'model', 'information', 'word', 'document'] 

 TOTAL NOUN PHRASES --> 7 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['model', 'call', 'multi-nomi', 'model', ',', 'addit', 'multi-vari', 'bernoulli', 'model', ',', 'also', 'captur', 'inform', 'mani', 'time', 'word', 'use', 'document', '.']

 TOTAL PORTER STEM WORDS ==> 19



 ---- SNOWBALL STEMMING ----

['model', 'call', 'multi-nomi', 'model', ',', 'addit', 'multi-vari', 'bernoulli', 'model', ',', 'also', 'captur', 'inform', 'mani', 'time', 'word', 'use', 'document', '.']

 TOTAL SNOWBALL STEM WORDS ==> 19



 ---- LEMMATIZATION ----

['model', 'called', 'multi-nomial', 'model', ',', 'addition', 'Multi-variate', 'Bernoulli', 'model', ',', 'also', 'capture', 'information', 'many', 'time', 'word', 'used', 'document', '.']

 TOTAL LEMMATIZE WORDS ==> 19

************************************************************************************************************************

213 --> Most  text categorization approaches to anti spam Email filtering have used multi variate Bernoulli  model (Androutsopoulos et al.,2000b) [47]  6.4 Information Extraction  Information extraction is concerned with identifying phrases of interest of textual data. 


 ---- TOKENS ----

 ['Most', 'text', 'categorization', 'approaches', 'to', 'anti', 'spam', 'Email', 'filtering', 'have', 'used', 'multi', 'variate', 'Bernoulli', 'model', '(', 'Androutsopoulos', 'et', 'al.,2000b', ')', '[', '47', ']', '6.4', 'Information', 'Extraction', 'Information', 'extraction', 'is', 'concerned', 'with', 'identifying', 'phrases', 'of', 'interest', 'of', 'textual', 'data', '.'] 

 TOTAL TOKENS ==> 39

 ---- POST ----

 [('Most', 'JJS'), ('text', 'JJ'), ('categorization', 'NN'), ('approaches', 'NNS'), ('to', 'TO'), ('anti', 'VB'), ('spam', 'JJ'), ('Email', 'NNP'), ('filtering', 'NN'), ('have', 'VBP'), ('used', 'VBN'), ('multi', 'NN'), ('variate', 'NN'), ('Bernoulli', 'NNP'), ('model', 'NN'), ('(', '('), ('Androutsopoulos', 'NNP'), ('et', 'RB'), ('al.,2000b', 'RB'), (')', ')'), ('[', 'VBZ'), ('47', 'CD'), (']', 'JJ'), ('6.4', 'CD'), ('Information', 'NNP'), ('Extraction', 'NNP'), ('Information', 'NNP'), ('extraction', 'NN'), ('is', 'VBZ'), ('concerned', 'VBN'), ('with', 'IN'), ('identifying', 'VBG'), ('phrases', 'NNS'), ('of', 'IN'), ('interest', 'NN'), ('of', 'IN'), ('textual', 'JJ'), ('data', 'NNS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['text', 'categorization', 'approaches', 'anti', 'spam', 'Email', 'filtering', 'used', 'multi', 'variate', 'Bernoulli', 'model', '(', 'Androutsopoulos', 'et', 'al.,2000b', ')', '[', '47', ']', '6.4', 'Information', 'Extraction', 'Information', 'extraction', 'concerned', 'identifying', 'phrases', 'interest', 'textual', 'data', '.']

 TOTAL FILTERED TOKENS ==>  32

 ---- POST FOR FILTERED TOKENS ----

 [('text', 'JJ'), ('categorization', 'NN'), ('approaches', 'NNS'), ('anti', 'VBP'), ('spam', 'JJ'), ('Email', 'NNP'), ('filtering', 'NN'), ('used', 'VBN'), ('multi', 'NN'), ('variate', 'NN'), ('Bernoulli', 'NNP'), ('model', 'NN'), ('(', '('), ('Androutsopoulos', 'NNP'), ('et', 'RB'), ('al.,2000b', 'RB'), (')', ')'), ('[', 'VBZ'), ('47', 'CD'), (']', 'JJ'), ('6.4', 'CD'), ('Information', 'NNP'), ('Extraction', 'NNP'), ('Information', 'NNP'), ('extraction', 'NN'), ('concerned', 'VBD'), ('identifying', 'JJ'), ('phrases', 'NNS'), ('interest', 'NN'), ('textual', 'JJ'), ('data', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['text categorization', 'categorization approaches', 'approaches anti', 'anti spam', 'spam Email', 'Email filtering', 'filtering used', 'used multi', 'multi variate', 'variate Bernoulli', 'Bernoulli model', 'model (', '( Androutsopoulos', 'Androutsopoulos et', 'et al.,2000b', 'al.,2000b )', ') [', '[ 47', '47 ]', '] 6.4', '6.4 Information', 'Information Extraction', 'Extraction Information', 'Information extraction', 'extraction concerned', 'concerned identifying', 'identifying phrases', 'phrases interest', 'interest textual', 'textual data', 'data .'] 

 TOTAL BIGRAMS --> 31 



 ---- TRI-GRAMS ---- 

 ['text categorization approaches', 'categorization approaches anti', 'approaches anti spam', 'anti spam Email', 'spam Email filtering', 'Email filtering used', 'filtering used multi', 'used multi variate', 'multi variate Bernoulli', 'variate Bernoulli model', 'Bernoulli model (', 'model ( Androutsopoulos', '( Androutsopoulos et', 'Androutsopoulos et al.,2000b', 'et al.,2000b )', 'al.,2000b ) [', ') [ 47', '[ 47 ]', '47 ] 6.4', '] 6.4 Information', '6.4 Information Extraction', 'Information Extraction Information', 'Extraction Information extraction', 'Information extraction concerned', 'extraction concerned identifying', 'concerned identifying phrases', 'identifying phrases interest', 'phrases interest textual', 'interest textual data', 'textual data .'] 

 TOTAL TRIGRAMS --> 30 



 ---- NOUN PHRASES ---- 

 ['text categorization', 'filtering', 'multi', 'variate', 'model', 'extraction', 'interest'] 

 TOTAL NOUN PHRASES --> 7 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> ['Bernoulli']
 TOTAL PERSON ENTITY --> 1 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['text', 'categor', 'approach', 'anti', 'spam', 'email', 'filter', 'use', 'multi', 'variat', 'bernoulli', 'model', '(', 'androutsopoulo', 'et', 'al.,2000b', ')', '[', '47', ']', '6.4', 'inform', 'extract', 'inform', 'extract', 'concern', 'identifi', 'phrase', 'interest', 'textual', 'data', '.']

 TOTAL PORTER STEM WORDS ==> 32



 ---- SNOWBALL STEMMING ----

['text', 'categor', 'approach', 'anti', 'spam', 'email', 'filter', 'use', 'multi', 'variat', 'bernoulli', 'model', '(', 'androutsopoulo', 'et', 'al.,2000b', ')', '[', '47', ']', '6.4', 'inform', 'extract', 'inform', 'extract', 'concern', 'identifi', 'phrase', 'interest', 'textual', 'data', '.']

 TOTAL SNOWBALL STEM WORDS ==> 32



 ---- LEMMATIZATION ----

['text', 'categorization', 'approach', 'anti', 'spam', 'Email', 'filtering', 'used', 'multi', 'variate', 'Bernoulli', 'model', '(', 'Androutsopoulos', 'et', 'al.,2000b', ')', '[', '47', ']', '6.4', 'Information', 'Extraction', 'Information', 'extraction', 'concerned', 'identifying', 'phrase', 'interest', 'textual', 'data', '.']

 TOTAL LEMMATIZE WORDS ==> 32

************************************************************************************************************************

214 --> For  many applications, extracting entities such as names, places, events, dates, times and prices is  a powerful way of summarize the information relevant to a user’s needs. 


 ---- TOKENS ----

 ['For', 'many', 'applications', ',', 'extracting', 'entities', 'such', 'as', 'names', ',', 'places', ',', 'events', ',', 'dates', ',', 'times', 'and', 'prices', 'is', 'a', 'powerful', 'way', 'of', 'summarize', 'the', 'information', 'relevant', 'to', 'a', 'user', '’', 's', 'needs', '.'] 

 TOTAL TOKENS ==> 35

 ---- POST ----

 [('For', 'IN'), ('many', 'JJ'), ('applications', 'NNS'), (',', ','), ('extracting', 'VBG'), ('entities', 'NNS'), ('such', 'JJ'), ('as', 'IN'), ('names', 'NNS'), (',', ','), ('places', 'NNS'), (',', ','), ('events', 'NNS'), (',', ','), ('dates', 'NNS'), (',', ','), ('times', 'NNS'), ('and', 'CC'), ('prices', 'NNS'), ('is', 'VBZ'), ('a', 'DT'), ('powerful', 'JJ'), ('way', 'NN'), ('of', 'IN'), ('summarize', 'VB'), ('the', 'DT'), ('information', 'NN'), ('relevant', 'NN'), ('to', 'TO'), ('a', 'DT'), ('user', 'NN'), ('’', 'NN'), ('s', 'NN'), ('needs', 'NNS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['many', 'applications', ',', 'extracting', 'entities', 'names', ',', 'places', ',', 'events', ',', 'dates', ',', 'times', 'prices', 'powerful', 'way', 'summarize', 'information', 'relevant', 'user', '’', 'needs', '.']

 TOTAL FILTERED TOKENS ==>  24

 ---- POST FOR FILTERED TOKENS ----

 [('many', 'JJ'), ('applications', 'NNS'), (',', ','), ('extracting', 'VBG'), ('entities', 'NNS'), ('names', 'RB'), (',', ','), ('places', 'NNS'), (',', ','), ('events', 'NNS'), (',', ','), ('dates', 'NNS'), (',', ','), ('times', 'NNS'), ('prices', 'NNS'), ('powerful', 'JJ'), ('way', 'NN'), ('summarize', 'JJ'), ('information', 'NN'), ('relevant', 'JJ'), ('user', 'NN'), ('’', 'NN'), ('needs', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['many applications', 'applications ,', ', extracting', 'extracting entities', 'entities names', 'names ,', ', places', 'places ,', ', events', 'events ,', ', dates', 'dates ,', ', times', 'times prices', 'prices powerful', 'powerful way', 'way summarize', 'summarize information', 'information relevant', 'relevant user', 'user ’', '’ needs', 'needs .'] 

 TOTAL BIGRAMS --> 23 



 ---- TRI-GRAMS ---- 

 ['many applications ,', 'applications , extracting', ', extracting entities', 'extracting entities names', 'entities names ,', 'names , places', ', places ,', 'places , events', ', events ,', 'events , dates', ', dates ,', 'dates , times', ', times prices', 'times prices powerful', 'prices powerful way', 'powerful way summarize', 'way summarize information', 'summarize information relevant', 'information relevant user', 'relevant user ’', 'user ’ needs', '’ needs .'] 

 TOTAL TRIGRAMS --> 22 



 ---- NOUN PHRASES ---- 

 ['powerful way', 'summarize information', 'relevant user', '’'] 

 TOTAL NOUN PHRASES --> 4 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['mani', 'applic', ',', 'extract', 'entiti', 'name', ',', 'place', ',', 'event', ',', 'date', ',', 'time', 'price', 'power', 'way', 'summar', 'inform', 'relev', 'user', '’', 'need', '.']

 TOTAL PORTER STEM WORDS ==> 24



 ---- SNOWBALL STEMMING ----

['mani', 'applic', ',', 'extract', 'entiti', 'name', ',', 'place', ',', 'event', ',', 'date', ',', 'time', 'price', 'power', 'way', 'summar', 'inform', 'relev', 'user', '’', 'need', '.']

 TOTAL SNOWBALL STEM WORDS ==> 24



 ---- LEMMATIZATION ----

['many', 'application', ',', 'extracting', 'entity', 'name', ',', 'place', ',', 'event', ',', 'date', ',', 'time', 'price', 'powerful', 'way', 'summarize', 'information', 'relevant', 'user', '’', 'need', '.']

 TOTAL LEMMATIZE WORDS ==> 24

************************************************************************************************************************

215 --> In the case of a  domain specific search engine, the automatic identification of important information can  increase accuracy and efficiency of a directed search. 


 ---- TOKENS ----

 ['In', 'the', 'case', 'of', 'a', 'domain', 'specific', 'search', 'engine', ',', 'the', 'automatic', 'identification', 'of', 'important', 'information', 'can', 'increase', 'accuracy', 'and', 'efficiency', 'of', 'a', 'directed', 'search', '.'] 

 TOTAL TOKENS ==> 26

 ---- POST ----

 [('In', 'IN'), ('the', 'DT'), ('case', 'NN'), ('of', 'IN'), ('a', 'DT'), ('domain', 'NN'), ('specific', 'JJ'), ('search', 'NN'), ('engine', 'NN'), (',', ','), ('the', 'DT'), ('automatic', 'JJ'), ('identification', 'NN'), ('of', 'IN'), ('important', 'JJ'), ('information', 'NN'), ('can', 'MD'), ('increase', 'VB'), ('accuracy', 'NN'), ('and', 'CC'), ('efficiency', 'NN'), ('of', 'IN'), ('a', 'DT'), ('directed', 'JJ'), ('search', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['case', 'domain', 'specific', 'search', 'engine', ',', 'automatic', 'identification', 'important', 'information', 'increase', 'accuracy', 'efficiency', 'directed', 'search', '.']

 TOTAL FILTERED TOKENS ==>  16

 ---- POST FOR FILTERED TOKENS ----

 [('case', 'NN'), ('domain', 'NN'), ('specific', 'JJ'), ('search', 'NN'), ('engine', 'NN'), (',', ','), ('automatic', 'JJ'), ('identification', 'NN'), ('important', 'JJ'), ('information', 'NN'), ('increase', 'NN'), ('accuracy', 'NN'), ('efficiency', 'NN'), ('directed', 'VBD'), ('search', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['case domain', 'domain specific', 'specific search', 'search engine', 'engine ,', ', automatic', 'automatic identification', 'identification important', 'important information', 'information increase', 'increase accuracy', 'accuracy efficiency', 'efficiency directed', 'directed search', 'search .'] 

 TOTAL BIGRAMS --> 15 



 ---- TRI-GRAMS ---- 

 ['case domain specific', 'domain specific search', 'specific search engine', 'search engine ,', 'engine , automatic', ', automatic identification', 'automatic identification important', 'identification important information', 'important information increase', 'information increase accuracy', 'increase accuracy efficiency', 'accuracy efficiency directed', 'efficiency directed search', 'directed search .'] 

 TOTAL TRIGRAMS --> 14 



 ---- NOUN PHRASES ---- 

 ['case', 'domain', 'specific search', 'engine', 'automatic identification', 'important information', 'increase', 'accuracy', 'efficiency', 'search'] 

 TOTAL NOUN PHRASES --> 10 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['case', 'domain', 'specif', 'search', 'engin', ',', 'automat', 'identif', 'import', 'inform', 'increas', 'accuraci', 'effici', 'direct', 'search', '.']

 TOTAL PORTER STEM WORDS ==> 16



 ---- SNOWBALL STEMMING ----

['case', 'domain', 'specif', 'search', 'engin', ',', 'automat', 'identif', 'import', 'inform', 'increas', 'accuraci', 'effici', 'direct', 'search', '.']

 TOTAL SNOWBALL STEM WORDS ==> 16



 ---- LEMMATIZATION ----

['case', 'domain', 'specific', 'search', 'engine', ',', 'automatic', 'identification', 'important', 'information', 'increase', 'accuracy', 'efficiency', 'directed', 'search', '.']

 TOTAL LEMMATIZE WORDS ==> 16

************************************************************************************************************************

216 --> There is use of hidden Markov models  (HMMs) to extract the relevant fields of research papers. 


 ---- TOKENS ----

 ['There', 'is', 'use', 'of', 'hidden', 'Markov', 'models', '(', 'HMMs', ')', 'to', 'extract', 'the', 'relevant', 'fields', 'of', 'research', 'papers', '.'] 

 TOTAL TOKENS ==> 19

 ---- POST ----

 [('There', 'EX'), ('is', 'VBZ'), ('use', 'NN'), ('of', 'IN'), ('hidden', 'JJ'), ('Markov', 'NNP'), ('models', 'NNS'), ('(', '('), ('HMMs', 'NNP'), (')', ')'), ('to', 'TO'), ('extract', 'VB'), ('the', 'DT'), ('relevant', 'JJ'), ('fields', 'NNS'), ('of', 'IN'), ('research', 'NN'), ('papers', 'NNS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['use', 'hidden', 'Markov', 'models', '(', 'HMMs', ')', 'extract', 'relevant', 'fields', 'research', 'papers', '.']

 TOTAL FILTERED TOKENS ==>  13

 ---- POST FOR FILTERED TOKENS ----

 [('use', 'NN'), ('hidden', 'JJ'), ('Markov', 'NNP'), ('models', 'NNS'), ('(', '('), ('HMMs', 'NNP'), (')', ')'), ('extract', 'NN'), ('relevant', 'JJ'), ('fields', 'NNS'), ('research', 'NN'), ('papers', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['use hidden', 'hidden Markov', 'Markov models', 'models (', '( HMMs', 'HMMs )', ') extract', 'extract relevant', 'relevant fields', 'fields research', 'research papers', 'papers .'] 

 TOTAL BIGRAMS --> 12 



 ---- TRI-GRAMS ---- 

 ['use hidden Markov', 'hidden Markov models', 'Markov models (', 'models ( HMMs', '( HMMs )', 'HMMs ) extract', ') extract relevant', 'extract relevant fields', 'relevant fields research', 'fields research papers', 'research papers .'] 

 TOTAL TRIGRAMS --> 11 



 ---- NOUN PHRASES ---- 

 ['use', 'extract', 'research'] 

 TOTAL NOUN PHRASES --> 3 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> ['Markov']
 TOTAL PERSON ENTITY --> 1 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['use', 'hidden', 'markov', 'model', '(', 'hmm', ')', 'extract', 'relev', 'field', 'research', 'paper', '.']

 TOTAL PORTER STEM WORDS ==> 13



 ---- SNOWBALL STEMMING ----

['use', 'hidden', 'markov', 'model', '(', 'hmms', ')', 'extract', 'relev', 'field', 'research', 'paper', '.']

 TOTAL SNOWBALL STEM WORDS ==> 13



 ---- LEMMATIZATION ----

['use', 'hidden', 'Markov', 'model', '(', 'HMMs', ')', 'extract', 'relevant', 'field', 'research', 'paper', '.']

 TOTAL LEMMATIZE WORDS ==> 13

************************************************************************************************************************

217 --> These extracted text segments are   used to allow searched over specific fields and to provide effective presentation of search  results and to match references to papers. 


 ---- TOKENS ----

 ['These', 'extracted', 'text', 'segments', 'are', 'used', 'to', 'allow', 'searched', 'over', 'specific', 'fields', 'and', 'to', 'provide', 'effective', 'presentation', 'of', 'search', 'results', 'and', 'to', 'match', 'references', 'to', 'papers', '.'] 

 TOTAL TOKENS ==> 27

 ---- POST ----

 [('These', 'DT'), ('extracted', 'VBD'), ('text', 'NN'), ('segments', 'NNS'), ('are', 'VBP'), ('used', 'VBN'), ('to', 'TO'), ('allow', 'VB'), ('searched', 'VBN'), ('over', 'IN'), ('specific', 'JJ'), ('fields', 'NNS'), ('and', 'CC'), ('to', 'TO'), ('provide', 'VB'), ('effective', 'JJ'), ('presentation', 'NN'), ('of', 'IN'), ('search', 'NN'), ('results', 'NNS'), ('and', 'CC'), ('to', 'TO'), ('match', 'VB'), ('references', 'NNS'), ('to', 'TO'), ('papers', 'NNS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['extracted', 'text', 'segments', 'used', 'allow', 'searched', 'specific', 'fields', 'provide', 'effective', 'presentation', 'search', 'results', 'match', 'references', 'papers', '.']

 TOTAL FILTERED TOKENS ==>  17

 ---- POST FOR FILTERED TOKENS ----

 [('extracted', 'VBN'), ('text', 'JJ'), ('segments', 'NNS'), ('used', 'VBN'), ('allow', 'VB'), ('searched', 'VBN'), ('specific', 'JJ'), ('fields', 'NNS'), ('provide', 'VBP'), ('effective', 'JJ'), ('presentation', 'NN'), ('search', 'NN'), ('results', 'NNS'), ('match', 'VB'), ('references', 'NNS'), ('papers', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['extracted text', 'text segments', 'segments used', 'used allow', 'allow searched', 'searched specific', 'specific fields', 'fields provide', 'provide effective', 'effective presentation', 'presentation search', 'search results', 'results match', 'match references', 'references papers', 'papers .'] 

 TOTAL BIGRAMS --> 16 



 ---- TRI-GRAMS ---- 

 ['extracted text segments', 'text segments used', 'segments used allow', 'used allow searched', 'allow searched specific', 'searched specific fields', 'specific fields provide', 'fields provide effective', 'provide effective presentation', 'effective presentation search', 'presentation search results', 'search results match', 'results match references', 'match references papers', 'references papers .'] 

 TOTAL TRIGRAMS --> 15 



 ---- NOUN PHRASES ---- 

 ['effective presentation', 'search'] 

 TOTAL NOUN PHRASES --> 2 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['extract', 'text', 'segment', 'use', 'allow', 'search', 'specif', 'field', 'provid', 'effect', 'present', 'search', 'result', 'match', 'refer', 'paper', '.']

 TOTAL PORTER STEM WORDS ==> 17



 ---- SNOWBALL STEMMING ----

['extract', 'text', 'segment', 'use', 'allow', 'search', 'specif', 'field', 'provid', 'effect', 'present', 'search', 'result', 'match', 'refer', 'paper', '.']

 TOTAL SNOWBALL STEM WORDS ==> 17



 ---- LEMMATIZATION ----

['extracted', 'text', 'segment', 'used', 'allow', 'searched', 'specific', 'field', 'provide', 'effective', 'presentation', 'search', 'result', 'match', 'reference', 'paper', '.']

 TOTAL LEMMATIZE WORDS ==> 17

************************************************************************************************************************

218 --> For example, noticing the pop up ads on any  websites showing the recent items you might have looked on an online store with discounts. 


 ---- TOKENS ----

 ['For', 'example', ',', 'noticing', 'the', 'pop', 'up', 'ads', 'on', 'any', 'websites', 'showing', 'the', 'recent', 'items', 'you', 'might', 'have', 'looked', 'on', 'an', 'online', 'store', 'with', 'discounts', '.'] 

 TOTAL TOKENS ==> 26

 ---- POST ----

 [('For', 'IN'), ('example', 'NN'), (',', ','), ('noticing', 'VBG'), ('the', 'DT'), ('pop', 'NN'), ('up', 'RP'), ('ads', 'NNS'), ('on', 'IN'), ('any', 'DT'), ('websites', 'NNS'), ('showing', 'VBG'), ('the', 'DT'), ('recent', 'JJ'), ('items', 'NNS'), ('you', 'PRP'), ('might', 'MD'), ('have', 'VB'), ('looked', 'VBN'), ('on', 'IN'), ('an', 'DT'), ('online', 'JJ'), ('store', 'NN'), ('with', 'IN'), ('discounts', 'NNS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['example', ',', 'noticing', 'pop', 'ads', 'websites', 'showing', 'recent', 'items', 'might', 'looked', 'online', 'store', 'discounts', '.']

 TOTAL FILTERED TOKENS ==>  15

 ---- POST FOR FILTERED TOKENS ----

 [('example', 'NN'), (',', ','), ('noticing', 'VBG'), ('pop', 'NN'), ('ads', 'NNS'), ('websites', 'VBZ'), ('showing', 'VBG'), ('recent', 'JJ'), ('items', 'NNS'), ('might', 'MD'), ('looked', 'VB'), ('online', 'JJ'), ('store', 'NN'), ('discounts', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['example ,', ', noticing', 'noticing pop', 'pop ads', 'ads websites', 'websites showing', 'showing recent', 'recent items', 'items might', 'might looked', 'looked online', 'online store', 'store discounts', 'discounts .'] 

 TOTAL BIGRAMS --> 14 



 ---- TRI-GRAMS ---- 

 ['example , noticing', ', noticing pop', 'noticing pop ads', 'pop ads websites', 'ads websites showing', 'websites showing recent', 'showing recent items', 'recent items might', 'items might looked', 'might looked online', 'looked online store', 'online store discounts', 'store discounts .'] 

 TOTAL TRIGRAMS --> 13 



 ---- NOUN PHRASES ---- 

 ['example', 'pop', 'online store'] 

 TOTAL NOUN PHRASES --> 3 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['exampl', ',', 'notic', 'pop', 'ad', 'websit', 'show', 'recent', 'item', 'might', 'look', 'onlin', 'store', 'discount', '.']

 TOTAL PORTER STEM WORDS ==> 15



 ---- SNOWBALL STEMMING ----

['exampl', ',', 'notic', 'pop', 'ad', 'websit', 'show', 'recent', 'item', 'might', 'look', 'onlin', 'store', 'discount', '.']

 TOTAL SNOWBALL STEM WORDS ==> 15



 ---- LEMMATIZATION ----

['example', ',', 'noticing', 'pop', 'ad', 'website', 'showing', 'recent', 'item', 'might', 'looked', 'online', 'store', 'discount', '.']

 TOTAL LEMMATIZE WORDS ==> 15

************************************************************************************************************************

219 --> In Information Retrieval two types of models have been used (McCallum and Nigam, 1998)  [55]. 


 ---- TOKENS ----

 ['In', 'Information', 'Retrieval', 'two', 'types', 'of', 'models', 'have', 'been', 'used', '(', 'McCallum', 'and', 'Nigam', ',', '1998', ')', '[', '55', ']', '.'] 

 TOTAL TOKENS ==> 21

 ---- POST ----

 [('In', 'IN'), ('Information', 'NNP'), ('Retrieval', 'NNP'), ('two', 'CD'), ('types', 'NNS'), ('of', 'IN'), ('models', 'NNS'), ('have', 'VBP'), ('been', 'VBN'), ('used', 'VBN'), ('(', '('), ('McCallum', 'NNP'), ('and', 'CC'), ('Nigam', 'NNP'), (',', ','), ('1998', 'CD'), (')', ')'), ('[', 'VBD'), ('55', 'CD'), (']', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Information', 'Retrieval', 'two', 'types', 'models', 'used', '(', 'McCallum', 'Nigam', ',', '1998', ')', '[', '55', ']', '.']

 TOTAL FILTERED TOKENS ==>  16

 ---- POST FOR FILTERED TOKENS ----

 [('Information', 'NN'), ('Retrieval', 'NNP'), ('two', 'CD'), ('types', 'NNS'), ('models', 'NNS'), ('used', 'VBN'), ('(', '('), ('McCallum', 'NNP'), ('Nigam', 'NNP'), (',', ','), ('1998', 'CD'), (')', ')'), ('[', 'VBD'), ('55', 'CD'), (']', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Information Retrieval', 'Retrieval two', 'two types', 'types models', 'models used', 'used (', '( McCallum', 'McCallum Nigam', 'Nigam ,', ', 1998', '1998 )', ') [', '[ 55', '55 ]', '] .'] 

 TOTAL BIGRAMS --> 15 



 ---- TRI-GRAMS ---- 

 ['Information Retrieval two', 'Retrieval two types', 'two types models', 'types models used', 'models used (', 'used ( McCallum', '( McCallum Nigam', 'McCallum Nigam ,', 'Nigam , 1998', ', 1998 )', '1998 ) [', ') [ 55', '[ 55 ]', '55 ] .'] 

 TOTAL TRIGRAMS --> 14 



 ---- NOUN PHRASES ---- 

 ['Information', ']'] 

 TOTAL NOUN PHRASES --> 2 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['inform', 'retriev', 'two', 'type', 'model', 'use', '(', 'mccallum', 'nigam', ',', '1998', ')', '[', '55', ']', '.']

 TOTAL PORTER STEM WORDS ==> 16



 ---- SNOWBALL STEMMING ----

['inform', 'retriev', 'two', 'type', 'model', 'use', '(', 'mccallum', 'nigam', ',', '1998', ')', '[', '55', ']', '.']

 TOTAL SNOWBALL STEM WORDS ==> 16



 ---- LEMMATIZATION ----

['Information', 'Retrieval', 'two', 'type', 'model', 'used', '(', 'McCallum', 'Nigam', ',', '1998', ')', '[', '55', ']', '.']

 TOTAL LEMMATIZE WORDS ==> 16

************************************************************************************************************************

220 --> Both modules assume that a fixed vocabulary is present. 


 ---- TOKENS ----

 ['Both', 'modules', 'assume', 'that', 'a', 'fixed', 'vocabulary', 'is', 'present', '.'] 

 TOTAL TOKENS ==> 10

 ---- POST ----

 [('Both', 'DT'), ('modules', 'NNS'), ('assume', 'VBP'), ('that', 'IN'), ('a', 'DT'), ('fixed', 'JJ'), ('vocabulary', 'NN'), ('is', 'VBZ'), ('present', 'JJ'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['modules', 'assume', 'fixed', 'vocabulary', 'present', '.']

 TOTAL FILTERED TOKENS ==>  6

 ---- POST FOR FILTERED TOKENS ----

 [('modules', 'NNS'), ('assume', 'VBP'), ('fixed', 'VBN'), ('vocabulary', 'JJ'), ('present', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['modules assume', 'assume fixed', 'fixed vocabulary', 'vocabulary present', 'present .'] 

 TOTAL BIGRAMS --> 5 



 ---- TRI-GRAMS ---- 

 ['modules assume fixed', 'assume fixed vocabulary', 'fixed vocabulary present', 'vocabulary present .'] 

 TOTAL TRIGRAMS --> 4 



 ---- NOUN PHRASES ---- 

 ['vocabulary present'] 

 TOTAL NOUN PHRASES --> 1 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['modul', 'assum', 'fix', 'vocabulari', 'present', '.']

 TOTAL PORTER STEM WORDS ==> 6



 ---- SNOWBALL STEMMING ----

['modul', 'assum', 'fix', 'vocabulari', 'present', '.']

 TOTAL SNOWBALL STEM WORDS ==> 6



 ---- LEMMATIZATION ----

['module', 'assume', 'fixed', 'vocabulary', 'present', '.']

 TOTAL LEMMATIZE WORDS ==> 6

************************************************************************************************************************

221 --> But in first model a document  is generated by first choosing a subset of vocabulary and then using the selected words any  number of times, at least once without any order. 


 ---- TOKENS ----

 ['But', 'in', 'first', 'model', 'a', 'document', 'is', 'generated', 'by', 'first', 'choosing', 'a', 'subset', 'of', 'vocabulary', 'and', 'then', 'using', 'the', 'selected', 'words', 'any', 'number', 'of', 'times', ',', 'at', 'least', 'once', 'without', 'any', 'order', '.'] 

 TOTAL TOKENS ==> 33

 ---- POST ----

 [('But', 'CC'), ('in', 'IN'), ('first', 'JJ'), ('model', 'NN'), ('a', 'DT'), ('document', 'NN'), ('is', 'VBZ'), ('generated', 'VBN'), ('by', 'IN'), ('first', 'JJ'), ('choosing', 'VBG'), ('a', 'DT'), ('subset', 'NN'), ('of', 'IN'), ('vocabulary', 'JJ'), ('and', 'CC'), ('then', 'RB'), ('using', 'VBG'), ('the', 'DT'), ('selected', 'VBN'), ('words', 'NNS'), ('any', 'DT'), ('number', 'NN'), ('of', 'IN'), ('times', 'NNS'), (',', ','), ('at', 'IN'), ('least', 'JJS'), ('once', 'RB'), ('without', 'IN'), ('any', 'DT'), ('order', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['first', 'model', 'document', 'generated', 'first', 'choosing', 'subset', 'vocabulary', 'using', 'selected', 'words', 'number', 'times', ',', 'least', 'without', 'order', '.']

 TOTAL FILTERED TOKENS ==>  18

 ---- POST FOR FILTERED TOKENS ----

 [('first', 'RB'), ('model', 'NN'), ('document', 'NN'), ('generated', 'VBD'), ('first', 'JJ'), ('choosing', 'VBG'), ('subset', 'NN'), ('vocabulary', 'JJ'), ('using', 'VBG'), ('selected', 'VBN'), ('words', 'NNS'), ('number', 'NN'), ('times', 'NNS'), (',', ','), ('least', 'JJS'), ('without', 'IN'), ('order', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['first model', 'model document', 'document generated', 'generated first', 'first choosing', 'choosing subset', 'subset vocabulary', 'vocabulary using', 'using selected', 'selected words', 'words number', 'number times', 'times ,', ', least', 'least without', 'without order', 'order .'] 

 TOTAL BIGRAMS --> 17 



 ---- TRI-GRAMS ---- 

 ['first model document', 'model document generated', 'document generated first', 'generated first choosing', 'first choosing subset', 'choosing subset vocabulary', 'subset vocabulary using', 'vocabulary using selected', 'using selected words', 'selected words number', 'words number times', 'number times ,', 'times , least', ', least without', 'least without order', 'without order .'] 

 TOTAL TRIGRAMS --> 16 



 ---- NOUN PHRASES ---- 

 ['model', 'document', 'subset', 'number', 'order'] 

 TOTAL NOUN PHRASES --> 5 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['first', 'model', 'document', 'gener', 'first', 'choos', 'subset', 'vocabulari', 'use', 'select', 'word', 'number', 'time', ',', 'least', 'without', 'order', '.']

 TOTAL PORTER STEM WORDS ==> 18



 ---- SNOWBALL STEMMING ----

['first', 'model', 'document', 'generat', 'first', 'choos', 'subset', 'vocabulari', 'use', 'select', 'word', 'number', 'time', ',', 'least', 'without', 'order', '.']

 TOTAL SNOWBALL STEM WORDS ==> 18



 ---- LEMMATIZATION ----

['first', 'model', 'document', 'generated', 'first', 'choosing', 'subset', 'vocabulary', 'using', 'selected', 'word', 'number', 'time', ',', 'least', 'without', 'order', '.']

 TOTAL LEMMATIZE WORDS ==> 18

************************************************************************************************************************

222 --> This is called Multi-variate Bernoulli  model. 


 ---- TOKENS ----

 ['This', 'is', 'called', 'Multi-variate', 'Bernoulli', 'model', '.'] 

 TOTAL TOKENS ==> 7

 ---- POST ----

 [('This', 'DT'), ('is', 'VBZ'), ('called', 'VBN'), ('Multi-variate', 'NNP'), ('Bernoulli', 'NNP'), ('model', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['called', 'Multi-variate', 'Bernoulli', 'model', '.']

 TOTAL FILTERED TOKENS ==>  5

 ---- POST FOR FILTERED TOKENS ----

 [('called', 'VBN'), ('Multi-variate', 'NNP'), ('Bernoulli', 'NNP'), ('model', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['called Multi-variate', 'Multi-variate Bernoulli', 'Bernoulli model', 'model .'] 

 TOTAL BIGRAMS --> 4 



 ---- TRI-GRAMS ---- 

 ['called Multi-variate Bernoulli', 'Multi-variate Bernoulli model', 'Bernoulli model .'] 

 TOTAL TRIGRAMS --> 3 



 ---- NOUN PHRASES ---- 

 ['model'] 

 TOTAL NOUN PHRASES --> 1 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['call', 'multi-vari', 'bernoulli', 'model', '.']

 TOTAL PORTER STEM WORDS ==> 5



 ---- SNOWBALL STEMMING ----

['call', 'multi-vari', 'bernoulli', 'model', '.']

 TOTAL SNOWBALL STEM WORDS ==> 5



 ---- LEMMATIZATION ----

['called', 'Multi-variate', 'Bernoulli', 'model', '.']

 TOTAL LEMMATIZE WORDS ==> 5

************************************************************************************************************************

223 --> It takes the information of which words are used in a document irrespective of number  of words and order. 


 ---- TOKENS ----

 ['It', 'takes', 'the', 'information', 'of', 'which', 'words', 'are', 'used', 'in', 'a', 'document', 'irrespective', 'of', 'number', 'of', 'words', 'and', 'order', '.'] 

 TOTAL TOKENS ==> 20

 ---- POST ----

 [('It', 'PRP'), ('takes', 'VBZ'), ('the', 'DT'), ('information', 'NN'), ('of', 'IN'), ('which', 'WDT'), ('words', 'NNS'), ('are', 'VBP'), ('used', 'VBN'), ('in', 'IN'), ('a', 'DT'), ('document', 'NN'), ('irrespective', 'NN'), ('of', 'IN'), ('number', 'NN'), ('of', 'IN'), ('words', 'NNS'), ('and', 'CC'), ('order', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['takes', 'information', 'words', 'used', 'document', 'irrespective', 'number', 'words', 'order', '.']

 TOTAL FILTERED TOKENS ==>  10

 ---- POST FOR FILTERED TOKENS ----

 [('takes', 'VBZ'), ('information', 'NN'), ('words', 'NNS'), ('used', 'VBN'), ('document', 'NN'), ('irrespective', 'JJ'), ('number', 'NN'), ('words', 'NNS'), ('order', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['takes information', 'information words', 'words used', 'used document', 'document irrespective', 'irrespective number', 'number words', 'words order', 'order .'] 

 TOTAL BIGRAMS --> 9 



 ---- TRI-GRAMS ---- 

 ['takes information words', 'information words used', 'words used document', 'used document irrespective', 'document irrespective number', 'irrespective number words', 'number words order', 'words order .'] 

 TOTAL TRIGRAMS --> 8 



 ---- NOUN PHRASES ---- 

 ['information', 'document', 'irrespective number', 'order'] 

 TOTAL NOUN PHRASES --> 4 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['take', 'inform', 'word', 'use', 'document', 'irrespect', 'number', 'word', 'order', '.']

 TOTAL PORTER STEM WORDS ==> 10



 ---- SNOWBALL STEMMING ----

['take', 'inform', 'word', 'use', 'document', 'irrespect', 'number', 'word', 'order', '.']

 TOTAL SNOWBALL STEM WORDS ==> 10



 ---- LEMMATIZATION ----

['take', 'information', 'word', 'used', 'document', 'irrespective', 'number', 'word', 'order', '.']

 TOTAL LEMMATIZE WORDS ==> 10

************************************************************************************************************************

224 --> In second model, a document is generated by choosing a set of word  occurrences and arranging them in any order. 


 ---- TOKENS ----

 ['In', 'second', 'model', ',', 'a', 'document', 'is', 'generated', 'by', 'choosing', 'a', 'set', 'of', 'word', 'occurrences', 'and', 'arranging', 'them', 'in', 'any', 'order', '.'] 

 TOTAL TOKENS ==> 22

 ---- POST ----

 [('In', 'IN'), ('second', 'JJ'), ('model', 'NN'), (',', ','), ('a', 'DT'), ('document', 'NN'), ('is', 'VBZ'), ('generated', 'VBN'), ('by', 'IN'), ('choosing', 'VBG'), ('a', 'DT'), ('set', 'NN'), ('of', 'IN'), ('word', 'NN'), ('occurrences', 'NNS'), ('and', 'CC'), ('arranging', 'VBG'), ('them', 'PRP'), ('in', 'IN'), ('any', 'DT'), ('order', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['second', 'model', ',', 'document', 'generated', 'choosing', 'set', 'word', 'occurrences', 'arranging', 'order', '.']

 TOTAL FILTERED TOKENS ==>  12

 ---- POST FOR FILTERED TOKENS ----

 [('second', 'JJ'), ('model', 'NN'), (',', ','), ('document', 'NN'), ('generated', 'VBD'), ('choosing', 'VBG'), ('set', 'VBN'), ('word', 'NN'), ('occurrences', 'NNS'), ('arranging', 'VBG'), ('order', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['second model', 'model ,', ', document', 'document generated', 'generated choosing', 'choosing set', 'set word', 'word occurrences', 'occurrences arranging', 'arranging order', 'order .'] 

 TOTAL BIGRAMS --> 11 



 ---- TRI-GRAMS ---- 

 ['second model ,', 'model , document', ', document generated', 'document generated choosing', 'generated choosing set', 'choosing set word', 'set word occurrences', 'word occurrences arranging', 'occurrences arranging order', 'arranging order .'] 

 TOTAL TRIGRAMS --> 10 



 ---- NOUN PHRASES ---- 

 ['second model', 'document', 'word', 'order'] 

 TOTAL NOUN PHRASES --> 4 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['second', 'model', ',', 'document', 'gener', 'choos', 'set', 'word', 'occurr', 'arrang', 'order', '.']

 TOTAL PORTER STEM WORDS ==> 12



 ---- SNOWBALL STEMMING ----

['second', 'model', ',', 'document', 'generat', 'choos', 'set', 'word', 'occurr', 'arrang', 'order', '.']

 TOTAL SNOWBALL STEM WORDS ==> 12



 ---- LEMMATIZATION ----

['second', 'model', ',', 'document', 'generated', 'choosing', 'set', 'word', 'occurrence', 'arranging', 'order', '.']

 TOTAL LEMMATIZE WORDS ==> 12

************************************************************************************************************************

225 --> this model is called multi-nomial model, in  addition to the Multi-variate Bernoulli model , it also captures information on how many  times a word is used in a document  Discovery of knowledge is becoming important areas of research over the recent years. 


 ---- TOKENS ----

 ['this', 'model', 'is', 'called', 'multi-nomial', 'model', ',', 'in', 'addition', 'to', 'the', 'Multi-variate', 'Bernoulli', 'model', ',', 'it', 'also', 'captures', 'information', 'on', 'how', 'many', 'times', 'a', 'word', 'is', 'used', 'in', 'a', 'document', 'Discovery', 'of', 'knowledge', 'is', 'becoming', 'important', 'areas', 'of', 'research', 'over', 'the', 'recent', 'years', '.'] 

 TOTAL TOKENS ==> 44

 ---- POST ----

 [('this', 'DT'), ('model', 'NN'), ('is', 'VBZ'), ('called', 'VBN'), ('multi-nomial', 'JJ'), ('model', 'NN'), (',', ','), ('in', 'IN'), ('addition', 'NN'), ('to', 'TO'), ('the', 'DT'), ('Multi-variate', 'NNP'), ('Bernoulli', 'NNP'), ('model', 'NN'), (',', ','), ('it', 'PRP'), ('also', 'RB'), ('captures', 'VBZ'), ('information', 'NN'), ('on', 'IN'), ('how', 'WRB'), ('many', 'JJ'), ('times', 'NNS'), ('a', 'DT'), ('word', 'NN'), ('is', 'VBZ'), ('used', 'VBN'), ('in', 'IN'), ('a', 'DT'), ('document', 'JJ'), ('Discovery', 'NNP'), ('of', 'IN'), ('knowledge', 'NN'), ('is', 'VBZ'), ('becoming', 'VBG'), ('important', 'JJ'), ('areas', 'NNS'), ('of', 'IN'), ('research', 'NN'), ('over', 'IN'), ('the', 'DT'), ('recent', 'JJ'), ('years', 'NNS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['model', 'called', 'multi-nomial', 'model', ',', 'addition', 'Multi-variate', 'Bernoulli', 'model', ',', 'also', 'captures', 'information', 'many', 'times', 'word', 'used', 'document', 'Discovery', 'knowledge', 'becoming', 'important', 'areas', 'research', 'recent', 'years', '.']

 TOTAL FILTERED TOKENS ==>  27

 ---- POST FOR FILTERED TOKENS ----

 [('model', 'NN'), ('called', 'VBN'), ('multi-nomial', 'JJ'), ('model', 'NN'), (',', ','), ('addition', 'NN'), ('Multi-variate', 'NNP'), ('Bernoulli', 'NNP'), ('model', 'NN'), (',', ','), ('also', 'RB'), ('captures', 'VBZ'), ('information', 'NN'), ('many', 'JJ'), ('times', 'NNS'), ('word', 'NN'), ('used', 'VBN'), ('document', 'JJ'), ('Discovery', 'NNP'), ('knowledge', 'NN'), ('becoming', 'VBG'), ('important', 'JJ'), ('areas', 'NNS'), ('research', 'NN'), ('recent', 'JJ'), ('years', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['model called', 'called multi-nomial', 'multi-nomial model', 'model ,', ', addition', 'addition Multi-variate', 'Multi-variate Bernoulli', 'Bernoulli model', 'model ,', ', also', 'also captures', 'captures information', 'information many', 'many times', 'times word', 'word used', 'used document', 'document Discovery', 'Discovery knowledge', 'knowledge becoming', 'becoming important', 'important areas', 'areas research', 'research recent', 'recent years', 'years .'] 

 TOTAL BIGRAMS --> 26 



 ---- TRI-GRAMS ---- 

 ['model called multi-nomial', 'called multi-nomial model', 'multi-nomial model ,', 'model , addition', ', addition Multi-variate', 'addition Multi-variate Bernoulli', 'Multi-variate Bernoulli model', 'Bernoulli model ,', 'model , also', ', also captures', 'also captures information', 'captures information many', 'information many times', 'many times word', 'times word used', 'word used document', 'used document Discovery', 'document Discovery knowledge', 'Discovery knowledge becoming', 'knowledge becoming important', 'becoming important areas', 'important areas research', 'areas research recent', 'research recent years', 'recent years .'] 

 TOTAL TRIGRAMS --> 25 



 ---- NOUN PHRASES ---- 

 ['model', 'multi-nomial model', 'addition', 'model', 'information', 'word', 'knowledge', 'research'] 

 TOTAL NOUN PHRASES --> 8 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['model', 'call', 'multi-nomi', 'model', ',', 'addit', 'multi-vari', 'bernoulli', 'model', ',', 'also', 'captur', 'inform', 'mani', 'time', 'word', 'use', 'document', 'discoveri', 'knowledg', 'becom', 'import', 'area', 'research', 'recent', 'year', '.']

 TOTAL PORTER STEM WORDS ==> 27



 ---- SNOWBALL STEMMING ----

['model', 'call', 'multi-nomi', 'model', ',', 'addit', 'multi-vari', 'bernoulli', 'model', ',', 'also', 'captur', 'inform', 'mani', 'time', 'word', 'use', 'document', 'discoveri', 'knowledg', 'becom', 'import', 'area', 'research', 'recent', 'year', '.']

 TOTAL SNOWBALL STEM WORDS ==> 27



 ---- LEMMATIZATION ----

['model', 'called', 'multi-nomial', 'model', ',', 'addition', 'Multi-variate', 'Bernoulli', 'model', ',', 'also', 'capture', 'information', 'many', 'time', 'word', 'used', 'document', 'Discovery', 'knowledge', 'becoming', 'important', 'area', 'research', 'recent', 'year', '.']

 TOTAL LEMMATIZE WORDS ==> 27

************************************************************************************************************************

226 --> Knowledge discovery research use a variety of techniques in order to extract useful  information from source documents like   Parts of Speech (POS) tagging, Chunking or Shadow Parsing, Stop-words (Keywords that  are used and must be removed before processing documents), Stemming (Mapping words to  some base for, it has two methods, dictionary based stemming and Porter style stemming  (Porter, 1980) [55]. 


 ---- TOKENS ----

 ['Knowledge', 'discovery', 'research', 'use', 'a', 'variety', 'of', 'techniques', 'in', 'order', 'to', 'extract', 'useful', 'information', 'from', 'source', 'documents', 'like', 'Parts', 'of', 'Speech', '(', 'POS', ')', 'tagging', ',', 'Chunking', 'or', 'Shadow', 'Parsing', ',', 'Stop-words', '(', 'Keywords', 'that', 'are', 'used', 'and', 'must', 'be', 'removed', 'before', 'processing', 'documents', ')', ',', 'Stemming', '(', 'Mapping', 'words', 'to', 'some', 'base', 'for', ',', 'it', 'has', 'two', 'methods', ',', 'dictionary', 'based', 'stemming', 'and', 'Porter', 'style', 'stemming', '(', 'Porter', ',', '1980', ')', '[', '55', ']', '.'] 

 TOTAL TOKENS ==> 76

 ---- POST ----

 [('Knowledge', 'NNP'), ('discovery', 'NN'), ('research', 'NN'), ('use', 'NN'), ('a', 'DT'), ('variety', 'NN'), ('of', 'IN'), ('techniques', 'NNS'), ('in', 'IN'), ('order', 'NN'), ('to', 'TO'), ('extract', 'VB'), ('useful', 'JJ'), ('information', 'NN'), ('from', 'IN'), ('source', 'NN'), ('documents', 'NNS'), ('like', 'IN'), ('Parts', 'NNS'), ('of', 'IN'), ('Speech', 'NNP'), ('(', '('), ('POS', 'NNP'), (')', ')'), ('tagging', 'NN'), (',', ','), ('Chunking', 'NNP'), ('or', 'CC'), ('Shadow', 'NNP'), ('Parsing', 'NNP'), (',', ','), ('Stop-words', 'NNP'), ('(', '('), ('Keywords', 'NNP'), ('that', 'WDT'), ('are', 'VBP'), ('used', 'VBN'), ('and', 'CC'), ('must', 'MD'), ('be', 'VB'), ('removed', 'VBN'), ('before', 'IN'), ('processing', 'VBG'), ('documents', 'NNS'), (')', ')'), (',', ','), ('Stemming', 'VBG'), ('(', '('), ('Mapping', 'VBG'), ('words', 'NNS'), ('to', 'TO'), ('some', 'DT'), ('base', 'NN'), ('for', 'IN'), (',', ','), ('it', 'PRP'), ('has', 'VBZ'), ('two', 'CD'), ('methods', 'NNS'), (',', ','), ('dictionary', 'JJ'), ('based', 'VBN'), ('stemming', 'NN'), ('and', 'CC'), ('Porter', 'NNP'), ('style', 'NN'), ('stemming', 'VBG'), ('(', '('), ('Porter', 'NNP'), (',', ','), ('1980', 'CD'), (')', ')'), ('[', 'VBD'), ('55', 'CD'), (']', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Knowledge', 'discovery', 'research', 'use', 'variety', 'techniques', 'order', 'extract', 'useful', 'information', 'source', 'documents', 'like', 'Parts', 'Speech', '(', 'POS', ')', 'tagging', ',', 'Chunking', 'Shadow', 'Parsing', ',', 'Stop-words', '(', 'Keywords', 'used', 'must', 'removed', 'processing', 'documents', ')', ',', 'Stemming', '(', 'Mapping', 'words', 'base', ',', 'two', 'methods', ',', 'dictionary', 'based', 'stemming', 'Porter', 'style', 'stemming', '(', 'Porter', ',', '1980', ')', '[', '55', ']', '.']

 TOTAL FILTERED TOKENS ==>  58

 ---- POST FOR FILTERED TOKENS ----

 [('Knowledge', 'NNP'), ('discovery', 'NN'), ('research', 'NN'), ('use', 'NN'), ('variety', 'NN'), ('techniques', 'NNS'), ('order', 'NN'), ('extract', 'JJ'), ('useful', 'JJ'), ('information', 'NN'), ('source', 'NN'), ('documents', 'NNS'), ('like', 'IN'), ('Parts', 'NNP'), ('Speech', 'NNP'), ('(', '('), ('POS', 'NNP'), (')', ')'), ('tagging', 'NN'), (',', ','), ('Chunking', 'VBG'), ('Shadow', 'NNP'), ('Parsing', 'NNP'), (',', ','), ('Stop-words', 'NNP'), ('(', '('), ('Keywords', 'NNP'), ('used', 'VBN'), ('must', 'MD'), ('removed', 'VB'), ('processing', 'NN'), ('documents', 'NNS'), (')', ')'), (',', ','), ('Stemming', 'VBG'), ('(', '('), ('Mapping', 'VBG'), ('words', 'NNS'), ('base', 'NN'), (',', ','), ('two', 'CD'), ('methods', 'NNS'), (',', ','), ('dictionary', 'JJ'), ('based', 'VBN'), ('stemming', 'VBG'), ('Porter', 'NNP'), ('style', 'NN'), ('stemming', 'VBG'), ('(', '('), ('Porter', 'NNP'), (',', ','), ('1980', 'CD'), (')', ')'), ('[', 'VBD'), ('55', 'CD'), (']', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Knowledge discovery', 'discovery research', 'research use', 'use variety', 'variety techniques', 'techniques order', 'order extract', 'extract useful', 'useful information', 'information source', 'source documents', 'documents like', 'like Parts', 'Parts Speech', 'Speech (', '( POS', 'POS )', ') tagging', 'tagging ,', ', Chunking', 'Chunking Shadow', 'Shadow Parsing', 'Parsing ,', ', Stop-words', 'Stop-words (', '( Keywords', 'Keywords used', 'used must', 'must removed', 'removed processing', 'processing documents', 'documents )', ') ,', ', Stemming', 'Stemming (', '( Mapping', 'Mapping words', 'words base', 'base ,', ', two', 'two methods', 'methods ,', ', dictionary', 'dictionary based', 'based stemming', 'stemming Porter', 'Porter style', 'style stemming', 'stemming (', '( Porter', 'Porter ,', ', 1980', '1980 )', ') [', '[ 55', '55 ]', '] .'] 

 TOTAL BIGRAMS --> 57 



 ---- TRI-GRAMS ---- 

 ['Knowledge discovery research', 'discovery research use', 'research use variety', 'use variety techniques', 'variety techniques order', 'techniques order extract', 'order extract useful', 'extract useful information', 'useful information source', 'information source documents', 'source documents like', 'documents like Parts', 'like Parts Speech', 'Parts Speech (', 'Speech ( POS', '( POS )', 'POS ) tagging', ') tagging ,', 'tagging , Chunking', ', Chunking Shadow', 'Chunking Shadow Parsing', 'Shadow Parsing ,', 'Parsing , Stop-words', ', Stop-words (', 'Stop-words ( Keywords', '( Keywords used', 'Keywords used must', 'used must removed', 'must removed processing', 'removed processing documents', 'processing documents )', 'documents ) ,', ') , Stemming', ', Stemming (', 'Stemming ( Mapping', '( Mapping words', 'Mapping words base', 'words base ,', 'base , two', ', two methods', 'two methods ,', 'methods , dictionary', ', dictionary based', 'dictionary based stemming', 'based stemming Porter', 'stemming Porter style', 'Porter style stemming', 'style stemming (', 'stemming ( Porter', '( Porter ,', 'Porter , 1980', ', 1980 )', '1980 ) [', ') [ 55', '[ 55 ]', '55 ] .'] 

 TOTAL TRIGRAMS --> 56 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 []

 ---- PORTER STEMMING ----

['knowledg', 'discoveri', 'research', 'use', 'varieti', 'techniqu', 'order', 'extract', 'use', 'inform', 'sourc', 'document', 'like', 'part', 'speech', '(', 'po', ')', 'tag', ',', 'chunk', 'shadow', 'pars', ',', 'stop-word', '(', 'keyword', 'use', 'must', 'remov', 'process', 'document', ')', ',', 'stem', '(', 'map', 'word', 'base', ',', 'two', 'method', ',', 'dictionari', 'base', 'stem', 'porter', 'style', 'stem', '(', 'porter', ',', '1980', ')', '[', '55', ']', '.']

 TOTAL PORTER STEM WORDS ==> 58



 ---- SNOWBALL STEMMING ----

['knowledg', 'discoveri', 'research', 'use', 'varieti', 'techniqu', 'order', 'extract', 'use', 'inform', 'sourc', 'document', 'like', 'part', 'speech', '(', 'pos', ')', 'tag', ',', 'chunk', 'shadow', 'pars', ',', 'stop-word', '(', 'keyword', 'use', 'must', 'remov', 'process', 'document', ')', ',', 'stem', '(', 'map', 'word', 'base', ',', 'two', 'method', ',', 'dictionari', 'base', 'stem', 'porter', 'style', 'stem', '(', 'porter', ',', '1980', ')', '[', '55', ']', '.']

 TOTAL SNOWBALL STEM WORDS ==> 58



 ---- LEMMATIZATION ----

['Knowledge', 'discovery', 'research', 'use', 'variety', 'technique', 'order', 'extract', 'useful', 'information', 'source', 'document', 'like', 'Parts', 'Speech', '(', 'POS', ')', 'tagging', ',', 'Chunking', 'Shadow', 'Parsing', ',', 'Stop-words', '(', 'Keywords', 'used', 'must', 'removed', 'processing', 'document', ')', ',', 'Stemming', '(', 'Mapping', 'word', 'base', ',', 'two', 'method', ',', 'dictionary', 'based', 'stemming', 'Porter', 'style', 'stemming', '(', 'Porter', ',', '1980', ')', '[', '55', ']', '.']

 TOTAL LEMMATIZE WORDS ==> 58

************************************************************************************************************************

227 --> Former one has higher accuracy but higher cost of implementation while  latter has lower implementation cost and is usually insufficient for IR). 


 ---- TOKENS ----

 ['Former', 'one', 'has', 'higher', 'accuracy', 'but', 'higher', 'cost', 'of', 'implementation', 'while', 'latter', 'has', 'lower', 'implementation', 'cost', 'and', 'is', 'usually', 'insufficient', 'for', 'IR', ')', '.'] 

 TOTAL TOKENS ==> 24

 ---- POST ----

 [('Former', 'NNP'), ('one', 'CD'), ('has', 'VBZ'), ('higher', 'JJR'), ('accuracy', 'NN'), ('but', 'CC'), ('higher', 'JJR'), ('cost', 'NN'), ('of', 'IN'), ('implementation', 'NN'), ('while', 'IN'), ('latter', 'NN'), ('has', 'VBZ'), ('lower', 'JJR'), ('implementation', 'NN'), ('cost', 'NN'), ('and', 'CC'), ('is', 'VBZ'), ('usually', 'RB'), ('insufficient', 'JJ'), ('for', 'IN'), ('IR', 'NNP'), (')', ')'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Former', 'one', 'higher', 'accuracy', 'higher', 'cost', 'implementation', 'latter', 'lower', 'implementation', 'cost', 'usually', 'insufficient', 'IR', ')', '.']

 TOTAL FILTERED TOKENS ==>  16

 ---- POST FOR FILTERED TOKENS ----

 [('Former', 'NNP'), ('one', 'CD'), ('higher', 'JJR'), ('accuracy', 'NN'), ('higher', 'JJR'), ('cost', 'NN'), ('implementation', 'NN'), ('latter', 'RBR'), ('lower', 'JJR'), ('implementation', 'NN'), ('cost', 'NN'), ('usually', 'RB'), ('insufficient', 'JJ'), ('IR', 'NNP'), (')', ')'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Former one', 'one higher', 'higher accuracy', 'accuracy higher', 'higher cost', 'cost implementation', 'implementation latter', 'latter lower', 'lower implementation', 'implementation cost', 'cost usually', 'usually insufficient', 'insufficient IR', 'IR )', ') .'] 

 TOTAL BIGRAMS --> 15 



 ---- TRI-GRAMS ---- 

 ['Former one higher', 'one higher accuracy', 'higher accuracy higher', 'accuracy higher cost', 'higher cost implementation', 'cost implementation latter', 'implementation latter lower', 'latter lower implementation', 'lower implementation cost', 'implementation cost usually', 'cost usually insufficient', 'usually insufficient IR', 'insufficient IR )', 'IR ) .'] 

 TOTAL TRIGRAMS --> 14 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 []

 ---- PORTER STEMMING ----

['former', 'one', 'higher', 'accuraci', 'higher', 'cost', 'implement', 'latter', 'lower', 'implement', 'cost', 'usual', 'insuffici', 'ir', ')', '.']

 TOTAL PORTER STEM WORDS ==> 16



 ---- SNOWBALL STEMMING ----

['former', 'one', 'higher', 'accuraci', 'higher', 'cost', 'implement', 'latter', 'lower', 'implement', 'cost', 'usual', 'insuffici', 'ir', ')', '.']

 TOTAL SNOWBALL STEM WORDS ==> 16



 ---- LEMMATIZATION ----

['Former', 'one', 'higher', 'accuracy', 'higher', 'cost', 'implementation', 'latter', 'lower', 'implementation', 'cost', 'usually', 'insufficient', 'IR', ')', '.']

 TOTAL LEMMATIZE WORDS ==> 16

************************************************************************************************************************

228 --> Compound or  Statistical Phrases (Compounds and statistical phrases index multi token units instead of  single tokens.) 


 ---- TOKENS ----

 ['Compound', 'or', 'Statistical', 'Phrases', '(', 'Compounds', 'and', 'statistical', 'phrases', 'index', 'multi', 'token', 'units', 'instead', 'of', 'single', 'tokens', '.', ')'] 

 TOTAL TOKENS ==> 19

 ---- POST ----

 [('Compound', 'NN'), ('or', 'CC'), ('Statistical', 'JJ'), ('Phrases', 'NNS'), ('(', '('), ('Compounds', 'NNPS'), ('and', 'CC'), ('statistical', 'JJ'), ('phrases', 'NNS'), ('index', 'NN'), ('multi', 'VBP'), ('token', 'VBN'), ('units', 'NNS'), ('instead', 'RB'), ('of', 'IN'), ('single', 'JJ'), ('tokens', 'NNS'), ('.', '.'), (')', ')')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Compound', 'Statistical', 'Phrases', '(', 'Compounds', 'statistical', 'phrases', 'index', 'multi', 'token', 'units', 'instead', 'single', 'tokens', '.', ')']

 TOTAL FILTERED TOKENS ==>  16

 ---- POST FOR FILTERED TOKENS ----

 [('Compound', 'NNP'), ('Statistical', 'NNP'), ('Phrases', 'NNP'), ('(', '('), ('Compounds', 'NNP'), ('statistical', 'JJ'), ('phrases', 'NNS'), ('index', 'NN'), ('multi', 'VBP'), ('token', 'VBN'), ('units', 'NNS'), ('instead', 'RB'), ('single', 'JJ'), ('tokens', 'NNS'), ('.', '.'), (')', ')')] 



 ---- BI-GRAMS ---- 

 ['Compound Statistical', 'Statistical Phrases', 'Phrases (', '( Compounds', 'Compounds statistical', 'statistical phrases', 'phrases index', 'index multi', 'multi token', 'token units', 'units instead', 'instead single', 'single tokens', 'tokens .', '. )'] 

 TOTAL BIGRAMS --> 15 



 ---- TRI-GRAMS ---- 

 ['Compound Statistical Phrases', 'Statistical Phrases (', 'Phrases ( Compounds', '( Compounds statistical', 'Compounds statistical phrases', 'statistical phrases index', 'phrases index multi', 'index multi token', 'multi token units', 'token units instead', 'units instead single', 'instead single tokens', 'single tokens .', 'tokens . )'] 

 TOTAL TRIGRAMS --> 14 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> ['Compound']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['compound', 'statist', 'phrase', '(', 'compound', 'statist', 'phrase', 'index', 'multi', 'token', 'unit', 'instead', 'singl', 'token', '.', ')']

 TOTAL PORTER STEM WORDS ==> 16



 ---- SNOWBALL STEMMING ----

['compound', 'statist', 'phrase', '(', 'compound', 'statist', 'phrase', 'index', 'multi', 'token', 'unit', 'instead', 'singl', 'token', '.', ')']

 TOTAL SNOWBALL STEM WORDS ==> 16



 ---- LEMMATIZATION ----

['Compound', 'Statistical', 'Phrases', '(', 'Compounds', 'statistical', 'phrase', 'index', 'multi', 'token', 'unit', 'instead', 'single', 'token', '.', ')']

 TOTAL LEMMATIZE WORDS ==> 16

************************************************************************************************************************

229 --> Word Sense Disambiguation (Word sense disambiguation is the task of  understanding the correct sense of a word in context. 


 ---- TOKENS ----

 ['Word', 'Sense', 'Disambiguation', '(', 'Word', 'sense', 'disambiguation', 'is', 'the', 'task', 'of', 'understanding', 'the', 'correct', 'sense', 'of', 'a', 'word', 'in', 'context', '.'] 

 TOTAL TOKENS ==> 21

 ---- POST ----

 [('Word', 'NNP'), ('Sense', 'NNP'), ('Disambiguation', 'NNP'), ('(', '('), ('Word', 'NNP'), ('sense', 'NN'), ('disambiguation', 'NN'), ('is', 'VBZ'), ('the', 'DT'), ('task', 'NN'), ('of', 'IN'), ('understanding', 'VBG'), ('the', 'DT'), ('correct', 'JJ'), ('sense', 'NN'), ('of', 'IN'), ('a', 'DT'), ('word', 'NN'), ('in', 'IN'), ('context', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Word', 'Sense', 'Disambiguation', '(', 'Word', 'sense', 'disambiguation', 'task', 'understanding', 'correct', 'sense', 'word', 'context', '.']

 TOTAL FILTERED TOKENS ==>  14

 ---- POST FOR FILTERED TOKENS ----

 [('Word', 'NNP'), ('Sense', 'NNP'), ('Disambiguation', 'NNP'), ('(', '('), ('Word', 'NNP'), ('sense', 'NN'), ('disambiguation', 'NN'), ('task', 'NN'), ('understanding', 'VBG'), ('correct', 'JJ'), ('sense', 'NN'), ('word', 'NN'), ('context', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Word Sense', 'Sense Disambiguation', 'Disambiguation (', '( Word', 'Word sense', 'sense disambiguation', 'disambiguation task', 'task understanding', 'understanding correct', 'correct sense', 'sense word', 'word context', 'context .'] 

 TOTAL BIGRAMS --> 13 



 ---- TRI-GRAMS ---- 

 ['Word Sense Disambiguation', 'Sense Disambiguation (', 'Disambiguation ( Word', '( Word sense', 'Word sense disambiguation', 'sense disambiguation task', 'disambiguation task understanding', 'task understanding correct', 'understanding correct sense', 'correct sense word', 'sense word context', 'word context .'] 

 TOTAL TRIGRAMS --> 12 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 []

 ---- PORTER STEMMING ----

['word', 'sens', 'disambigu', '(', 'word', 'sens', 'disambigu', 'task', 'understand', 'correct', 'sens', 'word', 'context', '.']

 TOTAL PORTER STEM WORDS ==> 14



 ---- SNOWBALL STEMMING ----

['word', 'sens', 'disambigu', '(', 'word', 'sens', 'disambigu', 'task', 'understand', 'correct', 'sens', 'word', 'context', '.']

 TOTAL SNOWBALL STEM WORDS ==> 14



 ---- LEMMATIZATION ----

['Word', 'Sense', 'Disambiguation', '(', 'Word', 'sense', 'disambiguation', 'task', 'understanding', 'correct', 'sense', 'word', 'context', '.']

 TOTAL LEMMATIZE WORDS ==> 14

************************************************************************************************************************

230 --> When used for information retrieval,  terms are replaced by their senses in the document vector.) 


 ---- TOKENS ----

 ['When', 'used', 'for', 'information', 'retrieval', ',', 'terms', 'are', 'replaced', 'by', 'their', 'senses', 'in', 'the', 'document', 'vector', '.', ')'] 

 TOTAL TOKENS ==> 18

 ---- POST ----

 [('When', 'WRB'), ('used', 'VBN'), ('for', 'IN'), ('information', 'NN'), ('retrieval', 'NN'), (',', ','), ('terms', 'NNS'), ('are', 'VBP'), ('replaced', 'VBN'), ('by', 'IN'), ('their', 'PRP$'), ('senses', 'NNS'), ('in', 'IN'), ('the', 'DT'), ('document', 'NN'), ('vector', 'NN'), ('.', '.'), (')', ')')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['used', 'information', 'retrieval', ',', 'terms', 'replaced', 'senses', 'document', 'vector', '.', ')']

 TOTAL FILTERED TOKENS ==>  11

 ---- POST FOR FILTERED TOKENS ----

 [('used', 'VBN'), ('information', 'NN'), ('retrieval', 'NN'), (',', ','), ('terms', 'NNS'), ('replaced', 'VBD'), ('senses', 'NNS'), ('document', 'JJ'), ('vector', 'NN'), ('.', '.'), (')', ')')] 



 ---- BI-GRAMS ---- 

 ['used information', 'information retrieval', 'retrieval ,', ', terms', 'terms replaced', 'replaced senses', 'senses document', 'document vector', 'vector .', '. )'] 

 TOTAL BIGRAMS --> 10 



 ---- TRI-GRAMS ---- 

 ['used information retrieval', 'information retrieval ,', 'retrieval , terms', ', terms replaced', 'terms replaced senses', 'replaced senses document', 'senses document vector', 'document vector .', 'vector . )'] 

 TOTAL TRIGRAMS --> 9 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 []

 ---- PORTER STEMMING ----

['use', 'inform', 'retriev', ',', 'term', 'replac', 'sens', 'document', 'vector', '.', ')']

 TOTAL PORTER STEM WORDS ==> 11



 ---- SNOWBALL STEMMING ----

['use', 'inform', 'retriev', ',', 'term', 'replac', 'sens', 'document', 'vector', '.', ')']

 TOTAL SNOWBALL STEM WORDS ==> 11



 ---- LEMMATIZATION ----

['used', 'information', 'retrieval', ',', 'term', 'replaced', 'sens', 'document', 'vector', '.', ')']

 TOTAL LEMMATIZE WORDS ==> 11

************************************************************************************************************************

231 --> Its extracted information can be applied on a variety of purpose, for example to prepare a  summary, to build databases, identify keywords, classifying text items according to some pre- defined categories etc. 


 ---- TOKENS ----

 ['Its', 'extracted', 'information', 'can', 'be', 'applied', 'on', 'a', 'variety', 'of', 'purpose', ',', 'for', 'example', 'to', 'prepare', 'a', 'summary', ',', 'to', 'build', 'databases', ',', 'identify', 'keywords', ',', 'classifying', 'text', 'items', 'according', 'to', 'some', 'pre-', 'defined', 'categories', 'etc', '.'] 

 TOTAL TOKENS ==> 37

 ---- POST ----

 [('Its', 'PRP$'), ('extracted', 'JJ'), ('information', 'NN'), ('can', 'MD'), ('be', 'VB'), ('applied', 'VBN'), ('on', 'IN'), ('a', 'DT'), ('variety', 'NN'), ('of', 'IN'), ('purpose', 'NN'), (',', ','), ('for', 'IN'), ('example', 'NN'), ('to', 'TO'), ('prepare', 'VB'), ('a', 'DT'), ('summary', 'JJ'), (',', ','), ('to', 'TO'), ('build', 'VB'), ('databases', 'NNS'), (',', ','), ('identify', 'VB'), ('keywords', 'NNS'), (',', ','), ('classifying', 'VBG'), ('text', 'NN'), ('items', 'NNS'), ('according', 'VBG'), ('to', 'TO'), ('some', 'DT'), ('pre-', 'JJ'), ('defined', 'JJ'), ('categories', 'NNS'), ('etc', 'VBP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['extracted', 'information', 'applied', 'variety', 'purpose', ',', 'example', 'prepare', 'summary', ',', 'build', 'databases', ',', 'identify', 'keywords', ',', 'classifying', 'text', 'items', 'according', 'pre-', 'defined', 'categories', 'etc', '.']

 TOTAL FILTERED TOKENS ==>  25

 ---- POST FOR FILTERED TOKENS ----

 [('extracted', 'VBN'), ('information', 'NN'), ('applied', 'VBN'), ('variety', 'NN'), ('purpose', 'NN'), (',', ','), ('example', 'NN'), ('prepare', 'NN'), ('summary', 'NN'), (',', ','), ('build', 'JJ'), ('databases', 'NNS'), (',', ','), ('identify', 'VB'), ('keywords', 'NNS'), (',', ','), ('classifying', 'VBG'), ('text', 'NN'), ('items', 'NNS'), ('according', 'VBG'), ('pre-', 'NN'), ('defined', 'JJ'), ('categories', 'NNS'), ('etc', 'VBP'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['extracted information', 'information applied', 'applied variety', 'variety purpose', 'purpose ,', ', example', 'example prepare', 'prepare summary', 'summary ,', ', build', 'build databases', 'databases ,', ', identify', 'identify keywords', 'keywords ,', ', classifying', 'classifying text', 'text items', 'items according', 'according pre-', 'pre- defined', 'defined categories', 'categories etc', 'etc .'] 

 TOTAL BIGRAMS --> 24 



 ---- TRI-GRAMS ---- 

 ['extracted information applied', 'information applied variety', 'applied variety purpose', 'variety purpose ,', 'purpose , example', ', example prepare', 'example prepare summary', 'prepare summary ,', 'summary , build', ', build databases', 'build databases ,', 'databases , identify', ', identify keywords', 'identify keywords ,', 'keywords , classifying', ', classifying text', 'classifying text items', 'text items according', 'items according pre-', 'according pre- defined', 'pre- defined categories', 'defined categories etc', 'categories etc .'] 

 TOTAL TRIGRAMS --> 23 



 ---- NOUN PHRASES ---- 

 ['information', 'variety', 'purpose', 'example', 'prepare', 'summary', 'text', 'pre-'] 

 TOTAL NOUN PHRASES --> 8 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['extract', 'inform', 'appli', 'varieti', 'purpos', ',', 'exampl', 'prepar', 'summari', ',', 'build', 'databas', ',', 'identifi', 'keyword', ',', 'classifi', 'text', 'item', 'accord', 'pre-', 'defin', 'categori', 'etc', '.']

 TOTAL PORTER STEM WORDS ==> 25



 ---- SNOWBALL STEMMING ----

['extract', 'inform', 'appli', 'varieti', 'purpos', ',', 'exampl', 'prepar', 'summari', ',', 'build', 'databas', ',', 'identifi', 'keyword', ',', 'classifi', 'text', 'item', 'accord', 'pre-', 'defin', 'categori', 'etc', '.']

 TOTAL SNOWBALL STEM WORDS ==> 25



 ---- LEMMATIZATION ----

['extracted', 'information', 'applied', 'variety', 'purpose', ',', 'example', 'prepare', 'summary', ',', 'build', 'database', ',', 'identify', 'keywords', ',', 'classifying', 'text', 'item', 'according', 'pre-', 'defined', 'category', 'etc', '.']

 TOTAL LEMMATIZE WORDS ==> 25

************************************************************************************************************************

232 --> For example   CONSTRUE, it was developed for Reuters, that is used  in classifying news stories (Hayes, 1992) [57]. 


 ---- TOKENS ----

 ['For', 'example', 'CONSTRUE', ',', 'it', 'was', 'developed', 'for', 'Reuters', ',', 'that', 'is', 'used', 'in', 'classifying', 'news', 'stories', '(', 'Hayes', ',', '1992', ')', '[', '57', ']', '.'] 

 TOTAL TOKENS ==> 26

 ---- POST ----

 [('For', 'IN'), ('example', 'NN'), ('CONSTRUE', 'NNP'), (',', ','), ('it', 'PRP'), ('was', 'VBD'), ('developed', 'VBN'), ('for', 'IN'), ('Reuters', 'NNP'), (',', ','), ('that', 'WDT'), ('is', 'VBZ'), ('used', 'VBN'), ('in', 'IN'), ('classifying', 'VBG'), ('news', 'NN'), ('stories', 'NNS'), ('(', '('), ('Hayes', 'NNP'), (',', ','), ('1992', 'CD'), (')', ')'), ('[', 'VBD'), ('57', 'CD'), (']', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['example', 'CONSTRUE', ',', 'developed', 'Reuters', ',', 'used', 'classifying', 'news', 'stories', '(', 'Hayes', ',', '1992', ')', '[', '57', ']', '.']

 TOTAL FILTERED TOKENS ==>  19

 ---- POST FOR FILTERED TOKENS ----

 [('example', 'NN'), ('CONSTRUE', 'NNP'), (',', ','), ('developed', 'VBD'), ('Reuters', 'NNP'), (',', ','), ('used', 'VBD'), ('classifying', 'VBG'), ('news', 'NN'), ('stories', 'NNS'), ('(', '('), ('Hayes', 'NNP'), (',', ','), ('1992', 'CD'), (')', ')'), ('[', 'VBD'), ('57', 'CD'), (']', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['example CONSTRUE', 'CONSTRUE ,', ', developed', 'developed Reuters', 'Reuters ,', ', used', 'used classifying', 'classifying news', 'news stories', 'stories (', '( Hayes', 'Hayes ,', ', 1992', '1992 )', ') [', '[ 57', '57 ]', '] .'] 

 TOTAL BIGRAMS --> 18 



 ---- TRI-GRAMS ---- 

 ['example CONSTRUE ,', 'CONSTRUE , developed', ', developed Reuters', 'developed Reuters ,', 'Reuters , used', ', used classifying', 'used classifying news', 'classifying news stories', 'news stories (', 'stories ( Hayes', '( Hayes ,', 'Hayes , 1992', ', 1992 )', '1992 ) [', ') [ 57', '[ 57 ]', '57 ] .'] 

 TOTAL TRIGRAMS --> 17 



 ---- NOUN PHRASES ---- 

 ['example', 'news', ']'] 

 TOTAL NOUN PHRASES --> 3 



 ---- NER ----

 
 ORGANIZATION ---> ['CONSTRUE', 'Reuters']
 TOTAL ORGANIZATION ENTITY --> 2 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['exampl', 'constru', ',', 'develop', 'reuter', ',', 'use', 'classifi', 'news', 'stori', '(', 'hay', ',', '1992', ')', '[', '57', ']', '.']

 TOTAL PORTER STEM WORDS ==> 19



 ---- SNOWBALL STEMMING ----

['exampl', 'constru', ',', 'develop', 'reuter', ',', 'use', 'classifi', 'news', 'stori', '(', 'hay', ',', '1992', ')', '[', '57', ']', '.']

 TOTAL SNOWBALL STEM WORDS ==> 19



 ---- LEMMATIZATION ----

['example', 'CONSTRUE', ',', 'developed', 'Reuters', ',', 'used', 'classifying', 'news', 'story', '(', 'Hayes', ',', '1992', ')', '[', '57', ']', '.']

 TOTAL LEMMATIZE WORDS ==> 19

************************************************************************************************************************

233 --> It has been suggested that many IE systems  can successfully extract terms from documents, acquiring relations between the terms is still a  difficulty. 


 ---- TOKENS ----

 ['It', 'has', 'been', 'suggested', 'that', 'many', 'IE', 'systems', 'can', 'successfully', 'extract', 'terms', 'from', 'documents', ',', 'acquiring', 'relations', 'between', 'the', 'terms', 'is', 'still', 'a', 'difficulty', '.'] 

 TOTAL TOKENS ==> 25

 ---- POST ----

 [('It', 'PRP'), ('has', 'VBZ'), ('been', 'VBN'), ('suggested', 'VBN'), ('that', 'IN'), ('many', 'JJ'), ('IE', 'NNP'), ('systems', 'NNS'), ('can', 'MD'), ('successfully', 'RB'), ('extract', 'VB'), ('terms', 'NNS'), ('from', 'IN'), ('documents', 'NNS'), (',', ','), ('acquiring', 'VBG'), ('relations', 'NNS'), ('between', 'IN'), ('the', 'DT'), ('terms', 'NNS'), ('is', 'VBZ'), ('still', 'RB'), ('a', 'DT'), ('difficulty', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['suggested', 'many', 'IE', 'systems', 'successfully', 'extract', 'terms', 'documents', ',', 'acquiring', 'relations', 'terms', 'still', 'difficulty', '.']

 TOTAL FILTERED TOKENS ==>  15

 ---- POST FOR FILTERED TOKENS ----

 [('suggested', 'VBD'), ('many', 'JJ'), ('IE', 'NNP'), ('systems', 'NNS'), ('successfully', 'RB'), ('extract', 'JJ'), ('terms', 'NNS'), ('documents', 'NNS'), (',', ','), ('acquiring', 'VBG'), ('relations', 'NNS'), ('terms', 'NNS'), ('still', 'RB'), ('difficulty', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['suggested many', 'many IE', 'IE systems', 'systems successfully', 'successfully extract', 'extract terms', 'terms documents', 'documents ,', ', acquiring', 'acquiring relations', 'relations terms', 'terms still', 'still difficulty', 'difficulty .'] 

 TOTAL BIGRAMS --> 14 



 ---- TRI-GRAMS ---- 

 ['suggested many IE', 'many IE systems', 'IE systems successfully', 'systems successfully extract', 'successfully extract terms', 'extract terms documents', 'terms documents ,', 'documents , acquiring', ', acquiring relations', 'acquiring relations terms', 'relations terms still', 'terms still difficulty', 'still difficulty .'] 

 TOTAL TRIGRAMS --> 13 



 ---- NOUN PHRASES ---- 

 ['difficulty'] 

 TOTAL NOUN PHRASES --> 1 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['suggest', 'mani', 'ie', 'system', 'success', 'extract', 'term', 'document', ',', 'acquir', 'relat', 'term', 'still', 'difficulti', '.']

 TOTAL PORTER STEM WORDS ==> 15



 ---- SNOWBALL STEMMING ----

['suggest', 'mani', 'ie', 'system', 'success', 'extract', 'term', 'document', ',', 'acquir', 'relat', 'term', 'still', 'difficulti', '.']

 TOTAL SNOWBALL STEM WORDS ==> 15



 ---- LEMMATIZATION ----

['suggested', 'many', 'IE', 'system', 'successfully', 'extract', 'term', 'document', ',', 'acquiring', 'relation', 'term', 'still', 'difficulty', '.']

 TOTAL LEMMATIZE WORDS ==> 15

************************************************************************************************************************

234 --> PROMETHEE is a system that extracts lexico-syntactic patterns relative to a  specific conceptual relation (Morin,1999) [58]. 


 ---- TOKENS ----

 ['PROMETHEE', 'is', 'a', 'system', 'that', 'extracts', 'lexico-syntactic', 'patterns', 'relative', 'to', 'a', 'specific', 'conceptual', 'relation', '(', 'Morin,1999', ')', '[', '58', ']', '.'] 

 TOTAL TOKENS ==> 21

 ---- POST ----

 [('PROMETHEE', 'NNP'), ('is', 'VBZ'), ('a', 'DT'), ('system', 'NN'), ('that', 'WDT'), ('extracts', 'VBZ'), ('lexico-syntactic', 'JJ'), ('patterns', 'NNS'), ('relative', 'VBP'), ('to', 'TO'), ('a', 'DT'), ('specific', 'JJ'), ('conceptual', 'JJ'), ('relation', 'NN'), ('(', '('), ('Morin,1999', 'NNP'), (')', ')'), ('[', 'VBD'), ('58', 'CD'), (']', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['PROMETHEE', 'system', 'extracts', 'lexico-syntactic', 'patterns', 'relative', 'specific', 'conceptual', 'relation', '(', 'Morin,1999', ')', '[', '58', ']', '.']

 TOTAL FILTERED TOKENS ==>  16

 ---- POST FOR FILTERED TOKENS ----

 [('PROMETHEE', 'NNP'), ('system', 'NN'), ('extracts', 'VBZ'), ('lexico-syntactic', 'JJ'), ('patterns', 'NNS'), ('relative', 'JJ'), ('specific', 'JJ'), ('conceptual', 'JJ'), ('relation', 'NN'), ('(', '('), ('Morin,1999', 'NNP'), (')', ')'), ('[', 'VBD'), ('58', 'CD'), (']', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['PROMETHEE system', 'system extracts', 'extracts lexico-syntactic', 'lexico-syntactic patterns', 'patterns relative', 'relative specific', 'specific conceptual', 'conceptual relation', 'relation (', '( Morin,1999', 'Morin,1999 )', ') [', '[ 58', '58 ]', '] .'] 

 TOTAL BIGRAMS --> 15 



 ---- TRI-GRAMS ---- 

 ['PROMETHEE system extracts', 'system extracts lexico-syntactic', 'extracts lexico-syntactic patterns', 'lexico-syntactic patterns relative', 'patterns relative specific', 'relative specific conceptual', 'specific conceptual relation', 'conceptual relation (', 'relation ( Morin,1999', '( Morin,1999 )', 'Morin,1999 ) [', ') [ 58', '[ 58 ]', '58 ] .'] 

 TOTAL TRIGRAMS --> 14 



 ---- NOUN PHRASES ---- 

 ['system', 'relative specific conceptual relation', ']'] 

 TOTAL NOUN PHRASES --> 3 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['promethe', 'system', 'extract', 'lexico-syntact', 'pattern', 'rel', 'specif', 'conceptu', 'relat', '(', 'morin,1999', ')', '[', '58', ']', '.']

 TOTAL PORTER STEM WORDS ==> 16



 ---- SNOWBALL STEMMING ----

['promethe', 'system', 'extract', 'lexico-syntact', 'pattern', 'relat', 'specif', 'conceptu', 'relat', '(', 'morin,1999', ')', '[', '58', ']', '.']

 TOTAL SNOWBALL STEM WORDS ==> 16



 ---- LEMMATIZATION ----

['PROMETHEE', 'system', 'extract', 'lexico-syntactic', 'pattern', 'relative', 'specific', 'conceptual', 'relation', '(', 'Morin,1999', ')', '[', '58', ']', '.']

 TOTAL LEMMATIZE WORDS ==> 16

************************************************************************************************************************

235 --> IE systems should work at many levels, from  word recognition to discourse analysis at the level of the complete document. 


 ---- TOKENS ----

 ['IE', 'systems', 'should', 'work', 'at', 'many', 'levels', ',', 'from', 'word', 'recognition', 'to', 'discourse', 'analysis', 'at', 'the', 'level', 'of', 'the', 'complete', 'document', '.'] 

 TOTAL TOKENS ==> 22

 ---- POST ----

 [('IE', 'NNP'), ('systems', 'NNS'), ('should', 'MD'), ('work', 'VB'), ('at', 'IN'), ('many', 'JJ'), ('levels', 'NNS'), (',', ','), ('from', 'IN'), ('word', 'NN'), ('recognition', 'NN'), ('to', 'TO'), ('discourse', 'VB'), ('analysis', 'NN'), ('at', 'IN'), ('the', 'DT'), ('level', 'NN'), ('of', 'IN'), ('the', 'DT'), ('complete', 'JJ'), ('document', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['IE', 'systems', 'work', 'many', 'levels', ',', 'word', 'recognition', 'discourse', 'analysis', 'level', 'complete', 'document', '.']

 TOTAL FILTERED TOKENS ==>  14

 ---- POST FOR FILTERED TOKENS ----

 [('IE', 'NNP'), ('systems', 'NNS'), ('work', 'VBP'), ('many', 'JJ'), ('levels', 'NNS'), (',', ','), ('word', 'NN'), ('recognition', 'NN'), ('discourse', 'NN'), ('analysis', 'NN'), ('level', 'NN'), ('complete', 'JJ'), ('document', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['IE systems', 'systems work', 'work many', 'many levels', 'levels ,', ', word', 'word recognition', 'recognition discourse', 'discourse analysis', 'analysis level', 'level complete', 'complete document', 'document .'] 

 TOTAL BIGRAMS --> 13 



 ---- TRI-GRAMS ---- 

 ['IE systems work', 'systems work many', 'work many levels', 'many levels ,', 'levels , word', ', word recognition', 'word recognition discourse', 'recognition discourse analysis', 'discourse analysis level', 'analysis level complete', 'level complete document', 'complete document .'] 

 TOTAL TRIGRAMS --> 12 



 ---- NOUN PHRASES ---- 

 ['word', 'recognition', 'discourse', 'analysis', 'level', 'complete document'] 

 TOTAL NOUN PHRASES --> 6 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['ie', 'system', 'work', 'mani', 'level', ',', 'word', 'recognit', 'discours', 'analysi', 'level', 'complet', 'document', '.']

 TOTAL PORTER STEM WORDS ==> 14



 ---- SNOWBALL STEMMING ----

['ie', 'system', 'work', 'mani', 'level', ',', 'word', 'recognit', 'discours', 'analysi', 'level', 'complet', 'document', '.']

 TOTAL SNOWBALL STEM WORDS ==> 14



 ---- LEMMATIZATION ----

['IE', 'system', 'work', 'many', 'level', ',', 'word', 'recognition', 'discourse', 'analysis', 'level', 'complete', 'document', '.']

 TOTAL LEMMATIZE WORDS ==> 14

************************************************************************************************************************

236 --> An application  of the Blank Slate Language Processor (BSLP) (Bondale et al., 1999) [59] approach for the  analysis of a real life natural language corpus that consists of responses to open-ended  questionnaires in the field of advertising. 


 ---- TOKENS ----

 ['An', 'application', 'of', 'the', 'Blank', 'Slate', 'Language', 'Processor', '(', 'BSLP', ')', '(', 'Bondale', 'et', 'al.', ',', '1999', ')', '[', '59', ']', 'approach', 'for', 'the', 'analysis', 'of', 'a', 'real', 'life', 'natural', 'language', 'corpus', 'that', 'consists', 'of', 'responses', 'to', 'open-ended', 'questionnaires', 'in', 'the', 'field', 'of', 'advertising', '.'] 

 TOTAL TOKENS ==> 45

 ---- POST ----

 [('An', 'DT'), ('application', 'NN'), ('of', 'IN'), ('the', 'DT'), ('Blank', 'NNP'), ('Slate', 'NNP'), ('Language', 'NNP'), ('Processor', 'NNP'), ('(', '('), ('BSLP', 'NNP'), (')', ')'), ('(', '('), ('Bondale', 'NNP'), ('et', 'FW'), ('al.', 'NN'), (',', ','), ('1999', 'CD'), (')', ')'), ('[', 'VBD'), ('59', 'CD'), (']', 'NNP'), ('approach', 'NN'), ('for', 'IN'), ('the', 'DT'), ('analysis', 'NN'), ('of', 'IN'), ('a', 'DT'), ('real', 'JJ'), ('life', 'NN'), ('natural', 'JJ'), ('language', 'NN'), ('corpus', 'NN'), ('that', 'IN'), ('consists', 'VBZ'), ('of', 'IN'), ('responses', 'NNS'), ('to', 'TO'), ('open-ended', 'JJ'), ('questionnaires', 'NNS'), ('in', 'IN'), ('the', 'DT'), ('field', 'NN'), ('of', 'IN'), ('advertising', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['application', 'Blank', 'Slate', 'Language', 'Processor', '(', 'BSLP', ')', '(', 'Bondale', 'et', 'al.', ',', '1999', ')', '[', '59', ']', 'approach', 'analysis', 'real', 'life', 'natural', 'language', 'corpus', 'consists', 'responses', 'open-ended', 'questionnaires', 'field', 'advertising', '.']

 TOTAL FILTERED TOKENS ==>  32

 ---- POST FOR FILTERED TOKENS ----

 [('application', 'NN'), ('Blank', 'NNP'), ('Slate', 'NNP'), ('Language', 'NNP'), ('Processor', 'NNP'), ('(', '('), ('BSLP', 'NNP'), (')', ')'), ('(', '('), ('Bondale', 'NNP'), ('et', 'FW'), ('al.', 'NN'), (',', ','), ('1999', 'CD'), (')', ')'), ('[', 'VBD'), ('59', 'CD'), (']', 'NNP'), ('approach', 'NN'), ('analysis', 'NN'), ('real', 'JJ'), ('life', 'NN'), ('natural', 'JJ'), ('language', 'NN'), ('corpus', 'NN'), ('consists', 'VBZ'), ('responses', 'VBZ'), ('open-ended', 'JJ'), ('questionnaires', 'NNS'), ('field', 'NN'), ('advertising', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['application Blank', 'Blank Slate', 'Slate Language', 'Language Processor', 'Processor (', '( BSLP', 'BSLP )', ') (', '( Bondale', 'Bondale et', 'et al.', 'al. ,', ', 1999', '1999 )', ') [', '[ 59', '59 ]', '] approach', 'approach analysis', 'analysis real', 'real life', 'life natural', 'natural language', 'language corpus', 'corpus consists', 'consists responses', 'responses open-ended', 'open-ended questionnaires', 'questionnaires field', 'field advertising', 'advertising .'] 

 TOTAL BIGRAMS --> 31 



 ---- TRI-GRAMS ---- 

 ['application Blank Slate', 'Blank Slate Language', 'Slate Language Processor', 'Language Processor (', 'Processor ( BSLP', '( BSLP )', 'BSLP ) (', ') ( Bondale', '( Bondale et', 'Bondale et al.', 'et al. ,', 'al. , 1999', ', 1999 )', '1999 ) [', ') [ 59', '[ 59 ]', '59 ] approach', '] approach analysis', 'approach analysis real', 'analysis real life', 'real life natural', 'life natural language', 'natural language corpus', 'language corpus consists', 'corpus consists responses', 'consists responses open-ended', 'responses open-ended questionnaires', 'open-ended questionnaires field', 'questionnaires field advertising', 'field advertising .'] 

 TOTAL TRIGRAMS --> 30 



 ---- NOUN PHRASES ---- 

 ['application', 'approach', 'analysis', 'real life', 'natural language', 'corpus', 'field', 'advertising'] 

 TOTAL NOUN PHRASES --> 8 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> ['Blank Slate Language Processor']
 TOTAL PERSON ENTITY --> 1 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['applic', 'blank', 'slate', 'languag', 'processor', '(', 'bslp', ')', '(', 'bondal', 'et', 'al.', ',', '1999', ')', '[', '59', ']', 'approach', 'analysi', 'real', 'life', 'natur', 'languag', 'corpu', 'consist', 'respons', 'open-end', 'questionnair', 'field', 'advertis', '.']

 TOTAL PORTER STEM WORDS ==> 32



 ---- SNOWBALL STEMMING ----

['applic', 'blank', 'slate', 'languag', 'processor', '(', 'bslp', ')', '(', 'bondal', 'et', 'al.', ',', '1999', ')', '[', '59', ']', 'approach', 'analysi', 'real', 'life', 'natur', 'languag', 'corpus', 'consist', 'respons', 'open-end', 'questionnair', 'field', 'advertis', '.']

 TOTAL SNOWBALL STEM WORDS ==> 32



 ---- LEMMATIZATION ----

['application', 'Blank', 'Slate', 'Language', 'Processor', '(', 'BSLP', ')', '(', 'Bondale', 'et', 'al.', ',', '1999', ')', '[', '59', ']', 'approach', 'analysis', 'real', 'life', 'natural', 'language', 'corpus', 'consists', 'response', 'open-ended', 'questionnaire', 'field', 'advertising', '.']

 TOTAL LEMMATIZE WORDS ==> 32

************************************************************************************************************************

237 --> There’s a system called MITA (Metlife’s Intelligent Text Analyzer) (Glasgow et al. 


 ---- TOKENS ----

 ['There', '’', 's', 'a', 'system', 'called', 'MITA', '(', 'Metlife', '’', 's', 'Intelligent', 'Text', 'Analyzer', ')', '(', 'Glasgow', 'et', 'al', '.'] 

 TOTAL TOKENS ==> 20

 ---- POST ----

 [('There', 'EX'), ('’', 'VBZ'), ('s', 'VB'), ('a', 'DT'), ('system', 'NN'), ('called', 'VBN'), ('MITA', 'NNP'), ('(', '('), ('Metlife', 'NNP'), ('’', 'NNP'), ('s', 'VBD'), ('Intelligent', 'NNP'), ('Text', 'NNP'), ('Analyzer', 'NNP'), (')', ')'), ('(', '('), ('Glasgow', 'NNP'), ('et', 'RB'), ('al', 'RB'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['’', 'system', 'called', 'MITA', '(', 'Metlife', '’', 'Intelligent', 'Text', 'Analyzer', ')', '(', 'Glasgow', 'et', 'al', '.']

 TOTAL FILTERED TOKENS ==>  16

 ---- POST FOR FILTERED TOKENS ----

 [('’', 'NN'), ('system', 'NN'), ('called', 'VBN'), ('MITA', 'NNP'), ('(', '('), ('Metlife', 'NNP'), ('’', 'NNP'), ('Intelligent', 'NNP'), ('Text', 'NNP'), ('Analyzer', 'NNP'), (')', ')'), ('(', '('), ('Glasgow', 'NNP'), ('et', 'RB'), ('al', 'RB'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['’ system', 'system called', 'called MITA', 'MITA (', '( Metlife', 'Metlife ’', '’ Intelligent', 'Intelligent Text', 'Text Analyzer', 'Analyzer )', ') (', '( Glasgow', 'Glasgow et', 'et al', 'al .'] 

 TOTAL BIGRAMS --> 15 



 ---- TRI-GRAMS ---- 

 ['’ system called', 'system called MITA', 'called MITA (', 'MITA ( Metlife', '( Metlife ’', 'Metlife ’ Intelligent', '’ Intelligent Text', 'Intelligent Text Analyzer', 'Text Analyzer )', 'Analyzer ) (', ') ( Glasgow', '( Glasgow et', 'Glasgow et al', 'et al .'] 

 TOTAL TRIGRAMS --> 14 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 []

 ---- PORTER STEMMING ----

['’', 'system', 'call', 'mita', '(', 'metlif', '’', 'intellig', 'text', 'analyz', ')', '(', 'glasgow', 'et', 'al', '.']

 TOTAL PORTER STEM WORDS ==> 16



 ---- SNOWBALL STEMMING ----

['’', 'system', 'call', 'mita', '(', 'metlif', '’', 'intellig', 'text', 'analyz', ')', '(', 'glasgow', 'et', 'al', '.']

 TOTAL SNOWBALL STEM WORDS ==> 16



 ---- LEMMATIZATION ----

['’', 'system', 'called', 'MITA', '(', 'Metlife', '’', 'Intelligent', 'Text', 'Analyzer', ')', '(', 'Glasgow', 'et', 'al', '.']

 TOTAL LEMMATIZE WORDS ==> 16

************************************************************************************************************************

238 --> (1998)  [60]) that extracts information from life insurance applications. 


 ---- TOKENS ----

 ['(', '1998', ')', '[', '60', ']', ')', 'that', 'extracts', 'information', 'from', 'life', 'insurance', 'applications', '.'] 

 TOTAL TOKENS ==> 15

 ---- POST ----

 [('(', '('), ('1998', 'CD'), (')', ')'), ('[', 'VBD'), ('60', 'CD'), (']', 'NN'), (')', ')'), ('that', 'WDT'), ('extracts', 'VBZ'), ('information', 'NN'), ('from', 'IN'), ('life', 'NN'), ('insurance', 'NN'), ('applications', 'NNS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['(', '1998', ')', '[', '60', ']', ')', 'extracts', 'information', 'life', 'insurance', 'applications', '.']

 TOTAL FILTERED TOKENS ==>  13

 ---- POST FOR FILTERED TOKENS ----

 [('(', '('), ('1998', 'CD'), (')', ')'), ('[', 'VBD'), ('60', 'CD'), (']', 'NN'), (')', ')'), ('extracts', 'VBZ'), ('information', 'NN'), ('life', 'NN'), ('insurance', 'NN'), ('applications', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['( 1998', '1998 )', ') [', '[ 60', '60 ]', '] )', ') extracts', 'extracts information', 'information life', 'life insurance', 'insurance applications', 'applications .'] 

 TOTAL BIGRAMS --> 12 



 ---- TRI-GRAMS ---- 

 ['( 1998 )', '1998 ) [', ') [ 60', '[ 60 ]', '60 ] )', '] ) extracts', ') extracts information', 'extracts information life', 'information life insurance', 'life insurance applications', 'insurance applications .'] 

 TOTAL TRIGRAMS --> 11 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 []

 ---- PORTER STEMMING ----

['(', '1998', ')', '[', '60', ']', ')', 'extract', 'inform', 'life', 'insur', 'applic', '.']

 TOTAL PORTER STEM WORDS ==> 13



 ---- SNOWBALL STEMMING ----

['(', '1998', ')', '[', '60', ']', ')', 'extract', 'inform', 'life', 'insur', 'applic', '.']

 TOTAL SNOWBALL STEM WORDS ==> 13



 ---- LEMMATIZATION ----

['(', '1998', ')', '[', '60', ']', ')', 'extract', 'information', 'life', 'insurance', 'application', '.']

 TOTAL LEMMATIZE WORDS ==> 13

************************************************************************************************************************

239 --> Ahonen et al. 


 ---- TOKENS ----

 ['Ahonen', 'et', 'al', '.'] 

 TOTAL TOKENS ==> 4

 ---- POST ----

 [('Ahonen', 'NNP'), ('et', 'CC'), ('al', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Ahonen', 'et', 'al', '.']

 TOTAL FILTERED TOKENS ==>  4

 ---- POST FOR FILTERED TOKENS ----

 [('Ahonen', 'NNP'), ('et', 'CC'), ('al', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Ahonen et', 'et al', 'al .'] 

 TOTAL BIGRAMS --> 3 



 ---- TRI-GRAMS ---- 

 ['Ahonen et al', 'et al .'] 

 TOTAL TRIGRAMS --> 2 



 ---- NOUN PHRASES ---- 

 ['al'] 

 TOTAL NOUN PHRASES --> 1 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> ['Ahonen']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['ahonen', 'et', 'al', '.']

 TOTAL PORTER STEM WORDS ==> 4



 ---- SNOWBALL STEMMING ----

['ahonen', 'et', 'al', '.']

 TOTAL SNOWBALL STEM WORDS ==> 4



 ---- LEMMATIZATION ----

['Ahonen', 'et', 'al', '.']

 TOTAL LEMMATIZE WORDS ==> 4

************************************************************************************************************************

240 --> (1998) [61]  suggested a mainstream framework for text mining that uses pragmatic and discourse level  analyses of text. 


 ---- TOKENS ----

 ['(', '1998', ')', '[', '61', ']', 'suggested', 'a', 'mainstream', 'framework', 'for', 'text', 'mining', 'that', 'uses', 'pragmatic', 'and', 'discourse', 'level', 'analyses', 'of', 'text', '.'] 

 TOTAL TOKENS ==> 23

 ---- POST ----

 [('(', '('), ('1998', 'CD'), (')', ')'), ('[', 'VBD'), ('61', 'CD'), (']', 'NNP'), ('suggested', 'VBD'), ('a', 'DT'), ('mainstream', 'NN'), ('framework', 'NN'), ('for', 'IN'), ('text', 'NN'), ('mining', 'NN'), ('that', 'WDT'), ('uses', 'VBZ'), ('pragmatic', 'JJ'), ('and', 'CC'), ('discourse', 'JJ'), ('level', 'NN'), ('analyses', 'NNS'), ('of', 'IN'), ('text', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['(', '1998', ')', '[', '61', ']', 'suggested', 'mainstream', 'framework', 'text', 'mining', 'uses', 'pragmatic', 'discourse', 'level', 'analyses', 'text', '.']

 TOTAL FILTERED TOKENS ==>  18

 ---- POST FOR FILTERED TOKENS ----

 [('(', '('), ('1998', 'CD'), (')', ')'), ('[', 'VBD'), ('61', 'CD'), (']', 'NN'), ('suggested', 'VBD'), ('mainstream', 'JJ'), ('framework', 'NN'), ('text', 'NN'), ('mining', 'NN'), ('uses', 'VBZ'), ('pragmatic', 'JJ'), ('discourse', 'NN'), ('level', 'NN'), ('analyses', 'VBZ'), ('text', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['( 1998', '1998 )', ') [', '[ 61', '61 ]', '] suggested', 'suggested mainstream', 'mainstream framework', 'framework text', 'text mining', 'mining uses', 'uses pragmatic', 'pragmatic discourse', 'discourse level', 'level analyses', 'analyses text', 'text .'] 

 TOTAL BIGRAMS --> 17 



 ---- TRI-GRAMS ---- 

 ['( 1998 )', '1998 ) [', ') [ 61', '[ 61 ]', '61 ] suggested', '] suggested mainstream', 'suggested mainstream framework', 'mainstream framework text', 'framework text mining', 'text mining uses', 'mining uses pragmatic', 'uses pragmatic discourse', 'pragmatic discourse level', 'discourse level analyses', 'level analyses text', 'analyses text .'] 

 TOTAL TRIGRAMS --> 16 



 ---- NOUN PHRASES ---- 

 [']', 'mainstream framework', 'text', 'mining', 'pragmatic discourse', 'level', 'text'] 

 TOTAL NOUN PHRASES --> 7 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['(', '1998', ')', '[', '61', ']', 'suggest', 'mainstream', 'framework', 'text', 'mine', 'use', 'pragmat', 'discours', 'level', 'analys', 'text', '.']

 TOTAL PORTER STEM WORDS ==> 18



 ---- SNOWBALL STEMMING ----

['(', '1998', ')', '[', '61', ']', 'suggest', 'mainstream', 'framework', 'text', 'mine', 'use', 'pragmat', 'discours', 'level', 'analys', 'text', '.']

 TOTAL SNOWBALL STEM WORDS ==> 18



 ---- LEMMATIZATION ----

['(', '1998', ')', '[', '61', ']', 'suggested', 'mainstream', 'framework', 'text', 'mining', 'us', 'pragmatic', 'discourse', 'level', 'analysis', 'text', '.']

 TOTAL LEMMATIZE WORDS ==> 18

************************************************************************************************************************

241 --> 6.5 Summarization  Overload of information is the real thing in this digital age, and already our reach and access  to knowledge and information exceeds our capacity to understand it. 


 ---- TOKENS ----

 ['6.5', 'Summarization', 'Overload', 'of', 'information', 'is', 'the', 'real', 'thing', 'in', 'this', 'digital', 'age', ',', 'and', 'already', 'our', 'reach', 'and', 'access', 'to', 'knowledge', 'and', 'information', 'exceeds', 'our', 'capacity', 'to', 'understand', 'it', '.'] 

 TOTAL TOKENS ==> 31

 ---- POST ----

 [('6.5', 'CD'), ('Summarization', 'NNP'), ('Overload', 'NNP'), ('of', 'IN'), ('information', 'NN'), ('is', 'VBZ'), ('the', 'DT'), ('real', 'JJ'), ('thing', 'NN'), ('in', 'IN'), ('this', 'DT'), ('digital', 'JJ'), ('age', 'NN'), (',', ','), ('and', 'CC'), ('already', 'RB'), ('our', 'PRP$'), ('reach', 'NN'), ('and', 'CC'), ('access', 'NN'), ('to', 'TO'), ('knowledge', 'VB'), ('and', 'CC'), ('information', 'NN'), ('exceeds', 'VBZ'), ('our', 'PRP$'), ('capacity', 'NN'), ('to', 'TO'), ('understand', 'VB'), ('it', 'PRP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['6.5', 'Summarization', 'Overload', 'information', 'real', 'thing', 'digital', 'age', ',', 'already', 'reach', 'access', 'knowledge', 'information', 'exceeds', 'capacity', 'understand', '.']

 TOTAL FILTERED TOKENS ==>  18

 ---- POST FOR FILTERED TOKENS ----

 [('6.5', 'CD'), ('Summarization', 'NNP'), ('Overload', 'NNP'), ('information', 'NN'), ('real', 'JJ'), ('thing', 'NN'), ('digital', 'JJ'), ('age', 'NN'), (',', ','), ('already', 'RB'), ('reach', 'VB'), ('access', 'NN'), ('knowledge', 'NN'), ('information', 'NN'), ('exceeds', 'VBZ'), ('capacity', 'NN'), ('understand', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['6.5 Summarization', 'Summarization Overload', 'Overload information', 'information real', 'real thing', 'thing digital', 'digital age', 'age ,', ', already', 'already reach', 'reach access', 'access knowledge', 'knowledge information', 'information exceeds', 'exceeds capacity', 'capacity understand', 'understand .'] 

 TOTAL BIGRAMS --> 17 



 ---- TRI-GRAMS ---- 

 ['6.5 Summarization Overload', 'Summarization Overload information', 'Overload information real', 'information real thing', 'real thing digital', 'thing digital age', 'digital age ,', 'age , already', ', already reach', 'already reach access', 'reach access knowledge', 'access knowledge information', 'knowledge information exceeds', 'information exceeds capacity', 'exceeds capacity understand', 'capacity understand .'] 

 TOTAL TRIGRAMS --> 16 



 ---- NOUN PHRASES ---- 

 ['information', 'real thing', 'digital age', 'access', 'knowledge', 'information', 'capacity', 'understand'] 

 TOTAL NOUN PHRASES --> 8 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['6.5', 'summar', 'overload', 'inform', 'real', 'thing', 'digit', 'age', ',', 'alreadi', 'reach', 'access', 'knowledg', 'inform', 'exce', 'capac', 'understand', '.']

 TOTAL PORTER STEM WORDS ==> 18



 ---- SNOWBALL STEMMING ----

['6.5', 'summar', 'overload', 'inform', 'real', 'thing', 'digit', 'age', ',', 'alreadi', 'reach', 'access', 'knowledg', 'inform', 'exceed', 'capac', 'understand', '.']

 TOTAL SNOWBALL STEM WORDS ==> 18



 ---- LEMMATIZATION ----

['6.5', 'Summarization', 'Overload', 'information', 'real', 'thing', 'digital', 'age', ',', 'already', 'reach', 'access', 'knowledge', 'information', 'exceeds', 'capacity', 'understand', '.']

 TOTAL LEMMATIZE WORDS ==> 18

************************************************************************************************************************

242 --> This trend is not slowing  down, so an ability to summarize the data while keeping the meaning intact is highly   required. 


 ---- TOKENS ----

 ['This', 'trend', 'is', 'not', 'slowing', 'down', ',', 'so', 'an', 'ability', 'to', 'summarize', 'the', 'data', 'while', 'keeping', 'the', 'meaning', 'intact', 'is', 'highly', 'required', '.'] 

 TOTAL TOKENS ==> 23

 ---- POST ----

 [('This', 'DT'), ('trend', 'NN'), ('is', 'VBZ'), ('not', 'RB'), ('slowing', 'VBG'), ('down', 'RP'), (',', ','), ('so', 'IN'), ('an', 'DT'), ('ability', 'NN'), ('to', 'TO'), ('summarize', 'VB'), ('the', 'DT'), ('data', 'NNS'), ('while', 'IN'), ('keeping', 'VBG'), ('the', 'DT'), ('meaning', 'NN'), ('intact', 'JJ'), ('is', 'VBZ'), ('highly', 'RB'), ('required', 'VBN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['trend', 'slowing', ',', 'ability', 'summarize', 'data', 'keeping', 'meaning', 'intact', 'highly', 'required', '.']

 TOTAL FILTERED TOKENS ==>  12

 ---- POST FOR FILTERED TOKENS ----

 [('trend', 'NN'), ('slowing', 'NN'), (',', ','), ('ability', 'NN'), ('summarize', 'VB'), ('data', 'NNS'), ('keeping', 'VBG'), ('meaning', 'VBG'), ('intact', 'JJ'), ('highly', 'RB'), ('required', 'VBN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['trend slowing', 'slowing ,', ', ability', 'ability summarize', 'summarize data', 'data keeping', 'keeping meaning', 'meaning intact', 'intact highly', 'highly required', 'required .'] 

 TOTAL BIGRAMS --> 11 



 ---- TRI-GRAMS ---- 

 ['trend slowing ,', 'slowing , ability', ', ability summarize', 'ability summarize data', 'summarize data keeping', 'data keeping meaning', 'keeping meaning intact', 'meaning intact highly', 'intact highly required', 'highly required .'] 

 TOTAL TRIGRAMS --> 10 



 ---- NOUN PHRASES ---- 

 ['trend', 'slowing', 'ability'] 

 TOTAL NOUN PHRASES --> 3 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['trend', 'slow', ',', 'abil', 'summar', 'data', 'keep', 'mean', 'intact', 'highli', 'requir', '.']

 TOTAL PORTER STEM WORDS ==> 12



 ---- SNOWBALL STEMMING ----

['trend', 'slow', ',', 'abil', 'summar', 'data', 'keep', 'mean', 'intact', 'high', 'requir', '.']

 TOTAL SNOWBALL STEM WORDS ==> 12



 ---- LEMMATIZATION ----

['trend', 'slowing', ',', 'ability', 'summarize', 'data', 'keeping', 'meaning', 'intact', 'highly', 'required', '.']

 TOTAL LEMMATIZE WORDS ==> 12

************************************************************************************************************************

243 --> This is important not just allowing us the ability to recognize the understand the  important information for a large set of data, it is used to understand the deeper emotional  meanings; For example, a company determine the general sentiment on social media and use  it on their latest product offering. 


 ---- TOKENS ----

 ['This', 'is', 'important', 'not', 'just', 'allowing', 'us', 'the', 'ability', 'to', 'recognize', 'the', 'understand', 'the', 'important', 'information', 'for', 'a', 'large', 'set', 'of', 'data', ',', 'it', 'is', 'used', 'to', 'understand', 'the', 'deeper', 'emotional', 'meanings', ';', 'For', 'example', ',', 'a', 'company', 'determine', 'the', 'general', 'sentiment', 'on', 'social', 'media', 'and', 'use', 'it', 'on', 'their', 'latest', 'product', 'offering', '.'] 

 TOTAL TOKENS ==> 54

 ---- POST ----

 [('This', 'DT'), ('is', 'VBZ'), ('important', 'JJ'), ('not', 'RB'), ('just', 'RB'), ('allowing', 'VBG'), ('us', 'PRP'), ('the', 'DT'), ('ability', 'NN'), ('to', 'TO'), ('recognize', 'VB'), ('the', 'DT'), ('understand', 'NN'), ('the', 'DT'), ('important', 'JJ'), ('information', 'NN'), ('for', 'IN'), ('a', 'DT'), ('large', 'JJ'), ('set', 'NN'), ('of', 'IN'), ('data', 'NNS'), (',', ','), ('it', 'PRP'), ('is', 'VBZ'), ('used', 'VBN'), ('to', 'TO'), ('understand', 'VB'), ('the', 'DT'), ('deeper', 'JJR'), ('emotional', 'JJ'), ('meanings', 'NNS'), (';', ':'), ('For', 'IN'), ('example', 'NN'), (',', ','), ('a', 'DT'), ('company', 'NN'), ('determine', 'VBZ'), ('the', 'DT'), ('general', 'JJ'), ('sentiment', 'NN'), ('on', 'IN'), ('social', 'JJ'), ('media', 'NNS'), ('and', 'CC'), ('use', 'VB'), ('it', 'PRP'), ('on', 'IN'), ('their', 'PRP$'), ('latest', 'JJS'), ('product', 'NN'), ('offering', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['important', 'allowing', 'us', 'ability', 'recognize', 'understand', 'important', 'information', 'large', 'set', 'data', ',', 'used', 'understand', 'deeper', 'emotional', 'meanings', ';', 'example', ',', 'company', 'determine', 'general', 'sentiment', 'social', 'media', 'use', 'latest', 'product', 'offering', '.']

 TOTAL FILTERED TOKENS ==>  31

 ---- POST FOR FILTERED TOKENS ----

 [('important', 'JJ'), ('allowing', 'VBG'), ('us', 'PRP'), ('ability', 'NN'), ('recognize', 'VBP'), ('understand', 'NN'), ('important', 'JJ'), ('information', 'NN'), ('large', 'JJ'), ('set', 'NN'), ('data', 'NNS'), (',', ','), ('used', 'VBD'), ('understand', 'JJ'), ('deeper', 'JJR'), ('emotional', 'JJ'), ('meanings', 'NNS'), (';', ':'), ('example', 'NN'), (',', ','), ('company', 'NN'), ('determine', 'VB'), ('general', 'JJ'), ('sentiment', 'NN'), ('social', 'JJ'), ('media', 'NNS'), ('use', 'VBP'), ('latest', 'JJS'), ('product', 'NN'), ('offering', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['important allowing', 'allowing us', 'us ability', 'ability recognize', 'recognize understand', 'understand important', 'important information', 'information large', 'large set', 'set data', 'data ,', ', used', 'used understand', 'understand deeper', 'deeper emotional', 'emotional meanings', 'meanings ;', '; example', 'example ,', ', company', 'company determine', 'determine general', 'general sentiment', 'sentiment social', 'social media', 'media use', 'use latest', 'latest product', 'product offering', 'offering .'] 

 TOTAL BIGRAMS --> 30 



 ---- TRI-GRAMS ---- 

 ['important allowing us', 'allowing us ability', 'us ability recognize', 'ability recognize understand', 'recognize understand important', 'understand important information', 'important information large', 'information large set', 'large set data', 'set data ,', 'data , used', ', used understand', 'used understand deeper', 'understand deeper emotional', 'deeper emotional meanings', 'emotional meanings ;', 'meanings ; example', '; example ,', 'example , company', ', company determine', 'company determine general', 'determine general sentiment', 'general sentiment social', 'sentiment social media', 'social media use', 'media use latest', 'use latest product', 'latest product offering', 'product offering .'] 

 TOTAL TRIGRAMS --> 29 



 ---- NOUN PHRASES ---- 

 ['ability', 'understand', 'important information', 'large set', 'example', 'company', 'general sentiment', 'product', 'offering'] 

 TOTAL NOUN PHRASES --> 9 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['import', 'allow', 'us', 'abil', 'recogn', 'understand', 'import', 'inform', 'larg', 'set', 'data', ',', 'use', 'understand', 'deeper', 'emot', 'mean', ';', 'exampl', ',', 'compani', 'determin', 'gener', 'sentiment', 'social', 'media', 'use', 'latest', 'product', 'offer', '.']

 TOTAL PORTER STEM WORDS ==> 31



 ---- SNOWBALL STEMMING ----

['import', 'allow', 'us', 'abil', 'recogn', 'understand', 'import', 'inform', 'larg', 'set', 'data', ',', 'use', 'understand', 'deeper', 'emot', 'mean', ';', 'exampl', ',', 'compani', 'determin', 'general', 'sentiment', 'social', 'media', 'use', 'latest', 'product', 'offer', '.']

 TOTAL SNOWBALL STEM WORDS ==> 31



 ---- LEMMATIZATION ----

['important', 'allowing', 'u', 'ability', 'recognize', 'understand', 'important', 'information', 'large', 'set', 'data', ',', 'used', 'understand', 'deeper', 'emotional', 'meaning', ';', 'example', ',', 'company', 'determine', 'general', 'sentiment', 'social', 'medium', 'use', 'latest', 'product', 'offering', '.']

 TOTAL LEMMATIZE WORDS ==> 31

************************************************************************************************************************

244 --> This application is useful as a valuable marketing asset. 


 ---- TOKENS ----

 ['This', 'application', 'is', 'useful', 'as', 'a', 'valuable', 'marketing', 'asset', '.'] 

 TOTAL TOKENS ==> 10

 ---- POST ----

 [('This', 'DT'), ('application', 'NN'), ('is', 'VBZ'), ('useful', 'JJ'), ('as', 'IN'), ('a', 'DT'), ('valuable', 'JJ'), ('marketing', 'NN'), ('asset', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['application', 'useful', 'valuable', 'marketing', 'asset', '.']

 TOTAL FILTERED TOKENS ==>  6

 ---- POST FOR FILTERED TOKENS ----

 [('application', 'NN'), ('useful', 'JJ'), ('valuable', 'JJ'), ('marketing', 'NN'), ('asset', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['application useful', 'useful valuable', 'valuable marketing', 'marketing asset', 'asset .'] 

 TOTAL BIGRAMS --> 5 



 ---- TRI-GRAMS ---- 

 ['application useful valuable', 'useful valuable marketing', 'valuable marketing asset', 'marketing asset .'] 

 TOTAL TRIGRAMS --> 4 



 ---- NOUN PHRASES ---- 

 ['application', 'useful valuable marketing', 'asset'] 

 TOTAL NOUN PHRASES --> 3 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['applic', 'use', 'valuabl', 'market', 'asset', '.']

 TOTAL PORTER STEM WORDS ==> 6



 ---- SNOWBALL STEMMING ----

['applic', 'use', 'valuabl', 'market', 'asset', '.']

 TOTAL SNOWBALL STEM WORDS ==> 6



 ---- LEMMATIZATION ----

['application', 'useful', 'valuable', 'marketing', 'asset', '.']

 TOTAL LEMMATIZE WORDS ==> 6

************************************************************************************************************************

245 --> The types of text summarization depends on the basis of the number of documents and  the  two important categories are single document summarization and multi document  summarization (Zajic et al. 


 ---- TOKENS ----

 ['The', 'types', 'of', 'text', 'summarization', 'depends', 'on', 'the', 'basis', 'of', 'the', 'number', 'of', 'documents', 'and', 'the', 'two', 'important', 'categories', 'are', 'single', 'document', 'summarization', 'and', 'multi', 'document', 'summarization', '(', 'Zajic', 'et', 'al', '.'] 

 TOTAL TOKENS ==> 32

 ---- POST ----

 [('The', 'DT'), ('types', 'NNS'), ('of', 'IN'), ('text', 'NN'), ('summarization', 'NN'), ('depends', 'VBZ'), ('on', 'IN'), ('the', 'DT'), ('basis', 'NN'), ('of', 'IN'), ('the', 'DT'), ('number', 'NN'), ('of', 'IN'), ('documents', 'NNS'), ('and', 'CC'), ('the', 'DT'), ('two', 'CD'), ('important', 'JJ'), ('categories', 'NNS'), ('are', 'VBP'), ('single', 'JJ'), ('document', 'NN'), ('summarization', 'NN'), ('and', 'CC'), ('multi', 'NN'), ('document', 'NN'), ('summarization', 'NN'), ('(', '('), ('Zajic', 'NNP'), ('et', 'RB'), ('al', 'RB'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['types', 'text', 'summarization', 'depends', 'basis', 'number', 'documents', 'two', 'important', 'categories', 'single', 'document', 'summarization', 'multi', 'document', 'summarization', '(', 'Zajic', 'et', 'al', '.']

 TOTAL FILTERED TOKENS ==>  21

 ---- POST FOR FILTERED TOKENS ----

 [('types', 'NNS'), ('text', 'JJ'), ('summarization', 'NN'), ('depends', 'VBZ'), ('basis', 'NN'), ('number', 'NN'), ('documents', 'NNS'), ('two', 'CD'), ('important', 'JJ'), ('categories', 'NNS'), ('single', 'JJ'), ('document', 'NN'), ('summarization', 'NN'), ('multi', 'FW'), ('document', 'NN'), ('summarization', 'NN'), ('(', '('), ('Zajic', 'NNP'), ('et', 'RB'), ('al', 'RB'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['types text', 'text summarization', 'summarization depends', 'depends basis', 'basis number', 'number documents', 'documents two', 'two important', 'important categories', 'categories single', 'single document', 'document summarization', 'summarization multi', 'multi document', 'document summarization', 'summarization (', '( Zajic', 'Zajic et', 'et al', 'al .'] 

 TOTAL BIGRAMS --> 20 



 ---- TRI-GRAMS ---- 

 ['types text summarization', 'text summarization depends', 'summarization depends basis', 'depends basis number', 'basis number documents', 'number documents two', 'documents two important', 'two important categories', 'important categories single', 'categories single document', 'single document summarization', 'document summarization multi', 'summarization multi document', 'multi document summarization', 'document summarization (', 'summarization ( Zajic', '( Zajic et', 'Zajic et al', 'et al .'] 

 TOTAL TRIGRAMS --> 19 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 []

 ---- PORTER STEMMING ----

['type', 'text', 'summar', 'depend', 'basi', 'number', 'document', 'two', 'import', 'categori', 'singl', 'document', 'summar', 'multi', 'document', 'summar', '(', 'zajic', 'et', 'al', '.']

 TOTAL PORTER STEM WORDS ==> 21



 ---- SNOWBALL STEMMING ----

['type', 'text', 'summar', 'depend', 'basi', 'number', 'document', 'two', 'import', 'categori', 'singl', 'document', 'summar', 'multi', 'document', 'summar', '(', 'zajic', 'et', 'al', '.']

 TOTAL SNOWBALL STEM WORDS ==> 21



 ---- LEMMATIZATION ----

['type', 'text', 'summarization', 'depends', 'basis', 'number', 'document', 'two', 'important', 'category', 'single', 'document', 'summarization', 'multi', 'document', 'summarization', '(', 'Zajic', 'et', 'al', '.']

 TOTAL LEMMATIZE WORDS ==> 21

************************************************************************************************************************

246 --> 2008 [62]; Fattah and Ren 2009 [63]). 


 ---- TOKENS ----

 ['2008', '[', '62', ']', ';', 'Fattah', 'and', 'Ren', '2009', '[', '63', ']', ')', '.'] 

 TOTAL TOKENS ==> 14

 ---- POST ----

 [('2008', 'CD'), ('[', '$'), ('62', 'CD'), (']', 'NNP'), (';', ':'), ('Fattah', 'NNP'), ('and', 'CC'), ('Ren', 'NNP'), ('2009', 'CD'), ('[', 'NNP'), ('63', 'CD'), (']', 'NN'), (')', ')'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['2008', '[', '62', ']', ';', 'Fattah', 'Ren', '2009', '[', '63', ']', ')', '.']

 TOTAL FILTERED TOKENS ==>  13

 ---- POST FOR FILTERED TOKENS ----

 [('2008', 'CD'), ('[', '$'), ('62', 'CD'), (']', 'NNP'), (';', ':'), ('Fattah', 'NNP'), ('Ren', 'NNP'), ('2009', 'CD'), ('[', 'NNP'), ('63', 'CD'), (']', 'NN'), (')', ')'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['2008 [', '[ 62', '62 ]', '] ;', '; Fattah', 'Fattah Ren', 'Ren 2009', '2009 [', '[ 63', '63 ]', '] )', ') .'] 

 TOTAL BIGRAMS --> 12 



 ---- TRI-GRAMS ---- 

 ['2008 [ 62', '[ 62 ]', '62 ] ;', '] ; Fattah', '; Fattah Ren', 'Fattah Ren 2009', 'Ren 2009 [', '2009 [ 63', '[ 63 ]', '63 ] )', '] ) .'] 

 TOTAL TRIGRAMS --> 11 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 []

 ---- PORTER STEMMING ----

['2008', '[', '62', ']', ';', 'fattah', 'ren', '2009', '[', '63', ']', ')', '.']

 TOTAL PORTER STEM WORDS ==> 13



 ---- SNOWBALL STEMMING ----

['2008', '[', '62', ']', ';', 'fattah', 'ren', '2009', '[', '63', ']', ')', '.']

 TOTAL SNOWBALL STEM WORDS ==> 13



 ---- LEMMATIZATION ----

['2008', '[', '62', ']', ';', 'Fattah', 'Ren', '2009', '[', '63', ']', ')', '.']

 TOTAL LEMMATIZE WORDS ==> 13

************************************************************************************************************************

247 --> Summaries can also be of  two types: generic or query-focused (Gong and Liu 2001 [64]; Dunlavy et al. 


 ---- TOKENS ----

 ['Summaries', 'can', 'also', 'be', 'of', 'two', 'types', ':', 'generic', 'or', 'query-focused', '(', 'Gong', 'and', 'Liu', '2001', '[', '64', ']', ';', 'Dunlavy', 'et', 'al', '.'] 

 TOTAL TOKENS ==> 24

 ---- POST ----

 [('Summaries', 'NNS'), ('can', 'MD'), ('also', 'RB'), ('be', 'VB'), ('of', 'IN'), ('two', 'CD'), ('types', 'NNS'), (':', ':'), ('generic', 'NN'), ('or', 'CC'), ('query-focused', 'JJ'), ('(', '('), ('Gong', 'NNP'), ('and', 'CC'), ('Liu', 'NNP'), ('2001', 'CD'), ('[', 'NNP'), ('64', 'CD'), (']', 'NNP'), (';', ':'), ('Dunlavy', 'NNP'), ('et', 'FW'), ('al', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Summaries', 'also', 'two', 'types', ':', 'generic', 'query-focused', '(', 'Gong', 'Liu', '2001', '[', '64', ']', ';', 'Dunlavy', 'et', 'al', '.']

 TOTAL FILTERED TOKENS ==>  19

 ---- POST FOR FILTERED TOKENS ----

 [('Summaries', 'NNS'), ('also', 'RB'), ('two', 'CD'), ('types', 'NNS'), (':', ':'), ('generic', 'JJ'), ('query-focused', 'JJ'), ('(', '('), ('Gong', 'NNP'), ('Liu', 'NNP'), ('2001', 'CD'), ('[', 'NNP'), ('64', 'CD'), (']', 'NNP'), (';', ':'), ('Dunlavy', 'NNP'), ('et', 'FW'), ('al', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Summaries also', 'also two', 'two types', 'types :', ': generic', 'generic query-focused', 'query-focused (', '( Gong', 'Gong Liu', 'Liu 2001', '2001 [', '[ 64', '64 ]', '] ;', '; Dunlavy', 'Dunlavy et', 'et al', 'al .'] 

 TOTAL BIGRAMS --> 18 



 ---- TRI-GRAMS ---- 

 ['Summaries also two', 'also two types', 'two types :', 'types : generic', ': generic query-focused', 'generic query-focused (', 'query-focused ( Gong', '( Gong Liu', 'Gong Liu 2001', 'Liu 2001 [', '2001 [ 64', '[ 64 ]', '64 ] ;', '] ; Dunlavy', '; Dunlavy et', 'Dunlavy et al', 'et al .'] 

 TOTAL TRIGRAMS --> 17 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 []

 ---- PORTER STEMMING ----

['summari', 'also', 'two', 'type', ':', 'gener', 'query-focus', '(', 'gong', 'liu', '2001', '[', '64', ']', ';', 'dunlavi', 'et', 'al', '.']

 TOTAL PORTER STEM WORDS ==> 19



 ---- SNOWBALL STEMMING ----

['summari', 'also', 'two', 'type', ':', 'generic', 'query-focus', '(', 'gong', 'liu', '2001', '[', '64', ']', ';', 'dunlavi', 'et', 'al', '.']

 TOTAL SNOWBALL STEM WORDS ==> 19



 ---- LEMMATIZATION ----

['Summaries', 'also', 'two', 'type', ':', 'generic', 'query-focused', '(', 'Gong', 'Liu', '2001', '[', '64', ']', ';', 'Dunlavy', 'et', 'al', '.']

 TOTAL LEMMATIZE WORDS ==> 19

************************************************************************************************************************

248 --> 2007 [65]; Wan  2008 [66]; Ouyang et al. 


 ---- TOKENS ----

 ['2007', '[', '65', ']', ';', 'Wan', '2008', '[', '66', ']', ';', 'Ouyang', 'et', 'al', '.'] 

 TOTAL TOKENS ==> 15

 ---- POST ----

 [('2007', 'CD'), ('[', '$'), ('65', 'CD'), (']', 'NNP'), (';', ':'), ('Wan', 'NNP'), ('2008', 'CD'), ('[', 'NNP'), ('66', 'CD'), (']', 'NNP'), (';', ':'), ('Ouyang', 'NNP'), ('et', 'FW'), ('al', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['2007', '[', '65', ']', ';', 'Wan', '2008', '[', '66', ']', ';', 'Ouyang', 'et', 'al', '.']

 TOTAL FILTERED TOKENS ==>  15

 ---- POST FOR FILTERED TOKENS ----

 [('2007', 'CD'), ('[', '$'), ('65', 'CD'), (']', 'NNP'), (';', ':'), ('Wan', 'NNP'), ('2008', 'CD'), ('[', 'NNP'), ('66', 'CD'), (']', 'NNP'), (';', ':'), ('Ouyang', 'NNP'), ('et', 'FW'), ('al', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['2007 [', '[ 65', '65 ]', '] ;', '; Wan', 'Wan 2008', '2008 [', '[ 66', '66 ]', '] ;', '; Ouyang', 'Ouyang et', 'et al', 'al .'] 

 TOTAL BIGRAMS --> 14 



 ---- TRI-GRAMS ---- 

 ['2007 [ 65', '[ 65 ]', '65 ] ;', '] ; Wan', '; Wan 2008', 'Wan 2008 [', '2008 [ 66', '[ 66 ]', '66 ] ;', '] ; Ouyang', '; Ouyang et', 'Ouyang et al', 'et al .'] 

 TOTAL TRIGRAMS --> 13 



 ---- NOUN PHRASES ---- 

 ['al'] 

 TOTAL NOUN PHRASES --> 1 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> ['Ouyang']
 TOTAL PERSON ENTITY --> 1 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['2007', '[', '65', ']', ';', 'wan', '2008', '[', '66', ']', ';', 'ouyang', 'et', 'al', '.']

 TOTAL PORTER STEM WORDS ==> 15



 ---- SNOWBALL STEMMING ----

['2007', '[', '65', ']', ';', 'wan', '2008', '[', '66', ']', ';', 'ouyang', 'et', 'al', '.']

 TOTAL SNOWBALL STEM WORDS ==> 15



 ---- LEMMATIZATION ----

['2007', '[', '65', ']', ';', 'Wan', '2008', '[', '66', ']', ';', 'Ouyang', 'et', 'al', '.']

 TOTAL LEMMATIZE WORDS ==> 15

************************************************************************************************************************

249 --> 2011 [67]). 


 ---- TOKENS ----

 ['2011', '[', '67', ']', ')', '.'] 

 TOTAL TOKENS ==> 6

 ---- POST ----

 [('2011', 'CD'), ('[', '$'), ('67', 'CD'), (']', 'NN'), (')', ')'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['2011', '[', '67', ']', ')', '.']

 TOTAL FILTERED TOKENS ==>  6

 ---- POST FOR FILTERED TOKENS ----

 [('2011', 'CD'), ('[', '$'), ('67', 'CD'), (']', 'NN'), (')', ')'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['2011 [', '[ 67', '67 ]', '] )', ') .'] 

 TOTAL BIGRAMS --> 5 



 ---- TRI-GRAMS ---- 

 ['2011 [ 67', '[ 67 ]', '67 ] )', '] ) .'] 

 TOTAL TRIGRAMS --> 4 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 []

 ---- PORTER STEMMING ----

['2011', '[', '67', ']', ')', '.']

 TOTAL PORTER STEM WORDS ==> 6



 ---- SNOWBALL STEMMING ----

['2011', '[', '67', ']', ')', '.']

 TOTAL SNOWBALL STEM WORDS ==> 6



 ---- LEMMATIZATION ----

['2011', '[', '67', ']', ')', '.']

 TOTAL LEMMATIZE WORDS ==> 6

************************************************************************************************************************

250 --> Summarization task can be either supervised or  unsupervised (Mani and Maybury 1999 [68]; Fattah and Ren 2009 [63]; Riedhammer et al. 


 ---- TOKENS ----

 ['Summarization', 'task', 'can', 'be', 'either', 'supervised', 'or', 'unsupervised', '(', 'Mani', 'and', 'Maybury', '1999', '[', '68', ']', ';', 'Fattah', 'and', 'Ren', '2009', '[', '63', ']', ';', 'Riedhammer', 'et', 'al', '.'] 

 TOTAL TOKENS ==> 29

 ---- POST ----

 [('Summarization', 'NN'), ('task', 'NN'), ('can', 'MD'), ('be', 'VB'), ('either', 'RB'), ('supervised', 'VBN'), ('or', 'CC'), ('unsupervised', 'VBN'), ('(', '('), ('Mani', 'NNP'), ('and', 'CC'), ('Maybury', 'NNP'), ('1999', 'CD'), ('[', 'NNP'), ('68', 'CD'), (']', 'NNP'), (';', ':'), ('Fattah', 'NNP'), ('and', 'CC'), ('Ren', 'NNP'), ('2009', 'CD'), ('[', 'NNP'), ('63', 'CD'), (']', 'NNP'), (';', ':'), ('Riedhammer', 'NNP'), ('et', 'FW'), ('al', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Summarization', 'task', 'either', 'supervised', 'unsupervised', '(', 'Mani', 'Maybury', '1999', '[', '68', ']', ';', 'Fattah', 'Ren', '2009', '[', '63', ']', ';', 'Riedhammer', 'et', 'al', '.']

 TOTAL FILTERED TOKENS ==>  24

 ---- POST FOR FILTERED TOKENS ----

 [('Summarization', 'NNP'), ('task', 'NN'), ('either', 'CC'), ('supervised', 'VBD'), ('unsupervised', 'JJ'), ('(', '('), ('Mani', 'NNP'), ('Maybury', 'NNP'), ('1999', 'CD'), ('[', 'NNP'), ('68', 'CD'), (']', 'NNP'), (';', ':'), ('Fattah', 'NNP'), ('Ren', 'NNP'), ('2009', 'CD'), ('[', 'NNP'), ('63', 'CD'), (']', 'NNP'), (';', ':'), ('Riedhammer', 'NNP'), ('et', 'FW'), ('al', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Summarization task', 'task either', 'either supervised', 'supervised unsupervised', 'unsupervised (', '( Mani', 'Mani Maybury', 'Maybury 1999', '1999 [', '[ 68', '68 ]', '] ;', '; Fattah', 'Fattah Ren', 'Ren 2009', '2009 [', '[ 63', '63 ]', '] ;', '; Riedhammer', 'Riedhammer et', 'et al', 'al .'] 

 TOTAL BIGRAMS --> 23 



 ---- TRI-GRAMS ---- 

 ['Summarization task either', 'task either supervised', 'either supervised unsupervised', 'supervised unsupervised (', 'unsupervised ( Mani', '( Mani Maybury', 'Mani Maybury 1999', 'Maybury 1999 [', '1999 [ 68', '[ 68 ]', '68 ] ;', '] ; Fattah', '; Fattah Ren', 'Fattah Ren 2009', 'Ren 2009 [', '2009 [ 63', '[ 63 ]', '63 ] ;', '] ; Riedhammer', '; Riedhammer et', 'Riedhammer et al', 'et al .'] 

 TOTAL TRIGRAMS --> 22 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 []

 ---- PORTER STEMMING ----

['summar', 'task', 'either', 'supervis', 'unsupervis', '(', 'mani', 'mayburi', '1999', '[', '68', ']', ';', 'fattah', 'ren', '2009', '[', '63', ']', ';', 'riedhamm', 'et', 'al', '.']

 TOTAL PORTER STEM WORDS ==> 24



 ---- SNOWBALL STEMMING ----

['summar', 'task', 'either', 'supervis', 'unsupervis', '(', 'mani', 'mayburi', '1999', '[', '68', ']', ';', 'fattah', 'ren', '2009', '[', '63', ']', ';', 'riedhamm', 'et', 'al', '.']

 TOTAL SNOWBALL STEM WORDS ==> 24



 ---- LEMMATIZATION ----

['Summarization', 'task', 'either', 'supervised', 'unsupervised', '(', 'Mani', 'Maybury', '1999', '[', '68', ']', ';', 'Fattah', 'Ren', '2009', '[', '63', ']', ';', 'Riedhammer', 'et', 'al', '.']

 TOTAL LEMMATIZE WORDS ==> 24

************************************************************************************************************************

251 --> 2010 [69]). 


 ---- TOKENS ----

 ['2010', '[', '69', ']', ')', '.'] 

 TOTAL TOKENS ==> 6

 ---- POST ----

 [('2010', 'CD'), ('[', '$'), ('69', 'CD'), (']', 'NN'), (')', ')'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['2010', '[', '69', ']', ')', '.']

 TOTAL FILTERED TOKENS ==>  6

 ---- POST FOR FILTERED TOKENS ----

 [('2010', 'CD'), ('[', '$'), ('69', 'CD'), (']', 'NN'), (')', ')'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['2010 [', '[ 69', '69 ]', '] )', ') .'] 

 TOTAL BIGRAMS --> 5 



 ---- TRI-GRAMS ---- 

 ['2010 [ 69', '[ 69 ]', '69 ] )', '] ) .'] 

 TOTAL TRIGRAMS --> 4 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 []

 ---- PORTER STEMMING ----

['2010', '[', '69', ']', ')', '.']

 TOTAL PORTER STEM WORDS ==> 6



 ---- SNOWBALL STEMMING ----

['2010', '[', '69', ']', ')', '.']

 TOTAL SNOWBALL STEM WORDS ==> 6



 ---- LEMMATIZATION ----

['2010', '[', '69', ']', ')', '.']

 TOTAL LEMMATIZE WORDS ==> 6

************************************************************************************************************************

252 --> Training data is required in a supervised system for selecting relevant material  from the documents. 


 ---- TOKENS ----

 ['Training', 'data', 'is', 'required', 'in', 'a', 'supervised', 'system', 'for', 'selecting', 'relevant', 'material', 'from', 'the', 'documents', '.'] 

 TOTAL TOKENS ==> 16

 ---- POST ----

 [('Training', 'VBG'), ('data', 'NNS'), ('is', 'VBZ'), ('required', 'VBN'), ('in', 'IN'), ('a', 'DT'), ('supervised', 'JJ'), ('system', 'NN'), ('for', 'IN'), ('selecting', 'VBG'), ('relevant', 'JJ'), ('material', 'NN'), ('from', 'IN'), ('the', 'DT'), ('documents', 'NNS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Training', 'data', 'required', 'supervised', 'system', 'selecting', 'relevant', 'material', 'documents', '.']

 TOTAL FILTERED TOKENS ==>  10

 ---- POST FOR FILTERED TOKENS ----

 [('Training', 'VBG'), ('data', 'NNS'), ('required', 'VBN'), ('supervised', 'JJ'), ('system', 'NN'), ('selecting', 'VBG'), ('relevant', 'JJ'), ('material', 'NN'), ('documents', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Training data', 'data required', 'required supervised', 'supervised system', 'system selecting', 'selecting relevant', 'relevant material', 'material documents', 'documents .'] 

 TOTAL BIGRAMS --> 9 



 ---- TRI-GRAMS ---- 

 ['Training data required', 'data required supervised', 'required supervised system', 'supervised system selecting', 'system selecting relevant', 'selecting relevant material', 'relevant material documents', 'material documents .'] 

 TOTAL TRIGRAMS --> 8 



 ---- NOUN PHRASES ---- 

 ['supervised system', 'relevant material'] 

 TOTAL NOUN PHRASES --> 2 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['train', 'data', 'requir', 'supervis', 'system', 'select', 'relev', 'materi', 'document', '.']

 TOTAL PORTER STEM WORDS ==> 10



 ---- SNOWBALL STEMMING ----

['train', 'data', 'requir', 'supervis', 'system', 'select', 'relev', 'materi', 'document', '.']

 TOTAL SNOWBALL STEM WORDS ==> 10



 ---- LEMMATIZATION ----

['Training', 'data', 'required', 'supervised', 'system', 'selecting', 'relevant', 'material', 'document', '.']

 TOTAL LEMMATIZE WORDS ==> 10

************************************************************************************************************************

253 --> Large amount of annotated data is needed for learning techniques. 


 ---- TOKENS ----

 ['Large', 'amount', 'of', 'annotated', 'data', 'is', 'needed', 'for', 'learning', 'techniques', '.'] 

 TOTAL TOKENS ==> 11

 ---- POST ----

 [('Large', 'JJ'), ('amount', 'NN'), ('of', 'IN'), ('annotated', 'VBN'), ('data', 'NN'), ('is', 'VBZ'), ('needed', 'VBN'), ('for', 'IN'), ('learning', 'VBG'), ('techniques', 'NNS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Large', 'amount', 'annotated', 'data', 'needed', 'learning', 'techniques', '.']

 TOTAL FILTERED TOKENS ==>  8

 ---- POST FOR FILTERED TOKENS ----

 [('Large', 'JJ'), ('amount', 'NN'), ('annotated', 'VBD'), ('data', 'NNS'), ('needed', 'VBN'), ('learning', 'NN'), ('techniques', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Large amount', 'amount annotated', 'annotated data', 'data needed', 'needed learning', 'learning techniques', 'techniques .'] 

 TOTAL BIGRAMS --> 7 



 ---- TRI-GRAMS ---- 

 ['Large amount annotated', 'amount annotated data', 'annotated data needed', 'data needed learning', 'needed learning techniques', 'learning techniques .'] 

 TOTAL TRIGRAMS --> 6 



 ---- NOUN PHRASES ---- 

 ['Large amount', 'learning'] 

 TOTAL NOUN PHRASES --> 2 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> ['Large']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['larg', 'amount', 'annot', 'data', 'need', 'learn', 'techniqu', '.']

 TOTAL PORTER STEM WORDS ==> 8



 ---- SNOWBALL STEMMING ----

['larg', 'amount', 'annot', 'data', 'need', 'learn', 'techniqu', '.']

 TOTAL SNOWBALL STEM WORDS ==> 8



 ---- LEMMATIZATION ----

['Large', 'amount', 'annotated', 'data', 'needed', 'learning', 'technique', '.']

 TOTAL LEMMATIZE WORDS ==> 8

************************************************************************************************************************

254 --> Few  techniques are as follows–  - Bayesian Sentence based Topic Model (BSTM) uses both term-sentences and term  document associations for summarizing multiple documents. 


 ---- TOKENS ----

 ['Few', 'techniques', 'are', 'as', 'follows–', '-', 'Bayesian', 'Sentence', 'based', 'Topic', 'Model', '(', 'BSTM', ')', 'uses', 'both', 'term-sentences', 'and', 'term', 'document', 'associations', 'for', 'summarizing', 'multiple', 'documents', '.'] 

 TOTAL TOKENS ==> 26

 ---- POST ----

 [('Few', 'JJ'), ('techniques', 'NNS'), ('are', 'VBP'), ('as', 'IN'), ('follows–', 'JJ'), ('-', ':'), ('Bayesian', 'JJ'), ('Sentence', 'NN'), ('based', 'VBN'), ('Topic', 'NNP'), ('Model', 'NNP'), ('(', '('), ('BSTM', 'NNP'), (')', ')'), ('uses', 'VBZ'), ('both', 'DT'), ('term-sentences', 'NNS'), ('and', 'CC'), ('term', 'NN'), ('document', 'NN'), ('associations', 'NNS'), ('for', 'IN'), ('summarizing', 'VBG'), ('multiple', 'JJ'), ('documents', 'NNS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['techniques', 'follows–', '-', 'Bayesian', 'Sentence', 'based', 'Topic', 'Model', '(', 'BSTM', ')', 'uses', 'term-sentences', 'term', 'document', 'associations', 'summarizing', 'multiple', 'documents', '.']

 TOTAL FILTERED TOKENS ==>  20

 ---- POST FOR FILTERED TOKENS ----

 [('techniques', 'NNS'), ('follows–', 'VBP'), ('-', ':'), ('Bayesian', 'JJ'), ('Sentence', 'NN'), ('based', 'VBN'), ('Topic', 'NNP'), ('Model', 'NNP'), ('(', '('), ('BSTM', 'NNP'), (')', ')'), ('uses', 'VBZ'), ('term-sentences', 'JJ'), ('term', 'NN'), ('document', 'NN'), ('associations', 'NNS'), ('summarizing', 'VBG'), ('multiple', 'JJ'), ('documents', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['techniques follows–', 'follows– -', '- Bayesian', 'Bayesian Sentence', 'Sentence based', 'based Topic', 'Topic Model', 'Model (', '( BSTM', 'BSTM )', ') uses', 'uses term-sentences', 'term-sentences term', 'term document', 'document associations', 'associations summarizing', 'summarizing multiple', 'multiple documents', 'documents .'] 

 TOTAL BIGRAMS --> 19 



 ---- TRI-GRAMS ---- 

 ['techniques follows– -', 'follows– - Bayesian', '- Bayesian Sentence', 'Bayesian Sentence based', 'Sentence based Topic', 'based Topic Model', 'Topic Model (', 'Model ( BSTM', '( BSTM )', 'BSTM ) uses', ') uses term-sentences', 'uses term-sentences term', 'term-sentences term document', 'term document associations', 'document associations summarizing', 'associations summarizing multiple', 'summarizing multiple documents', 'multiple documents .'] 

 TOTAL TRIGRAMS --> 18 



 ---- NOUN PHRASES ---- 

 ['Bayesian Sentence', 'term-sentences term', 'document'] 

 TOTAL NOUN PHRASES --> 3 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> ['Topic Model']
 TOTAL PERSON ENTITY --> 1 


 GPE ---> ['Bayesian']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['techniqu', 'follows–', '-', 'bayesian', 'sentenc', 'base', 'topic', 'model', '(', 'bstm', ')', 'use', 'term-sent', 'term', 'document', 'associ', 'summar', 'multipl', 'document', '.']

 TOTAL PORTER STEM WORDS ==> 20



 ---- SNOWBALL STEMMING ----

['techniqu', 'follows–', '-', 'bayesian', 'sentenc', 'base', 'topic', 'model', '(', 'bstm', ')', 'use', 'term-sent', 'term', 'document', 'associ', 'summar', 'multipl', 'document', '.']

 TOTAL SNOWBALL STEM WORDS ==> 20



 ---- LEMMATIZATION ----

['technique', 'follows–', '-', 'Bayesian', 'Sentence', 'based', 'Topic', 'Model', '(', 'BSTM', ')', 'us', 'term-sentences', 'term', 'document', 'association', 'summarizing', 'multiple', 'document', '.']

 TOTAL LEMMATIZE WORDS ==> 20

************************************************************************************************************************

255 --> (Wang et al. 


 ---- TOKENS ----

 ['(', 'Wang', 'et', 'al', '.'] 

 TOTAL TOKENS ==> 5

 ---- POST ----

 [('(', '('), ('Wang', 'NNP'), ('et', 'RB'), ('al', 'RB'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['(', 'Wang', 'et', 'al', '.']

 TOTAL FILTERED TOKENS ==>  5

 ---- POST FOR FILTERED TOKENS ----

 [('(', '('), ('Wang', 'NNP'), ('et', 'RB'), ('al', 'RB'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['( Wang', 'Wang et', 'et al', 'al .'] 

 TOTAL BIGRAMS --> 4 



 ---- TRI-GRAMS ---- 

 ['( Wang et', 'Wang et al', 'et al .'] 

 TOTAL TRIGRAMS --> 3 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 []

 ---- PORTER STEMMING ----

['(', 'wang', 'et', 'al', '.']

 TOTAL PORTER STEM WORDS ==> 5



 ---- SNOWBALL STEMMING ----

['(', 'wang', 'et', 'al', '.']

 TOTAL SNOWBALL STEM WORDS ==> 5



 ---- LEMMATIZATION ----

['(', 'Wang', 'et', 'al', '.']

 TOTAL LEMMATIZE WORDS ==> 5

************************************************************************************************************************

256 --> 2009  [70])    - Factorization with Given Bases (FGB) is a language model where sentence bases  are the given bases and it utilizes document-term and sentence term matrices. 


 ---- TOKENS ----

 ['2009', '[', '70', ']', ')', '-', 'Factorization', 'with', 'Given', 'Bases', '(', 'FGB', ')', 'is', 'a', 'language', 'model', 'where', 'sentence', 'bases', 'are', 'the', 'given', 'bases', 'and', 'it', 'utilizes', 'document-term', 'and', 'sentence', 'term', 'matrices', '.'] 

 TOTAL TOKENS ==> 33

 ---- POST ----

 [('2009', 'CD'), ('[', '$'), ('70', 'CD'), (']', 'NN'), (')', ')'), ('-', ':'), ('Factorization', 'NN'), ('with', 'IN'), ('Given', 'NNP'), ('Bases', 'NNP'), ('(', '('), ('FGB', 'NNP'), (')', ')'), ('is', 'VBZ'), ('a', 'DT'), ('language', 'NN'), ('model', 'NN'), ('where', 'WRB'), ('sentence', 'NN'), ('bases', 'NNS'), ('are', 'VBP'), ('the', 'DT'), ('given', 'VBN'), ('bases', 'NNS'), ('and', 'CC'), ('it', 'PRP'), ('utilizes', 'VBZ'), ('document-term', 'JJ'), ('and', 'CC'), ('sentence', 'JJ'), ('term', 'NN'), ('matrices', 'NNS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['2009', '[', '70', ']', ')', '-', 'Factorization', 'Given', 'Bases', '(', 'FGB', ')', 'language', 'model', 'sentence', 'bases', 'given', 'bases', 'utilizes', 'document-term', 'sentence', 'term', 'matrices', '.']

 TOTAL FILTERED TOKENS ==>  24

 ---- POST FOR FILTERED TOKENS ----

 [('2009', 'CD'), ('[', '$'), ('70', 'CD'), (']', 'NN'), (')', ')'), ('-', ':'), ('Factorization', 'NN'), ('Given', 'VBN'), ('Bases', 'NNP'), ('(', '('), ('FGB', 'NNP'), (')', ')'), ('language', 'NN'), ('model', 'NN'), ('sentence', 'NN'), ('bases', 'NNS'), ('given', 'VBN'), ('bases', 'NNS'), ('utilizes', 'JJ'), ('document-term', 'JJ'), ('sentence', 'NN'), ('term', 'NN'), ('matrices', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['2009 [', '[ 70', '70 ]', '] )', ') -', '- Factorization', 'Factorization Given', 'Given Bases', 'Bases (', '( FGB', 'FGB )', ') language', 'language model', 'model sentence', 'sentence bases', 'bases given', 'given bases', 'bases utilizes', 'utilizes document-term', 'document-term sentence', 'sentence term', 'term matrices', 'matrices .'] 

 TOTAL BIGRAMS --> 23 



 ---- TRI-GRAMS ---- 

 ['2009 [ 70', '[ 70 ]', '70 ] )', '] ) -', ') - Factorization', '- Factorization Given', 'Factorization Given Bases', 'Given Bases (', 'Bases ( FGB', '( FGB )', 'FGB ) language', ') language model', 'language model sentence', 'model sentence bases', 'sentence bases given', 'bases given bases', 'given bases utilizes', 'bases utilizes document-term', 'utilizes document-term sentence', 'document-term sentence term', 'sentence term matrices', 'term matrices .'] 

 TOTAL TRIGRAMS --> 22 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 []

 ---- PORTER STEMMING ----

['2009', '[', '70', ']', ')', '-', 'factor', 'given', 'base', '(', 'fgb', ')', 'languag', 'model', 'sentenc', 'base', 'given', 'base', 'util', 'document-term', 'sentenc', 'term', 'matric', '.']

 TOTAL PORTER STEM WORDS ==> 24



 ---- SNOWBALL STEMMING ----

['2009', '[', '70', ']', ')', '-', 'factor', 'given', 'base', '(', 'fgb', ')', 'languag', 'model', 'sentenc', 'base', 'given', 'base', 'util', 'document-term', 'sentenc', 'term', 'matric', '.']

 TOTAL SNOWBALL STEM WORDS ==> 24



 ---- LEMMATIZATION ----

['2009', '[', '70', ']', ')', '-', 'Factorization', 'Given', 'Bases', '(', 'FGB', ')', 'language', 'model', 'sentence', 'base', 'given', 'base', 'utilizes', 'document-term', 'sentence', 'term', 'matrix', '.']

 TOTAL LEMMATIZE WORDS ==> 24

************************************************************************************************************************

257 --> This approach groups and summarizes the documents simultaneously. 


 ---- TOKENS ----

 ['This', 'approach', 'groups', 'and', 'summarizes', 'the', 'documents', 'simultaneously', '.'] 

 TOTAL TOKENS ==> 9

 ---- POST ----

 [('This', 'DT'), ('approach', 'NN'), ('groups', 'NNS'), ('and', 'CC'), ('summarizes', 'VBZ'), ('the', 'DT'), ('documents', 'NNS'), ('simultaneously', 'RB'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['approach', 'groups', 'summarizes', 'documents', 'simultaneously', '.']

 TOTAL FILTERED TOKENS ==>  6

 ---- POST FOR FILTERED TOKENS ----

 [('approach', 'NN'), ('groups', 'NNS'), ('summarizes', 'VBZ'), ('documents', 'NNS'), ('simultaneously', 'RB'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['approach groups', 'groups summarizes', 'summarizes documents', 'documents simultaneously', 'simultaneously .'] 

 TOTAL BIGRAMS --> 5 



 ---- TRI-GRAMS ---- 

 ['approach groups summarizes', 'groups summarizes documents', 'summarizes documents simultaneously', 'documents simultaneously .'] 

 TOTAL TRIGRAMS --> 4 



 ---- NOUN PHRASES ---- 

 ['approach'] 

 TOTAL NOUN PHRASES --> 1 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['approach', 'group', 'summar', 'document', 'simultan', '.']

 TOTAL PORTER STEM WORDS ==> 6



 ---- SNOWBALL STEMMING ----

['approach', 'group', 'summar', 'document', 'simultan', '.']

 TOTAL SNOWBALL STEM WORDS ==> 6



 ---- LEMMATIZATION ----

['approach', 'group', 'summarizes', 'document', 'simultaneously', '.']

 TOTAL LEMMATIZE WORDS ==> 6

************************************************************************************************************************

258 --> (Wang et  al. 


 ---- TOKENS ----

 ['(', 'Wang', 'et', 'al', '.'] 

 TOTAL TOKENS ==> 5

 ---- POST ----

 [('(', '('), ('Wang', 'NNP'), ('et', 'RB'), ('al', 'RB'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['(', 'Wang', 'et', 'al', '.']

 TOTAL FILTERED TOKENS ==>  5

 ---- POST FOR FILTERED TOKENS ----

 [('(', '('), ('Wang', 'NNP'), ('et', 'RB'), ('al', 'RB'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['( Wang', 'Wang et', 'et al', 'al .'] 

 TOTAL BIGRAMS --> 4 



 ---- TRI-GRAMS ---- 

 ['( Wang et', 'Wang et al', 'et al .'] 

 TOTAL TRIGRAMS --> 3 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 []

 ---- PORTER STEMMING ----

['(', 'wang', 'et', 'al', '.']

 TOTAL PORTER STEM WORDS ==> 5



 ---- SNOWBALL STEMMING ----

['(', 'wang', 'et', 'al', '.']

 TOTAL SNOWBALL STEM WORDS ==> 5



 ---- LEMMATIZATION ----

['(', 'Wang', 'et', 'al', '.']

 TOTAL LEMMATIZE WORDS ==> 5

************************************************************************************************************************

259 --> 2011) [71])  - Topic Aspect-Oriented Summarization (TAOS) is based on topic factors. 


 ---- TOKENS ----

 ['2011', ')', '[', '71', ']', ')', '-', 'Topic', 'Aspect-Oriented', 'Summarization', '(', 'TAOS', ')', 'is', 'based', 'on', 'topic', 'factors', '.'] 

 TOTAL TOKENS ==> 19

 ---- POST ----

 [('2011', 'CD'), (')', ')'), ('[', 'VBD'), ('71', 'CD'), (']', 'NN'), (')', ')'), ('-', ':'), ('Topic', 'NN'), ('Aspect-Oriented', 'JJ'), ('Summarization', 'NNP'), ('(', '('), ('TAOS', 'NNP'), (')', ')'), ('is', 'VBZ'), ('based', 'VBN'), ('on', 'IN'), ('topic', 'NN'), ('factors', 'NNS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['2011', ')', '[', '71', ']', ')', '-', 'Topic', 'Aspect-Oriented', 'Summarization', '(', 'TAOS', ')', 'based', 'topic', 'factors', '.']

 TOTAL FILTERED TOKENS ==>  17

 ---- POST FOR FILTERED TOKENS ----

 [('2011', 'CD'), (')', ')'), ('[', 'VBD'), ('71', 'CD'), (']', 'NN'), (')', ')'), ('-', ':'), ('Topic', 'NN'), ('Aspect-Oriented', 'JJ'), ('Summarization', 'NNP'), ('(', '('), ('TAOS', 'NNP'), (')', ')'), ('based', 'VBN'), ('topic', 'NN'), ('factors', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['2011 )', ') [', '[ 71', '71 ]', '] )', ') -', '- Topic', 'Topic Aspect-Oriented', 'Aspect-Oriented Summarization', 'Summarization (', '( TAOS', 'TAOS )', ') based', 'based topic', 'topic factors', 'factors .'] 

 TOTAL BIGRAMS --> 16 



 ---- TRI-GRAMS ---- 

 ['2011 ) [', ') [ 71', '[ 71 ]', '71 ] )', '] ) -', ') - Topic', '- Topic Aspect-Oriented', 'Topic Aspect-Oriented Summarization', 'Aspect-Oriented Summarization (', 'Summarization ( TAOS', '( TAOS )', 'TAOS ) based', ') based topic', 'based topic factors', 'topic factors .'] 

 TOTAL TRIGRAMS --> 15 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 []

 ---- PORTER STEMMING ----

['2011', ')', '[', '71', ']', ')', '-', 'topic', 'aspect-ori', 'summar', '(', 'tao', ')', 'base', 'topic', 'factor', '.']

 TOTAL PORTER STEM WORDS ==> 17



 ---- SNOWBALL STEMMING ----

['2011', ')', '[', '71', ']', ')', '-', 'topic', 'aspect-ori', 'summar', '(', 'tao', ')', 'base', 'topic', 'factor', '.']

 TOTAL SNOWBALL STEM WORDS ==> 17



 ---- LEMMATIZATION ----

['2011', ')', '[', '71', ']', ')', '-', 'Topic', 'Aspect-Oriented', 'Summarization', '(', 'TAOS', ')', 'based', 'topic', 'factor', '.']

 TOTAL LEMMATIZE WORDS ==> 17

************************************************************************************************************************

260 --> These  topic factors are various features that describe topics such as capital words are  used to represent entity. 


 ---- TOKENS ----

 ['These', 'topic', 'factors', 'are', 'various', 'features', 'that', 'describe', 'topics', 'such', 'as', 'capital', 'words', 'are', 'used', 'to', 'represent', 'entity', '.'] 

 TOTAL TOKENS ==> 19

 ---- POST ----

 [('These', 'DT'), ('topic', 'NN'), ('factors', 'NNS'), ('are', 'VBP'), ('various', 'JJ'), ('features', 'NNS'), ('that', 'WDT'), ('describe', 'VBP'), ('topics', 'NNS'), ('such', 'JJ'), ('as', 'IN'), ('capital', 'NN'), ('words', 'NNS'), ('are', 'VBP'), ('used', 'VBN'), ('to', 'TO'), ('represent', 'VB'), ('entity', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['topic', 'factors', 'various', 'features', 'describe', 'topics', 'capital', 'words', 'used', 'represent', 'entity', '.']

 TOTAL FILTERED TOKENS ==>  12

 ---- POST FOR FILTERED TOKENS ----

 [('topic', 'NN'), ('factors', 'NNS'), ('various', 'JJ'), ('features', 'NNS'), ('describe', 'VBP'), ('topics', 'NNS'), ('capital', 'NN'), ('words', 'NNS'), ('used', 'VBN'), ('represent', 'JJ'), ('entity', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['topic factors', 'factors various', 'various features', 'features describe', 'describe topics', 'topics capital', 'capital words', 'words used', 'used represent', 'represent entity', 'entity .'] 

 TOTAL BIGRAMS --> 11 



 ---- TRI-GRAMS ---- 

 ['topic factors various', 'factors various features', 'various features describe', 'features describe topics', 'describe topics capital', 'topics capital words', 'capital words used', 'words used represent', 'used represent entity', 'represent entity .'] 

 TOTAL TRIGRAMS --> 10 



 ---- NOUN PHRASES ---- 

 ['topic', 'capital', 'represent entity'] 

 TOTAL NOUN PHRASES --> 3 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['topic', 'factor', 'variou', 'featur', 'describ', 'topic', 'capit', 'word', 'use', 'repres', 'entiti', '.']

 TOTAL PORTER STEM WORDS ==> 12



 ---- SNOWBALL STEMMING ----

['topic', 'factor', 'various', 'featur', 'describ', 'topic', 'capit', 'word', 'use', 'repres', 'entiti', '.']

 TOTAL SNOWBALL STEM WORDS ==> 12



 ---- LEMMATIZATION ----

['topic', 'factor', 'various', 'feature', 'describe', 'topic', 'capital', 'word', 'used', 'represent', 'entity', '.']

 TOTAL LEMMATIZE WORDS ==> 12

************************************************************************************************************************

261 --> Various topics can have various aspects and various  preferences of features are used to represent various aspects. 


 ---- TOKENS ----

 ['Various', 'topics', 'can', 'have', 'various', 'aspects', 'and', 'various', 'preferences', 'of', 'features', 'are', 'used', 'to', 'represent', 'various', 'aspects', '.'] 

 TOTAL TOKENS ==> 18

 ---- POST ----

 [('Various', 'JJ'), ('topics', 'NNS'), ('can', 'MD'), ('have', 'VB'), ('various', 'JJ'), ('aspects', 'NNS'), ('and', 'CC'), ('various', 'JJ'), ('preferences', 'NNS'), ('of', 'IN'), ('features', 'NNS'), ('are', 'VBP'), ('used', 'VBN'), ('to', 'TO'), ('represent', 'VB'), ('various', 'JJ'), ('aspects', 'NNS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Various', 'topics', 'various', 'aspects', 'various', 'preferences', 'features', 'used', 'represent', 'various', 'aspects', '.']

 TOTAL FILTERED TOKENS ==>  12

 ---- POST FOR FILTERED TOKENS ----

 [('Various', 'JJ'), ('topics', 'NNS'), ('various', 'JJ'), ('aspects', 'NNS'), ('various', 'JJ'), ('preferences', 'NNS'), ('features', 'NNS'), ('used', 'VBN'), ('represent', 'VBP'), ('various', 'JJ'), ('aspects', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Various topics', 'topics various', 'various aspects', 'aspects various', 'various preferences', 'preferences features', 'features used', 'used represent', 'represent various', 'various aspects', 'aspects .'] 

 TOTAL BIGRAMS --> 11 



 ---- TRI-GRAMS ---- 

 ['Various topics various', 'topics various aspects', 'various aspects various', 'aspects various preferences', 'various preferences features', 'preferences features used', 'features used represent', 'used represent various', 'represent various aspects', 'various aspects .'] 

 TOTAL TRIGRAMS --> 10 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> ['Various']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['variou', 'topic', 'variou', 'aspect', 'variou', 'prefer', 'featur', 'use', 'repres', 'variou', 'aspect', '.']

 TOTAL PORTER STEM WORDS ==> 12



 ---- SNOWBALL STEMMING ----

['various', 'topic', 'various', 'aspect', 'various', 'prefer', 'featur', 'use', 'repres', 'various', 'aspect', '.']

 TOTAL SNOWBALL STEM WORDS ==> 12



 ---- LEMMATIZATION ----

['Various', 'topic', 'various', 'aspect', 'various', 'preference', 'feature', 'used', 'represent', 'various', 'aspect', '.']

 TOTAL LEMMATIZE WORDS ==> 12

************************************************************************************************************************

262 --> (Fang et al. 


 ---- TOKENS ----

 ['(', 'Fang', 'et', 'al', '.'] 

 TOTAL TOKENS ==> 5

 ---- POST ----

 [('(', '('), ('Fang', 'NNP'), ('et', 'RB'), ('al', 'RB'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['(', 'Fang', 'et', 'al', '.']

 TOTAL FILTERED TOKENS ==>  5

 ---- POST FOR FILTERED TOKENS ----

 [('(', '('), ('Fang', 'NNP'), ('et', 'RB'), ('al', 'RB'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['( Fang', 'Fang et', 'et al', 'al .'] 

 TOTAL BIGRAMS --> 4 



 ---- TRI-GRAMS ---- 

 ['( Fang et', 'Fang et al', 'et al .'] 

 TOTAL TRIGRAMS --> 3 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 []

 ---- PORTER STEMMING ----

['(', 'fang', 'et', 'al', '.']

 TOTAL PORTER STEM WORDS ==> 5



 ---- SNOWBALL STEMMING ----

['(', 'fang', 'et', 'al', '.']

 TOTAL SNOWBALL STEM WORDS ==> 5



 ---- LEMMATIZATION ----

['(', 'Fang', 'et', 'al', '.']

 TOTAL LEMMATIZE WORDS ==> 5

************************************************************************************************************************

263 --> 2015 [72])    6.6 Dialogue System  Perhaps the most desirable application of the future, in the systems envisioned by large  providers of end user applications, Dialogue systems, which focuses on a narrowly defined  applications (like refrigerator or home theater systems) currently uses the phonetic and lexical  levels of language. 


 ---- TOKENS ----

 ['2015', '[', '72', ']', ')', '6.6', 'Dialogue', 'System', 'Perhaps', 'the', 'most', 'desirable', 'application', 'of', 'the', 'future', ',', 'in', 'the', 'systems', 'envisioned', 'by', 'large', 'providers', 'of', 'end', 'user', 'applications', ',', 'Dialogue', 'systems', ',', 'which', 'focuses', 'on', 'a', 'narrowly', 'defined', 'applications', '(', 'like', 'refrigerator', 'or', 'home', 'theater', 'systems', ')', 'currently', 'uses', 'the', 'phonetic', 'and', 'lexical', 'levels', 'of', 'language', '.'] 

 TOTAL TOKENS ==> 57

 ---- POST ----

 [('2015', 'CD'), ('[', '$'), ('72', 'CD'), (']', 'NNP'), (')', ')'), ('6.6', 'CD'), ('Dialogue', 'NNP'), ('System', 'NNP'), ('Perhaps', 'RB'), ('the', 'DT'), ('most', 'RBS'), ('desirable', 'JJ'), ('application', 'NN'), ('of', 'IN'), ('the', 'DT'), ('future', 'NN'), (',', ','), ('in', 'IN'), ('the', 'DT'), ('systems', 'NNS'), ('envisioned', 'VBN'), ('by', 'IN'), ('large', 'JJ'), ('providers', 'NNS'), ('of', 'IN'), ('end', 'NN'), ('user', 'NN'), ('applications', 'NNS'), (',', ','), ('Dialogue', 'NNP'), ('systems', 'NNS'), (',', ','), ('which', 'WDT'), ('focuses', 'VBZ'), ('on', 'IN'), ('a', 'DT'), ('narrowly', 'RB'), ('defined', 'VBN'), ('applications', 'NNS'), ('(', '('), ('like', 'IN'), ('refrigerator', 'NN'), ('or', 'CC'), ('home', 'NN'), ('theater', 'NN'), ('systems', 'NNS'), (')', ')'), ('currently', 'RB'), ('uses', 'VBZ'), ('the', 'DT'), ('phonetic', 'JJ'), ('and', 'CC'), ('lexical', 'JJ'), ('levels', 'NNS'), ('of', 'IN'), ('language', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['2015', '[', '72', ']', ')', '6.6', 'Dialogue', 'System', 'Perhaps', 'desirable', 'application', 'future', ',', 'systems', 'envisioned', 'large', 'providers', 'end', 'user', 'applications', ',', 'Dialogue', 'systems', ',', 'focuses', 'narrowly', 'defined', 'applications', '(', 'like', 'refrigerator', 'home', 'theater', 'systems', ')', 'currently', 'uses', 'phonetic', 'lexical', 'levels', 'language', '.']

 TOTAL FILTERED TOKENS ==>  42

 ---- POST FOR FILTERED TOKENS ----

 [('2015', 'CD'), ('[', '$'), ('72', 'CD'), (']', 'NNP'), (')', ')'), ('6.6', 'CD'), ('Dialogue', 'NNP'), ('System', 'NNP'), ('Perhaps', 'RB'), ('desirable', 'JJ'), ('application', 'NN'), ('future', 'NN'), (',', ','), ('systems', 'NNS'), ('envisioned', 'VBD'), ('large', 'JJ'), ('providers', 'NNS'), ('end', 'VBP'), ('user', 'NN'), ('applications', 'NNS'), (',', ','), ('Dialogue', 'NNP'), ('systems', 'NNS'), (',', ','), ('focuses', 'VBZ'), ('narrowly', 'RB'), ('defined', 'VBN'), ('applications', 'NNS'), ('(', '('), ('like', 'IN'), ('refrigerator', 'NN'), ('home', 'NN'), ('theater', 'NN'), ('systems', 'NNS'), (')', ')'), ('currently', 'RB'), ('uses', 'VBZ'), ('phonetic', 'JJ'), ('lexical', 'JJ'), ('levels', 'NNS'), ('language', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['2015 [', '[ 72', '72 ]', '] )', ') 6.6', '6.6 Dialogue', 'Dialogue System', 'System Perhaps', 'Perhaps desirable', 'desirable application', 'application future', 'future ,', ', systems', 'systems envisioned', 'envisioned large', 'large providers', 'providers end', 'end user', 'user applications', 'applications ,', ', Dialogue', 'Dialogue systems', 'systems ,', ', focuses', 'focuses narrowly', 'narrowly defined', 'defined applications', 'applications (', '( like', 'like refrigerator', 'refrigerator home', 'home theater', 'theater systems', 'systems )', ') currently', 'currently uses', 'uses phonetic', 'phonetic lexical', 'lexical levels', 'levels language', 'language .'] 

 TOTAL BIGRAMS --> 41 



 ---- TRI-GRAMS ---- 

 ['2015 [ 72', '[ 72 ]', '72 ] )', '] ) 6.6', ') 6.6 Dialogue', '6.6 Dialogue System', 'Dialogue System Perhaps', 'System Perhaps desirable', 'Perhaps desirable application', 'desirable application future', 'application future ,', 'future , systems', ', systems envisioned', 'systems envisioned large', 'envisioned large providers', 'large providers end', 'providers end user', 'end user applications', 'user applications ,', 'applications , Dialogue', ', Dialogue systems', 'Dialogue systems ,', 'systems , focuses', ', focuses narrowly', 'focuses narrowly defined', 'narrowly defined applications', 'defined applications (', 'applications ( like', '( like refrigerator', 'like refrigerator home', 'refrigerator home theater', 'home theater systems', 'theater systems )', 'systems ) currently', ') currently uses', 'currently uses phonetic', 'uses phonetic lexical', 'phonetic lexical levels', 'lexical levels language', 'levels language .'] 

 TOTAL TRIGRAMS --> 40 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 []

 ---- PORTER STEMMING ----

['2015', '[', '72', ']', ')', '6.6', 'dialogu', 'system', 'perhap', 'desir', 'applic', 'futur', ',', 'system', 'envis', 'larg', 'provid', 'end', 'user', 'applic', ',', 'dialogu', 'system', ',', 'focus', 'narrowli', 'defin', 'applic', '(', 'like', 'refriger', 'home', 'theater', 'system', ')', 'current', 'use', 'phonet', 'lexic', 'level', 'languag', '.']

 TOTAL PORTER STEM WORDS ==> 42



 ---- SNOWBALL STEMMING ----

['2015', '[', '72', ']', ')', '6.6', 'dialogu', 'system', 'perhap', 'desir', 'applic', 'futur', ',', 'system', 'envis', 'larg', 'provid', 'end', 'user', 'applic', ',', 'dialogu', 'system', ',', 'focus', 'narrowli', 'defin', 'applic', '(', 'like', 'refriger', 'home', 'theater', 'system', ')', 'current', 'use', 'phonet', 'lexic', 'level', 'languag', '.']

 TOTAL SNOWBALL STEM WORDS ==> 42



 ---- LEMMATIZATION ----

['2015', '[', '72', ']', ')', '6.6', 'Dialogue', 'System', 'Perhaps', 'desirable', 'application', 'future', ',', 'system', 'envisioned', 'large', 'provider', 'end', 'user', 'application', ',', 'Dialogue', 'system', ',', 'focus', 'narrowly', 'defined', 'application', '(', 'like', 'refrigerator', 'home', 'theater', 'system', ')', 'currently', 'us', 'phonetic', 'lexical', 'level', 'language', '.']

 TOTAL LEMMATIZE WORDS ==> 42

************************************************************************************************************************

264 --> It is believed that these dialogue systems when utilizing all levels of  language processing offer potential for fully automated dialog systems. 


 ---- TOKENS ----

 ['It', 'is', 'believed', 'that', 'these', 'dialogue', 'systems', 'when', 'utilizing', 'all', 'levels', 'of', 'language', 'processing', 'offer', 'potential', 'for', 'fully', 'automated', 'dialog', 'systems', '.'] 

 TOTAL TOKENS ==> 22

 ---- POST ----

 [('It', 'PRP'), ('is', 'VBZ'), ('believed', 'VBN'), ('that', 'IN'), ('these', 'DT'), ('dialogue', 'NN'), ('systems', 'NNS'), ('when', 'WRB'), ('utilizing', 'VBG'), ('all', 'DT'), ('levels', 'NNS'), ('of', 'IN'), ('language', 'NN'), ('processing', 'NN'), ('offer', 'VBP'), ('potential', 'JJ'), ('for', 'IN'), ('fully', 'RB'), ('automated', 'VBN'), ('dialog', 'NN'), ('systems', 'NNS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['believed', 'dialogue', 'systems', 'utilizing', 'levels', 'language', 'processing', 'offer', 'potential', 'fully', 'automated', 'dialog', 'systems', '.']

 TOTAL FILTERED TOKENS ==>  14

 ---- POST FOR FILTERED TOKENS ----

 [('believed', 'VBN'), ('dialogue', 'NN'), ('systems', 'NNS'), ('utilizing', 'JJ'), ('levels', 'NNS'), ('language', 'NN'), ('processing', 'NN'), ('offer', 'VBP'), ('potential', 'JJ'), ('fully', 'RB'), ('automated', 'VBN'), ('dialog', 'NN'), ('systems', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['believed dialogue', 'dialogue systems', 'systems utilizing', 'utilizing levels', 'levels language', 'language processing', 'processing offer', 'offer potential', 'potential fully', 'fully automated', 'automated dialog', 'dialog systems', 'systems .'] 

 TOTAL BIGRAMS --> 13 



 ---- TRI-GRAMS ---- 

 ['believed dialogue systems', 'dialogue systems utilizing', 'systems utilizing levels', 'utilizing levels language', 'levels language processing', 'language processing offer', 'processing offer potential', 'offer potential fully', 'potential fully automated', 'fully automated dialog', 'automated dialog systems', 'dialog systems .'] 

 TOTAL TRIGRAMS --> 12 



 ---- NOUN PHRASES ---- 

 ['dialogue', 'language', 'processing', 'dialog'] 

 TOTAL NOUN PHRASES --> 4 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['believ', 'dialogu', 'system', 'util', 'level', 'languag', 'process', 'offer', 'potenti', 'fulli', 'autom', 'dialog', 'system', '.']

 TOTAL PORTER STEM WORDS ==> 14



 ---- SNOWBALL STEMMING ----

['believ', 'dialogu', 'system', 'util', 'level', 'languag', 'process', 'offer', 'potenti', 'fulli', 'autom', 'dialog', 'system', '.']

 TOTAL SNOWBALL STEM WORDS ==> 14



 ---- LEMMATIZATION ----

['believed', 'dialogue', 'system', 'utilizing', 'level', 'language', 'processing', 'offer', 'potential', 'fully', 'automated', 'dialog', 'system', '.']

 TOTAL LEMMATIZE WORDS ==> 14

************************************************************************************************************************

265 --> (Elizabeth D. Liddy,  2001) [7]. 


 ---- TOKENS ----

 ['(', 'Elizabeth', 'D.', 'Liddy', ',', '2001', ')', '[', '7', ']', '.'] 

 TOTAL TOKENS ==> 11

 ---- POST ----

 [('(', '('), ('Elizabeth', 'NNP'), ('D.', 'NNP'), ('Liddy', 'NNP'), (',', ','), ('2001', 'CD'), (')', ')'), ('[', 'VBD'), ('7', 'CD'), (']', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['(', 'Elizabeth', 'D.', 'Liddy', ',', '2001', ')', '[', '7', ']', '.']

 TOTAL FILTERED TOKENS ==>  11

 ---- POST FOR FILTERED TOKENS ----

 [('(', '('), ('Elizabeth', 'NNP'), ('D.', 'NNP'), ('Liddy', 'NNP'), (',', ','), ('2001', 'CD'), (')', ')'), ('[', 'VBD'), ('7', 'CD'), (']', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['( Elizabeth', 'Elizabeth D.', 'D. Liddy', 'Liddy ,', ', 2001', '2001 )', ') [', '[ 7', '7 ]', '] .'] 

 TOTAL BIGRAMS --> 10 



 ---- TRI-GRAMS ---- 

 ['( Elizabeth D.', 'Elizabeth D. Liddy', 'D. Liddy ,', 'Liddy , 2001', ', 2001 )', '2001 ) [', ') [ 7', '[ 7 ]', '7 ] .'] 

 TOTAL TRIGRAMS --> 9 



 ---- NOUN PHRASES ---- 

 [']'] 

 TOTAL NOUN PHRASES --> 1 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['(', 'elizabeth', 'd.', 'liddi', ',', '2001', ')', '[', '7', ']', '.']

 TOTAL PORTER STEM WORDS ==> 11



 ---- SNOWBALL STEMMING ----

['(', 'elizabeth', 'd.', 'liddi', ',', '2001', ')', '[', '7', ']', '.']

 TOTAL SNOWBALL STEM WORDS ==> 11



 ---- LEMMATIZATION ----

['(', 'Elizabeth', 'D.', 'Liddy', ',', '2001', ')', '[', '7', ']', '.']

 TOTAL LEMMATIZE WORDS ==> 11

************************************************************************************************************************

266 --> Whether on text or via voice. 


 ---- TOKENS ----

 ['Whether', 'on', 'text', 'or', 'via', 'voice', '.'] 

 TOTAL TOKENS ==> 7

 ---- POST ----

 [('Whether', 'NNP'), ('on', 'IN'), ('text', 'NN'), ('or', 'CC'), ('via', 'IN'), ('voice', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Whether', 'text', 'via', 'voice', '.']

 TOTAL FILTERED TOKENS ==>  5

 ---- POST FOR FILTERED TOKENS ----

 [('Whether', 'NNP'), ('text', 'NN'), ('via', 'IN'), ('voice', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Whether text', 'text via', 'via voice', 'voice .'] 

 TOTAL BIGRAMS --> 4 



 ---- TRI-GRAMS ---- 

 ['Whether text via', 'text via voice', 'via voice .'] 

 TOTAL TRIGRAMS --> 3 



 ---- NOUN PHRASES ---- 

 ['text', 'voice'] 

 TOTAL NOUN PHRASES --> 2 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> ['Whether']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['whether', 'text', 'via', 'voic', '.']

 TOTAL PORTER STEM WORDS ==> 5



 ---- SNOWBALL STEMMING ----

['whether', 'text', 'via', 'voic', '.']

 TOTAL SNOWBALL STEM WORDS ==> 5



 ---- LEMMATIZATION ----

['Whether', 'text', 'via', 'voice', '.']

 TOTAL LEMMATIZE WORDS ==> 5

************************************************************************************************************************

267 --> This could lead to produce systems that can enable  robots to interact with humans in natural languages. 


 ---- TOKENS ----

 ['This', 'could', 'lead', 'to', 'produce', 'systems', 'that', 'can', 'enable', 'robots', 'to', 'interact', 'with', 'humans', 'in', 'natural', 'languages', '.'] 

 TOTAL TOKENS ==> 18

 ---- POST ----

 [('This', 'DT'), ('could', 'MD'), ('lead', 'VB'), ('to', 'TO'), ('produce', 'VB'), ('systems', 'NNS'), ('that', 'WDT'), ('can', 'MD'), ('enable', 'VB'), ('robots', 'NNS'), ('to', 'TO'), ('interact', 'VB'), ('with', 'IN'), ('humans', 'NNS'), ('in', 'IN'), ('natural', 'JJ'), ('languages', 'NNS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['could', 'lead', 'produce', 'systems', 'enable', 'robots', 'interact', 'humans', 'natural', 'languages', '.']

 TOTAL FILTERED TOKENS ==>  11

 ---- POST FOR FILTERED TOKENS ----

 [('could', 'MD'), ('lead', 'VB'), ('produce', 'VB'), ('systems', 'NNS'), ('enable', 'JJ'), ('robots', 'NNS'), ('interact', 'VBP'), ('humans', 'NNS'), ('natural', 'JJ'), ('languages', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['could lead', 'lead produce', 'produce systems', 'systems enable', 'enable robots', 'robots interact', 'interact humans', 'humans natural', 'natural languages', 'languages .'] 

 TOTAL BIGRAMS --> 10 



 ---- TRI-GRAMS ---- 

 ['could lead produce', 'lead produce systems', 'produce systems enable', 'systems enable robots', 'enable robots interact', 'robots interact humans', 'interact humans natural', 'humans natural languages', 'natural languages .'] 

 TOTAL TRIGRAMS --> 9 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['could', 'lead', 'produc', 'system', 'enabl', 'robot', 'interact', 'human', 'natur', 'languag', '.']

 TOTAL PORTER STEM WORDS ==> 11



 ---- SNOWBALL STEMMING ----

['could', 'lead', 'produc', 'system', 'enabl', 'robot', 'interact', 'human', 'natur', 'languag', '.']

 TOTAL SNOWBALL STEM WORDS ==> 11



 ---- LEMMATIZATION ----

['could', 'lead', 'produce', 'system', 'enable', 'robot', 'interact', 'human', 'natural', 'language', '.']

 TOTAL LEMMATIZE WORDS ==> 11

************************************************************************************************************************

268 --> Examples like Google’s assistant,  Windows Cortana, Apple’s Siri and Amazon’s Alexa are the software and devices that follow  Dialogue systems. 


 ---- TOKENS ----

 ['Examples', 'like', 'Google', '’', 's', 'assistant', ',', 'Windows', 'Cortana', ',', 'Apple', '’', 's', 'Siri', 'and', 'Amazon', '’', 's', 'Alexa', 'are', 'the', 'software', 'and', 'devices', 'that', 'follow', 'Dialogue', 'systems', '.'] 

 TOTAL TOKENS ==> 29

 ---- POST ----

 [('Examples', 'NNS'), ('like', 'IN'), ('Google', 'NNP'), ('’', 'NNP'), ('s', 'NN'), ('assistant', 'NN'), (',', ','), ('Windows', 'NNP'), ('Cortana', 'NNP'), (',', ','), ('Apple', 'NNP'), ('’', 'NNP'), ('s', 'VBD'), ('Siri', 'NNP'), ('and', 'CC'), ('Amazon', 'NNP'), ('’', 'NNP'), ('s', 'NN'), ('Alexa', 'NNP'), ('are', 'VBP'), ('the', 'DT'), ('software', 'NN'), ('and', 'CC'), ('devices', 'NNS'), ('that', 'WDT'), ('follow', 'VBP'), ('Dialogue', 'NNPS'), ('systems', 'NNS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Examples', 'like', 'Google', '’', 'assistant', ',', 'Windows', 'Cortana', ',', 'Apple', '’', 'Siri', 'Amazon', '’', 'Alexa', 'software', 'devices', 'follow', 'Dialogue', 'systems', '.']

 TOTAL FILTERED TOKENS ==>  21

 ---- POST FOR FILTERED TOKENS ----

 [('Examples', 'NNS'), ('like', 'IN'), ('Google', 'NNP'), ('’', 'NNP'), ('assistant', 'NN'), (',', ','), ('Windows', 'NNP'), ('Cortana', 'NNP'), (',', ','), ('Apple', 'NNP'), ('’', 'NNP'), ('Siri', 'NNP'), ('Amazon', 'NNP'), ('’', 'NNP'), ('Alexa', 'NNP'), ('software', 'NN'), ('devices', 'NNS'), ('follow', 'VBP'), ('Dialogue', 'NNP'), ('systems', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Examples like', 'like Google', 'Google ’', '’ assistant', 'assistant ,', ', Windows', 'Windows Cortana', 'Cortana ,', ', Apple', 'Apple ’', '’ Siri', 'Siri Amazon', 'Amazon ’', '’ Alexa', 'Alexa software', 'software devices', 'devices follow', 'follow Dialogue', 'Dialogue systems', 'systems .'] 

 TOTAL BIGRAMS --> 20 



 ---- TRI-GRAMS ---- 

 ['Examples like Google', 'like Google ’', 'Google ’ assistant', '’ assistant ,', 'assistant , Windows', ', Windows Cortana', 'Windows Cortana ,', 'Cortana , Apple', ', Apple ’', 'Apple ’ Siri', '’ Siri Amazon', 'Siri Amazon ’', 'Amazon ’ Alexa', '’ Alexa software', 'Alexa software devices', 'software devices follow', 'devices follow Dialogue', 'follow Dialogue systems', 'Dialogue systems .'] 

 TOTAL TRIGRAMS --> 19 



 ---- NOUN PHRASES ---- 

 ['assistant', 'software'] 

 TOTAL NOUN PHRASES --> 2 



 ---- NER ----

 
 ORGANIZATION ---> ['Dialogue']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> ['Google', 'Windows Cortana', 'Apple', 'Amazon', 'Alexa']
 TOTAL PERSON ENTITY --> 5 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['exampl', 'like', 'googl', '’', 'assist', ',', 'window', 'cortana', ',', 'appl', '’', 'siri', 'amazon', '’', 'alexa', 'softwar', 'devic', 'follow', 'dialogu', 'system', '.']

 TOTAL PORTER STEM WORDS ==> 21



 ---- SNOWBALL STEMMING ----

['exampl', 'like', 'googl', '’', 'assist', ',', 'window', 'cortana', ',', 'appl', '’', 'siri', 'amazon', '’', 'alexa', 'softwar', 'devic', 'follow', 'dialogu', 'system', '.']

 TOTAL SNOWBALL STEM WORDS ==> 21



 ---- LEMMATIZATION ----

['Examples', 'like', 'Google', '’', 'assistant', ',', 'Windows', 'Cortana', ',', 'Apple', '’', 'Siri', 'Amazon', '’', 'Alexa', 'software', 'device', 'follow', 'Dialogue', 'system', '.']

 TOTAL LEMMATIZE WORDS ==> 21

************************************************************************************************************************

269 --> 6.7 Medicine  NLP is applied in medicine field as well. 


 ---- TOKENS ----

 ['6.7', 'Medicine', 'NLP', 'is', 'applied', 'in', 'medicine', 'field', 'as', 'well', '.'] 

 TOTAL TOKENS ==> 11

 ---- POST ----

 [('6.7', 'CD'), ('Medicine', 'NNP'), ('NLP', 'NNP'), ('is', 'VBZ'), ('applied', 'VBN'), ('in', 'IN'), ('medicine', 'JJ'), ('field', 'NN'), ('as', 'RB'), ('well', 'RB'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['6.7', 'Medicine', 'NLP', 'applied', 'medicine', 'field', 'well', '.']

 TOTAL FILTERED TOKENS ==>  8

 ---- POST FOR FILTERED TOKENS ----

 [('6.7', 'CD'), ('Medicine', 'NNP'), ('NLP', 'NNP'), ('applied', 'VBD'), ('medicine', 'JJ'), ('field', 'NN'), ('well', 'RB'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['6.7 Medicine', 'Medicine NLP', 'NLP applied', 'applied medicine', 'medicine field', 'field well', 'well .'] 

 TOTAL BIGRAMS --> 7 



 ---- TRI-GRAMS ---- 

 ['6.7 Medicine NLP', 'Medicine NLP applied', 'NLP applied medicine', 'applied medicine field', 'medicine field well', 'field well .'] 

 TOTAL TRIGRAMS --> 6 



 ---- NOUN PHRASES ---- 

 ['medicine field'] 

 TOTAL NOUN PHRASES --> 1 



 ---- NER ----

 
 ORGANIZATION ---> ['Medicine']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['6.7', 'medicin', 'nlp', 'appli', 'medicin', 'field', 'well', '.']

 TOTAL PORTER STEM WORDS ==> 8



 ---- SNOWBALL STEMMING ----

['6.7', 'medicin', 'nlp', 'appli', 'medicin', 'field', 'well', '.']

 TOTAL SNOWBALL STEM WORDS ==> 8



 ---- LEMMATIZATION ----

['6.7', 'Medicine', 'NLP', 'applied', 'medicine', 'field', 'well', '.']

 TOTAL LEMMATIZE WORDS ==> 8

************************************************************************************************************************

270 --> The Linguistic String Project-Medical Language  Processor is one the large scale projects of NLP in the field of medicine [74][75][76][77][78]. 


 ---- TOKENS ----

 ['The', 'Linguistic', 'String', 'Project-Medical', 'Language', 'Processor', 'is', 'one', 'the', 'large', 'scale', 'projects', 'of', 'NLP', 'in', 'the', 'field', 'of', 'medicine', '[', '74', ']', '[', '75', ']', '[', '76', ']', '[', '77', ']', '[', '78', ']', '.'] 

 TOTAL TOKENS ==> 35

 ---- POST ----

 [('The', 'DT'), ('Linguistic', 'JJ'), ('String', 'NNP'), ('Project-Medical', 'JJ'), ('Language', 'NNP'), ('Processor', 'NNP'), ('is', 'VBZ'), ('one', 'CD'), ('the', 'DT'), ('large', 'JJ'), ('scale', 'JJ'), ('projects', 'NNS'), ('of', 'IN'), ('NLP', 'NNP'), ('in', 'IN'), ('the', 'DT'), ('field', 'NN'), ('of', 'IN'), ('medicine', 'JJ'), ('[', 'FW'), ('74', 'CD'), (']', 'NNP'), ('[', 'VBD'), ('75', 'CD'), (']', 'NNP'), ('[', 'VBD'), ('76', 'CD'), (']', 'NNP'), ('[', 'VBD'), ('77', 'CD'), (']', 'NNP'), ('[', 'VBD'), ('78', 'CD'), (']', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Linguistic', 'String', 'Project-Medical', 'Language', 'Processor', 'one', 'large', 'scale', 'projects', 'NLP', 'field', 'medicine', '[', '74', ']', '[', '75', ']', '[', '76', ']', '[', '77', ']', '[', '78', ']', '.']

 TOTAL FILTERED TOKENS ==>  28

 ---- POST FOR FILTERED TOKENS ----

 [('Linguistic', 'JJ'), ('String', 'NNP'), ('Project-Medical', 'JJ'), ('Language', 'NNP'), ('Processor', 'NNP'), ('one', 'CD'), ('large', 'JJ'), ('scale', 'JJ'), ('projects', 'NNS'), ('NLP', 'NNP'), ('field', 'NN'), ('medicine', 'NN'), ('[', '$'), ('74', 'CD'), (']', 'NNP'), ('[', 'VBD'), ('75', 'CD'), (']', 'NNP'), ('[', 'VBD'), ('76', 'CD'), (']', 'NNP'), ('[', 'VBD'), ('77', 'CD'), (']', 'NNP'), ('[', 'VBD'), ('78', 'CD'), (']', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Linguistic String', 'String Project-Medical', 'Project-Medical Language', 'Language Processor', 'Processor one', 'one large', 'large scale', 'scale projects', 'projects NLP', 'NLP field', 'field medicine', 'medicine [', '[ 74', '74 ]', '] [', '[ 75', '75 ]', '] [', '[ 76', '76 ]', '] [', '[ 77', '77 ]', '] [', '[ 78', '78 ]', '] .'] 

 TOTAL BIGRAMS --> 27 



 ---- TRI-GRAMS ---- 

 ['Linguistic String Project-Medical', 'String Project-Medical Language', 'Project-Medical Language Processor', 'Language Processor one', 'Processor one large', 'one large scale', 'large scale projects', 'scale projects NLP', 'projects NLP field', 'NLP field medicine', 'field medicine [', 'medicine [ 74', '[ 74 ]', '74 ] [', '] [ 75', '[ 75 ]', '75 ] [', '] [ 76', '[ 76 ]', '76 ] [', '] [ 77', '[ 77 ]', '77 ] [', '] [ 78', '[ 78 ]', '78 ] .'] 

 TOTAL TRIGRAMS --> 26 



 ---- NOUN PHRASES ---- 

 ['field', 'medicine', ']'] 

 TOTAL NOUN PHRASES --> 3 



 ---- NER ----

 
 ORGANIZATION ---> ['NLP']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> ['Linguistic']
 TOTAL PERSON ENTITY --> 1 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['linguist', 'string', 'project-med', 'languag', 'processor', 'one', 'larg', 'scale', 'project', 'nlp', 'field', 'medicin', '[', '74', ']', '[', '75', ']', '[', '76', ']', '[', '77', ']', '[', '78', ']', '.']

 TOTAL PORTER STEM WORDS ==> 28



 ---- SNOWBALL STEMMING ----

['linguist', 'string', 'project-med', 'languag', 'processor', 'one', 'larg', 'scale', 'project', 'nlp', 'field', 'medicin', '[', '74', ']', '[', '75', ']', '[', '76', ']', '[', '77', ']', '[', '78', ']', '.']

 TOTAL SNOWBALL STEM WORDS ==> 28



 ---- LEMMATIZATION ----

['Linguistic', 'String', 'Project-Medical', 'Language', 'Processor', 'one', 'large', 'scale', 'project', 'NLP', 'field', 'medicine', '[', '74', ']', '[', '75', ']', '[', '76', ']', '[', '77', ']', '[', '78', ']', '.']

 TOTAL LEMMATIZE WORDS ==> 28

************************************************************************************************************************

271 --> The LSP-MLP helps enabling physicians to extract and summarize information of any signs  or symptoms, drug dosage and response data with aim of identifying possible side effects of  any medicine while highlighting or flagging data items [74]. 


 ---- TOKENS ----

 ['The', 'LSP-MLP', 'helps', 'enabling', 'physicians', 'to', 'extract', 'and', 'summarize', 'information', 'of', 'any', 'signs', 'or', 'symptoms', ',', 'drug', 'dosage', 'and', 'response', 'data', 'with', 'aim', 'of', 'identifying', 'possible', 'side', 'effects', 'of', 'any', 'medicine', 'while', 'highlighting', 'or', 'flagging', 'data', 'items', '[', '74', ']', '.'] 

 TOTAL TOKENS ==> 41

 ---- POST ----

 [('The', 'DT'), ('LSP-MLP', 'NNP'), ('helps', 'VBZ'), ('enabling', 'VBG'), ('physicians', 'NNS'), ('to', 'TO'), ('extract', 'VB'), ('and', 'CC'), ('summarize', 'VB'), ('information', 'NN'), ('of', 'IN'), ('any', 'DT'), ('signs', 'NNS'), ('or', 'CC'), ('symptoms', 'NNS'), (',', ','), ('drug', 'NN'), ('dosage', 'NN'), ('and', 'CC'), ('response', 'NN'), ('data', 'NNS'), ('with', 'IN'), ('aim', 'NN'), ('of', 'IN'), ('identifying', 'VBG'), ('possible', 'JJ'), ('side', 'NN'), ('effects', 'NNS'), ('of', 'IN'), ('any', 'DT'), ('medicine', 'NN'), ('while', 'IN'), ('highlighting', 'VBG'), ('or', 'CC'), ('flagging', 'VBG'), ('data', 'NNS'), ('items', 'NNS'), ('[', 'VBD'), ('74', 'CD'), (']', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['LSP-MLP', 'helps', 'enabling', 'physicians', 'extract', 'summarize', 'information', 'signs', 'symptoms', ',', 'drug', 'dosage', 'response', 'data', 'aim', 'identifying', 'possible', 'side', 'effects', 'medicine', 'highlighting', 'flagging', 'data', 'items', '[', '74', ']', '.']

 TOTAL FILTERED TOKENS ==>  28

 ---- POST FOR FILTERED TOKENS ----

 [('LSP-MLP', 'NNP'), ('helps', 'VBZ'), ('enabling', 'VBG'), ('physicians', 'NNS'), ('extract', 'JJ'), ('summarize', 'JJ'), ('information', 'NN'), ('signs', 'NNS'), ('symptoms', 'NNS'), (',', ','), ('drug', 'NN'), ('dosage', 'NN'), ('response', 'NN'), ('data', 'NNS'), ('aim', 'NN'), ('identifying', 'VBG'), ('possible', 'JJ'), ('side', 'NN'), ('effects', 'NNS'), ('medicine', 'VBP'), ('highlighting', 'VBG'), ('flagging', 'VBG'), ('data', 'NNS'), ('items', 'NNS'), ('[', 'VBD'), ('74', 'CD'), (']', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['LSP-MLP helps', 'helps enabling', 'enabling physicians', 'physicians extract', 'extract summarize', 'summarize information', 'information signs', 'signs symptoms', 'symptoms ,', ', drug', 'drug dosage', 'dosage response', 'response data', 'data aim', 'aim identifying', 'identifying possible', 'possible side', 'side effects', 'effects medicine', 'medicine highlighting', 'highlighting flagging', 'flagging data', 'data items', 'items [', '[ 74', '74 ]', '] .'] 

 TOTAL BIGRAMS --> 27 



 ---- TRI-GRAMS ---- 

 ['LSP-MLP helps enabling', 'helps enabling physicians', 'enabling physicians extract', 'physicians extract summarize', 'extract summarize information', 'summarize information signs', 'information signs symptoms', 'signs symptoms ,', 'symptoms , drug', ', drug dosage', 'drug dosage response', 'dosage response data', 'response data aim', 'data aim identifying', 'aim identifying possible', 'identifying possible side', 'possible side effects', 'side effects medicine', 'effects medicine highlighting', 'medicine highlighting flagging', 'highlighting flagging data', 'flagging data items', 'data items [', 'items [ 74', '[ 74 ]', '74 ] .'] 

 TOTAL TRIGRAMS --> 26 



 ---- NOUN PHRASES ---- 

 ['extract summarize information', 'drug', 'dosage', 'response', 'aim', 'possible side', ']'] 

 TOTAL NOUN PHRASES --> 7 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['lsp-mlp', 'help', 'enabl', 'physician', 'extract', 'summar', 'inform', 'sign', 'symptom', ',', 'drug', 'dosag', 'respons', 'data', 'aim', 'identifi', 'possibl', 'side', 'effect', 'medicin', 'highlight', 'flag', 'data', 'item', '[', '74', ']', '.']

 TOTAL PORTER STEM WORDS ==> 28



 ---- SNOWBALL STEMMING ----

['lsp-mlp', 'help', 'enabl', 'physician', 'extract', 'summar', 'inform', 'sign', 'symptom', ',', 'drug', 'dosag', 'respons', 'data', 'aim', 'identifi', 'possibl', 'side', 'effect', 'medicin', 'highlight', 'flag', 'data', 'item', '[', '74', ']', '.']

 TOTAL SNOWBALL STEM WORDS ==> 28



 ---- LEMMATIZATION ----

['LSP-MLP', 'help', 'enabling', 'physician', 'extract', 'summarize', 'information', 'sign', 'symptom', ',', 'drug', 'dosage', 'response', 'data', 'aim', 'identifying', 'possible', 'side', 'effect', 'medicine', 'highlighting', 'flagging', 'data', 'item', '[', '74', ']', '.']

 TOTAL LEMMATIZE WORDS ==> 28

************************************************************************************************************************

272 --> The National Library of  Medicine is developing The Specialist System [79][80][81][82][83]. 


 ---- TOKENS ----

 ['The', 'National', 'Library', 'of', 'Medicine', 'is', 'developing', 'The', 'Specialist', 'System', '[', '79', ']', '[', '80', ']', '[', '81', ']', '[', '82', ']', '[', '83', ']', '.'] 

 TOTAL TOKENS ==> 26

 ---- POST ----

 [('The', 'DT'), ('National', 'NNP'), ('Library', 'NNP'), ('of', 'IN'), ('Medicine', 'NNP'), ('is', 'VBZ'), ('developing', 'VBG'), ('The', 'DT'), ('Specialist', 'NN'), ('System', 'NNP'), ('[', 'VBZ'), ('79', 'CD'), (']', 'NN'), ('[', 'VBD'), ('80', 'CD'), (']', 'NNP'), ('[', 'VBD'), ('81', 'CD'), (']', 'NNP'), ('[', 'VBD'), ('82', 'CD'), (']', 'NNP'), ('[', 'VBD'), ('83', 'CD'), (']', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['National', 'Library', 'Medicine', 'developing', 'Specialist', 'System', '[', '79', ']', '[', '80', ']', '[', '81', ']', '[', '82', ']', '[', '83', ']', '.']

 TOTAL FILTERED TOKENS ==>  22

 ---- POST FOR FILTERED TOKENS ----

 [('National', 'NNP'), ('Library', 'NNP'), ('Medicine', 'NNP'), ('developing', 'VBG'), ('Specialist', 'NNP'), ('System', 'NNP'), ('[', 'VBD'), ('79', 'CD'), (']', 'NNP'), ('[', 'VBD'), ('80', 'CD'), (']', 'NNP'), ('[', 'VBD'), ('81', 'CD'), (']', 'NNP'), ('[', 'VBD'), ('82', 'CD'), (']', 'NNP'), ('[', 'VBD'), ('83', 'CD'), (']', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['National Library', 'Library Medicine', 'Medicine developing', 'developing Specialist', 'Specialist System', 'System [', '[ 79', '79 ]', '] [', '[ 80', '80 ]', '] [', '[ 81', '81 ]', '] [', '[ 82', '82 ]', '] [', '[ 83', '83 ]', '] .'] 

 TOTAL BIGRAMS --> 21 



 ---- TRI-GRAMS ---- 

 ['National Library Medicine', 'Library Medicine developing', 'Medicine developing Specialist', 'developing Specialist System', 'Specialist System [', 'System [ 79', '[ 79 ]', '79 ] [', '] [ 80', '[ 80 ]', '80 ] [', '] [ 81', '[ 81 ]', '81 ] [', '] [ 82', '[ 82 ]', '82 ] [', '] [ 83', '[ 83 ]', '83 ] .'] 

 TOTAL TRIGRAMS --> 20 



 ---- NOUN PHRASES ---- 

 [']'] 

 TOTAL NOUN PHRASES --> 1 



 ---- NER ----

 
 ORGANIZATION ---> ['National', 'Specialist System']
 TOTAL ORGANIZATION ENTITY --> 2 


 PERSON ---> ['Library Medicine']
 TOTAL PERSON ENTITY --> 1 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['nation', 'librari', 'medicin', 'develop', 'specialist', 'system', '[', '79', ']', '[', '80', ']', '[', '81', ']', '[', '82', ']', '[', '83', ']', '.']

 TOTAL PORTER STEM WORDS ==> 22



 ---- SNOWBALL STEMMING ----

['nation', 'librari', 'medicin', 'develop', 'specialist', 'system', '[', '79', ']', '[', '80', ']', '[', '81', ']', '[', '82', ']', '[', '83', ']', '.']

 TOTAL SNOWBALL STEM WORDS ==> 22



 ---- LEMMATIZATION ----

['National', 'Library', 'Medicine', 'developing', 'Specialist', 'System', '[', '79', ']', '[', '80', ']', '[', '81', ']', '[', '82', ']', '[', '83', ']', '.']

 TOTAL LEMMATIZE WORDS ==> 22

************************************************************************************************************************

273 --> It is expected to function  as Information Extraction tool for Biomedical Knowledge Bases, particularly Medline   abstracts. 


 ---- TOKENS ----

 ['It', 'is', 'expected', 'to', 'function', 'as', 'Information', 'Extraction', 'tool', 'for', 'Biomedical', 'Knowledge', 'Bases', ',', 'particularly', 'Medline', 'abstracts', '.'] 

 TOTAL TOKENS ==> 18

 ---- POST ----

 [('It', 'PRP'), ('is', 'VBZ'), ('expected', 'VBN'), ('to', 'TO'), ('function', 'VB'), ('as', 'IN'), ('Information', 'NNP'), ('Extraction', 'NNP'), ('tool', 'NN'), ('for', 'IN'), ('Biomedical', 'NNP'), ('Knowledge', 'NNP'), ('Bases', 'NNP'), (',', ','), ('particularly', 'RB'), ('Medline', 'JJ'), ('abstracts', 'NNS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['expected', 'function', 'Information', 'Extraction', 'tool', 'Biomedical', 'Knowledge', 'Bases', ',', 'particularly', 'Medline', 'abstracts', '.']

 TOTAL FILTERED TOKENS ==>  13

 ---- POST FOR FILTERED TOKENS ----

 [('expected', 'VBN'), ('function', 'NN'), ('Information', 'NNP'), ('Extraction', 'NNP'), ('tool', 'NN'), ('Biomedical', 'NNP'), ('Knowledge', 'NNP'), ('Bases', 'NNP'), (',', ','), ('particularly', 'RB'), ('Medline', 'JJ'), ('abstracts', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['expected function', 'function Information', 'Information Extraction', 'Extraction tool', 'tool Biomedical', 'Biomedical Knowledge', 'Knowledge Bases', 'Bases ,', ', particularly', 'particularly Medline', 'Medline abstracts', 'abstracts .'] 

 TOTAL BIGRAMS --> 12 



 ---- TRI-GRAMS ---- 

 ['expected function Information', 'function Information Extraction', 'Information Extraction tool', 'Extraction tool Biomedical', 'tool Biomedical Knowledge', 'Biomedical Knowledge Bases', 'Knowledge Bases ,', 'Bases , particularly', ', particularly Medline', 'particularly Medline abstracts', 'Medline abstracts .'] 

 TOTAL TRIGRAMS --> 11 



 ---- NOUN PHRASES ---- 

 ['function', 'tool'] 

 TOTAL NOUN PHRASES --> 2 



 ---- NER ----

 
 ORGANIZATION ---> ['Biomedical', 'Medline']
 TOTAL ORGANIZATION ENTITY --> 2 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['expect', 'function', 'inform', 'extract', 'tool', 'biomed', 'knowledg', 'base', ',', 'particularli', 'medlin', 'abstract', '.']

 TOTAL PORTER STEM WORDS ==> 13



 ---- SNOWBALL STEMMING ----

['expect', 'function', 'inform', 'extract', 'tool', 'biomed', 'knowledg', 'base', ',', 'particular', 'medlin', 'abstract', '.']

 TOTAL SNOWBALL STEM WORDS ==> 13



 ---- LEMMATIZATION ----

['expected', 'function', 'Information', 'Extraction', 'tool', 'Biomedical', 'Knowledge', 'Bases', ',', 'particularly', 'Medline', 'abstract', '.']

 TOTAL LEMMATIZE WORDS ==> 13

************************************************************************************************************************

274 --> The lexicon was created using MeSH (Medical Subject Headings), Dorland’s  Illustrated Medical Dictionary and general English Dictionaries. 


 ---- TOKENS ----

 ['The', 'lexicon', 'was', 'created', 'using', 'MeSH', '(', 'Medical', 'Subject', 'Headings', ')', ',', 'Dorland', '’', 's', 'Illustrated', 'Medical', 'Dictionary', 'and', 'general', 'English', 'Dictionaries', '.'] 

 TOTAL TOKENS ==> 23

 ---- POST ----

 [('The', 'DT'), ('lexicon', 'NN'), ('was', 'VBD'), ('created', 'VBN'), ('using', 'VBG'), ('MeSH', 'NNP'), ('(', '('), ('Medical', 'NNP'), ('Subject', 'NNP'), ('Headings', 'NNP'), (')', ')'), (',', ','), ('Dorland', 'NNP'), ('’', 'NNP'), ('s', 'NN'), ('Illustrated', 'NNP'), ('Medical', 'NNP'), ('Dictionary', 'NNP'), ('and', 'CC'), ('general', 'JJ'), ('English', 'NNP'), ('Dictionaries', 'NNP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['lexicon', 'created', 'using', 'MeSH', '(', 'Medical', 'Subject', 'Headings', ')', ',', 'Dorland', '’', 'Illustrated', 'Medical', 'Dictionary', 'general', 'English', 'Dictionaries', '.']

 TOTAL FILTERED TOKENS ==>  19

 ---- POST FOR FILTERED TOKENS ----

 [('lexicon', 'NN'), ('created', 'VBD'), ('using', 'VBG'), ('MeSH', 'NNP'), ('(', '('), ('Medical', 'NNP'), ('Subject', 'NNP'), ('Headings', 'NNP'), (')', ')'), (',', ','), ('Dorland', 'NNP'), ('’', 'NNP'), ('Illustrated', 'NNP'), ('Medical', 'NNP'), ('Dictionary', 'NNP'), ('general', 'JJ'), ('English', 'NNP'), ('Dictionaries', 'NNP'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['lexicon created', 'created using', 'using MeSH', 'MeSH (', '( Medical', 'Medical Subject', 'Subject Headings', 'Headings )', ') ,', ', Dorland', 'Dorland ’', '’ Illustrated', 'Illustrated Medical', 'Medical Dictionary', 'Dictionary general', 'general English', 'English Dictionaries', 'Dictionaries .'] 

 TOTAL BIGRAMS --> 18 



 ---- TRI-GRAMS ---- 

 ['lexicon created using', 'created using MeSH', 'using MeSH (', 'MeSH ( Medical', '( Medical Subject', 'Medical Subject Headings', 'Subject Headings )', 'Headings ) ,', ') , Dorland', ', Dorland ’', 'Dorland ’ Illustrated', '’ Illustrated Medical', 'Illustrated Medical Dictionary', 'Medical Dictionary general', 'Dictionary general English', 'general English Dictionaries', 'English Dictionaries .'] 

 TOTAL TRIGRAMS --> 17 



 ---- NOUN PHRASES ---- 

 ['lexicon'] 

 TOTAL NOUN PHRASES --> 1 



 ---- NER ----

 
 ORGANIZATION ---> ['MeSH']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> ['Dorland', 'English Dictionaries']
 TOTAL PERSON ENTITY --> 2 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['lexicon', 'creat', 'use', 'mesh', '(', 'medic', 'subject', 'head', ')', ',', 'dorland', '’', 'illustr', 'medic', 'dictionari', 'gener', 'english', 'dictionari', '.']

 TOTAL PORTER STEM WORDS ==> 19



 ---- SNOWBALL STEMMING ----

['lexicon', 'creat', 'use', 'mesh', '(', 'medic', 'subject', 'head', ')', ',', 'dorland', '’', 'illustr', 'medic', 'dictionari', 'general', 'english', 'dictionari', '.']

 TOTAL SNOWBALL STEM WORDS ==> 19



 ---- LEMMATIZATION ----

['lexicon', 'created', 'using', 'MeSH', '(', 'Medical', 'Subject', 'Headings', ')', ',', 'Dorland', '’', 'Illustrated', 'Medical', 'Dictionary', 'general', 'English', 'Dictionaries', '.']

 TOTAL LEMMATIZE WORDS ==> 19

************************************************************************************************************************

275 --> The Centre d’Informatique  Hospitaliere of the Hopital Cantonal de Geneve is working on an electronic archiving  environment with NLP features [84][85]. 


 ---- TOKENS ----

 ['The', 'Centre', 'd', '’', 'Informatique', 'Hospitaliere', 'of', 'the', 'Hopital', 'Cantonal', 'de', 'Geneve', 'is', 'working', 'on', 'an', 'electronic', 'archiving', 'environment', 'with', 'NLP', 'features', '[', '84', ']', '[', '85', ']', '.'] 

 TOTAL TOKENS ==> 29

 ---- POST ----

 [('The', 'DT'), ('Centre', 'NNP'), ('d', 'NN'), ('’', 'NNP'), ('Informatique', 'NNP'), ('Hospitaliere', 'NNP'), ('of', 'IN'), ('the', 'DT'), ('Hopital', 'NNP'), ('Cantonal', 'NNP'), ('de', 'IN'), ('Geneve', 'NNP'), ('is', 'VBZ'), ('working', 'VBG'), ('on', 'IN'), ('an', 'DT'), ('electronic', 'JJ'), ('archiving', 'NN'), ('environment', 'NN'), ('with', 'IN'), ('NLP', 'NNP'), ('features', 'NNS'), ('[', 'VBP'), ('84', 'CD'), (']', 'NNP'), ('[', 'VBD'), ('85', 'CD'), (']', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Centre', '’', 'Informatique', 'Hospitaliere', 'Hopital', 'Cantonal', 'de', 'Geneve', 'working', 'electronic', 'archiving', 'environment', 'NLP', 'features', '[', '84', ']', '[', '85', ']', '.']

 TOTAL FILTERED TOKENS ==>  21

 ---- POST FOR FILTERED TOKENS ----

 [('Centre', 'NNP'), ('’', 'NNP'), ('Informatique', 'NNP'), ('Hospitaliere', 'NNP'), ('Hopital', 'NNP'), ('Cantonal', 'NNP'), ('de', 'IN'), ('Geneve', 'NNP'), ('working', 'VBG'), ('electronic', 'JJ'), ('archiving', 'VBG'), ('environment', 'NN'), ('NLP', 'NNP'), ('features', 'VBZ'), ('[', '$'), ('84', 'CD'), (']', 'NNP'), ('[', 'VBD'), ('85', 'CD'), (']', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Centre ’', '’ Informatique', 'Informatique Hospitaliere', 'Hospitaliere Hopital', 'Hopital Cantonal', 'Cantonal de', 'de Geneve', 'Geneve working', 'working electronic', 'electronic archiving', 'archiving environment', 'environment NLP', 'NLP features', 'features [', '[ 84', '84 ]', '] [', '[ 85', '85 ]', '] .'] 

 TOTAL BIGRAMS --> 20 



 ---- TRI-GRAMS ---- 

 ['Centre ’ Informatique', '’ Informatique Hospitaliere', 'Informatique Hospitaliere Hopital', 'Hospitaliere Hopital Cantonal', 'Hopital Cantonal de', 'Cantonal de Geneve', 'de Geneve working', 'Geneve working electronic', 'working electronic archiving', 'electronic archiving environment', 'archiving environment NLP', 'environment NLP features', 'NLP features [', 'features [ 84', '[ 84 ]', '84 ] [', '] [ 85', '[ 85 ]', '85 ] .'] 

 TOTAL TRIGRAMS --> 19 



 ---- NOUN PHRASES ---- 

 ['environment', ']'] 

 TOTAL NOUN PHRASES --> 2 



 ---- NER ----

 
 ORGANIZATION ---> ['NLP']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> ['Centre']
 TOTAL PERSON ENTITY --> 1 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['centr', '’', 'informatiqu', 'hospitalier', 'hopit', 'canton', 'de', 'genev', 'work', 'electron', 'archiv', 'environ', 'nlp', 'featur', '[', '84', ']', '[', '85', ']', '.']

 TOTAL PORTER STEM WORDS ==> 21



 ---- SNOWBALL STEMMING ----

['centr', '’', 'informatiqu', 'hospitalier', 'hopit', 'canton', 'de', 'genev', 'work', 'electron', 'archiv', 'environ', 'nlp', 'featur', '[', '84', ']', '[', '85', ']', '.']

 TOTAL SNOWBALL STEM WORDS ==> 21



 ---- LEMMATIZATION ----

['Centre', '’', 'Informatique', 'Hospitaliere', 'Hopital', 'Cantonal', 'de', 'Geneve', 'working', 'electronic', 'archiving', 'environment', 'NLP', 'feature', '[', '84', ']', '[', '85', ']', '.']

 TOTAL LEMMATIZE WORDS ==> 21

************************************************************************************************************************

276 --> In first phase, patient records were archived . 


 ---- TOKENS ----

 ['In', 'first', 'phase', ',', 'patient', 'records', 'were', 'archived', '.'] 

 TOTAL TOKENS ==> 9

 ---- POST ----

 [('In', 'IN'), ('first', 'JJ'), ('phase', 'NN'), (',', ','), ('patient', 'NN'), ('records', 'NNS'), ('were', 'VBD'), ('archived', 'VBN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['first', 'phase', ',', 'patient', 'records', 'archived', '.']

 TOTAL FILTERED TOKENS ==>  7

 ---- POST FOR FILTERED TOKENS ----

 [('first', 'JJ'), ('phase', 'NN'), (',', ','), ('patient', 'NN'), ('records', 'NNS'), ('archived', 'VBD'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['first phase', 'phase ,', ', patient', 'patient records', 'records archived', 'archived .'] 

 TOTAL BIGRAMS --> 6 



 ---- TRI-GRAMS ---- 

 ['first phase ,', 'phase , patient', ', patient records', 'patient records archived', 'records archived .'] 

 TOTAL TRIGRAMS --> 5 



 ---- NOUN PHRASES ---- 

 ['first phase', 'patient'] 

 TOTAL NOUN PHRASES --> 2 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['first', 'phase', ',', 'patient', 'record', 'archiv', '.']

 TOTAL PORTER STEM WORDS ==> 7



 ---- SNOWBALL STEMMING ----

['first', 'phase', ',', 'patient', 'record', 'archiv', '.']

 TOTAL SNOWBALL STEM WORDS ==> 7



 ---- LEMMATIZATION ----

['first', 'phase', ',', 'patient', 'record', 'archived', '.']

 TOTAL LEMMATIZE WORDS ==> 7

************************************************************************************************************************

277 --> At  later stage the LSP-MLP has been adapted for French [86][87][88][89] , and finally , a proper  NLP system called RECIT  [90][91][92][93] has been developed using a method called  Proximity Processing [94]. 


 ---- TOKENS ----

 ['At', 'later', 'stage', 'the', 'LSP-MLP', 'has', 'been', 'adapted', 'for', 'French', '[', '86', ']', '[', '87', ']', '[', '88', ']', '[', '89', ']', ',', 'and', 'finally', ',', 'a', 'proper', 'NLP', 'system', 'called', 'RECIT', '[', '90', ']', '[', '91', ']', '[', '92', ']', '[', '93', ']', 'has', 'been', 'developed', 'using', 'a', 'method', 'called', 'Proximity', 'Processing', '[', '94', ']', '.'] 

 TOTAL TOKENS ==> 57

 ---- POST ----

 [('At', 'IN'), ('later', 'RB'), ('stage', 'VB'), ('the', 'DT'), ('LSP-MLP', 'NNP'), ('has', 'VBZ'), ('been', 'VBN'), ('adapted', 'VBN'), ('for', 'IN'), ('French', 'JJ'), ('[', 'NNP'), ('86', 'CD'), (']', 'NNP'), ('[', 'VBD'), ('87', 'CD'), (']', 'NNP'), ('[', 'VBD'), ('88', 'CD'), (']', 'NNP'), ('[', 'VBD'), ('89', 'CD'), (']', 'NN'), (',', ','), ('and', 'CC'), ('finally', 'RB'), (',', ','), ('a', 'DT'), ('proper', 'JJ'), ('NLP', 'NNP'), ('system', 'NN'), ('called', 'VBD'), ('RECIT', 'NNP'), ('[', 'NNP'), ('90', 'CD'), (']', 'NNP'), ('[', 'VBD'), ('91', 'CD'), (']', 'NNP'), ('[', 'VBD'), ('92', 'CD'), (']', 'NNP'), ('[', 'VBD'), ('93', 'CD'), (']', 'NN'), ('has', 'VBZ'), ('been', 'VBN'), ('developed', 'VBN'), ('using', 'VBG'), ('a', 'DT'), ('method', 'NN'), ('called', 'VBN'), ('Proximity', 'NNP'), ('Processing', 'NNP'), ('[', 'VBD'), ('94', 'CD'), (']', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['later', 'stage', 'LSP-MLP', 'adapted', 'French', '[', '86', ']', '[', '87', ']', '[', '88', ']', '[', '89', ']', ',', 'finally', ',', 'proper', 'NLP', 'system', 'called', 'RECIT', '[', '90', ']', '[', '91', ']', '[', '92', ']', '[', '93', ']', 'developed', 'using', 'method', 'called', 'Proximity', 'Processing', '[', '94', ']', '.']

 TOTAL FILTERED TOKENS ==>  47

 ---- POST FOR FILTERED TOKENS ----

 [('later', 'RB'), ('stage', 'NN'), ('LSP-MLP', 'JJ'), ('adapted', 'JJ'), ('French', 'JJ'), ('[', 'NN'), ('86', 'CD'), (']', 'NNP'), ('[', 'VBD'), ('87', 'CD'), (']', 'NNP'), ('[', 'VBD'), ('88', 'CD'), (']', 'NNP'), ('[', 'VBD'), ('89', 'CD'), (']', 'NN'), (',', ','), ('finally', 'RB'), (',', ','), ('proper', 'JJ'), ('NLP', 'NNP'), ('system', 'NN'), ('called', 'VBD'), ('RECIT', 'NNP'), ('[', 'NNP'), ('90', 'CD'), (']', 'NNP'), ('[', 'VBD'), ('91', 'CD'), (']', 'NNP'), ('[', 'VBD'), ('92', 'CD'), (']', 'NNP'), ('[', 'VBD'), ('93', 'CD'), (']', 'NNP'), ('developed', 'VBD'), ('using', 'VBG'), ('method', 'NN'), ('called', 'VBN'), ('Proximity', 'NNP'), ('Processing', 'NNP'), ('[', 'VBD'), ('94', 'CD'), (']', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['later stage', 'stage LSP-MLP', 'LSP-MLP adapted', 'adapted French', 'French [', '[ 86', '86 ]', '] [', '[ 87', '87 ]', '] [', '[ 88', '88 ]', '] [', '[ 89', '89 ]', '] ,', ', finally', 'finally ,', ', proper', 'proper NLP', 'NLP system', 'system called', 'called RECIT', 'RECIT [', '[ 90', '90 ]', '] [', '[ 91', '91 ]', '] [', '[ 92', '92 ]', '] [', '[ 93', '93 ]', '] developed', 'developed using', 'using method', 'method called', 'called Proximity', 'Proximity Processing', 'Processing [', '[ 94', '94 ]', '] .'] 

 TOTAL BIGRAMS --> 46 



 ---- TRI-GRAMS ---- 

 ['later stage LSP-MLP', 'stage LSP-MLP adapted', 'LSP-MLP adapted French', 'adapted French [', 'French [ 86', '[ 86 ]', '86 ] [', '] [ 87', '[ 87 ]', '87 ] [', '] [ 88', '[ 88 ]', '88 ] [', '] [ 89', '[ 89 ]', '89 ] ,', '] , finally', ', finally ,', 'finally , proper', ', proper NLP', 'proper NLP system', 'NLP system called', 'system called RECIT', 'called RECIT [', 'RECIT [ 90', '[ 90 ]', '90 ] [', '] [ 91', '[ 91 ]', '91 ] [', '] [ 92', '[ 92 ]', '92 ] [', '] [ 93', '[ 93 ]', '93 ] developed', '] developed using', 'developed using method', 'using method called', 'method called Proximity', 'called Proximity Processing', 'Proximity Processing [', 'Processing [ 94', '[ 94 ]', '94 ] .'] 

 TOTAL TRIGRAMS --> 45 



 ---- NOUN PHRASES ---- 

 ['stage', 'LSP-MLP adapted French [', ']', 'system', 'method', ']'] 

 TOTAL NOUN PHRASES --> 6 



 ---- NER ----

 
 ORGANIZATION ---> ['NLP', 'RECIT', 'Proximity']
 TOTAL ORGANIZATION ENTITY --> 3 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> ['French']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['later', 'stage', 'lsp-mlp', 'adapt', 'french', '[', '86', ']', '[', '87', ']', '[', '88', ']', '[', '89', ']', ',', 'final', ',', 'proper', 'nlp', 'system', 'call', 'recit', '[', '90', ']', '[', '91', ']', '[', '92', ']', '[', '93', ']', 'develop', 'use', 'method', 'call', 'proxim', 'process', '[', '94', ']', '.']

 TOTAL PORTER STEM WORDS ==> 47



 ---- SNOWBALL STEMMING ----

['later', 'stage', 'lsp-mlp', 'adapt', 'french', '[', '86', ']', '[', '87', ']', '[', '88', ']', '[', '89', ']', ',', 'final', ',', 'proper', 'nlp', 'system', 'call', 'recit', '[', '90', ']', '[', '91', ']', '[', '92', ']', '[', '93', ']', 'develop', 'use', 'method', 'call', 'proxim', 'process', '[', '94', ']', '.']

 TOTAL SNOWBALL STEM WORDS ==> 47



 ---- LEMMATIZATION ----

['later', 'stage', 'LSP-MLP', 'adapted', 'French', '[', '86', ']', '[', '87', ']', '[', '88', ']', '[', '89', ']', ',', 'finally', ',', 'proper', 'NLP', 'system', 'called', 'RECIT', '[', '90', ']', '[', '91', ']', '[', '92', ']', '[', '93', ']', 'developed', 'using', 'method', 'called', 'Proximity', 'Processing', '[', '94', ']', '.']

 TOTAL LEMMATIZE WORDS ==> 47

************************************************************************************************************************

278 --> It’s task was to implement a robust and multilingual system able  to analyze/comprehend medical sentences, and to preserve a knowledge of free text into a  language independent knowledge representation [95][96]. 


 ---- TOKENS ----

 ['It', '’', 's', 'task', 'was', 'to', 'implement', 'a', 'robust', 'and', 'multilingual', 'system', 'able', 'to', 'analyze/comprehend', 'medical', 'sentences', ',', 'and', 'to', 'preserve', 'a', 'knowledge', 'of', 'free', 'text', 'into', 'a', 'language', 'independent', 'knowledge', 'representation', '[', '95', ']', '[', '96', ']', '.'] 

 TOTAL TOKENS ==> 39

 ---- POST ----

 [('It', 'PRP'), ('’', 'VBZ'), ('s', 'JJ'), ('task', 'NN'), ('was', 'VBD'), ('to', 'TO'), ('implement', 'VB'), ('a', 'DT'), ('robust', 'JJ'), ('and', 'CC'), ('multilingual', 'JJ'), ('system', 'NN'), ('able', 'JJ'), ('to', 'TO'), ('analyze/comprehend', 'VB'), ('medical', 'JJ'), ('sentences', 'NNS'), (',', ','), ('and', 'CC'), ('to', 'TO'), ('preserve', 'VB'), ('a', 'DT'), ('knowledge', 'NN'), ('of', 'IN'), ('free', 'JJ'), ('text', 'NN'), ('into', 'IN'), ('a', 'DT'), ('language', 'NN'), ('independent', 'JJ'), ('knowledge', 'NN'), ('representation', 'NN'), ('[', 'VBD'), ('95', 'CD'), (']', 'NNP'), ('[', 'VBD'), ('96', 'CD'), (']', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['’', 'task', 'implement', 'robust', 'multilingual', 'system', 'able', 'analyze/comprehend', 'medical', 'sentences', ',', 'preserve', 'knowledge', 'free', 'text', 'language', 'independent', 'knowledge', 'representation', '[', '95', ']', '[', '96', ']', '.']

 TOTAL FILTERED TOKENS ==>  26

 ---- POST FOR FILTERED TOKENS ----

 [('’', 'JJ'), ('task', 'NN'), ('implement', 'NN'), ('robust', 'VBP'), ('multilingual', 'JJ'), ('system', 'NN'), ('able', 'JJ'), ('analyze/comprehend', 'RB'), ('medical', 'JJ'), ('sentences', 'NNS'), (',', ','), ('preserve', 'VB'), ('knowledge', 'NNP'), ('free', 'JJ'), ('text', 'NN'), ('language', 'NN'), ('independent', 'JJ'), ('knowledge', 'NN'), ('representation', 'NN'), ('[', 'VBD'), ('95', 'CD'), (']', 'NNP'), ('[', 'VBD'), ('96', 'CD'), (']', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['’ task', 'task implement', 'implement robust', 'robust multilingual', 'multilingual system', 'system able', 'able analyze/comprehend', 'analyze/comprehend medical', 'medical sentences', 'sentences ,', ', preserve', 'preserve knowledge', 'knowledge free', 'free text', 'text language', 'language independent', 'independent knowledge', 'knowledge representation', 'representation [', '[ 95', '95 ]', '] [', '[ 96', '96 ]', '] .'] 

 TOTAL BIGRAMS --> 25 



 ---- TRI-GRAMS ---- 

 ['’ task implement', 'task implement robust', 'implement robust multilingual', 'robust multilingual system', 'multilingual system able', 'system able analyze/comprehend', 'able analyze/comprehend medical', 'analyze/comprehend medical sentences', 'medical sentences ,', 'sentences , preserve', ', preserve knowledge', 'preserve knowledge free', 'knowledge free text', 'free text language', 'text language independent', 'language independent knowledge', 'independent knowledge representation', 'knowledge representation [', 'representation [ 95', '[ 95 ]', '95 ] [', '] [ 96', '[ 96 ]', '96 ] .'] 

 TOTAL TRIGRAMS --> 24 



 ---- NOUN PHRASES ---- 

 ['’ task', 'implement', 'multilingual system', 'free text', 'language', 'independent knowledge', 'representation', ']'] 

 TOTAL NOUN PHRASES --> 8 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['’', 'task', 'implement', 'robust', 'multilingu', 'system', 'abl', 'analyze/comprehend', 'medic', 'sentenc', ',', 'preserv', 'knowledg', 'free', 'text', 'languag', 'independ', 'knowledg', 'represent', '[', '95', ']', '[', '96', ']', '.']

 TOTAL PORTER STEM WORDS ==> 26



 ---- SNOWBALL STEMMING ----

['’', 'task', 'implement', 'robust', 'multilingu', 'system', 'abl', 'analyze/comprehend', 'medic', 'sentenc', ',', 'preserv', 'knowledg', 'free', 'text', 'languag', 'independ', 'knowledg', 'represent', '[', '95', ']', '[', '96', ']', '.']

 TOTAL SNOWBALL STEM WORDS ==> 26



 ---- LEMMATIZATION ----

['’', 'task', 'implement', 'robust', 'multilingual', 'system', 'able', 'analyze/comprehend', 'medical', 'sentence', ',', 'preserve', 'knowledge', 'free', 'text', 'language', 'independent', 'knowledge', 'representation', '[', '95', ']', '[', '96', ']', '.']

 TOTAL LEMMATIZE WORDS ==> 26

************************************************************************************************************************

279 --> The Columbia university of New  York has developed an NLP system called MEDLEE (MEDical Language Extraction and  Encoding System) that identifies clinical information in narrative reports and transforms the  textual information into structured representation [97]. 


 ---- TOKENS ----

 ['The', 'Columbia', 'university', 'of', 'New', 'York', 'has', 'developed', 'an', 'NLP', 'system', 'called', 'MEDLEE', '(', 'MEDical', 'Language', 'Extraction', 'and', 'Encoding', 'System', ')', 'that', 'identifies', 'clinical', 'information', 'in', 'narrative', 'reports', 'and', 'transforms', 'the', 'textual', 'information', 'into', 'structured', 'representation', '[', '97', ']', '.'] 

 TOTAL TOKENS ==> 40

 ---- POST ----

 [('The', 'DT'), ('Columbia', 'NNP'), ('university', 'NN'), ('of', 'IN'), ('New', 'NNP'), ('York', 'NNP'), ('has', 'VBZ'), ('developed', 'VBN'), ('an', 'DT'), ('NLP', 'NNP'), ('system', 'NN'), ('called', 'VBN'), ('MEDLEE', 'NNP'), ('(', '('), ('MEDical', 'NNP'), ('Language', 'NNP'), ('Extraction', 'NNP'), ('and', 'CC'), ('Encoding', 'NNP'), ('System', 'NNP'), (')', ')'), ('that', 'IN'), ('identifies', 'VBZ'), ('clinical', 'JJ'), ('information', 'NN'), ('in', 'IN'), ('narrative', 'JJ'), ('reports', 'NNS'), ('and', 'CC'), ('transforms', 'VBZ'), ('the', 'DT'), ('textual', 'JJ'), ('information', 'NN'), ('into', 'IN'), ('structured', 'JJ'), ('representation', 'NN'), ('[', 'VBD'), ('97', 'CD'), (']', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Columbia', 'university', 'New', 'York', 'developed', 'NLP', 'system', 'called', 'MEDLEE', '(', 'MEDical', 'Language', 'Extraction', 'Encoding', 'System', ')', 'identifies', 'clinical', 'information', 'narrative', 'reports', 'transforms', 'textual', 'information', 'structured', 'representation', '[', '97', ']', '.']

 TOTAL FILTERED TOKENS ==>  30

 ---- POST FOR FILTERED TOKENS ----

 [('Columbia', 'NNP'), ('university', 'JJ'), ('New', 'NNP'), ('York', 'NNP'), ('developed', 'VBD'), ('NLP', 'NNP'), ('system', 'NN'), ('called', 'VBN'), ('MEDLEE', 'NNP'), ('(', '('), ('MEDical', 'NNP'), ('Language', 'NNP'), ('Extraction', 'NNP'), ('Encoding', 'NNP'), ('System', 'NNP'), (')', ')'), ('identifies', 'VBZ'), ('clinical', 'JJ'), ('information', 'NN'), ('narrative', 'JJ'), ('reports', 'NNS'), ('transforms', 'NNS'), ('textual', 'JJ'), ('information', 'NN'), ('structured', 'VBN'), ('representation', 'NN'), ('[', 'VBZ'), ('97', 'CD'), (']', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Columbia university', 'university New', 'New York', 'York developed', 'developed NLP', 'NLP system', 'system called', 'called MEDLEE', 'MEDLEE (', '( MEDical', 'MEDical Language', 'Language Extraction', 'Extraction Encoding', 'Encoding System', 'System )', ') identifies', 'identifies clinical', 'clinical information', 'information narrative', 'narrative reports', 'reports transforms', 'transforms textual', 'textual information', 'information structured', 'structured representation', 'representation [', '[ 97', '97 ]', '] .'] 

 TOTAL BIGRAMS --> 29 



 ---- TRI-GRAMS ---- 

 ['Columbia university New', 'university New York', 'New York developed', 'York developed NLP', 'developed NLP system', 'NLP system called', 'system called MEDLEE', 'called MEDLEE (', 'MEDLEE ( MEDical', '( MEDical Language', 'MEDical Language Extraction', 'Language Extraction Encoding', 'Extraction Encoding System', 'Encoding System )', 'System ) identifies', ') identifies clinical', 'identifies clinical information', 'clinical information narrative', 'information narrative reports', 'narrative reports transforms', 'reports transforms textual', 'transforms textual information', 'textual information structured', 'information structured representation', 'structured representation [', 'representation [ 97', '[ 97 ]', '97 ] .'] 

 TOTAL TRIGRAMS --> 28 



 ---- NOUN PHRASES ---- 

 ['system', 'clinical information', 'textual information', 'representation', ']'] 

 TOTAL NOUN PHRASES --> 5 



 ---- NER ----

 
 ORGANIZATION ---> ['NLP', 'MEDLEE']
 TOTAL ORGANIZATION ENTITY --> 2 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> ['Columbia', 'New York']
 TOTAL GPE ENTITY --> 2 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['columbia', 'univers', 'new', 'york', 'develop', 'nlp', 'system', 'call', 'medle', '(', 'medic', 'languag', 'extract', 'encod', 'system', ')', 'identifi', 'clinic', 'inform', 'narr', 'report', 'transform', 'textual', 'inform', 'structur', 'represent', '[', '97', ']', '.']

 TOTAL PORTER STEM WORDS ==> 30



 ---- SNOWBALL STEMMING ----

['columbia', 'univers', 'new', 'york', 'develop', 'nlp', 'system', 'call', 'medle', '(', 'medic', 'languag', 'extract', 'encod', 'system', ')', 'identifi', 'clinic', 'inform', 'narrat', 'report', 'transform', 'textual', 'inform', 'structur', 'represent', '[', '97', ']', '.']

 TOTAL SNOWBALL STEM WORDS ==> 30



 ---- LEMMATIZATION ----

['Columbia', 'university', 'New', 'York', 'developed', 'NLP', 'system', 'called', 'MEDLEE', '(', 'MEDical', 'Language', 'Extraction', 'Encoding', 'System', ')', 'identifies', 'clinical', 'information', 'narrative', 'report', 'transforms', 'textual', 'information', 'structured', 'representation', '[', '97', ']', '.']

 TOTAL LEMMATIZE WORDS ==> 30

************************************************************************************************************************

280 --> 7. 


 ---- TOKENS ----

 ['7', '.'] 

 TOTAL TOKENS ==> 2

 ---- POST ----

 [('7', 'CD'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['7', '.']

 TOTAL FILTERED TOKENS ==>  2

 ---- POST FOR FILTERED TOKENS ----

 [('7', 'CD'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['7 .'] 

 TOTAL BIGRAMS --> 1 



 ---- TRI-GRAMS ---- 

 [] 

 TOTAL TRIGRAMS --> 0 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['7', '.']

 TOTAL PORTER STEM WORDS ==> 2



 ---- SNOWBALL STEMMING ----

['7', '.']

 TOTAL SNOWBALL STEM WORDS ==> 2



 ---- LEMMATIZATION ----

['7', '.']

 TOTAL LEMMATIZE WORDS ==> 2

************************************************************************************************************************

281 --> Approaches  Rationalist approach or symbolic approach assume that crucial part of the knowledge in the  human mind is not derived by the sense but is firm in advance, probably by genetic in  heritance. 


 ---- TOKENS ----

 ['Approaches', 'Rationalist', 'approach', 'or', 'symbolic', 'approach', 'assume', 'that', 'crucial', 'part', 'of', 'the', 'knowledge', 'in', 'the', 'human', 'mind', 'is', 'not', 'derived', 'by', 'the', 'sense', 'but', 'is', 'firm', 'in', 'advance', ',', 'probably', 'by', 'genetic', 'in', 'heritance', '.'] 

 TOTAL TOKENS ==> 35

 ---- POST ----

 [('Approaches', 'NNS'), ('Rationalist', 'NNP'), ('approach', 'NN'), ('or', 'CC'), ('symbolic', 'JJ'), ('approach', 'NN'), ('assume', 'VBP'), ('that', 'IN'), ('crucial', 'JJ'), ('part', 'NN'), ('of', 'IN'), ('the', 'DT'), ('knowledge', 'NN'), ('in', 'IN'), ('the', 'DT'), ('human', 'JJ'), ('mind', 'NN'), ('is', 'VBZ'), ('not', 'RB'), ('derived', 'VBN'), ('by', 'IN'), ('the', 'DT'), ('sense', 'NN'), ('but', 'CC'), ('is', 'VBZ'), ('firm', 'NN'), ('in', 'IN'), ('advance', 'NN'), (',', ','), ('probably', 'RB'), ('by', 'IN'), ('genetic', 'JJ'), ('in', 'IN'), ('heritance', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Approaches', 'Rationalist', 'approach', 'symbolic', 'approach', 'assume', 'crucial', 'part', 'knowledge', 'human', 'mind', 'derived', 'sense', 'firm', 'advance', ',', 'probably', 'genetic', 'heritance', '.']

 TOTAL FILTERED TOKENS ==>  20

 ---- POST FOR FILTERED TOKENS ----

 [('Approaches', 'NNS'), ('Rationalist', 'NNP'), ('approach', 'NN'), ('symbolic', 'JJ'), ('approach', 'NN'), ('assume', 'VBP'), ('crucial', 'JJ'), ('part', 'NN'), ('knowledge', 'NN'), ('human', 'JJ'), ('mind', 'NN'), ('derived', 'VBD'), ('sense', 'JJ'), ('firm', 'NN'), ('advance', 'NN'), (',', ','), ('probably', 'RB'), ('genetic', 'JJ'), ('heritance', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Approaches Rationalist', 'Rationalist approach', 'approach symbolic', 'symbolic approach', 'approach assume', 'assume crucial', 'crucial part', 'part knowledge', 'knowledge human', 'human mind', 'mind derived', 'derived sense', 'sense firm', 'firm advance', 'advance ,', ', probably', 'probably genetic', 'genetic heritance', 'heritance .'] 

 TOTAL BIGRAMS --> 19 



 ---- TRI-GRAMS ---- 

 ['Approaches Rationalist approach', 'Rationalist approach symbolic', 'approach symbolic approach', 'symbolic approach assume', 'approach assume crucial', 'assume crucial part', 'crucial part knowledge', 'part knowledge human', 'knowledge human mind', 'human mind derived', 'mind derived sense', 'derived sense firm', 'sense firm advance', 'firm advance ,', 'advance , probably', ', probably genetic', 'probably genetic heritance', 'genetic heritance .'] 

 TOTAL TRIGRAMS --> 18 



 ---- NOUN PHRASES ---- 

 ['approach', 'symbolic approach', 'crucial part', 'knowledge', 'human mind', 'sense firm', 'advance', 'genetic heritance'] 

 TOTAL NOUN PHRASES --> 8 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['approach', 'rationalist', 'approach', 'symbol', 'approach', 'assum', 'crucial', 'part', 'knowledg', 'human', 'mind', 'deriv', 'sens', 'firm', 'advanc', ',', 'probabl', 'genet', 'herit', '.']

 TOTAL PORTER STEM WORDS ==> 20



 ---- SNOWBALL STEMMING ----

['approach', 'rationalist', 'approach', 'symbol', 'approach', 'assum', 'crucial', 'part', 'knowledg', 'human', 'mind', 'deriv', 'sens', 'firm', 'advanc', ',', 'probabl', 'genet', 'herit', '.']

 TOTAL SNOWBALL STEM WORDS ==> 20



 ---- LEMMATIZATION ----

['Approaches', 'Rationalist', 'approach', 'symbolic', 'approach', 'assume', 'crucial', 'part', 'knowledge', 'human', 'mind', 'derived', 'sense', 'firm', 'advance', ',', 'probably', 'genetic', 'heritance', '.']

 TOTAL LEMMATIZE WORDS ==> 20

************************************************************************************************************************

282 --> Noam Chomsky was the strongest advocate of this approach. 


 ---- TOKENS ----

 ['Noam', 'Chomsky', 'was', 'the', 'strongest', 'advocate', 'of', 'this', 'approach', '.'] 

 TOTAL TOKENS ==> 10

 ---- POST ----

 [('Noam', 'NNP'), ('Chomsky', 'NNP'), ('was', 'VBD'), ('the', 'DT'), ('strongest', 'JJS'), ('advocate', 'NN'), ('of', 'IN'), ('this', 'DT'), ('approach', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Noam', 'Chomsky', 'strongest', 'advocate', 'approach', '.']

 TOTAL FILTERED TOKENS ==>  6

 ---- POST FOR FILTERED TOKENS ----

 [('Noam', 'NNP'), ('Chomsky', 'NNP'), ('strongest', 'JJS'), ('advocate', 'NN'), ('approach', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Noam Chomsky', 'Chomsky strongest', 'strongest advocate', 'advocate approach', 'approach .'] 

 TOTAL BIGRAMS --> 5 



 ---- TRI-GRAMS ---- 

 ['Noam Chomsky strongest', 'Chomsky strongest advocate', 'strongest advocate approach', 'advocate approach .'] 

 TOTAL TRIGRAMS --> 4 



 ---- NOUN PHRASES ---- 

 ['advocate', 'approach'] 

 TOTAL NOUN PHRASES --> 2 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> ['Noam', 'Chomsky']
 TOTAL PERSON ENTITY --> 2 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['noam', 'chomski', 'strongest', 'advoc', 'approach', '.']

 TOTAL PORTER STEM WORDS ==> 6



 ---- SNOWBALL STEMMING ----

['noam', 'chomski', 'strongest', 'advoc', 'approach', '.']

 TOTAL SNOWBALL STEM WORDS ==> 6



 ---- LEMMATIZATION ----

['Noam', 'Chomsky', 'strongest', 'advocate', 'approach', '.']

 TOTAL LEMMATIZE WORDS ==> 6

************************************************************************************************************************

283 --> It was trusted that  machine can be  made to function like human brain by giving some fundamental knowledge  and reasoning mechanism linguistics  knowledge is directly encoded in rule or other forms of  representation. 


 ---- TOKENS ----

 ['It', 'was', 'trusted', 'that', 'machine', 'can', 'be', 'made', 'to', 'function', 'like', 'human', 'brain', 'by', 'giving', 'some', 'fundamental', 'knowledge', 'and', 'reasoning', 'mechanism', 'linguistics', 'knowledge', 'is', 'directly', 'encoded', 'in', 'rule', 'or', 'other', 'forms', 'of', 'representation', '.'] 

 TOTAL TOKENS ==> 34

 ---- POST ----

 [('It', 'PRP'), ('was', 'VBD'), ('trusted', 'VBN'), ('that', 'IN'), ('machine', 'NN'), ('can', 'MD'), ('be', 'VB'), ('made', 'VBN'), ('to', 'TO'), ('function', 'VB'), ('like', 'IN'), ('human', 'JJ'), ('brain', 'NN'), ('by', 'IN'), ('giving', 'VBG'), ('some', 'DT'), ('fundamental', 'JJ'), ('knowledge', 'NN'), ('and', 'CC'), ('reasoning', 'VBG'), ('mechanism', 'NN'), ('linguistics', 'NNS'), ('knowledge', 'NN'), ('is', 'VBZ'), ('directly', 'RB'), ('encoded', 'VBN'), ('in', 'IN'), ('rule', 'NN'), ('or', 'CC'), ('other', 'JJ'), ('forms', 'NNS'), ('of', 'IN'), ('representation', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['trusted', 'machine', 'made', 'function', 'like', 'human', 'brain', 'giving', 'fundamental', 'knowledge', 'reasoning', 'mechanism', 'linguistics', 'knowledge', 'directly', 'encoded', 'rule', 'forms', 'representation', '.']

 TOTAL FILTERED TOKENS ==>  20

 ---- POST FOR FILTERED TOKENS ----

 [('trusted', 'VBN'), ('machine', 'NN'), ('made', 'VBN'), ('function', 'NN'), ('like', 'IN'), ('human', 'JJ'), ('brain', 'NN'), ('giving', 'VBG'), ('fundamental', 'JJ'), ('knowledge', 'NN'), ('reasoning', 'VBG'), ('mechanism', 'NN'), ('linguistics', 'NNS'), ('knowledge', 'VBP'), ('directly', 'RB'), ('encoded', 'VBN'), ('rule', 'NN'), ('forms', 'NNS'), ('representation', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['trusted machine', 'machine made', 'made function', 'function like', 'like human', 'human brain', 'brain giving', 'giving fundamental', 'fundamental knowledge', 'knowledge reasoning', 'reasoning mechanism', 'mechanism linguistics', 'linguistics knowledge', 'knowledge directly', 'directly encoded', 'encoded rule', 'rule forms', 'forms representation', 'representation .'] 

 TOTAL BIGRAMS --> 19 



 ---- TRI-GRAMS ---- 

 ['trusted machine made', 'machine made function', 'made function like', 'function like human', 'like human brain', 'human brain giving', 'brain giving fundamental', 'giving fundamental knowledge', 'fundamental knowledge reasoning', 'knowledge reasoning mechanism', 'reasoning mechanism linguistics', 'mechanism linguistics knowledge', 'linguistics knowledge directly', 'knowledge directly encoded', 'directly encoded rule', 'encoded rule forms', 'rule forms representation', 'forms representation .'] 

 TOTAL TRIGRAMS --> 18 



 ---- NOUN PHRASES ---- 

 ['machine', 'function', 'human brain', 'fundamental knowledge', 'mechanism', 'rule', 'representation'] 

 TOTAL NOUN PHRASES --> 7 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['trust', 'machin', 'made', 'function', 'like', 'human', 'brain', 'give', 'fundament', 'knowledg', 'reason', 'mechan', 'linguist', 'knowledg', 'directli', 'encod', 'rule', 'form', 'represent', '.']

 TOTAL PORTER STEM WORDS ==> 20



 ---- SNOWBALL STEMMING ----

['trust', 'machin', 'made', 'function', 'like', 'human', 'brain', 'give', 'fundament', 'knowledg', 'reason', 'mechan', 'linguist', 'knowledg', 'direct', 'encod', 'rule', 'form', 'represent', '.']

 TOTAL SNOWBALL STEM WORDS ==> 20



 ---- LEMMATIZATION ----

['trusted', 'machine', 'made', 'function', 'like', 'human', 'brain', 'giving', 'fundamental', 'knowledge', 'reasoning', 'mechanism', 'linguistics', 'knowledge', 'directly', 'encoded', 'rule', 'form', 'representation', '.']

 TOTAL LEMMATIZE WORDS ==> 20

************************************************************************************************************************

284 --> This helps automatic process of natural languages. 


 ---- TOKENS ----

 ['This', 'helps', 'automatic', 'process', 'of', 'natural', 'languages', '.'] 

 TOTAL TOKENS ==> 8

 ---- POST ----

 [('This', 'DT'), ('helps', 'VBZ'), ('automatic', 'JJ'), ('process', 'NN'), ('of', 'IN'), ('natural', 'JJ'), ('languages', 'NNS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['helps', 'automatic', 'process', 'natural', 'languages', '.']

 TOTAL FILTERED TOKENS ==>  6

 ---- POST FOR FILTERED TOKENS ----

 [('helps', 'VBZ'), ('automatic', 'JJ'), ('process', 'NN'), ('natural', 'JJ'), ('languages', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['helps automatic', 'automatic process', 'process natural', 'natural languages', 'languages .'] 

 TOTAL BIGRAMS --> 5 



 ---- TRI-GRAMS ---- 

 ['helps automatic process', 'automatic process natural', 'process natural languages', 'natural languages .'] 

 TOTAL TRIGRAMS --> 4 



 ---- NOUN PHRASES ---- 

 ['automatic process'] 

 TOTAL NOUN PHRASES --> 1 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['help', 'automat', 'process', 'natur', 'languag', '.']

 TOTAL PORTER STEM WORDS ==> 6



 ---- SNOWBALL STEMMING ----

['help', 'automat', 'process', 'natur', 'languag', '.']

 TOTAL SNOWBALL STEM WORDS ==> 6



 ---- LEMMATIZATION ----

['help', 'automatic', 'process', 'natural', 'language', '.']

 TOTAL LEMMATIZE WORDS ==> 6

************************************************************************************************************************

285 --> [98] Statistical and  machine learning entail evolution of algorithms that allow a program to infer patterns. 


 ---- TOKENS ----

 ['[', '98', ']', 'Statistical', 'and', 'machine', 'learning', 'entail', 'evolution', 'of', 'algorithms', 'that', 'allow', 'a', 'program', 'to', 'infer', 'patterns', '.'] 

 TOTAL TOKENS ==> 19

 ---- POST ----

 [('[', 'RB'), ('98', 'CD'), (']', 'JJ'), ('Statistical', 'NNP'), ('and', 'CC'), ('machine', 'NN'), ('learning', 'NN'), ('entail', 'JJ'), ('evolution', 'NN'), ('of', 'IN'), ('algorithms', 'NN'), ('that', 'WDT'), ('allow', 'VBP'), ('a', 'DT'), ('program', 'NN'), ('to', 'TO'), ('infer', 'VB'), ('patterns', 'NNS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['[', '98', ']', 'Statistical', 'machine', 'learning', 'entail', 'evolution', 'algorithms', 'allow', 'program', 'infer', 'patterns', '.']

 TOTAL FILTERED TOKENS ==>  14

 ---- POST FOR FILTERED TOKENS ----

 [('[', 'RB'), ('98', 'CD'), (']', 'JJ'), ('Statistical', 'NNP'), ('machine', 'NN'), ('learning', 'VBG'), ('entail', 'JJ'), ('evolution', 'NN'), ('algorithms', 'NN'), ('allow', 'JJ'), ('program', 'NN'), ('infer', 'NN'), ('patterns', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['[ 98', '98 ]', '] Statistical', 'Statistical machine', 'machine learning', 'learning entail', 'entail evolution', 'evolution algorithms', 'algorithms allow', 'allow program', 'program infer', 'infer patterns', 'patterns .'] 

 TOTAL BIGRAMS --> 13 



 ---- TRI-GRAMS ---- 

 ['[ 98 ]', '98 ] Statistical', '] Statistical machine', 'Statistical machine learning', 'machine learning entail', 'learning entail evolution', 'entail evolution algorithms', 'evolution algorithms allow', 'algorithms allow program', 'allow program infer', 'program infer patterns', 'infer patterns .'] 

 TOTAL TRIGRAMS --> 12 



 ---- NOUN PHRASES ---- 

 ['machine', 'entail evolution', 'algorithms', 'allow program', 'infer'] 

 TOTAL NOUN PHRASES --> 5 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['[', '98', ']', 'statist', 'machin', 'learn', 'entail', 'evolut', 'algorithm', 'allow', 'program', 'infer', 'pattern', '.']

 TOTAL PORTER STEM WORDS ==> 14



 ---- SNOWBALL STEMMING ----

['[', '98', ']', 'statist', 'machin', 'learn', 'entail', 'evolut', 'algorithm', 'allow', 'program', 'infer', 'pattern', '.']

 TOTAL SNOWBALL STEM WORDS ==> 14



 ---- LEMMATIZATION ----

['[', '98', ']', 'Statistical', 'machine', 'learning', 'entail', 'evolution', 'algorithm', 'allow', 'program', 'infer', 'pattern', '.']

 TOTAL LEMMATIZE WORDS ==> 14

************************************************************************************************************************

286 --> An  iterative process is used to characterize a given algorithm’s underlying algorithm that are  optimised by a numerical measure that characterize numerical parameters and learning phase. 


 ---- TOKENS ----

 ['An', 'iterative', 'process', 'is', 'used', 'to', 'characterize', 'a', 'given', 'algorithm', '’', 's', 'underlying', 'algorithm', 'that', 'are', 'optimised', 'by', 'a', 'numerical', 'measure', 'that', 'characterize', 'numerical', 'parameters', 'and', 'learning', 'phase', '.'] 

 TOTAL TOKENS ==> 29

 ---- POST ----

 [('An', 'DT'), ('iterative', 'JJ'), ('process', 'NN'), ('is', 'VBZ'), ('used', 'VBN'), ('to', 'TO'), ('characterize', 'VB'), ('a', 'DT'), ('given', 'VBN'), ('algorithm', 'NN'), ('’', 'NNP'), ('s', 'NN'), ('underlying', 'VBG'), ('algorithm', 'NN'), ('that', 'WDT'), ('are', 'VBP'), ('optimised', 'VBN'), ('by', 'IN'), ('a', 'DT'), ('numerical', 'JJ'), ('measure', 'NN'), ('that', 'WDT'), ('characterize', 'VBZ'), ('numerical', 'JJ'), ('parameters', 'NNS'), ('and', 'CC'), ('learning', 'VBG'), ('phase', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['iterative', 'process', 'used', 'characterize', 'given', 'algorithm', '’', 'underlying', 'algorithm', 'optimised', 'numerical', 'measure', 'characterize', 'numerical', 'parameters', 'learning', 'phase', '.']

 TOTAL FILTERED TOKENS ==>  18

 ---- POST FOR FILTERED TOKENS ----

 [('iterative', 'JJ'), ('process', 'NN'), ('used', 'VBN'), ('characterize', 'VB'), ('given', 'VBN'), ('algorithm', 'RP'), ('’', 'JJ'), ('underlying', 'VBG'), ('algorithm', 'NN'), ('optimised', 'VBD'), ('numerical', 'JJ'), ('measure', 'NN'), ('characterize', 'VB'), ('numerical', 'JJ'), ('parameters', 'NNS'), ('learning', 'VBG'), ('phase', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['iterative process', 'process used', 'used characterize', 'characterize given', 'given algorithm', 'algorithm ’', '’ underlying', 'underlying algorithm', 'algorithm optimised', 'optimised numerical', 'numerical measure', 'measure characterize', 'characterize numerical', 'numerical parameters', 'parameters learning', 'learning phase', 'phase .'] 

 TOTAL BIGRAMS --> 17 



 ---- TRI-GRAMS ---- 

 ['iterative process used', 'process used characterize', 'used characterize given', 'characterize given algorithm', 'given algorithm ’', 'algorithm ’ underlying', '’ underlying algorithm', 'underlying algorithm optimised', 'algorithm optimised numerical', 'optimised numerical measure', 'numerical measure characterize', 'measure characterize numerical', 'characterize numerical parameters', 'numerical parameters learning', 'parameters learning phase', 'learning phase .'] 

 TOTAL TRIGRAMS --> 16 



 ---- NOUN PHRASES ---- 

 ['iterative process', 'algorithm', 'numerical measure', 'phase'] 

 TOTAL NOUN PHRASES --> 4 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['iter', 'process', 'use', 'character', 'given', 'algorithm', '’', 'underli', 'algorithm', 'optimis', 'numer', 'measur', 'character', 'numer', 'paramet', 'learn', 'phase', '.']

 TOTAL PORTER STEM WORDS ==> 18



 ---- SNOWBALL STEMMING ----

['iter', 'process', 'use', 'character', 'given', 'algorithm', '’', 'under', 'algorithm', 'optimis', 'numer', 'measur', 'character', 'numer', 'paramet', 'learn', 'phase', '.']

 TOTAL SNOWBALL STEM WORDS ==> 18



 ---- LEMMATIZATION ----

['iterative', 'process', 'used', 'characterize', 'given', 'algorithm', '’', 'underlying', 'algorithm', 'optimised', 'numerical', 'measure', 'characterize', 'numerical', 'parameter', 'learning', 'phase', '.']

 TOTAL LEMMATIZE WORDS ==> 18

************************************************************************************************************************

287 --> Machine-learning models can be predominantly categorized as either generative or  discriminative. 


 ---- TOKENS ----

 ['Machine-learning', 'models', 'can', 'be', 'predominantly', 'categorized', 'as', 'either', 'generative', 'or', 'discriminative', '.'] 

 TOTAL TOKENS ==> 12

 ---- POST ----

 [('Machine-learning', 'JJ'), ('models', 'NNS'), ('can', 'MD'), ('be', 'VB'), ('predominantly', 'RB'), ('categorized', 'VBN'), ('as', 'IN'), ('either', 'DT'), ('generative', 'NN'), ('or', 'CC'), ('discriminative', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Machine-learning', 'models', 'predominantly', 'categorized', 'either', 'generative', 'discriminative', '.']

 TOTAL FILTERED TOKENS ==>  8

 ---- POST FOR FILTERED TOKENS ----

 [('Machine-learning', 'JJ'), ('models', 'NNS'), ('predominantly', 'RB'), ('categorized', 'VBD'), ('either', 'CC'), ('generative', 'JJ'), ('discriminative', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Machine-learning models', 'models predominantly', 'predominantly categorized', 'categorized either', 'either generative', 'generative discriminative', 'discriminative .'] 

 TOTAL BIGRAMS --> 7 



 ---- TRI-GRAMS ---- 

 ['Machine-learning models predominantly', 'models predominantly categorized', 'predominantly categorized either', 'categorized either generative', 'either generative discriminative', 'generative discriminative .'] 

 TOTAL TRIGRAMS --> 6 



 ---- NOUN PHRASES ---- 

 ['generative discriminative'] 

 TOTAL NOUN PHRASES --> 1 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['machine-learn', 'model', 'predominantli', 'categor', 'either', 'gener', 'discrimin', '.']

 TOTAL PORTER STEM WORDS ==> 8



 ---- SNOWBALL STEMMING ----

['machine-learn', 'model', 'predomin', 'categor', 'either', 'generat', 'discrimin', '.']

 TOTAL SNOWBALL STEM WORDS ==> 8



 ---- LEMMATIZATION ----

['Machine-learning', 'model', 'predominantly', 'categorized', 'either', 'generative', 'discriminative', '.']

 TOTAL LEMMATIZE WORDS ==> 8

************************************************************************************************************************

288 --> Generative methods can generate synthetic data because of which they create  rich models of probability distributions. 


 ---- TOKENS ----

 ['Generative', 'methods', 'can', 'generate', 'synthetic', 'data', 'because', 'of', 'which', 'they', 'create', 'rich', 'models', 'of', 'probability', 'distributions', '.'] 

 TOTAL TOKENS ==> 17

 ---- POST ----

 [('Generative', 'JJ'), ('methods', 'NNS'), ('can', 'MD'), ('generate', 'VB'), ('synthetic', 'JJ'), ('data', 'NNS'), ('because', 'IN'), ('of', 'IN'), ('which', 'WDT'), ('they', 'PRP'), ('create', 'VBP'), ('rich', 'JJ'), ('models', 'NNS'), ('of', 'IN'), ('probability', 'NN'), ('distributions', 'NNS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Generative', 'methods', 'generate', 'synthetic', 'data', 'create', 'rich', 'models', 'probability', 'distributions', '.']

 TOTAL FILTERED TOKENS ==>  11

 ---- POST FOR FILTERED TOKENS ----

 [('Generative', 'JJ'), ('methods', 'NNS'), ('generate', 'VBP'), ('synthetic', 'JJ'), ('data', 'NNS'), ('create', 'NN'), ('rich', 'JJ'), ('models', 'NNS'), ('probability', 'NN'), ('distributions', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Generative methods', 'methods generate', 'generate synthetic', 'synthetic data', 'data create', 'create rich', 'rich models', 'models probability', 'probability distributions', 'distributions .'] 

 TOTAL BIGRAMS --> 10 



 ---- TRI-GRAMS ---- 

 ['Generative methods generate', 'methods generate synthetic', 'generate synthetic data', 'synthetic data create', 'data create rich', 'create rich models', 'rich models probability', 'models probability distributions', 'probability distributions .'] 

 TOTAL TRIGRAMS --> 9 



 ---- NOUN PHRASES ---- 

 ['create', 'probability'] 

 TOTAL NOUN PHRASES --> 2 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['gener', 'method', 'gener', 'synthet', 'data', 'creat', 'rich', 'model', 'probabl', 'distribut', '.']

 TOTAL PORTER STEM WORDS ==> 11



 ---- SNOWBALL STEMMING ----

['generat', 'method', 'generat', 'synthet', 'data', 'creat', 'rich', 'model', 'probabl', 'distribut', '.']

 TOTAL SNOWBALL STEM WORDS ==> 11



 ---- LEMMATIZATION ----

['Generative', 'method', 'generate', 'synthetic', 'data', 'create', 'rich', 'model', 'probability', 'distribution', '.']

 TOTAL LEMMATIZE WORDS ==> 11

************************************************************************************************************************

289 --> Discriminative methods are more functional and  have right estimating posterior probabilities and are based on observations. 


 ---- TOKENS ----

 ['Discriminative', 'methods', 'are', 'more', 'functional', 'and', 'have', 'right', 'estimating', 'posterior', 'probabilities', 'and', 'are', 'based', 'on', 'observations', '.'] 

 TOTAL TOKENS ==> 17

 ---- POST ----

 [('Discriminative', 'JJ'), ('methods', 'NNS'), ('are', 'VBP'), ('more', 'RBR'), ('functional', 'JJ'), ('and', 'CC'), ('have', 'VB'), ('right', 'JJ'), ('estimating', 'VBG'), ('posterior', 'JJ'), ('probabilities', 'NNS'), ('and', 'CC'), ('are', 'VBP'), ('based', 'VBN'), ('on', 'IN'), ('observations', 'NNS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Discriminative', 'methods', 'functional', 'right', 'estimating', 'posterior', 'probabilities', 'based', 'observations', '.']

 TOTAL FILTERED TOKENS ==>  10

 ---- POST FOR FILTERED TOKENS ----

 [('Discriminative', 'JJ'), ('methods', 'NNS'), ('functional', 'JJ'), ('right', 'JJ'), ('estimating', 'VBG'), ('posterior', 'JJ'), ('probabilities', 'NNS'), ('based', 'VBN'), ('observations', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Discriminative methods', 'methods functional', 'functional right', 'right estimating', 'estimating posterior', 'posterior probabilities', 'probabilities based', 'based observations', 'observations .'] 

 TOTAL BIGRAMS --> 9 



 ---- TRI-GRAMS ---- 

 ['Discriminative methods functional', 'methods functional right', 'functional right estimating', 'right estimating posterior', 'estimating posterior probabilities', 'posterior probabilities based', 'probabilities based observations', 'based observations .'] 

 TOTAL TRIGRAMS --> 8 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['discrimin', 'method', 'function', 'right', 'estim', 'posterior', 'probabl', 'base', 'observ', '.']

 TOTAL PORTER STEM WORDS ==> 10



 ---- SNOWBALL STEMMING ----

['discrimin', 'method', 'function', 'right', 'estim', 'posterior', 'probabl', 'base', 'observ', '.']

 TOTAL SNOWBALL STEM WORDS ==> 10



 ---- LEMMATIZATION ----

['Discriminative', 'method', 'functional', 'right', 'estimating', 'posterior', 'probability', 'based', 'observation', '.']

 TOTAL LEMMATIZE WORDS ==> 10

************************************************************************************************************************

290 --> Srihari [99] explains the different generative models as one with a resemblance that is used to  spot an unknown speaker’s language and would bid the deep knowledge of numerous  language to perform the match. 


 ---- TOKENS ----

 ['Srihari', '[', '99', ']', 'explains', 'the', 'different', 'generative', 'models', 'as', 'one', 'with', 'a', 'resemblance', 'that', 'is', 'used', 'to', 'spot', 'an', 'unknown', 'speaker', '’', 's', 'language', 'and', 'would', 'bid', 'the', 'deep', 'knowledge', 'of', 'numerous', 'language', 'to', 'perform', 'the', 'match', '.'] 

 TOTAL TOKENS ==> 39

 ---- POST ----

 [('Srihari', 'NNP'), ('[', 'VBD'), ('99', 'CD'), (']', 'NN'), ('explains', 'VBZ'), ('the', 'DT'), ('different', 'JJ'), ('generative', 'JJ'), ('models', 'NNS'), ('as', 'IN'), ('one', 'CD'), ('with', 'IN'), ('a', 'DT'), ('resemblance', 'NN'), ('that', 'WDT'), ('is', 'VBZ'), ('used', 'VBN'), ('to', 'TO'), ('spot', 'VB'), ('an', 'DT'), ('unknown', 'JJ'), ('speaker', 'NN'), ('’', 'NNP'), ('s', 'NN'), ('language', 'NN'), ('and', 'CC'), ('would', 'MD'), ('bid', 'VB'), ('the', 'DT'), ('deep', 'JJ'), ('knowledge', 'NN'), ('of', 'IN'), ('numerous', 'JJ'), ('language', 'NN'), ('to', 'TO'), ('perform', 'VB'), ('the', 'DT'), ('match', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Srihari', '[', '99', ']', 'explains', 'different', 'generative', 'models', 'one', 'resemblance', 'used', 'spot', 'unknown', 'speaker', '’', 'language', 'would', 'bid', 'deep', 'knowledge', 'numerous', 'language', 'perform', 'match', '.']

 TOTAL FILTERED TOKENS ==>  25

 ---- POST FOR FILTERED TOKENS ----

 [('Srihari', 'NNP'), ('[', 'VBD'), ('99', 'CD'), (']', 'JJ'), ('explains', 'NNS'), ('different', 'JJ'), ('generative', 'JJ'), ('models', 'NNS'), ('one', 'CD'), ('resemblance', 'NN'), ('used', 'VBN'), ('spot', 'NN'), ('unknown', 'JJ'), ('speaker', 'NN'), ('’', 'NNP'), ('language', 'NN'), ('would', 'MD'), ('bid', 'VB'), ('deep', 'JJ'), ('knowledge', 'NN'), ('numerous', 'JJ'), ('language', 'NN'), ('perform', 'NN'), ('match', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Srihari [', '[ 99', '99 ]', '] explains', 'explains different', 'different generative', 'generative models', 'models one', 'one resemblance', 'resemblance used', 'used spot', 'spot unknown', 'unknown speaker', 'speaker ’', '’ language', 'language would', 'would bid', 'bid deep', 'deep knowledge', 'knowledge numerous', 'numerous language', 'language perform', 'perform match', 'match .'] 

 TOTAL BIGRAMS --> 24 



 ---- TRI-GRAMS ---- 

 ['Srihari [ 99', '[ 99 ]', '99 ] explains', '] explains different', 'explains different generative', 'different generative models', 'generative models one', 'models one resemblance', 'one resemblance used', 'resemblance used spot', 'used spot unknown', 'spot unknown speaker', 'unknown speaker ’', 'speaker ’ language', '’ language would', 'language would bid', 'would bid deep', 'bid deep knowledge', 'deep knowledge numerous', 'knowledge numerous language', 'numerous language perform', 'language perform match', 'perform match .'] 

 TOTAL TRIGRAMS --> 23 



 ---- NOUN PHRASES ---- 

 ['resemblance', 'spot', 'unknown speaker', 'language', 'deep knowledge', 'numerous language', 'perform', 'match'] 

 TOTAL NOUN PHRASES --> 8 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> ['Srihari']
 TOTAL PERSON ENTITY --> 1 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['srihari', '[', '99', ']', 'explain', 'differ', 'gener', 'model', 'one', 'resembl', 'use', 'spot', 'unknown', 'speaker', '’', 'languag', 'would', 'bid', 'deep', 'knowledg', 'numer', 'languag', 'perform', 'match', '.']

 TOTAL PORTER STEM WORDS ==> 25



 ---- SNOWBALL STEMMING ----

['srihari', '[', '99', ']', 'explain', 'differ', 'generat', 'model', 'one', 'resembl', 'use', 'spot', 'unknown', 'speaker', '’', 'languag', 'would', 'bid', 'deep', 'knowledg', 'numer', 'languag', 'perform', 'match', '.']

 TOTAL SNOWBALL STEM WORDS ==> 25



 ---- LEMMATIZATION ----

['Srihari', '[', '99', ']', 'explains', 'different', 'generative', 'model', 'one', 'resemblance', 'used', 'spot', 'unknown', 'speaker', '’', 'language', 'would', 'bid', 'deep', 'knowledge', 'numerous', 'language', 'perform', 'match', '.']

 TOTAL LEMMATIZE WORDS ==> 25

************************************************************************************************************************

291 --> Whereas discriminative methods rely on a less knowledge- intensive approach and using distinction between language. 


 ---- TOKENS ----

 ['Whereas', 'discriminative', 'methods', 'rely', 'on', 'a', 'less', 'knowledge-', 'intensive', 'approach', 'and', 'using', 'distinction', 'between', 'language', '.'] 

 TOTAL TOKENS ==> 16

 ---- POST ----

 [('Whereas', 'NNP'), ('discriminative', 'JJ'), ('methods', 'NNS'), ('rely', 'RB'), ('on', 'IN'), ('a', 'DT'), ('less', 'RBR'), ('knowledge-', 'JJ'), ('intensive', 'JJ'), ('approach', 'NN'), ('and', 'CC'), ('using', 'VBG'), ('distinction', 'NN'), ('between', 'IN'), ('language', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Whereas', 'discriminative', 'methods', 'rely', 'less', 'knowledge-', 'intensive', 'approach', 'using', 'distinction', 'language', '.']

 TOTAL FILTERED TOKENS ==>  12

 ---- POST FOR FILTERED TOKENS ----

 [('Whereas', 'NNP'), ('discriminative', 'JJ'), ('methods', 'NNS'), ('rely', 'RB'), ('less', 'RBR'), ('knowledge-', 'JJ'), ('intensive', 'JJ'), ('approach', 'NN'), ('using', 'VBG'), ('distinction', 'NN'), ('language', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Whereas discriminative', 'discriminative methods', 'methods rely', 'rely less', 'less knowledge-', 'knowledge- intensive', 'intensive approach', 'approach using', 'using distinction', 'distinction language', 'language .'] 

 TOTAL BIGRAMS --> 11 



 ---- TRI-GRAMS ---- 

 ['Whereas discriminative methods', 'discriminative methods rely', 'methods rely less', 'rely less knowledge-', 'less knowledge- intensive', 'knowledge- intensive approach', 'intensive approach using', 'approach using distinction', 'using distinction language', 'distinction language .'] 

 TOTAL TRIGRAMS --> 10 



 ---- NOUN PHRASES ---- 

 ['knowledge- intensive approach', 'distinction', 'language'] 

 TOTAL NOUN PHRASES --> 3 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> ['Whereas']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['wherea', 'discrimin', 'method', 'reli', 'less', 'knowledge-', 'intens', 'approach', 'use', 'distinct', 'languag', '.']

 TOTAL PORTER STEM WORDS ==> 12



 ---- SNOWBALL STEMMING ----

['wherea', 'discrimin', 'method', 'reli', 'less', 'knowledge-', 'intens', 'approach', 'use', 'distinct', 'languag', '.']

 TOTAL SNOWBALL STEM WORDS ==> 12



 ---- LEMMATIZATION ----

['Whereas', 'discriminative', 'method', 'rely', 'le', 'knowledge-', 'intensive', 'approach', 'using', 'distinction', 'language', '.']

 TOTAL LEMMATIZE WORDS ==> 12

************************************************************************************************************************

292 --> Whereas generative models, can  become troublesome when many features are used and discriminative models allow use of  more features. 


 ---- TOKENS ----

 ['Whereas', 'generative', 'models', ',', 'can', 'become', 'troublesome', 'when', 'many', 'features', 'are', 'used', 'and', 'discriminative', 'models', 'allow', 'use', 'of', 'more', 'features', '.'] 

 TOTAL TOKENS ==> 21

 ---- POST ----

 [('Whereas', 'NNP'), ('generative', 'JJ'), ('models', 'NNS'), (',', ','), ('can', 'MD'), ('become', 'VB'), ('troublesome', 'JJ'), ('when', 'WRB'), ('many', 'JJ'), ('features', 'NNS'), ('are', 'VBP'), ('used', 'VBN'), ('and', 'CC'), ('discriminative', 'JJ'), ('models', 'NNS'), ('allow', 'VBP'), ('use', 'NN'), ('of', 'IN'), ('more', 'JJR'), ('features', 'NNS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Whereas', 'generative', 'models', ',', 'become', 'troublesome', 'many', 'features', 'used', 'discriminative', 'models', 'allow', 'use', 'features', '.']

 TOTAL FILTERED TOKENS ==>  15

 ---- POST FOR FILTERED TOKENS ----

 [('Whereas', 'NNP'), ('generative', 'JJ'), ('models', 'NNS'), (',', ','), ('become', 'VBP'), ('troublesome', 'JJ'), ('many', 'JJ'), ('features', 'NNS'), ('used', 'VBD'), ('discriminative', 'JJ'), ('models', 'NNS'), ('allow', 'VBP'), ('use', 'NN'), ('features', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Whereas generative', 'generative models', 'models ,', ', become', 'become troublesome', 'troublesome many', 'many features', 'features used', 'used discriminative', 'discriminative models', 'models allow', 'allow use', 'use features', 'features .'] 

 TOTAL BIGRAMS --> 14 



 ---- TRI-GRAMS ---- 

 ['Whereas generative models', 'generative models ,', 'models , become', ', become troublesome', 'become troublesome many', 'troublesome many features', 'many features used', 'features used discriminative', 'used discriminative models', 'discriminative models allow', 'models allow use', 'allow use features', 'use features .'] 

 TOTAL TRIGRAMS --> 13 



 ---- NOUN PHRASES ---- 

 ['use'] 

 TOTAL NOUN PHRASES --> 1 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> ['Whereas']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['wherea', 'gener', 'model', ',', 'becom', 'troublesom', 'mani', 'featur', 'use', 'discrimin', 'model', 'allow', 'use', 'featur', '.']

 TOTAL PORTER STEM WORDS ==> 15



 ---- SNOWBALL STEMMING ----

['wherea', 'generat', 'model', ',', 'becom', 'troublesom', 'mani', 'featur', 'use', 'discrimin', 'model', 'allow', 'use', 'featur', '.']

 TOTAL SNOWBALL STEM WORDS ==> 15



 ---- LEMMATIZATION ----

['Whereas', 'generative', 'model', ',', 'become', 'troublesome', 'many', 'feature', 'used', 'discriminative', 'model', 'allow', 'use', 'feature', '.']

 TOTAL LEMMATIZE WORDS ==> 15

************************************************************************************************************************

293 --> [100] Few of the examples of discriminative methods are Logistic regression  and conditional random fields (CRFs), generative methods are Naive Bayes classifiers and  hidden Markov models (HMMs). 


 ---- TOKENS ----

 ['[', '100', ']', 'Few', 'of', 'the', 'examples', 'of', 'discriminative', 'methods', 'are', 'Logistic', 'regression', 'and', 'conditional', 'random', 'fields', '(', 'CRFs', ')', ',', 'generative', 'methods', 'are', 'Naive', 'Bayes', 'classifiers', 'and', 'hidden', 'Markov', 'models', '(', 'HMMs', ')', '.'] 

 TOTAL TOKENS ==> 35

 ---- POST ----

 [('[', 'RB'), ('100', 'CD'), (']', 'JJ'), ('Few', 'NNP'), ('of', 'IN'), ('the', 'DT'), ('examples', 'NNS'), ('of', 'IN'), ('discriminative', 'JJ'), ('methods', 'NNS'), ('are', 'VBP'), ('Logistic', 'JJ'), ('regression', 'NN'), ('and', 'CC'), ('conditional', 'JJ'), ('random', 'NN'), ('fields', 'NNS'), ('(', '('), ('CRFs', 'NNP'), (')', ')'), (',', ','), ('generative', 'JJ'), ('methods', 'NNS'), ('are', 'VBP'), ('Naive', 'JJ'), ('Bayes', 'NNP'), ('classifiers', 'NNS'), ('and', 'CC'), ('hidden', 'JJ'), ('Markov', 'NNP'), ('models', 'NNS'), ('(', '('), ('HMMs', 'NNP'), (')', ')'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['[', '100', ']', 'examples', 'discriminative', 'methods', 'Logistic', 'regression', 'conditional', 'random', 'fields', '(', 'CRFs', ')', ',', 'generative', 'methods', 'Naive', 'Bayes', 'classifiers', 'hidden', 'Markov', 'models', '(', 'HMMs', ')', '.']

 TOTAL FILTERED TOKENS ==>  27

 ---- POST FOR FILTERED TOKENS ----

 [('[', 'RB'), ('100', 'CD'), (']', 'JJ'), ('examples', 'NNS'), ('discriminative', 'JJ'), ('methods', 'NNS'), ('Logistic', 'JJ'), ('regression', 'NN'), ('conditional', 'JJ'), ('random', 'NN'), ('fields', 'NNS'), ('(', '('), ('CRFs', 'NNP'), (')', ')'), (',', ','), ('generative', 'JJ'), ('methods', 'NNS'), ('Naive', 'JJ'), ('Bayes', 'NNP'), ('classifiers', 'NNS'), ('hidden', 'VBP'), ('Markov', 'NNP'), ('models', 'NNS'), ('(', '('), ('HMMs', 'NNP'), (')', ')'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['[ 100', '100 ]', '] examples', 'examples discriminative', 'discriminative methods', 'methods Logistic', 'Logistic regression', 'regression conditional', 'conditional random', 'random fields', 'fields (', '( CRFs', 'CRFs )', ') ,', ', generative', 'generative methods', 'methods Naive', 'Naive Bayes', 'Bayes classifiers', 'classifiers hidden', 'hidden Markov', 'Markov models', 'models (', '( HMMs', 'HMMs )', ') .'] 

 TOTAL BIGRAMS --> 26 



 ---- TRI-GRAMS ---- 

 ['[ 100 ]', '100 ] examples', '] examples discriminative', 'examples discriminative methods', 'discriminative methods Logistic', 'methods Logistic regression', 'Logistic regression conditional', 'regression conditional random', 'conditional random fields', 'random fields (', 'fields ( CRFs', '( CRFs )', 'CRFs ) ,', ') , generative', ', generative methods', 'generative methods Naive', 'methods Naive Bayes', 'Naive Bayes classifiers', 'Bayes classifiers hidden', 'classifiers hidden Markov', 'hidden Markov models', 'Markov models (', 'models ( HMMs', '( HMMs )', 'HMMs ) .'] 

 TOTAL TRIGRAMS --> 25 



 ---- NOUN PHRASES ---- 

 ['Logistic regression', 'conditional random'] 

 TOTAL NOUN PHRASES --> 2 



 ---- NER ----

 
 ORGANIZATION ---> ['Logistic']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> ['Naive Bayes', 'Markov']
 TOTAL PERSON ENTITY --> 2 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['[', '100', ']', 'exampl', 'discrimin', 'method', 'logist', 'regress', 'condit', 'random', 'field', '(', 'crf', ')', ',', 'gener', 'method', 'naiv', 'bay', 'classifi', 'hidden', 'markov', 'model', '(', 'hmm', ')', '.']

 TOTAL PORTER STEM WORDS ==> 27



 ---- SNOWBALL STEMMING ----

['[', '100', ']', 'exampl', 'discrimin', 'method', 'logist', 'regress', 'condit', 'random', 'field', '(', 'crfs', ')', ',', 'generat', 'method', 'naiv', 'bay', 'classifi', 'hidden', 'markov', 'model', '(', 'hmms', ')', '.']

 TOTAL SNOWBALL STEM WORDS ==> 27



 ---- LEMMATIZATION ----

['[', '100', ']', 'example', 'discriminative', 'method', 'Logistic', 'regression', 'conditional', 'random', 'field', '(', 'CRFs', ')', ',', 'generative', 'method', 'Naive', 'Bayes', 'classifier', 'hidden', 'Markov', 'model', '(', 'HMMs', ')', '.']

 TOTAL LEMMATIZE WORDS ==> 27

************************************************************************************************************************

294 --> 7.1 Hidden Markov Model (HMM)  An HMM is a system where a shifting takes place between several states, generating feasible  output symbols with each switch. 


 ---- TOKENS ----

 ['7.1', 'Hidden', 'Markov', 'Model', '(', 'HMM', ')', 'An', 'HMM', 'is', 'a', 'system', 'where', 'a', 'shifting', 'takes', 'place', 'between', 'several', 'states', ',', 'generating', 'feasible', 'output', 'symbols', 'with', 'each', 'switch', '.'] 

 TOTAL TOKENS ==> 29

 ---- POST ----

 [('7.1', 'CD'), ('Hidden', 'NNP'), ('Markov', 'NNP'), ('Model', 'NNP'), ('(', '('), ('HMM', 'NNP'), (')', ')'), ('An', 'DT'), ('HMM', 'NNP'), ('is', 'VBZ'), ('a', 'DT'), ('system', 'NN'), ('where', 'WRB'), ('a', 'DT'), ('shifting', 'NN'), ('takes', 'VBZ'), ('place', 'NN'), ('between', 'IN'), ('several', 'JJ'), ('states', 'NNS'), (',', ','), ('generating', 'VBG'), ('feasible', 'JJ'), ('output', 'NN'), ('symbols', 'NNS'), ('with', 'IN'), ('each', 'DT'), ('switch', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['7.1', 'Hidden', 'Markov', 'Model', '(', 'HMM', ')', 'HMM', 'system', 'shifting', 'takes', 'place', 'several', 'states', ',', 'generating', 'feasible', 'output', 'symbols', 'switch', '.']

 TOTAL FILTERED TOKENS ==>  21

 ---- POST FOR FILTERED TOKENS ----

 [('7.1', 'CD'), ('Hidden', 'NNP'), ('Markov', 'NNP'), ('Model', 'NNP'), ('(', '('), ('HMM', 'NNP'), (')', ')'), ('HMM', 'NNP'), ('system', 'NN'), ('shifting', 'VBG'), ('takes', 'VBZ'), ('place', 'NN'), ('several', 'JJ'), ('states', 'NNS'), (',', ','), ('generating', 'VBG'), ('feasible', 'JJ'), ('output', 'NN'), ('symbols', 'NNS'), ('switch', 'VBP'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['7.1 Hidden', 'Hidden Markov', 'Markov Model', 'Model (', '( HMM', 'HMM )', ') HMM', 'HMM system', 'system shifting', 'shifting takes', 'takes place', 'place several', 'several states', 'states ,', ', generating', 'generating feasible', 'feasible output', 'output symbols', 'symbols switch', 'switch .'] 

 TOTAL BIGRAMS --> 20 



 ---- TRI-GRAMS ---- 

 ['7.1 Hidden Markov', 'Hidden Markov Model', 'Markov Model (', 'Model ( HMM', '( HMM )', 'HMM ) HMM', ') HMM system', 'HMM system shifting', 'system shifting takes', 'shifting takes place', 'takes place several', 'place several states', 'several states ,', 'states , generating', ', generating feasible', 'generating feasible output', 'feasible output symbols', 'output symbols switch', 'symbols switch .'] 

 TOTAL TRIGRAMS --> 19 



 ---- NOUN PHRASES ---- 

 ['system', 'place', 'feasible output'] 

 TOTAL NOUN PHRASES --> 3 



 ---- NER ----

 
 ORGANIZATION ---> ['HMM']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> ['Hidden Markov Model']
 TOTAL PERSON ENTITY --> 1 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['7.1', 'hidden', 'markov', 'model', '(', 'hmm', ')', 'hmm', 'system', 'shift', 'take', 'place', 'sever', 'state', ',', 'gener', 'feasibl', 'output', 'symbol', 'switch', '.']

 TOTAL PORTER STEM WORDS ==> 21



 ---- SNOWBALL STEMMING ----

['7.1', 'hidden', 'markov', 'model', '(', 'hmm', ')', 'hmm', 'system', 'shift', 'take', 'place', 'sever', 'state', ',', 'generat', 'feasibl', 'output', 'symbol', 'switch', '.']

 TOTAL SNOWBALL STEM WORDS ==> 21



 ---- LEMMATIZATION ----

['7.1', 'Hidden', 'Markov', 'Model', '(', 'HMM', ')', 'HMM', 'system', 'shifting', 'take', 'place', 'several', 'state', ',', 'generating', 'feasible', 'output', 'symbol', 'switch', '.']

 TOTAL LEMMATIZE WORDS ==> 21

************************************************************************************************************************

295 --> The sets of viable states and unique symbols may be large,  but finite and known. 


 ---- TOKENS ----

 ['The', 'sets', 'of', 'viable', 'states', 'and', 'unique', 'symbols', 'may', 'be', 'large', ',', 'but', 'finite', 'and', 'known', '.'] 

 TOTAL TOKENS ==> 17

 ---- POST ----

 [('The', 'DT'), ('sets', 'NNS'), ('of', 'IN'), ('viable', 'JJ'), ('states', 'NNS'), ('and', 'CC'), ('unique', 'JJ'), ('symbols', 'NNS'), ('may', 'MD'), ('be', 'VB'), ('large', 'JJ'), (',', ','), ('but', 'CC'), ('finite', 'JJ'), ('and', 'CC'), ('known', 'VBN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['sets', 'viable', 'states', 'unique', 'symbols', 'may', 'large', ',', 'finite', 'known', '.']

 TOTAL FILTERED TOKENS ==>  11

 ---- POST FOR FILTERED TOKENS ----

 [('sets', 'NNS'), ('viable', 'JJ'), ('states', 'NNS'), ('unique', 'JJ'), ('symbols', 'NNS'), ('may', 'MD'), ('large', 'JJ'), (',', ','), ('finite', 'JJ'), ('known', 'VBN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['sets viable', 'viable states', 'states unique', 'unique symbols', 'symbols may', 'may large', 'large ,', ', finite', 'finite known', 'known .'] 

 TOTAL BIGRAMS --> 10 



 ---- TRI-GRAMS ---- 

 ['sets viable states', 'viable states unique', 'states unique symbols', 'unique symbols may', 'symbols may large', 'may large ,', 'large , finite', ', finite known', 'finite known .'] 

 TOTAL TRIGRAMS --> 9 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['set', 'viabl', 'state', 'uniqu', 'symbol', 'may', 'larg', ',', 'finit', 'known', '.']

 TOTAL PORTER STEM WORDS ==> 11



 ---- SNOWBALL STEMMING ----

['set', 'viabl', 'state', 'uniqu', 'symbol', 'may', 'larg', ',', 'finit', 'known', '.']

 TOTAL SNOWBALL STEM WORDS ==> 11



 ---- LEMMATIZATION ----

['set', 'viable', 'state', 'unique', 'symbol', 'may', 'large', ',', 'finite', 'known', '.']

 TOTAL LEMMATIZE WORDS ==> 11

************************************************************************************************************************

296 --> We can descry the outputs, but the system’s internals are hidden. 


 ---- TOKENS ----

 ['We', 'can', 'descry', 'the', 'outputs', ',', 'but', 'the', 'system', '’', 's', 'internals', 'are', 'hidden', '.'] 

 TOTAL TOKENS ==> 15

 ---- POST ----

 [('We', 'PRP'), ('can', 'MD'), ('descry', 'VB'), ('the', 'DT'), ('outputs', 'NNS'), (',', ','), ('but', 'CC'), ('the', 'DT'), ('system', 'NN'), ('’', 'NNP'), ('s', 'NN'), ('internals', 'NNS'), ('are', 'VBP'), ('hidden', 'VBN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['descry', 'outputs', ',', 'system', '’', 'internals', 'hidden', '.']

 TOTAL FILTERED TOKENS ==>  8

 ---- POST FOR FILTERED TOKENS ----

 [('descry', 'NN'), ('outputs', 'NNS'), (',', ','), ('system', 'NN'), ('’', 'JJ'), ('internals', 'NNS'), ('hidden', 'VBP'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['descry outputs', 'outputs ,', ', system', 'system ’', '’ internals', 'internals hidden', 'hidden .'] 

 TOTAL BIGRAMS --> 7 



 ---- TRI-GRAMS ---- 

 ['descry outputs ,', 'outputs , system', ', system ’', 'system ’ internals', '’ internals hidden', 'internals hidden .'] 

 TOTAL TRIGRAMS --> 6 



 ---- NOUN PHRASES ---- 

 ['descry', 'system'] 

 TOTAL NOUN PHRASES --> 2 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['descri', 'output', ',', 'system', '’', 'intern', 'hidden', '.']

 TOTAL PORTER STEM WORDS ==> 8



 ---- SNOWBALL STEMMING ----

['descri', 'output', ',', 'system', '’', 'intern', 'hidden', '.']

 TOTAL SNOWBALL STEM WORDS ==> 8



 ---- LEMMATIZATION ----

['descry', 'output', ',', 'system', '’', 'internals', 'hidden', '.']

 TOTAL LEMMATIZE WORDS ==> 8

************************************************************************************************************************

297 --> Few  of the problem could be solved are by Inference A certain sequence of output symbols,  compute the probabilities of one or more candidate states with sequences. 


 ---- TOKENS ----

 ['Few', 'of', 'the', 'problem', 'could', 'be', 'solved', 'are', 'by', 'Inference', 'A', 'certain', 'sequence', 'of', 'output', 'symbols', ',', 'compute', 'the', 'probabilities', 'of', 'one', 'or', 'more', 'candidate', 'states', 'with', 'sequences', '.'] 

 TOTAL TOKENS ==> 29

 ---- POST ----

 [('Few', 'JJ'), ('of', 'IN'), ('the', 'DT'), ('problem', 'NN'), ('could', 'MD'), ('be', 'VB'), ('solved', 'VBN'), ('are', 'VBP'), ('by', 'IN'), ('Inference', 'NNP'), ('A', 'NNP'), ('certain', 'JJ'), ('sequence', 'NN'), ('of', 'IN'), ('output', 'NN'), ('symbols', 'NNS'), (',', ','), ('compute', 'VBP'), ('the', 'DT'), ('probabilities', 'NNS'), ('of', 'IN'), ('one', 'CD'), ('or', 'CC'), ('more', 'JJR'), ('candidate', 'JJ'), ('states', 'NNS'), ('with', 'IN'), ('sequences', 'NNS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['problem', 'could', 'solved', 'Inference', 'certain', 'sequence', 'output', 'symbols', ',', 'compute', 'probabilities', 'one', 'candidate', 'states', 'sequences', '.']

 TOTAL FILTERED TOKENS ==>  16

 ---- POST FOR FILTERED TOKENS ----

 [('problem', 'NN'), ('could', 'MD'), ('solved', 'VB'), ('Inference', 'NNP'), ('certain', 'JJ'), ('sequence', 'NN'), ('output', 'NN'), ('symbols', 'NNS'), (',', ','), ('compute', 'NN'), ('probabilities', 'NNS'), ('one', 'CD'), ('candidate', 'NN'), ('states', 'NNS'), ('sequences', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['problem could', 'could solved', 'solved Inference', 'Inference certain', 'certain sequence', 'sequence output', 'output symbols', 'symbols ,', ', compute', 'compute probabilities', 'probabilities one', 'one candidate', 'candidate states', 'states sequences', 'sequences .'] 

 TOTAL BIGRAMS --> 15 



 ---- TRI-GRAMS ---- 

 ['problem could solved', 'could solved Inference', 'solved Inference certain', 'Inference certain sequence', 'certain sequence output', 'sequence output symbols', 'output symbols ,', 'symbols , compute', ', compute probabilities', 'compute probabilities one', 'probabilities one candidate', 'one candidate states', 'candidate states sequences', 'states sequences .'] 

 TOTAL TRIGRAMS --> 14 



 ---- NOUN PHRASES ---- 

 ['problem', 'certain sequence', 'output', 'compute', 'candidate'] 

 TOTAL NOUN PHRASES --> 5 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['problem', 'could', 'solv', 'infer', 'certain', 'sequenc', 'output', 'symbol', ',', 'comput', 'probabl', 'one', 'candid', 'state', 'sequenc', '.']

 TOTAL PORTER STEM WORDS ==> 16



 ---- SNOWBALL STEMMING ----

['problem', 'could', 'solv', 'infer', 'certain', 'sequenc', 'output', 'symbol', ',', 'comput', 'probabl', 'one', 'candid', 'state', 'sequenc', '.']

 TOTAL SNOWBALL STEM WORDS ==> 16



 ---- LEMMATIZATION ----

['problem', 'could', 'solved', 'Inference', 'certain', 'sequence', 'output', 'symbol', ',', 'compute', 'probability', 'one', 'candidate', 'state', 'sequence', '.']

 TOTAL LEMMATIZE WORDS ==> 16

************************************************************************************************************************

298 --> Pattern matching  the state-switch sequence is realised are most likely to have generated a particular output-  symbol sequence. 


 ---- TOKENS ----

 ['Pattern', 'matching', 'the', 'state-switch', 'sequence', 'is', 'realised', 'are', 'most', 'likely', 'to', 'have', 'generated', 'a', 'particular', 'output-', 'symbol', 'sequence', '.'] 

 TOTAL TOKENS ==> 19

 ---- POST ----

 [('Pattern', 'NNP'), ('matching', 'VBG'), ('the', 'DT'), ('state-switch', 'JJ'), ('sequence', 'NN'), ('is', 'VBZ'), ('realised', 'VBN'), ('are', 'VBP'), ('most', 'RBS'), ('likely', 'JJ'), ('to', 'TO'), ('have', 'VB'), ('generated', 'VBN'), ('a', 'DT'), ('particular', 'JJ'), ('output-', 'JJ'), ('symbol', 'NN'), ('sequence', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Pattern', 'matching', 'state-switch', 'sequence', 'realised', 'likely', 'generated', 'particular', 'output-', 'symbol', 'sequence', '.']

 TOTAL FILTERED TOKENS ==>  12

 ---- POST FOR FILTERED TOKENS ----

 [('Pattern', 'JJ'), ('matching', 'VBG'), ('state-switch', 'JJ'), ('sequence', 'NN'), ('realised', 'VBD'), ('likely', 'RB'), ('generated', 'VBN'), ('particular', 'JJ'), ('output-', 'JJ'), ('symbol', 'NN'), ('sequence', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Pattern matching', 'matching state-switch', 'state-switch sequence', 'sequence realised', 'realised likely', 'likely generated', 'generated particular', 'particular output-', 'output- symbol', 'symbol sequence', 'sequence .'] 

 TOTAL BIGRAMS --> 11 



 ---- TRI-GRAMS ---- 

 ['Pattern matching state-switch', 'matching state-switch sequence', 'state-switch sequence realised', 'sequence realised likely', 'realised likely generated', 'likely generated particular', 'generated particular output-', 'particular output- symbol', 'output- symbol sequence', 'symbol sequence .'] 

 TOTAL TRIGRAMS --> 10 



 ---- NOUN PHRASES ---- 

 ['state-switch sequence', 'particular output- symbol', 'sequence'] 

 TOTAL NOUN PHRASES --> 3 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> ['Pattern']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['pattern', 'match', 'state-switch', 'sequenc', 'realis', 'like', 'gener', 'particular', 'output-', 'symbol', 'sequenc', '.']

 TOTAL PORTER STEM WORDS ==> 12



 ---- SNOWBALL STEMMING ----

['pattern', 'match', 'state-switch', 'sequenc', 'realis', 'like', 'generat', 'particular', 'output-', 'symbol', 'sequenc', '.']

 TOTAL SNOWBALL STEM WORDS ==> 12



 ---- LEMMATIZATION ----

['Pattern', 'matching', 'state-switch', 'sequence', 'realised', 'likely', 'generated', 'particular', 'output-', 'symbol', 'sequence', '.']

 TOTAL LEMMATIZE WORDS ==> 12

************************************************************************************************************************

299 --> Training the output-symbol chain data, reckon the state-switch/output  probabilities that fit this data best. 


 ---- TOKENS ----

 ['Training', 'the', 'output-symbol', 'chain', 'data', ',', 'reckon', 'the', 'state-switch/output', 'probabilities', 'that', 'fit', 'this', 'data', 'best', '.'] 

 TOTAL TOKENS ==> 16

 ---- POST ----

 [('Training', 'VBG'), ('the', 'DT'), ('output-symbol', 'NN'), ('chain', 'NN'), ('data', 'NNS'), (',', ','), ('reckon', 'VB'), ('the', 'DT'), ('state-switch/output', 'JJ'), ('probabilities', 'NNS'), ('that', 'WDT'), ('fit', 'VBP'), ('this', 'DT'), ('data', 'NN'), ('best', 'JJS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Training', 'output-symbol', 'chain', 'data', ',', 'reckon', 'state-switch/output', 'probabilities', 'fit', 'data', 'best', '.']

 TOTAL FILTERED TOKENS ==>  12

 ---- POST FOR FILTERED TOKENS ----

 [('Training', 'VBG'), ('output-symbol', 'NN'), ('chain', 'NN'), ('data', 'NNS'), (',', ','), ('reckon', 'VB'), ('state-switch/output', 'JJ'), ('probabilities', 'NNS'), ('fit', 'VBP'), ('data', 'NN'), ('best', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Training output-symbol', 'output-symbol chain', 'chain data', 'data ,', ', reckon', 'reckon state-switch/output', 'state-switch/output probabilities', 'probabilities fit', 'fit data', 'data best', 'best .'] 

 TOTAL BIGRAMS --> 11 



 ---- TRI-GRAMS ---- 

 ['Training output-symbol chain', 'output-symbol chain data', 'chain data ,', 'data , reckon', ', reckon state-switch/output', 'reckon state-switch/output probabilities', 'state-switch/output probabilities fit', 'probabilities fit data', 'fit data best', 'data best .'] 

 TOTAL TRIGRAMS --> 10 



 ---- NOUN PHRASES ---- 

 ['output-symbol', 'chain', 'data', 'best'] 

 TOTAL NOUN PHRASES --> 4 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['train', 'output-symbol', 'chain', 'data', ',', 'reckon', 'state-switch/output', 'probabl', 'fit', 'data', 'best', '.']

 TOTAL PORTER STEM WORDS ==> 12



 ---- SNOWBALL STEMMING ----

['train', 'output-symbol', 'chain', 'data', ',', 'reckon', 'state-switch/output', 'probabl', 'fit', 'data', 'best', '.']

 TOTAL SNOWBALL STEM WORDS ==> 12



 ---- LEMMATIZATION ----

['Training', 'output-symbol', 'chain', 'data', ',', 'reckon', 'state-switch/output', 'probability', 'fit', 'data', 'best', '.']

 TOTAL LEMMATIZE WORDS ==> 12

************************************************************************************************************************

300 --> Hidden Markov Models are extensively used for speech recognition, where the output  sequence is matched to the sequence of individual phonemes. 


 ---- TOKENS ----

 ['Hidden', 'Markov', 'Models', 'are', 'extensively', 'used', 'for', 'speech', 'recognition', ',', 'where', 'the', 'output', 'sequence', 'is', 'matched', 'to', 'the', 'sequence', 'of', 'individual', 'phonemes', '.'] 

 TOTAL TOKENS ==> 23

 ---- POST ----

 [('Hidden', 'NNP'), ('Markov', 'NNP'), ('Models', 'NNP'), ('are', 'VBP'), ('extensively', 'RB'), ('used', 'VBN'), ('for', 'IN'), ('speech', 'NN'), ('recognition', 'NN'), (',', ','), ('where', 'WRB'), ('the', 'DT'), ('output', 'NN'), ('sequence', 'NN'), ('is', 'VBZ'), ('matched', 'VBN'), ('to', 'TO'), ('the', 'DT'), ('sequence', 'NN'), ('of', 'IN'), ('individual', 'JJ'), ('phonemes', 'NNS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Hidden', 'Markov', 'Models', 'extensively', 'used', 'speech', 'recognition', ',', 'output', 'sequence', 'matched', 'sequence', 'individual', 'phonemes', '.']

 TOTAL FILTERED TOKENS ==>  15

 ---- POST FOR FILTERED TOKENS ----

 [('Hidden', 'NNP'), ('Markov', 'NNP'), ('Models', 'NNP'), ('extensively', 'RB'), ('used', 'VBD'), ('speech', 'NN'), ('recognition', 'NN'), (',', ','), ('output', 'NN'), ('sequence', 'NN'), ('matched', 'VBD'), ('sequence', 'NN'), ('individual', 'JJ'), ('phonemes', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Hidden Markov', 'Markov Models', 'Models extensively', 'extensively used', 'used speech', 'speech recognition', 'recognition ,', ', output', 'output sequence', 'sequence matched', 'matched sequence', 'sequence individual', 'individual phonemes', 'phonemes .'] 

 TOTAL BIGRAMS --> 14 



 ---- TRI-GRAMS ---- 

 ['Hidden Markov Models', 'Markov Models extensively', 'Models extensively used', 'extensively used speech', 'used speech recognition', 'speech recognition ,', 'recognition , output', ', output sequence', 'output sequence matched', 'sequence matched sequence', 'matched sequence individual', 'sequence individual phonemes', 'individual phonemes .'] 

 TOTAL TRIGRAMS --> 13 



 ---- NOUN PHRASES ---- 

 ['speech', 'recognition', 'output', 'sequence', 'sequence'] 

 TOTAL NOUN PHRASES --> 5 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> ['Hidden', 'Markov Models']
 TOTAL PERSON ENTITY --> 2 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['hidden', 'markov', 'model', 'extens', 'use', 'speech', 'recognit', ',', 'output', 'sequenc', 'match', 'sequenc', 'individu', 'phonem', '.']

 TOTAL PORTER STEM WORDS ==> 15



 ---- SNOWBALL STEMMING ----

['hidden', 'markov', 'model', 'extens', 'use', 'speech', 'recognit', ',', 'output', 'sequenc', 'match', 'sequenc', 'individu', 'phonem', '.']

 TOTAL SNOWBALL STEM WORDS ==> 15



 ---- LEMMATIZATION ----

['Hidden', 'Markov', 'Models', 'extensively', 'used', 'speech', 'recognition', ',', 'output', 'sequence', 'matched', 'sequence', 'individual', 'phoneme', '.']

 TOTAL LEMMATIZE WORDS ==> 15

************************************************************************************************************************

301 --> Frederick Jelinek, a statistical- NLP advocate who first instigated HMMs at IBM’s Speech Recognition Group, reportedly  joked, every time a linguist leaves my group, the speech recognizer’s performance improves. 


 ---- TOKENS ----

 ['Frederick', 'Jelinek', ',', 'a', 'statistical-', 'NLP', 'advocate', 'who', 'first', 'instigated', 'HMMs', 'at', 'IBM', '’', 's', 'Speech', 'Recognition', 'Group', ',', 'reportedly', 'joked', ',', 'every', 'time', 'a', 'linguist', 'leaves', 'my', 'group', ',', 'the', 'speech', 'recognizer', '’', 's', 'performance', 'improves', '.'] 

 TOTAL TOKENS ==> 38

 ---- POST ----

 [('Frederick', 'NNP'), ('Jelinek', 'NNP'), (',', ','), ('a', 'DT'), ('statistical-', 'JJ'), ('NLP', 'NNP'), ('advocate', 'NN'), ('who', 'WP'), ('first', 'RB'), ('instigated', 'VBD'), ('HMMs', 'NNP'), ('at', 'IN'), ('IBM', 'NNP'), ('’', 'NNP'), ('s', 'VBD'), ('Speech', 'NNP'), ('Recognition', 'NNP'), ('Group', 'NNP'), (',', ','), ('reportedly', 'RB'), ('joked', 'VBD'), (',', ','), ('every', 'DT'), ('time', 'NN'), ('a', 'DT'), ('linguist', 'NN'), ('leaves', 'VBZ'), ('my', 'PRP$'), ('group', 'NN'), (',', ','), ('the', 'DT'), ('speech', 'NN'), ('recognizer', 'NN'), ('’', 'NNP'), ('s', 'NN'), ('performance', 'NN'), ('improves', 'NNS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Frederick', 'Jelinek', ',', 'statistical-', 'NLP', 'advocate', 'first', 'instigated', 'HMMs', 'IBM', '’', 'Speech', 'Recognition', 'Group', ',', 'reportedly', 'joked', ',', 'every', 'time', 'linguist', 'leaves', 'group', ',', 'speech', 'recognizer', '’', 'performance', 'improves', '.']

 TOTAL FILTERED TOKENS ==>  30

 ---- POST FOR FILTERED TOKENS ----

 [('Frederick', 'NNP'), ('Jelinek', 'NNP'), (',', ','), ('statistical-', 'JJ'), ('NLP', 'NNP'), ('advocate', 'NN'), ('first', 'RB'), ('instigated', 'VBD'), ('HMMs', 'NNP'), ('IBM', 'NNP'), ('’', 'NNP'), ('Speech', 'NNP'), ('Recognition', 'NNP'), ('Group', 'NNP'), (',', ','), ('reportedly', 'RB'), ('joked', 'VBD'), (',', ','), ('every', 'DT'), ('time', 'NN'), ('linguist', 'JJ'), ('leaves', 'NNS'), ('group', 'NN'), (',', ','), ('speech', 'NN'), ('recognizer', 'NN'), ('’', 'JJ'), ('performance', 'NN'), ('improves', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Frederick Jelinek', 'Jelinek ,', ', statistical-', 'statistical- NLP', 'NLP advocate', 'advocate first', 'first instigated', 'instigated HMMs', 'HMMs IBM', 'IBM ’', '’ Speech', 'Speech Recognition', 'Recognition Group', 'Group ,', ', reportedly', 'reportedly joked', 'joked ,', ', every', 'every time', 'time linguist', 'linguist leaves', 'leaves group', 'group ,', ', speech', 'speech recognizer', 'recognizer ’', '’ performance', 'performance improves', 'improves .'] 

 TOTAL BIGRAMS --> 29 



 ---- TRI-GRAMS ---- 

 ['Frederick Jelinek ,', 'Jelinek , statistical-', ', statistical- NLP', 'statistical- NLP advocate', 'NLP advocate first', 'advocate first instigated', 'first instigated HMMs', 'instigated HMMs IBM', 'HMMs IBM ’', 'IBM ’ Speech', '’ Speech Recognition', 'Speech Recognition Group', 'Recognition Group ,', 'Group , reportedly', ', reportedly joked', 'reportedly joked ,', 'joked , every', ', every time', 'every time linguist', 'time linguist leaves', 'linguist leaves group', 'leaves group ,', 'group , speech', ', speech recognizer', 'speech recognizer ’', 'recognizer ’ performance', '’ performance improves', 'performance improves .'] 

 TOTAL TRIGRAMS --> 28 



 ---- NOUN PHRASES ---- 

 ['advocate', 'every time', 'group', 'speech', 'recognizer', '’ performance'] 

 TOTAL NOUN PHRASES --> 6 



 ---- NER ----

 
 ORGANIZATION ---> ['Jelinek', 'NLP', 'HMMs']
 TOTAL ORGANIZATION ENTITY --> 3 


 PERSON ---> ['Frederick']
 TOTAL PERSON ENTITY --> 1 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['frederick', 'jelinek', ',', 'statistical-', 'nlp', 'advoc', 'first', 'instig', 'hmm', 'ibm', '’', 'speech', 'recognit', 'group', ',', 'reportedli', 'joke', ',', 'everi', 'time', 'linguist', 'leav', 'group', ',', 'speech', 'recogn', '’', 'perform', 'improv', '.']

 TOTAL PORTER STEM WORDS ==> 30



 ---- SNOWBALL STEMMING ----

['frederick', 'jelinek', ',', 'statistical-', 'nlp', 'advoc', 'first', 'instig', 'hmms', 'ibm', '’', 'speech', 'recognit', 'group', ',', 'report', 'joke', ',', 'everi', 'time', 'linguist', 'leav', 'group', ',', 'speech', 'recogn', '’', 'perform', 'improv', '.']

 TOTAL SNOWBALL STEM WORDS ==> 30



 ---- LEMMATIZATION ----

['Frederick', 'Jelinek', ',', 'statistical-', 'NLP', 'advocate', 'first', 'instigated', 'HMMs', 'IBM', '’', 'Speech', 'Recognition', 'Group', ',', 'reportedly', 'joked', ',', 'every', 'time', 'linguist', 'leaf', 'group', ',', 'speech', 'recognizer', '’', 'performance', 'improves', '.']

 TOTAL LEMMATIZE WORDS ==> 30

************************************************************************************************************************

302 --> [101] HMM is not restricted to this application it has several others such as bioinformatics  problems, for example, multiple sequence alignment [102]. 


 ---- TOKENS ----

 ['[', '101', ']', 'HMM', 'is', 'not', 'restricted', 'to', 'this', 'application', 'it', 'has', 'several', 'others', 'such', 'as', 'bioinformatics', 'problems', ',', 'for', 'example', ',', 'multiple', 'sequence', 'alignment', '[', '102', ']', '.'] 

 TOTAL TOKENS ==> 29

 ---- POST ----

 [('[', 'RB'), ('101', 'CD'), (']', 'JJ'), ('HMM', 'NNP'), ('is', 'VBZ'), ('not', 'RB'), ('restricted', 'VBN'), ('to', 'TO'), ('this', 'DT'), ('application', 'NN'), ('it', 'PRP'), ('has', 'VBZ'), ('several', 'JJ'), ('others', 'NNS'), ('such', 'JJ'), ('as', 'IN'), ('bioinformatics', 'NNS'), ('problems', 'NNS'), (',', ','), ('for', 'IN'), ('example', 'NN'), (',', ','), ('multiple', 'JJ'), ('sequence', 'NN'), ('alignment', 'NN'), ('[', 'VBD'), ('102', 'CD'), (']', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['[', '101', ']', 'HMM', 'restricted', 'application', 'several', 'others', 'bioinformatics', 'problems', ',', 'example', ',', 'multiple', 'sequence', 'alignment', '[', '102', ']', '.']

 TOTAL FILTERED TOKENS ==>  20

 ---- POST FOR FILTERED TOKENS ----

 [('[', 'RB'), ('101', 'CD'), (']', 'JJ'), ('HMM', 'NNP'), ('restricted', 'VBD'), ('application', 'NN'), ('several', 'JJ'), ('others', 'NNS'), ('bioinformatics', 'NNS'), ('problems', 'NNS'), (',', ','), ('example', 'NN'), (',', ','), ('multiple', 'JJ'), ('sequence', 'NN'), ('alignment', 'NN'), ('[', 'VBD'), ('102', 'CD'), (']', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['[ 101', '101 ]', '] HMM', 'HMM restricted', 'restricted application', 'application several', 'several others', 'others bioinformatics', 'bioinformatics problems', 'problems ,', ', example', 'example ,', ', multiple', 'multiple sequence', 'sequence alignment', 'alignment [', '[ 102', '102 ]', '] .'] 

 TOTAL BIGRAMS --> 19 



 ---- TRI-GRAMS ---- 

 ['[ 101 ]', '101 ] HMM', '] HMM restricted', 'HMM restricted application', 'restricted application several', 'application several others', 'several others bioinformatics', 'others bioinformatics problems', 'bioinformatics problems ,', 'problems , example', ', example ,', 'example , multiple', ', multiple sequence', 'multiple sequence alignment', 'sequence alignment [', 'alignment [ 102', '[ 102 ]', '102 ] .'] 

 TOTAL TRIGRAMS --> 18 



 ---- NOUN PHRASES ---- 

 ['application', 'example', 'multiple sequence', 'alignment', ']'] 

 TOTAL NOUN PHRASES --> 5 



 ---- NER ----

 
 ORGANIZATION ---> ['HMM']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['[', '101', ']', 'hmm', 'restrict', 'applic', 'sever', 'other', 'bioinformat', 'problem', ',', 'exampl', ',', 'multipl', 'sequenc', 'align', '[', '102', ']', '.']

 TOTAL PORTER STEM WORDS ==> 20



 ---- SNOWBALL STEMMING ----

['[', '101', ']', 'hmm', 'restrict', 'applic', 'sever', 'other', 'bioinformat', 'problem', ',', 'exampl', ',', 'multipl', 'sequenc', 'align', '[', '102', ']', '.']

 TOTAL SNOWBALL STEM WORDS ==> 20



 ---- LEMMATIZATION ----

['[', '101', ']', 'HMM', 'restricted', 'application', 'several', 'others', 'bioinformatics', 'problem', ',', 'example', ',', 'multiple', 'sequence', 'alignment', '[', '102', ']', '.']

 TOTAL LEMMATIZE WORDS ==> 20

************************************************************************************************************************

303 --> Sonnhammer mentioned that  Pfam hold multiple alignments and hidden Markov model based profiles (HMM-profiles) of  entire protein domains. 


 ---- TOKENS ----

 ['Sonnhammer', 'mentioned', 'that', 'Pfam', 'hold', 'multiple', 'alignments', 'and', 'hidden', 'Markov', 'model', 'based', 'profiles', '(', 'HMM-profiles', ')', 'of', 'entire', 'protein', 'domains', '.'] 

 TOTAL TOKENS ==> 21

 ---- POST ----

 [('Sonnhammer', 'NN'), ('mentioned', 'VBD'), ('that', 'IN'), ('Pfam', 'NNP'), ('hold', 'VBP'), ('multiple', 'JJ'), ('alignments', 'NNS'), ('and', 'CC'), ('hidden', 'JJ'), ('Markov', 'NNP'), ('model', 'NN'), ('based', 'VBN'), ('profiles', 'NNS'), ('(', '('), ('HMM-profiles', 'NNP'), (')', ')'), ('of', 'IN'), ('entire', 'JJ'), ('protein', 'NN'), ('domains', 'NNS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Sonnhammer', 'mentioned', 'Pfam', 'hold', 'multiple', 'alignments', 'hidden', 'Markov', 'model', 'based', 'profiles', '(', 'HMM-profiles', ')', 'entire', 'protein', 'domains', '.']

 TOTAL FILTERED TOKENS ==>  18

 ---- POST FOR FILTERED TOKENS ----

 [('Sonnhammer', 'NNP'), ('mentioned', 'VBD'), ('Pfam', 'NNP'), ('hold', 'VB'), ('multiple', 'JJ'), ('alignments', 'NNS'), ('hidden', 'JJ'), ('Markov', 'NNP'), ('model', 'NN'), ('based', 'VBN'), ('profiles', 'NNS'), ('(', '('), ('HMM-profiles', 'NNP'), (')', ')'), ('entire', 'JJ'), ('protein', 'NN'), ('domains', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Sonnhammer mentioned', 'mentioned Pfam', 'Pfam hold', 'hold multiple', 'multiple alignments', 'alignments hidden', 'hidden Markov', 'Markov model', 'model based', 'based profiles', 'profiles (', '( HMM-profiles', 'HMM-profiles )', ') entire', 'entire protein', 'protein domains', 'domains .'] 

 TOTAL BIGRAMS --> 17 



 ---- TRI-GRAMS ---- 

 ['Sonnhammer mentioned Pfam', 'mentioned Pfam hold', 'Pfam hold multiple', 'hold multiple alignments', 'multiple alignments hidden', 'alignments hidden Markov', 'hidden Markov model', 'Markov model based', 'model based profiles', 'based profiles (', 'profiles ( HMM-profiles', '( HMM-profiles )', 'HMM-profiles ) entire', ') entire protein', 'entire protein domains', 'protein domains .'] 

 TOTAL TRIGRAMS --> 16 



 ---- NOUN PHRASES ---- 

 ['model', 'entire protein'] 

 TOTAL NOUN PHRASES --> 2 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> ['Sonnhammer', 'Pfam', 'Markov']
 TOTAL PERSON ENTITY --> 3 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['sonnhamm', 'mention', 'pfam', 'hold', 'multipl', 'align', 'hidden', 'markov', 'model', 'base', 'profil', '(', 'hmm-profil', ')', 'entir', 'protein', 'domain', '.']

 TOTAL PORTER STEM WORDS ==> 18



 ---- SNOWBALL STEMMING ----

['sonnhamm', 'mention', 'pfam', 'hold', 'multipl', 'align', 'hidden', 'markov', 'model', 'base', 'profil', '(', 'hmm-profil', ')', 'entir', 'protein', 'domain', '.']

 TOTAL SNOWBALL STEM WORDS ==> 18



 ---- LEMMATIZATION ----

['Sonnhammer', 'mentioned', 'Pfam', 'hold', 'multiple', 'alignment', 'hidden', 'Markov', 'model', 'based', 'profile', '(', 'HMM-profiles', ')', 'entire', 'protein', 'domain', '.']

 TOTAL LEMMATIZE WORDS ==> 18

************************************************************************************************************************

304 --> The cue of domain boundaries, family members and alignment is  done semi-automatically found on expert knowledge, sequence similarity, other protein  family databases and the capability of HMM-profiles to correctly identify and align the  members. 


 ---- TOKENS ----

 ['The', 'cue', 'of', 'domain', 'boundaries', ',', 'family', 'members', 'and', 'alignment', 'is', 'done', 'semi-automatically', 'found', 'on', 'expert', 'knowledge', ',', 'sequence', 'similarity', ',', 'other', 'protein', 'family', 'databases', 'and', 'the', 'capability', 'of', 'HMM-profiles', 'to', 'correctly', 'identify', 'and', 'align', 'the', 'members', '.'] 

 TOTAL TOKENS ==> 38

 ---- POST ----

 [('The', 'DT'), ('cue', 'NN'), ('of', 'IN'), ('domain', 'NN'), ('boundaries', 'NNS'), (',', ','), ('family', 'NN'), ('members', 'NNS'), ('and', 'CC'), ('alignment', 'NN'), ('is', 'VBZ'), ('done', 'VBN'), ('semi-automatically', 'RB'), ('found', 'VBN'), ('on', 'IN'), ('expert', 'NN'), ('knowledge', 'NN'), (',', ','), ('sequence', 'NN'), ('similarity', 'NN'), (',', ','), ('other', 'JJ'), ('protein', 'FW'), ('family', 'NN'), ('databases', 'NNS'), ('and', 'CC'), ('the', 'DT'), ('capability', 'NN'), ('of', 'IN'), ('HMM-profiles', 'NNP'), ('to', 'TO'), ('correctly', 'VB'), ('identify', 'VB'), ('and', 'CC'), ('align', 'VB'), ('the', 'DT'), ('members', 'NNS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['cue', 'domain', 'boundaries', ',', 'family', 'members', 'alignment', 'done', 'semi-automatically', 'found', 'expert', 'knowledge', ',', 'sequence', 'similarity', ',', 'protein', 'family', 'databases', 'capability', 'HMM-profiles', 'correctly', 'identify', 'align', 'members', '.']

 TOTAL FILTERED TOKENS ==>  26

 ---- POST FOR FILTERED TOKENS ----

 [('cue', 'NN'), ('domain', 'NN'), ('boundaries', 'NNS'), (',', ','), ('family', 'NN'), ('members', 'NNS'), ('alignment', 'VBP'), ('done', 'VBN'), ('semi-automatically', 'RB'), ('found', 'VBN'), ('expert', 'NN'), ('knowledge', 'NN'), (',', ','), ('sequence', 'NN'), ('similarity', 'NN'), (',', ','), ('protein', 'VBP'), ('family', 'NN'), ('databases', 'NNS'), ('capability', 'NN'), ('HMM-profiles', 'NNP'), ('correctly', 'RB'), ('identify', 'VBZ'), ('align', 'JJ'), ('members', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['cue domain', 'domain boundaries', 'boundaries ,', ', family', 'family members', 'members alignment', 'alignment done', 'done semi-automatically', 'semi-automatically found', 'found expert', 'expert knowledge', 'knowledge ,', ', sequence', 'sequence similarity', 'similarity ,', ', protein', 'protein family', 'family databases', 'databases capability', 'capability HMM-profiles', 'HMM-profiles correctly', 'correctly identify', 'identify align', 'align members', 'members .'] 

 TOTAL BIGRAMS --> 25 



 ---- TRI-GRAMS ---- 

 ['cue domain boundaries', 'domain boundaries ,', 'boundaries , family', ', family members', 'family members alignment', 'members alignment done', 'alignment done semi-automatically', 'done semi-automatically found', 'semi-automatically found expert', 'found expert knowledge', 'expert knowledge ,', 'knowledge , sequence', ', sequence similarity', 'sequence similarity ,', 'similarity , protein', ', protein family', 'protein family databases', 'family databases capability', 'databases capability HMM-profiles', 'capability HMM-profiles correctly', 'HMM-profiles correctly identify', 'correctly identify align', 'identify align members', 'align members .'] 

 TOTAL TRIGRAMS --> 24 



 ---- NOUN PHRASES ---- 

 ['cue', 'domain', 'family', 'expert', 'knowledge', 'sequence', 'similarity', 'family', 'capability'] 

 TOTAL NOUN PHRASES --> 9 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['cue', 'domain', 'boundari', ',', 'famili', 'member', 'align', 'done', 'semi-automat', 'found', 'expert', 'knowledg', ',', 'sequenc', 'similar', ',', 'protein', 'famili', 'databas', 'capabl', 'hmm-profil', 'correctli', 'identifi', 'align', 'member', '.']

 TOTAL PORTER STEM WORDS ==> 26



 ---- SNOWBALL STEMMING ----

['cue', 'domain', 'boundari', ',', 'famili', 'member', 'align', 'done', 'semi-automat', 'found', 'expert', 'knowledg', ',', 'sequenc', 'similar', ',', 'protein', 'famili', 'databas', 'capabl', 'hmm-profil', 'correct', 'identifi', 'align', 'member', '.']

 TOTAL SNOWBALL STEM WORDS ==> 26



 ---- LEMMATIZATION ----

['cue', 'domain', 'boundary', ',', 'family', 'member', 'alignment', 'done', 'semi-automatically', 'found', 'expert', 'knowledge', ',', 'sequence', 'similarity', ',', 'protein', 'family', 'database', 'capability', 'HMM-profiles', 'correctly', 'identify', 'align', 'member', '.']

 TOTAL LEMMATIZE WORDS ==> 26

************************************************************************************************************************

305 --> [103]   7.2 Naive Bayes Classifiers   The choice of area is wide ranging covering usual items like word segmentation and  translation but also unusual areas like segmentation for infant learning and identifying  documents for opinions and facts. 


 ---- TOKENS ----

 ['[', '103', ']', '7.2', 'Naive', 'Bayes', 'Classifiers', 'The', 'choice', 'of', 'area', 'is', 'wide', 'ranging', 'covering', 'usual', 'items', 'like', 'word', 'segmentation', 'and', 'translation', 'but', 'also', 'unusual', 'areas', 'like', 'segmentation', 'for', 'infant', 'learning', 'and', 'identifying', 'documents', 'for', 'opinions', 'and', 'facts', '.'] 

 TOTAL TOKENS ==> 39

 ---- POST ----

 [('[', 'RB'), ('103', 'CD'), (']', 'JJ'), ('7.2', 'CD'), ('Naive', 'JJ'), ('Bayes', 'NNP'), ('Classifiers', 'NNPS'), ('The', 'DT'), ('choice', 'NN'), ('of', 'IN'), ('area', 'NN'), ('is', 'VBZ'), ('wide', 'JJ'), ('ranging', 'VBG'), ('covering', 'VBG'), ('usual', 'JJ'), ('items', 'NNS'), ('like', 'IN'), ('word', 'NN'), ('segmentation', 'NN'), ('and', 'CC'), ('translation', 'NN'), ('but', 'CC'), ('also', 'RB'), ('unusual', 'JJ'), ('areas', 'NNS'), ('like', 'IN'), ('segmentation', 'NN'), ('for', 'IN'), ('infant', 'NN'), ('learning', 'NN'), ('and', 'CC'), ('identifying', 'NN'), ('documents', 'NNS'), ('for', 'IN'), ('opinions', 'NNS'), ('and', 'CC'), ('facts', 'NNS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['[', '103', ']', '7.2', 'Naive', 'Bayes', 'Classifiers', 'choice', 'area', 'wide', 'ranging', 'covering', 'usual', 'items', 'like', 'word', 'segmentation', 'translation', 'also', 'unusual', 'areas', 'like', 'segmentation', 'infant', 'learning', 'identifying', 'documents', 'opinions', 'facts', '.']

 TOTAL FILTERED TOKENS ==>  30

 ---- POST FOR FILTERED TOKENS ----

 [('[', 'RB'), ('103', 'CD'), (']', 'JJ'), ('7.2', 'CD'), ('Naive', 'JJ'), ('Bayes', 'NNP'), ('Classifiers', 'NNP'), ('choice', 'NN'), ('area', 'NN'), ('wide', 'JJ'), ('ranging', 'VBG'), ('covering', 'VBG'), ('usual', 'JJ'), ('items', 'NNS'), ('like', 'IN'), ('word', 'NN'), ('segmentation', 'NN'), ('translation', 'NN'), ('also', 'RB'), ('unusual', 'JJ'), ('areas', 'NNS'), ('like', 'IN'), ('segmentation', 'NN'), ('infant', 'NN'), ('learning', 'VBG'), ('identifying', 'VBG'), ('documents', 'NNS'), ('opinions', 'NNS'), ('facts', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['[ 103', '103 ]', '] 7.2', '7.2 Naive', 'Naive Bayes', 'Bayes Classifiers', 'Classifiers choice', 'choice area', 'area wide', 'wide ranging', 'ranging covering', 'covering usual', 'usual items', 'items like', 'like word', 'word segmentation', 'segmentation translation', 'translation also', 'also unusual', 'unusual areas', 'areas like', 'like segmentation', 'segmentation infant', 'infant learning', 'learning identifying', 'identifying documents', 'documents opinions', 'opinions facts', 'facts .'] 

 TOTAL BIGRAMS --> 29 



 ---- TRI-GRAMS ---- 

 ['[ 103 ]', '103 ] 7.2', '] 7.2 Naive', '7.2 Naive Bayes', 'Naive Bayes Classifiers', 'Bayes Classifiers choice', 'Classifiers choice area', 'choice area wide', 'area wide ranging', 'wide ranging covering', 'ranging covering usual', 'covering usual items', 'usual items like', 'items like word', 'like word segmentation', 'word segmentation translation', 'segmentation translation also', 'translation also unusual', 'also unusual areas', 'unusual areas like', 'areas like segmentation', 'like segmentation infant', 'segmentation infant learning', 'infant learning identifying', 'learning identifying documents', 'identifying documents opinions', 'documents opinions facts', 'opinions facts .'] 

 TOTAL TRIGRAMS --> 28 



 ---- NOUN PHRASES ---- 

 ['choice', 'area', 'word', 'segmentation', 'translation', 'segmentation', 'infant'] 

 TOTAL NOUN PHRASES --> 7 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> ['Bayes Classifiers']
 TOTAL PERSON ENTITY --> 1 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['[', '103', ']', '7.2', 'naiv', 'bay', 'classifi', 'choic', 'area', 'wide', 'rang', 'cover', 'usual', 'item', 'like', 'word', 'segment', 'translat', 'also', 'unusu', 'area', 'like', 'segment', 'infant', 'learn', 'identifi', 'document', 'opinion', 'fact', '.']

 TOTAL PORTER STEM WORDS ==> 30



 ---- SNOWBALL STEMMING ----

['[', '103', ']', '7.2', 'naiv', 'bay', 'classifi', 'choic', 'area', 'wide', 'rang', 'cover', 'usual', 'item', 'like', 'word', 'segment', 'translat', 'also', 'unusu', 'area', 'like', 'segment', 'infant', 'learn', 'identifi', 'document', 'opinion', 'fact', '.']

 TOTAL SNOWBALL STEM WORDS ==> 30



 ---- LEMMATIZATION ----

['[', '103', ']', '7.2', 'Naive', 'Bayes', 'Classifiers', 'choice', 'area', 'wide', 'ranging', 'covering', 'usual', 'item', 'like', 'word', 'segmentation', 'translation', 'also', 'unusual', 'area', 'like', 'segmentation', 'infant', 'learning', 'identifying', 'document', 'opinion', 'fact', '.']

 TOTAL LEMMATIZE WORDS ==> 30

************************************************************************************************************************

306 --> In addition, exclusive article was selected for its use of  Bayesian methods to aid the research in designing algorithms for their investigation. 


 ---- TOKENS ----

 ['In', 'addition', ',', 'exclusive', 'article', 'was', 'selected', 'for', 'its', 'use', 'of', 'Bayesian', 'methods', 'to', 'aid', 'the', 'research', 'in', 'designing', 'algorithms', 'for', 'their', 'investigation', '.'] 

 TOTAL TOKENS ==> 24

 ---- POST ----

 [('In', 'IN'), ('addition', 'NN'), (',', ','), ('exclusive', 'JJ'), ('article', 'NN'), ('was', 'VBD'), ('selected', 'VBN'), ('for', 'IN'), ('its', 'PRP$'), ('use', 'NN'), ('of', 'IN'), ('Bayesian', 'JJ'), ('methods', 'NNS'), ('to', 'TO'), ('aid', 'VB'), ('the', 'DT'), ('research', 'NN'), ('in', 'IN'), ('designing', 'VBG'), ('algorithms', 'NN'), ('for', 'IN'), ('their', 'PRP$'), ('investigation', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['addition', ',', 'exclusive', 'article', 'selected', 'use', 'Bayesian', 'methods', 'aid', 'research', 'designing', 'algorithms', 'investigation', '.']

 TOTAL FILTERED TOKENS ==>  14

 ---- POST FOR FILTERED TOKENS ----

 [('addition', 'NN'), (',', ','), ('exclusive', 'JJ'), ('article', 'NN'), ('selected', 'VBN'), ('use', 'NN'), ('Bayesian', 'JJ'), ('methods', 'NNS'), ('aid', 'VBD'), ('research', 'NN'), ('designing', 'VBG'), ('algorithms', 'JJ'), ('investigation', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['addition ,', ', exclusive', 'exclusive article', 'article selected', 'selected use', 'use Bayesian', 'Bayesian methods', 'methods aid', 'aid research', 'research designing', 'designing algorithms', 'algorithms investigation', 'investigation .'] 

 TOTAL BIGRAMS --> 13 



 ---- TRI-GRAMS ---- 

 ['addition , exclusive', ', exclusive article', 'exclusive article selected', 'article selected use', 'selected use Bayesian', 'use Bayesian methods', 'Bayesian methods aid', 'methods aid research', 'aid research designing', 'research designing algorithms', 'designing algorithms investigation', 'algorithms investigation .'] 

 TOTAL TRIGRAMS --> 12 



 ---- NOUN PHRASES ---- 

 ['addition', 'exclusive article', 'use', 'research', 'algorithms investigation'] 

 TOTAL NOUN PHRASES --> 5 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> ['Bayesian']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['addit', ',', 'exclus', 'articl', 'select', 'use', 'bayesian', 'method', 'aid', 'research', 'design', 'algorithm', 'investig', '.']

 TOTAL PORTER STEM WORDS ==> 14



 ---- SNOWBALL STEMMING ----

['addit', ',', 'exclus', 'articl', 'select', 'use', 'bayesian', 'method', 'aid', 'research', 'design', 'algorithm', 'investig', '.']

 TOTAL SNOWBALL STEM WORDS ==> 14



 ---- LEMMATIZATION ----

['addition', ',', 'exclusive', 'article', 'selected', 'use', 'Bayesian', 'method', 'aid', 'research', 'designing', 'algorithm', 'investigation', '.']

 TOTAL LEMMATIZE WORDS ==> 14

************************************************************************************************************************

307 --> 8. 


 ---- TOKENS ----

 ['8', '.'] 

 TOTAL TOKENS ==> 2

 ---- POST ----

 [('8', 'CD'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['8', '.']

 TOTAL FILTERED TOKENS ==>  2

 ---- POST FOR FILTERED TOKENS ----

 [('8', 'CD'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['8 .'] 

 TOTAL BIGRAMS --> 1 



 ---- TRI-GRAMS ---- 

 [] 

 TOTAL TRIGRAMS --> 0 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['8', '.']

 TOTAL PORTER STEM WORDS ==> 2



 ---- SNOWBALL STEMMING ----

['8', '.']

 TOTAL SNOWBALL STEM WORDS ==> 2



 ---- LEMMATIZATION ----

['8', '.']

 TOTAL LEMMATIZE WORDS ==> 2

************************************************************************************************************************

308 --> NLP in Talk  This section discusses the recent developments in the NLP projects implemented by various  companies and these are as follows:  8.1 ACE Powered GDPR Robot Launched by RAVN Systems [104]  RAVN Systems, an leading expert in Artificial Intelligence (AI), Search and Knowledge  Management Solutions, announced the launch of a RAVN ("Applied Cognitive Engine") i.e  powered software Robot to help and facilitate the GDPR ("General Data Protection  Regulation") compliance. 


 ---- TOKENS ----

 ['NLP', 'in', 'Talk', 'This', 'section', 'discusses', 'the', 'recent', 'developments', 'in', 'the', 'NLP', 'projects', 'implemented', 'by', 'various', 'companies', 'and', 'these', 'are', 'as', 'follows', ':', '8.1', 'ACE', 'Powered', 'GDPR', 'Robot', 'Launched', 'by', 'RAVN', 'Systems', '[', '104', ']', 'RAVN', 'Systems', ',', 'an', 'leading', 'expert', 'in', 'Artificial', 'Intelligence', '(', 'AI', ')', ',', 'Search', 'and', 'Knowledge', 'Management', 'Solutions', ',', 'announced', 'the', 'launch', 'of', 'a', 'RAVN', '(', '``', 'Applied', 'Cognitive', 'Engine', "''", ')', 'i.e', 'powered', 'software', 'Robot', 'to', 'help', 'and', 'facilitate', 'the', 'GDPR', '(', '``', 'General', 'Data', 'Protection', 'Regulation', "''", ')', 'compliance', '.'] 

 TOTAL TOKENS ==> 87

 ---- POST ----

 [('NLP', 'NNP'), ('in', 'IN'), ('Talk', 'NNP'), ('This', 'DT'), ('section', 'NN'), ('discusses', 'VBZ'), ('the', 'DT'), ('recent', 'JJ'), ('developments', 'NNS'), ('in', 'IN'), ('the', 'DT'), ('NLP', 'NNP'), ('projects', 'NNS'), ('implemented', 'VBN'), ('by', 'IN'), ('various', 'JJ'), ('companies', 'NNS'), ('and', 'CC'), ('these', 'DT'), ('are', 'VBP'), ('as', 'IN'), ('follows', 'VBZ'), (':', ':'), ('8.1', 'CD'), ('ACE', 'NNP'), ('Powered', 'NNP'), ('GDPR', 'NNP'), ('Robot', 'NNP'), ('Launched', 'VBN'), ('by', 'IN'), ('RAVN', 'NNP'), ('Systems', 'NNPS'), ('[', 'VBD'), ('104', 'CD'), (']', 'JJ'), ('RAVN', 'NNP'), ('Systems', 'NNP'), (',', ','), ('an', 'DT'), ('leading', 'VBG'), ('expert', 'NN'), ('in', 'IN'), ('Artificial', 'NNP'), ('Intelligence', 'NNP'), ('(', '('), ('AI', 'NNP'), (')', ')'), (',', ','), ('Search', 'NNP'), ('and', 'CC'), ('Knowledge', 'NNP'), ('Management', 'NNP'), ('Solutions', 'NNP'), (',', ','), ('announced', 'VBD'), ('the', 'DT'), ('launch', 'NN'), ('of', 'IN'), ('a', 'DT'), ('RAVN', 'NNP'), ('(', '('), ('``', '``'), ('Applied', 'NNP'), ('Cognitive', 'NNP'), ('Engine', 'NNP'), ("''", "''"), (')', ')'), ('i.e', 'NN'), ('powered', 'VBN'), ('software', 'NN'), ('Robot', 'NNP'), ('to', 'TO'), ('help', 'VB'), ('and', 'CC'), ('facilitate', 'VB'), ('the', 'DT'), ('GDPR', 'NNP'), ('(', '('), ('``', '``'), ('General', 'NNP'), ('Data', 'NNP'), ('Protection', 'NNP'), ('Regulation', 'NNP'), ("''", "''"), (')', ')'), ('compliance', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['NLP', 'Talk', 'section', 'discusses', 'recent', 'developments', 'NLP', 'projects', 'implemented', 'various', 'companies', 'follows', ':', '8.1', 'ACE', 'Powered', 'GDPR', 'Robot', 'Launched', 'RAVN', 'Systems', '[', '104', ']', 'RAVN', 'Systems', ',', 'leading', 'expert', 'Artificial', 'Intelligence', '(', 'AI', ')', ',', 'Search', 'Knowledge', 'Management', 'Solutions', ',', 'announced', 'launch', 'RAVN', '(', '``', 'Applied', 'Cognitive', 'Engine', "''", ')', 'i.e', 'powered', 'software', 'Robot', 'help', 'facilitate', 'GDPR', '(', '``', 'General', 'Data', 'Protection', 'Regulation', "''", ')', 'compliance', '.']

 TOTAL FILTERED TOKENS ==>  67

 ---- POST FOR FILTERED TOKENS ----

 [('NLP', 'NNP'), ('Talk', 'NNP'), ('section', 'NN'), ('discusses', 'VBZ'), ('recent', 'JJ'), ('developments', 'NNS'), ('NLP', 'NNP'), ('projects', 'NNS'), ('implemented', 'VBD'), ('various', 'JJ'), ('companies', 'NNS'), ('follows', 'VBZ'), (':', ':'), ('8.1', 'CD'), ('ACE', 'NNP'), ('Powered', 'NNP'), ('GDPR', 'NNP'), ('Robot', 'NNP'), ('Launched', 'NNP'), ('RAVN', 'NNP'), ('Systems', 'NNP'), ('[', 'NNP'), ('104', 'CD'), (']', 'NNP'), ('RAVN', 'NNP'), ('Systems', 'NNPS'), (',', ','), ('leading', 'VBG'), ('expert', 'JJ'), ('Artificial', 'JJ'), ('Intelligence', 'NNP'), ('(', '('), ('AI', 'NNP'), (')', ')'), (',', ','), ('Search', 'NNP'), ('Knowledge', 'NNP'), ('Management', 'NNP'), ('Solutions', 'NNP'), (',', ','), ('announced', 'VBD'), ('launch', 'JJ'), ('RAVN', 'NNP'), ('(', '('), ('``', '``'), ('Applied', 'NNP'), ('Cognitive', 'NNP'), ('Engine', 'NNP'), ("''", "''"), (')', ')'), ('i.e', 'NN'), ('powered', 'VBN'), ('software', 'NN'), ('Robot', 'NNP'), ('help', 'NN'), ('facilitate', 'VB'), ('GDPR', 'NNP'), ('(', '('), ('``', '``'), ('General', 'NNP'), ('Data', 'NNP'), ('Protection', 'NNP'), ('Regulation', 'NNP'), ("''", "''"), (')', ')'), ('compliance', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['NLP Talk', 'Talk section', 'section discusses', 'discusses recent', 'recent developments', 'developments NLP', 'NLP projects', 'projects implemented', 'implemented various', 'various companies', 'companies follows', 'follows :', ': 8.1', '8.1 ACE', 'ACE Powered', 'Powered GDPR', 'GDPR Robot', 'Robot Launched', 'Launched RAVN', 'RAVN Systems', 'Systems [', '[ 104', '104 ]', '] RAVN', 'RAVN Systems', 'Systems ,', ', leading', 'leading expert', 'expert Artificial', 'Artificial Intelligence', 'Intelligence (', '( AI', 'AI )', ') ,', ', Search', 'Search Knowledge', 'Knowledge Management', 'Management Solutions', 'Solutions ,', ', announced', 'announced launch', 'launch RAVN', 'RAVN (', '( ``', '`` Applied', 'Applied Cognitive', 'Cognitive Engine', "Engine ''", "'' )", ') i.e', 'i.e powered', 'powered software', 'software Robot', 'Robot help', 'help facilitate', 'facilitate GDPR', 'GDPR (', '( ``', '`` General', 'General Data', 'Data Protection', 'Protection Regulation', "Regulation ''", "'' )", ') compliance', 'compliance .'] 

 TOTAL BIGRAMS --> 66 



 ---- TRI-GRAMS ---- 

 ['NLP Talk section', 'Talk section discusses', 'section discusses recent', 'discusses recent developments', 'recent developments NLP', 'developments NLP projects', 'NLP projects implemented', 'projects implemented various', 'implemented various companies', 'various companies follows', 'companies follows :', 'follows : 8.1', ': 8.1 ACE', '8.1 ACE Powered', 'ACE Powered GDPR', 'Powered GDPR Robot', 'GDPR Robot Launched', 'Robot Launched RAVN', 'Launched RAVN Systems', 'RAVN Systems [', 'Systems [ 104', '[ 104 ]', '104 ] RAVN', '] RAVN Systems', 'RAVN Systems ,', 'Systems , leading', ', leading expert', 'leading expert Artificial', 'expert Artificial Intelligence', 'Artificial Intelligence (', 'Intelligence ( AI', '( AI )', 'AI ) ,', ') , Search', ', Search Knowledge', 'Search Knowledge Management', 'Knowledge Management Solutions', 'Management Solutions ,', 'Solutions , announced', ', announced launch', 'announced launch RAVN', 'launch RAVN (', 'RAVN ( ``', '( `` Applied', '`` Applied Cognitive', 'Applied Cognitive Engine', "Cognitive Engine ''", "Engine '' )", "'' ) i.e", ') i.e powered', 'i.e powered software', 'powered software Robot', 'software Robot help', 'Robot help facilitate', 'help facilitate GDPR', 'facilitate GDPR (', 'GDPR ( ``', '( `` General', '`` General Data', 'General Data Protection', 'Data Protection Regulation', "Protection Regulation ''", "Regulation '' )", "'' ) compliance", ') compliance .'] 

 TOTAL TRIGRAMS --> 65 



 ---- NOUN PHRASES ---- 

 ['section', 'i.e', 'software', 'help', 'compliance'] 

 TOTAL NOUN PHRASES --> 5 



 ---- NER ----

 
 ORGANIZATION ---> ['NLP Talk', 'NLP', 'ACE Powered', 'RAVN Systems', 'Artificial Intelligence', 'RAVN', 'GDPR']
 TOTAL ORGANIZATION ENTITY --> 7 


 PERSON ---> ['Robot Launched', 'Search Knowledge', 'Robot']
 TOTAL PERSON ENTITY --> 3 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['nlp', 'talk', 'section', 'discuss', 'recent', 'develop', 'nlp', 'project', 'implement', 'variou', 'compani', 'follow', ':', '8.1', 'ace', 'power', 'gdpr', 'robot', 'launch', 'ravn', 'system', '[', '104', ']', 'ravn', 'system', ',', 'lead', 'expert', 'artifici', 'intellig', '(', 'ai', ')', ',', 'search', 'knowledg', 'manag', 'solut', ',', 'announc', 'launch', 'ravn', '(', '``', 'appli', 'cognit', 'engin', "''", ')', 'i.e', 'power', 'softwar', 'robot', 'help', 'facilit', 'gdpr', '(', '``', 'gener', 'data', 'protect', 'regul', "''", ')', 'complianc', '.']

 TOTAL PORTER STEM WORDS ==> 67



 ---- SNOWBALL STEMMING ----

['nlp', 'talk', 'section', 'discuss', 'recent', 'develop', 'nlp', 'project', 'implement', 'various', 'compani', 'follow', ':', '8.1', 'ace', 'power', 'gdpr', 'robot', 'launch', 'ravn', 'system', '[', '104', ']', 'ravn', 'system', ',', 'lead', 'expert', 'artifici', 'intellig', '(', 'ai', ')', ',', 'search', 'knowledg', 'manag', 'solut', ',', 'announc', 'launch', 'ravn', '(', '``', 'appli', 'cognit', 'engin', "''", ')', 'i.e', 'power', 'softwar', 'robot', 'help', 'facilit', 'gdpr', '(', '``', 'general', 'data', 'protect', 'regul', "''", ')', 'complianc', '.']

 TOTAL SNOWBALL STEM WORDS ==> 67



 ---- LEMMATIZATION ----

['NLP', 'Talk', 'section', 'discus', 'recent', 'development', 'NLP', 'project', 'implemented', 'various', 'company', 'follows', ':', '8.1', 'ACE', 'Powered', 'GDPR', 'Robot', 'Launched', 'RAVN', 'Systems', '[', '104', ']', 'RAVN', 'Systems', ',', 'leading', 'expert', 'Artificial', 'Intelligence', '(', 'AI', ')', ',', 'Search', 'Knowledge', 'Management', 'Solutions', ',', 'announced', 'launch', 'RAVN', '(', '``', 'Applied', 'Cognitive', 'Engine', "''", ')', 'i.e', 'powered', 'software', 'Robot', 'help', 'facilitate', 'GDPR', '(', '``', 'General', 'Data', 'Protection', 'Regulation', "''", ')', 'compliance', '.']

 TOTAL LEMMATIZE WORDS ==> 67

************************************************************************************************************************

309 --> The Robot uses AI techniques to automatically analyse documents and other types of data in  any business system which is subject to GDPR rules. 


 ---- TOKENS ----

 ['The', 'Robot', 'uses', 'AI', 'techniques', 'to', 'automatically', 'analyse', 'documents', 'and', 'other', 'types', 'of', 'data', 'in', 'any', 'business', 'system', 'which', 'is', 'subject', 'to', 'GDPR', 'rules', '.'] 

 TOTAL TOKENS ==> 25

 ---- POST ----

 [('The', 'DT'), ('Robot', 'NNP'), ('uses', 'VBZ'), ('AI', 'NNP'), ('techniques', 'NNS'), ('to', 'TO'), ('automatically', 'RB'), ('analyse', 'VB'), ('documents', 'NNS'), ('and', 'CC'), ('other', 'JJ'), ('types', 'NNS'), ('of', 'IN'), ('data', 'NNS'), ('in', 'IN'), ('any', 'DT'), ('business', 'NN'), ('system', 'NN'), ('which', 'WDT'), ('is', 'VBZ'), ('subject', 'JJ'), ('to', 'TO'), ('GDPR', 'NNP'), ('rules', 'NNS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Robot', 'uses', 'AI', 'techniques', 'automatically', 'analyse', 'documents', 'types', 'data', 'business', 'system', 'subject', 'GDPR', 'rules', '.']

 TOTAL FILTERED TOKENS ==>  15

 ---- POST FOR FILTERED TOKENS ----

 [('Robot', 'NNP'), ('uses', 'VBZ'), ('AI', 'NNP'), ('techniques', 'NNS'), ('automatically', 'RB'), ('analyse', 'JJ'), ('documents', 'NNS'), ('types', 'VBZ'), ('data', 'NNS'), ('business', 'NN'), ('system', 'NN'), ('subject', 'JJ'), ('GDPR', 'NNP'), ('rules', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Robot uses', 'uses AI', 'AI techniques', 'techniques automatically', 'automatically analyse', 'analyse documents', 'documents types', 'types data', 'data business', 'business system', 'system subject', 'subject GDPR', 'GDPR rules', 'rules .'] 

 TOTAL BIGRAMS --> 14 



 ---- TRI-GRAMS ---- 

 ['Robot uses AI', 'uses AI techniques', 'AI techniques automatically', 'techniques automatically analyse', 'automatically analyse documents', 'analyse documents types', 'documents types data', 'types data business', 'data business system', 'business system subject', 'system subject GDPR', 'subject GDPR rules', 'GDPR rules .'] 

 TOTAL TRIGRAMS --> 13 



 ---- NOUN PHRASES ---- 

 ['business', 'system'] 

 TOTAL NOUN PHRASES --> 2 



 ---- NER ----

 
 ORGANIZATION ---> ['GDPR']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> ['Robot']
 TOTAL PERSON ENTITY --> 1 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['robot', 'use', 'ai', 'techniqu', 'automat', 'analys', 'document', 'type', 'data', 'busi', 'system', 'subject', 'gdpr', 'rule', '.']

 TOTAL PORTER STEM WORDS ==> 15



 ---- SNOWBALL STEMMING ----

['robot', 'use', 'ai', 'techniqu', 'automat', 'analys', 'document', 'type', 'data', 'busi', 'system', 'subject', 'gdpr', 'rule', '.']

 TOTAL SNOWBALL STEM WORDS ==> 15



 ---- LEMMATIZATION ----

['Robot', 'us', 'AI', 'technique', 'automatically', 'analyse', 'document', 'type', 'data', 'business', 'system', 'subject', 'GDPR', 'rule', '.']

 TOTAL LEMMATIZE WORDS ==> 15

************************************************************************************************************************

310 --> It allows users to quickly and easily  search, retrieve, flag, classify and report on data mediated to be supersensitive under GDPR. 


 ---- TOKENS ----

 ['It', 'allows', 'users', 'to', 'quickly', 'and', 'easily', 'search', ',', 'retrieve', ',', 'flag', ',', 'classify', 'and', 'report', 'on', 'data', 'mediated', 'to', 'be', 'supersensitive', 'under', 'GDPR', '.'] 

 TOTAL TOKENS ==> 25

 ---- POST ----

 [('It', 'PRP'), ('allows', 'VBZ'), ('users', 'NNS'), ('to', 'TO'), ('quickly', 'RB'), ('and', 'CC'), ('easily', 'RB'), ('search', 'NN'), (',', ','), ('retrieve', 'VBP'), (',', ','), ('flag', 'NN'), (',', ','), ('classify', 'NN'), ('and', 'CC'), ('report', 'NN'), ('on', 'IN'), ('data', 'NNS'), ('mediated', 'VBN'), ('to', 'TO'), ('be', 'VB'), ('supersensitive', 'JJ'), ('under', 'IN'), ('GDPR', 'NNP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['allows', 'users', 'quickly', 'easily', 'search', ',', 'retrieve', ',', 'flag', ',', 'classify', 'report', 'data', 'mediated', 'supersensitive', 'GDPR', '.']

 TOTAL FILTERED TOKENS ==>  17

 ---- POST FOR FILTERED TOKENS ----

 [('allows', 'NNS'), ('users', 'NNS'), ('quickly', 'RB'), ('easily', 'RB'), ('search', 'NN'), (',', ','), ('retrieve', 'VBP'), (',', ','), ('flag', 'NN'), (',', ','), ('classify', 'VB'), ('report', 'NN'), ('data', 'NNS'), ('mediated', 'VBD'), ('supersensitive', 'JJ'), ('GDPR', 'NNP'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['allows users', 'users quickly', 'quickly easily', 'easily search', 'search ,', ', retrieve', 'retrieve ,', ', flag', 'flag ,', ', classify', 'classify report', 'report data', 'data mediated', 'mediated supersensitive', 'supersensitive GDPR', 'GDPR .'] 

 TOTAL BIGRAMS --> 16 



 ---- TRI-GRAMS ---- 

 ['allows users quickly', 'users quickly easily', 'quickly easily search', 'easily search ,', 'search , retrieve', ', retrieve ,', 'retrieve , flag', ', flag ,', 'flag , classify', ', classify report', 'classify report data', 'report data mediated', 'data mediated supersensitive', 'mediated supersensitive GDPR', 'supersensitive GDPR .'] 

 TOTAL TRIGRAMS --> 15 



 ---- NOUN PHRASES ---- 

 ['search', 'flag', 'report'] 

 TOTAL NOUN PHRASES --> 3 



 ---- NER ----

 
 ORGANIZATION ---> ['GDPR']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['allow', 'user', 'quickli', 'easili', 'search', ',', 'retriev', ',', 'flag', ',', 'classifi', 'report', 'data', 'mediat', 'supersensit', 'gdpr', '.']

 TOTAL PORTER STEM WORDS ==> 17



 ---- SNOWBALL STEMMING ----

['allow', 'user', 'quick', 'easili', 'search', ',', 'retriev', ',', 'flag', ',', 'classifi', 'report', 'data', 'mediat', 'supersensit', 'gdpr', '.']

 TOTAL SNOWBALL STEM WORDS ==> 17



 ---- LEMMATIZATION ----

['allows', 'user', 'quickly', 'easily', 'search', ',', 'retrieve', ',', 'flag', ',', 'classify', 'report', 'data', 'mediated', 'supersensitive', 'GDPR', '.']

 TOTAL LEMMATIZE WORDS ==> 17

************************************************************************************************************************

311 --> Users also have the ability to identify personal data from documents, view feeds on the latest  personal data that requires attention and provide reports on the data suggested to be deleted or  secured. 


 ---- TOKENS ----

 ['Users', 'also', 'have', 'the', 'ability', 'to', 'identify', 'personal', 'data', 'from', 'documents', ',', 'view', 'feeds', 'on', 'the', 'latest', 'personal', 'data', 'that', 'requires', 'attention', 'and', 'provide', 'reports', 'on', 'the', 'data', 'suggested', 'to', 'be', 'deleted', 'or', 'secured', '.'] 

 TOTAL TOKENS ==> 35

 ---- POST ----

 [('Users', 'NNS'), ('also', 'RB'), ('have', 'VBP'), ('the', 'DT'), ('ability', 'NN'), ('to', 'TO'), ('identify', 'VB'), ('personal', 'JJ'), ('data', 'NNS'), ('from', 'IN'), ('documents', 'NNS'), (',', ','), ('view', 'NN'), ('feeds', 'NNS'), ('on', 'IN'), ('the', 'DT'), ('latest', 'JJS'), ('personal', 'JJ'), ('data', 'NNS'), ('that', 'WDT'), ('requires', 'VBZ'), ('attention', 'NN'), ('and', 'CC'), ('provide', 'VB'), ('reports', 'NNS'), ('on', 'IN'), ('the', 'DT'), ('data', 'NNS'), ('suggested', 'VBD'), ('to', 'TO'), ('be', 'VB'), ('deleted', 'VBN'), ('or', 'CC'), ('secured', 'VBN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Users', 'also', 'ability', 'identify', 'personal', 'data', 'documents', ',', 'view', 'feeds', 'latest', 'personal', 'data', 'requires', 'attention', 'provide', 'reports', 'data', 'suggested', 'deleted', 'secured', '.']

 TOTAL FILTERED TOKENS ==>  22

 ---- POST FOR FILTERED TOKENS ----

 [('Users', 'NNS'), ('also', 'RB'), ('ability', 'NN'), ('identify', 'VBP'), ('personal', 'JJ'), ('data', 'NN'), ('documents', 'NNS'), (',', ','), ('view', 'NN'), ('feeds', 'NNS'), ('latest', 'JJS'), ('personal', 'JJ'), ('data', 'NN'), ('requires', 'VBZ'), ('attention', 'NN'), ('provide', 'NN'), ('reports', 'NNS'), ('data', 'NNS'), ('suggested', 'VBD'), ('deleted', 'JJ'), ('secured', 'VBN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Users also', 'also ability', 'ability identify', 'identify personal', 'personal data', 'data documents', 'documents ,', ', view', 'view feeds', 'feeds latest', 'latest personal', 'personal data', 'data requires', 'requires attention', 'attention provide', 'provide reports', 'reports data', 'data suggested', 'suggested deleted', 'deleted secured', 'secured .'] 

 TOTAL BIGRAMS --> 21 



 ---- TRI-GRAMS ---- 

 ['Users also ability', 'also ability identify', 'ability identify personal', 'identify personal data', 'personal data documents', 'data documents ,', 'documents , view', ', view feeds', 'view feeds latest', 'feeds latest personal', 'latest personal data', 'personal data requires', 'data requires attention', 'requires attention provide', 'attention provide reports', 'provide reports data', 'reports data suggested', 'data suggested deleted', 'suggested deleted secured', 'deleted secured .'] 

 TOTAL TRIGRAMS --> 20 



 ---- NOUN PHRASES ---- 

 ['ability', 'personal data', 'view', 'personal data', 'attention', 'provide'] 

 TOTAL NOUN PHRASES --> 6 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['user', 'also', 'abil', 'identifi', 'person', 'data', 'document', ',', 'view', 'feed', 'latest', 'person', 'data', 'requir', 'attent', 'provid', 'report', 'data', 'suggest', 'delet', 'secur', '.']

 TOTAL PORTER STEM WORDS ==> 22



 ---- SNOWBALL STEMMING ----

['user', 'also', 'abil', 'identifi', 'person', 'data', 'document', ',', 'view', 'feed', 'latest', 'person', 'data', 'requir', 'attent', 'provid', 'report', 'data', 'suggest', 'delet', 'secur', '.']

 TOTAL SNOWBALL STEM WORDS ==> 22



 ---- LEMMATIZATION ----

['Users', 'also', 'ability', 'identify', 'personal', 'data', 'document', ',', 'view', 'feed', 'latest', 'personal', 'data', 'requires', 'attention', 'provide', 'report', 'data', 'suggested', 'deleted', 'secured', '.']

 TOTAL LEMMATIZE WORDS ==> 22

************************************************************************************************************************

312 --> RAVN's GDPR Robot is also able to hasten requests for information (Data Subject  Access Requests - "DSAR") in a simple and efficient way, removing the need for a physical  approach to these requests which tends to be very labour thorough. 


 ---- TOKENS ----

 ['RAVN', "'s", 'GDPR', 'Robot', 'is', 'also', 'able', 'to', 'hasten', 'requests', 'for', 'information', '(', 'Data', 'Subject', 'Access', 'Requests', '-', '``', 'DSAR', "''", ')', 'in', 'a', 'simple', 'and', 'efficient', 'way', ',', 'removing', 'the', 'need', 'for', 'a', 'physical', 'approach', 'to', 'these', 'requests', 'which', 'tends', 'to', 'be', 'very', 'labour', 'thorough', '.'] 

 TOTAL TOKENS ==> 47

 ---- POST ----

 [('RAVN', 'NNP'), ("'s", 'POS'), ('GDPR', 'NNP'), ('Robot', 'NNP'), ('is', 'VBZ'), ('also', 'RB'), ('able', 'JJ'), ('to', 'TO'), ('hasten', 'VB'), ('requests', 'NNS'), ('for', 'IN'), ('information', 'NN'), ('(', '('), ('Data', 'NNP'), ('Subject', 'NNP'), ('Access', 'NNP'), ('Requests', 'NNP'), ('-', ':'), ('``', '``'), ('DSAR', 'NNP'), ("''", "''"), (')', ')'), ('in', 'IN'), ('a', 'DT'), ('simple', 'JJ'), ('and', 'CC'), ('efficient', 'JJ'), ('way', 'NN'), (',', ','), ('removing', 'VBG'), ('the', 'DT'), ('need', 'NN'), ('for', 'IN'), ('a', 'DT'), ('physical', 'JJ'), ('approach', 'NN'), ('to', 'TO'), ('these', 'DT'), ('requests', 'NNS'), ('which', 'WDT'), ('tends', 'VBZ'), ('to', 'TO'), ('be', 'VB'), ('very', 'RB'), ('labour', 'JJ'), ('thorough', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['RAVN', "'s", 'GDPR', 'Robot', 'also', 'able', 'hasten', 'requests', 'information', '(', 'Data', 'Subject', 'Access', 'Requests', '-', '``', 'DSAR', "''", ')', 'simple', 'efficient', 'way', ',', 'removing', 'need', 'physical', 'approach', 'requests', 'tends', 'labour', 'thorough', '.']

 TOTAL FILTERED TOKENS ==>  32

 ---- POST FOR FILTERED TOKENS ----

 [('RAVN', 'NNP'), ("'s", 'POS'), ('GDPR', 'NNP'), ('Robot', 'NNP'), ('also', 'RB'), ('able', 'JJ'), ('hasten', 'JJ'), ('requests', 'NNS'), ('information', 'NN'), ('(', '('), ('Data', 'NNP'), ('Subject', 'NNP'), ('Access', 'NNP'), ('Requests', 'NNP'), ('-', ':'), ('``', '``'), ('DSAR', 'NNP'), ("''", "''"), (')', ')'), ('simple', 'JJ'), ('efficient', 'JJ'), ('way', 'NN'), (',', ','), ('removing', 'VBG'), ('need', 'NN'), ('physical', 'JJ'), ('approach', 'NN'), ('requests', 'NNS'), ('tends', 'VBZ'), ('labour', 'JJ'), ('thorough', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ["RAVN 's", "'s GDPR", 'GDPR Robot', 'Robot also', 'also able', 'able hasten', 'hasten requests', 'requests information', 'information (', '( Data', 'Data Subject', 'Subject Access', 'Access Requests', 'Requests -', '- ``', '`` DSAR', "DSAR ''", "'' )", ') simple', 'simple efficient', 'efficient way', 'way ,', ', removing', 'removing need', 'need physical', 'physical approach', 'approach requests', 'requests tends', 'tends labour', 'labour thorough', 'thorough .'] 

 TOTAL BIGRAMS --> 31 



 ---- TRI-GRAMS ---- 

 ["RAVN 's GDPR", "'s GDPR Robot", 'GDPR Robot also', 'Robot also able', 'also able hasten', 'able hasten requests', 'hasten requests information', 'requests information (', 'information ( Data', '( Data Subject', 'Data Subject Access', 'Subject Access Requests', 'Access Requests -', 'Requests - ``', '- `` DSAR', "`` DSAR ''", "DSAR '' )", "'' ) simple", ') simple efficient', 'simple efficient way', 'efficient way ,', 'way , removing', ', removing need', 'removing need physical', 'need physical approach', 'physical approach requests', 'approach requests tends', 'requests tends labour', 'tends labour thorough', 'labour thorough .'] 

 TOTAL TRIGRAMS --> 30 



 ---- NOUN PHRASES ---- 

 ['information', 'simple efficient way', 'need', 'physical approach', 'labour thorough'] 

 TOTAL NOUN PHRASES --> 5 



 ---- NER ----

 
 ORGANIZATION ---> ['RAVN', 'GDPR Robot']
 TOTAL ORGANIZATION ENTITY --> 2 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['ravn', "'s", 'gdpr', 'robot', 'also', 'abl', 'hasten', 'request', 'inform', '(', 'data', 'subject', 'access', 'request', '-', '``', 'dsar', "''", ')', 'simpl', 'effici', 'way', ',', 'remov', 'need', 'physic', 'approach', 'request', 'tend', 'labour', 'thorough', '.']

 TOTAL PORTER STEM WORDS ==> 32



 ---- SNOWBALL STEMMING ----

['ravn', "'s", 'gdpr', 'robot', 'also', 'abl', 'hasten', 'request', 'inform', '(', 'data', 'subject', 'access', 'request', '-', '``', 'dsar', "''", ')', 'simpl', 'effici', 'way', ',', 'remov', 'need', 'physic', 'approach', 'request', 'tend', 'labour', 'thorough', '.']

 TOTAL SNOWBALL STEM WORDS ==> 32



 ---- LEMMATIZATION ----

['RAVN', "'s", 'GDPR', 'Robot', 'also', 'able', 'hasten', 'request', 'information', '(', 'Data', 'Subject', 'Access', 'Requests', '-', '``', 'DSAR', "''", ')', 'simple', 'efficient', 'way', ',', 'removing', 'need', 'physical', 'approach', 'request', 'tends', 'labour', 'thorough', '.']

 TOTAL LEMMATIZE WORDS ==> 32

************************************************************************************************************************

313 --> Peter Wallqvist, CSO at  RAVN Systems commented, "GDPR compliance is of universal paramountcy as it will  exploit to any organisation that control and process data concerning EU citizens. 


 ---- TOKENS ----

 ['Peter', 'Wallqvist', ',', 'CSO', 'at', 'RAVN', 'Systems', 'commented', ',', '``', 'GDPR', 'compliance', 'is', 'of', 'universal', 'paramountcy', 'as', 'it', 'will', 'exploit', 'to', 'any', 'organisation', 'that', 'control', 'and', 'process', 'data', 'concerning', 'EU', 'citizens', '.'] 

 TOTAL TOKENS ==> 32

 ---- POST ----

 [('Peter', 'NNP'), ('Wallqvist', 'NNP'), (',', ','), ('CSO', 'NNP'), ('at', 'IN'), ('RAVN', 'NNP'), ('Systems', 'NNP'), ('commented', 'VBD'), (',', ','), ('``', '``'), ('GDPR', 'NNP'), ('compliance', 'NN'), ('is', 'VBZ'), ('of', 'IN'), ('universal', 'JJ'), ('paramountcy', 'NN'), ('as', 'IN'), ('it', 'PRP'), ('will', 'MD'), ('exploit', 'VB'), ('to', 'TO'), ('any', 'DT'), ('organisation', 'NN'), ('that', 'IN'), ('control', 'NN'), ('and', 'CC'), ('process', 'NN'), ('data', 'NNS'), ('concerning', 'VBG'), ('EU', 'NNP'), ('citizens', 'NNS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Peter', 'Wallqvist', ',', 'CSO', 'RAVN', 'Systems', 'commented', ',', '``', 'GDPR', 'compliance', 'universal', 'paramountcy', 'exploit', 'organisation', 'control', 'process', 'data', 'concerning', 'EU', 'citizens', '.']

 TOTAL FILTERED TOKENS ==>  22

 ---- POST FOR FILTERED TOKENS ----

 [('Peter', 'NNP'), ('Wallqvist', 'NNP'), (',', ','), ('CSO', 'NNP'), ('RAVN', 'NNP'), ('Systems', 'NNP'), ('commented', 'VBD'), (',', ','), ('``', '``'), ('GDPR', 'NNP'), ('compliance', 'NN'), ('universal', 'JJ'), ('paramountcy', 'NN'), ('exploit', 'JJ'), ('organisation', 'NN'), ('control', 'NN'), ('process', 'NN'), ('data', 'NNS'), ('concerning', 'VBG'), ('EU', 'NNP'), ('citizens', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Peter Wallqvist', 'Wallqvist ,', ', CSO', 'CSO RAVN', 'RAVN Systems', 'Systems commented', 'commented ,', ', ``', '`` GDPR', 'GDPR compliance', 'compliance universal', 'universal paramountcy', 'paramountcy exploit', 'exploit organisation', 'organisation control', 'control process', 'process data', 'data concerning', 'concerning EU', 'EU citizens', 'citizens .'] 

 TOTAL BIGRAMS --> 21 



 ---- TRI-GRAMS ---- 

 ['Peter Wallqvist ,', 'Wallqvist , CSO', ', CSO RAVN', 'CSO RAVN Systems', 'RAVN Systems commented', 'Systems commented ,', 'commented , ``', ', `` GDPR', '`` GDPR compliance', 'GDPR compliance universal', 'compliance universal paramountcy', 'universal paramountcy exploit', 'paramountcy exploit organisation', 'exploit organisation control', 'organisation control process', 'control process data', 'process data concerning', 'data concerning EU', 'concerning EU citizens', 'EU citizens .'] 

 TOTAL TRIGRAMS --> 20 



 ---- NOUN PHRASES ---- 

 ['compliance', 'universal paramountcy', 'exploit organisation', 'control', 'process'] 

 TOTAL NOUN PHRASES --> 5 



 ---- NER ----

 
 ORGANIZATION ---> ['CSO', 'GDPR']
 TOTAL ORGANIZATION ENTITY --> 2 


 PERSON ---> ['Peter Wallqvist', 'Systems']
 TOTAL PERSON ENTITY --> 2 


 GPE ---> ['EU']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['peter', 'wallqvist', ',', 'cso', 'ravn', 'system', 'comment', ',', '``', 'gdpr', 'complianc', 'univers', 'paramountci', 'exploit', 'organis', 'control', 'process', 'data', 'concern', 'eu', 'citizen', '.']

 TOTAL PORTER STEM WORDS ==> 22



 ---- SNOWBALL STEMMING ----

['peter', 'wallqvist', ',', 'cso', 'ravn', 'system', 'comment', ',', '``', 'gdpr', 'complianc', 'univers', 'paramountci', 'exploit', 'organis', 'control', 'process', 'data', 'concern', 'eu', 'citizen', '.']

 TOTAL SNOWBALL STEM WORDS ==> 22



 ---- LEMMATIZATION ----

['Peter', 'Wallqvist', ',', 'CSO', 'RAVN', 'Systems', 'commented', ',', '``', 'GDPR', 'compliance', 'universal', 'paramountcy', 'exploit', 'organisation', 'control', 'process', 'data', 'concerning', 'EU', 'citizen', '.']

 TOTAL LEMMATIZE WORDS ==> 22

************************************************************************************************************************

314 --> LINK:http://markets.financialcontent.com/stocks/news/read/33888795/RAVN_Systems_Launch_the_ACE_Po wered_GDPR_Robot  8.2 Eno A Natural Language Chatbot Launched by Capital One [105]  http://markets.financialcontent.com/stocks/news/read/33888795/RAVN_Systems_Launch_the_ACE_Powered_GDPR_Robot http://markets.financialcontent.com/stocks/news/read/33888795/RAVN_Systems_Launch_the_ACE_Powered_GDPR_Robot  Capital one announces chatbot for customers called Eno. 


 ---- TOKENS ----

 ['LINK', ':', 'http', ':', '//markets.financialcontent.com/stocks/news/read/33888795/RAVN_Systems_Launch_the_ACE_Po', 'wered_GDPR_Robot', '8.2', 'Eno', 'A', 'Natural', 'Language', 'Chatbot', 'Launched', 'by', 'Capital', 'One', '[', '105', ']', 'http', ':', '//markets.financialcontent.com/stocks/news/read/33888795/RAVN_Systems_Launch_the_ACE_Powered_GDPR_Robot', 'http', ':', '//markets.financialcontent.com/stocks/news/read/33888795/RAVN_Systems_Launch_the_ACE_Powered_GDPR_Robot', 'Capital', 'one', 'announces', 'chatbot', 'for', 'customers', 'called', 'Eno', '.'] 

 TOTAL TOKENS ==> 34

 ---- POST ----

 [('LINK', 'NN'), (':', ':'), ('http', 'NN'), (':', ':'), ('//markets.financialcontent.com/stocks/news/read/33888795/RAVN_Systems_Launch_the_ACE_Po', 'JJ'), ('wered_GDPR_Robot', 'NN'), ('8.2', 'CD'), ('Eno', 'NNP'), ('A', 'NNP'), ('Natural', 'NNP'), ('Language', 'NNP'), ('Chatbot', 'NNP'), ('Launched', 'VBN'), ('by', 'IN'), ('Capital', 'NNP'), ('One', 'NNP'), ('[', 'VBZ'), ('105', 'CD'), (']', 'JJ'), ('http', 'NN'), (':', ':'), ('//markets.financialcontent.com/stocks/news/read/33888795/RAVN_Systems_Launch_the_ACE_Powered_GDPR_Robot', 'JJ'), ('http', 'NN'), (':', ':'), ('//markets.financialcontent.com/stocks/news/read/33888795/RAVN_Systems_Launch_the_ACE_Powered_GDPR_Robot', 'JJ'), ('Capital', 'NNP'), ('one', 'CD'), ('announces', 'VBZ'), ('chatbot', 'NN'), ('for', 'IN'), ('customers', 'NNS'), ('called', 'VBN'), ('Eno', 'NNP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['LINK', ':', 'http', ':', '//markets.financialcontent.com/stocks/news/read/33888795/RAVN_Systems_Launch_the_ACE_Po', 'wered_GDPR_Robot', '8.2', 'Eno', 'Natural', 'Language', 'Chatbot', 'Launched', 'Capital', 'One', '[', '105', ']', 'http', ':', '//markets.financialcontent.com/stocks/news/read/33888795/RAVN_Systems_Launch_the_ACE_Powered_GDPR_Robot', 'http', ':', '//markets.financialcontent.com/stocks/news/read/33888795/RAVN_Systems_Launch_the_ACE_Powered_GDPR_Robot', 'Capital', 'one', 'announces', 'chatbot', 'customers', 'called', 'Eno', '.']

 TOTAL FILTERED TOKENS ==>  31

 ---- POST FOR FILTERED TOKENS ----

 [('LINK', 'NN'), (':', ':'), ('http', 'NN'), (':', ':'), ('//markets.financialcontent.com/stocks/news/read/33888795/RAVN_Systems_Launch_the_ACE_Po', 'JJ'), ('wered_GDPR_Robot', 'NN'), ('8.2', 'CD'), ('Eno', 'NNP'), ('Natural', 'NNP'), ('Language', 'NNP'), ('Chatbot', 'NNP'), ('Launched', 'NNP'), ('Capital', 'NNP'), ('One', 'NNP'), ('[', 'VBZ'), ('105', 'CD'), (']', 'JJ'), ('http', 'NN'), (':', ':'), ('//markets.financialcontent.com/stocks/news/read/33888795/RAVN_Systems_Launch_the_ACE_Powered_GDPR_Robot', 'JJ'), ('http', 'NN'), (':', ':'), ('//markets.financialcontent.com/stocks/news/read/33888795/RAVN_Systems_Launch_the_ACE_Powered_GDPR_Robot', 'JJ'), ('Capital', 'NNP'), ('one', 'CD'), ('announces', 'VBZ'), ('chatbot', 'NN'), ('customers', 'NNS'), ('called', 'VBN'), ('Eno', 'NNP'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['LINK :', ': http', 'http :', ': //markets.financialcontent.com/stocks/news/read/33888795/RAVN_Systems_Launch_the_ACE_Po', '//markets.financialcontent.com/stocks/news/read/33888795/RAVN_Systems_Launch_the_ACE_Po wered_GDPR_Robot', 'wered_GDPR_Robot 8.2', '8.2 Eno', 'Eno Natural', 'Natural Language', 'Language Chatbot', 'Chatbot Launched', 'Launched Capital', 'Capital One', 'One [', '[ 105', '105 ]', '] http', 'http :', ': //markets.financialcontent.com/stocks/news/read/33888795/RAVN_Systems_Launch_the_ACE_Powered_GDPR_Robot', '//markets.financialcontent.com/stocks/news/read/33888795/RAVN_Systems_Launch_the_ACE_Powered_GDPR_Robot http', 'http :', ': //markets.financialcontent.com/stocks/news/read/33888795/RAVN_Systems_Launch_the_ACE_Powered_GDPR_Robot', '//markets.financialcontent.com/stocks/news/read/33888795/RAVN_Systems_Launch_the_ACE_Powered_GDPR_Robot Capital', 'Capital one', 'one announces', 'announces chatbot', 'chatbot customers', 'customers called', 'called Eno', 'Eno .'] 

 TOTAL BIGRAMS --> 30 



 ---- TRI-GRAMS ---- 

 ['LINK : http', ': http :', 'http : //markets.financialcontent.com/stocks/news/read/33888795/RAVN_Systems_Launch_the_ACE_Po', ': //markets.financialcontent.com/stocks/news/read/33888795/RAVN_Systems_Launch_the_ACE_Po wered_GDPR_Robot', '//markets.financialcontent.com/stocks/news/read/33888795/RAVN_Systems_Launch_the_ACE_Po wered_GDPR_Robot 8.2', 'wered_GDPR_Robot 8.2 Eno', '8.2 Eno Natural', 'Eno Natural Language', 'Natural Language Chatbot', 'Language Chatbot Launched', 'Chatbot Launched Capital', 'Launched Capital One', 'Capital One [', 'One [ 105', '[ 105 ]', '105 ] http', '] http :', 'http : //markets.financialcontent.com/stocks/news/read/33888795/RAVN_Systems_Launch_the_ACE_Powered_GDPR_Robot', ': //markets.financialcontent.com/stocks/news/read/33888795/RAVN_Systems_Launch_the_ACE_Powered_GDPR_Robot http', '//markets.financialcontent.com/stocks/news/read/33888795/RAVN_Systems_Launch_the_ACE_Powered_GDPR_Robot http :', 'http : //markets.financialcontent.com/stocks/news/read/33888795/RAVN_Systems_Launch_the_ACE_Powered_GDPR_Robot', ': //markets.financialcontent.com/stocks/news/read/33888795/RAVN_Systems_Launch_the_ACE_Powered_GDPR_Robot Capital', '//markets.financialcontent.com/stocks/news/read/33888795/RAVN_Systems_Launch_the_ACE_Powered_GDPR_Robot Capital one', 'Capital one announces', 'one announces chatbot', 'announces chatbot customers', 'chatbot customers called', 'customers called Eno', 'called Eno .'] 

 TOTAL TRIGRAMS --> 29 



 ---- NOUN PHRASES ---- 

 ['LINK', 'http', ' wered_GDPR_Robot', '] http', ' http', 'chatbot'] 

 TOTAL NOUN PHRASES --> 6 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> ['Eno']
 TOTAL PERSON ENTITY --> 1 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['link', ':', 'http', ':', '//markets.financialcontent.com/stocks/news/read/33888795/ravn_systems_launch_the_ace_po', 'wered_gdpr_robot', '8.2', 'eno', 'natur', 'languag', 'chatbot', 'launch', 'capit', 'one', '[', '105', ']', 'http', ':', '//markets.financialcontent.com/stocks/news/read/33888795/ravn_systems_launch_the_ace_powered_gdpr_robot', 'http', ':', '//markets.financialcontent.com/stocks/news/read/33888795/ravn_systems_launch_the_ace_powered_gdpr_robot', 'capit', 'one', 'announc', 'chatbot', 'custom', 'call', 'eno', '.']

 TOTAL PORTER STEM WORDS ==> 31



 ---- SNOWBALL STEMMING ----

['link', ':', 'http', ':', '//markets.financialcontent.com/stocks/news/read/33888795/ravn_systems_launch_the_ace_po', 'wered_gdpr_robot', '8.2', 'eno', 'natur', 'languag', 'chatbot', 'launch', 'capit', 'one', '[', '105', ']', 'http', ':', '//markets.financialcontent.com/stocks/news/read/33888795/ravn_systems_launch_the_ace_powered_gdpr_robot', 'http', ':', '//markets.financialcontent.com/stocks/news/read/33888795/ravn_systems_launch_the_ace_powered_gdpr_robot', 'capit', 'one', 'announc', 'chatbot', 'custom', 'call', 'eno', '.']

 TOTAL SNOWBALL STEM WORDS ==> 31



 ---- LEMMATIZATION ----

['LINK', ':', 'http', ':', '//markets.financialcontent.com/stocks/news/read/33888795/RAVN_Systems_Launch_the_ACE_Po', 'wered_GDPR_Robot', '8.2', 'Eno', 'Natural', 'Language', 'Chatbot', 'Launched', 'Capital', 'One', '[', '105', ']', 'http', ':', '//markets.financialcontent.com/stocks/news/read/33888795/RAVN_Systems_Launch_the_ACE_Powered_GDPR_Robot', 'http', ':', '//markets.financialcontent.com/stocks/news/read/33888795/RAVN_Systems_Launch_the_ACE_Powered_GDPR_Robot', 'Capital', 'one', 'announces', 'chatbot', 'customer', 'called', 'Eno', '.']

 TOTAL LEMMATIZE WORDS ==> 31

************************************************************************************************************************

315 --> Eno is a natural language chatbot  that people socialize through texting. 


 ---- TOKENS ----

 ['Eno', 'is', 'a', 'natural', 'language', 'chatbot', 'that', 'people', 'socialize', 'through', 'texting', '.'] 

 TOTAL TOKENS ==> 12

 ---- POST ----

 [('Eno', 'NNP'), ('is', 'VBZ'), ('a', 'DT'), ('natural', 'JJ'), ('language', 'NN'), ('chatbot', 'NN'), ('that', 'WDT'), ('people', 'NNS'), ('socialize', 'VBP'), ('through', 'IN'), ('texting', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Eno', 'natural', 'language', 'chatbot', 'people', 'socialize', 'texting', '.']

 TOTAL FILTERED TOKENS ==>  8

 ---- POST FOR FILTERED TOKENS ----

 [('Eno', 'NNP'), ('natural', 'JJ'), ('language', 'NN'), ('chatbot', 'NN'), ('people', 'NNS'), ('socialize', 'VBP'), ('texting', 'VBG'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Eno natural', 'natural language', 'language chatbot', 'chatbot people', 'people socialize', 'socialize texting', 'texting .'] 

 TOTAL BIGRAMS --> 7 



 ---- TRI-GRAMS ---- 

 ['Eno natural language', 'natural language chatbot', 'language chatbot people', 'chatbot people socialize', 'people socialize texting', 'socialize texting .'] 

 TOTAL TRIGRAMS --> 6 



 ---- NOUN PHRASES ---- 

 ['natural language', 'chatbot'] 

 TOTAL NOUN PHRASES --> 2 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> ['Eno']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['eno', 'natur', 'languag', 'chatbot', 'peopl', 'social', 'text', '.']

 TOTAL PORTER STEM WORDS ==> 8



 ---- SNOWBALL STEMMING ----

['eno', 'natur', 'languag', 'chatbot', 'peopl', 'social', 'text', '.']

 TOTAL SNOWBALL STEM WORDS ==> 8



 ---- LEMMATIZATION ----

['Eno', 'natural', 'language', 'chatbot', 'people', 'socialize', 'texting', '.']

 TOTAL LEMMATIZE WORDS ==> 8

************************************************************************************************************************

316 --> Capital one claims that Eno is First natural language  SMS chatbot from a U.S. bank that allows customer to ask questions using natural language. 


 ---- TOKENS ----

 ['Capital', 'one', 'claims', 'that', 'Eno', 'is', 'First', 'natural', 'language', 'SMS', 'chatbot', 'from', 'a', 'U.S.', 'bank', 'that', 'allows', 'customer', 'to', 'ask', 'questions', 'using', 'natural', 'language', '.'] 

 TOTAL TOKENS ==> 25

 ---- POST ----

 [('Capital', 'NN'), ('one', 'CD'), ('claims', 'VBZ'), ('that', 'IN'), ('Eno', 'NNP'), ('is', 'VBZ'), ('First', 'NNP'), ('natural', 'JJ'), ('language', 'NN'), ('SMS', 'NNP'), ('chatbot', 'NN'), ('from', 'IN'), ('a', 'DT'), ('U.S.', 'NNP'), ('bank', 'NN'), ('that', 'WDT'), ('allows', 'VBZ'), ('customer', 'NN'), ('to', 'TO'), ('ask', 'VB'), ('questions', 'NNS'), ('using', 'VBG'), ('natural', 'JJ'), ('language', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Capital', 'one', 'claims', 'Eno', 'First', 'natural', 'language', 'SMS', 'chatbot', 'U.S.', 'bank', 'allows', 'customer', 'ask', 'questions', 'using', 'natural', 'language', '.']

 TOTAL FILTERED TOKENS ==>  19

 ---- POST FOR FILTERED TOKENS ----

 [('Capital', 'NN'), ('one', 'CD'), ('claims', 'VBZ'), ('Eno', 'NNP'), ('First', 'NNP'), ('natural', 'JJ'), ('language', 'NN'), ('SMS', 'NNP'), ('chatbot', 'VBZ'), ('U.S.', 'NNP'), ('bank', 'NN'), ('allows', 'VBZ'), ('customer', 'NN'), ('ask', 'NN'), ('questions', 'NNS'), ('using', 'VBG'), ('natural', 'JJ'), ('language', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Capital one', 'one claims', 'claims Eno', 'Eno First', 'First natural', 'natural language', 'language SMS', 'SMS chatbot', 'chatbot U.S.', 'U.S. bank', 'bank allows', 'allows customer', 'customer ask', 'ask questions', 'questions using', 'using natural', 'natural language', 'language .'] 

 TOTAL BIGRAMS --> 18 



 ---- TRI-GRAMS ---- 

 ['Capital one claims', 'one claims Eno', 'claims Eno First', 'Eno First natural', 'First natural language', 'natural language SMS', 'language SMS chatbot', 'SMS chatbot U.S.', 'chatbot U.S. bank', 'U.S. bank allows', 'bank allows customer', 'allows customer ask', 'customer ask questions', 'ask questions using', 'questions using natural', 'using natural language', 'natural language .'] 

 TOTAL TRIGRAMS --> 17 



 ---- NOUN PHRASES ---- 

 ['Capital', 'natural language', 'bank', 'customer', 'ask', 'natural language'] 

 TOTAL NOUN PHRASES --> 6 



 ---- NER ----

 
 ORGANIZATION ---> ['SMS']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> ['Eno First']
 TOTAL PERSON ENTITY --> 1 


 GPE ---> ['U.S.']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['capit', 'one', 'claim', 'eno', 'first', 'natur', 'languag', 'sm', 'chatbot', 'u.s.', 'bank', 'allow', 'custom', 'ask', 'question', 'use', 'natur', 'languag', '.']

 TOTAL PORTER STEM WORDS ==> 19



 ---- SNOWBALL STEMMING ----

['capit', 'one', 'claim', 'eno', 'first', 'natur', 'languag', 'sms', 'chatbot', 'u.s.', 'bank', 'allow', 'custom', 'ask', 'question', 'use', 'natur', 'languag', '.']

 TOTAL SNOWBALL STEM WORDS ==> 19



 ---- LEMMATIZATION ----

['Capital', 'one', 'claim', 'Eno', 'First', 'natural', 'language', 'SMS', 'chatbot', 'U.S.', 'bank', 'allows', 'customer', 'ask', 'question', 'using', 'natural', 'language', '.']

 TOTAL LEMMATIZE WORDS ==> 19

************************************************************************************************************************

317 --> Customers can interact with Eno asking questions about their savings and others using a text  interface. 


 ---- TOKENS ----

 ['Customers', 'can', 'interact', 'with', 'Eno', 'asking', 'questions', 'about', 'their', 'savings', 'and', 'others', 'using', 'a', 'text', 'interface', '.'] 

 TOTAL TOKENS ==> 17

 ---- POST ----

 [('Customers', 'NNS'), ('can', 'MD'), ('interact', 'VB'), ('with', 'IN'), ('Eno', 'NNP'), ('asking', 'VBG'), ('questions', 'NNS'), ('about', 'IN'), ('their', 'PRP$'), ('savings', 'NNS'), ('and', 'CC'), ('others', 'NNS'), ('using', 'VBG'), ('a', 'DT'), ('text', 'JJ'), ('interface', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Customers', 'interact', 'Eno', 'asking', 'questions', 'savings', 'others', 'using', 'text', 'interface', '.']

 TOTAL FILTERED TOKENS ==>  11

 ---- POST FOR FILTERED TOKENS ----

 [('Customers', 'NNS'), ('interact', 'VBP'), ('Eno', 'NNP'), ('asking', 'VBG'), ('questions', 'NNS'), ('savings', 'NNS'), ('others', 'NNS'), ('using', 'VBG'), ('text', 'JJ'), ('interface', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Customers interact', 'interact Eno', 'Eno asking', 'asking questions', 'questions savings', 'savings others', 'others using', 'using text', 'text interface', 'interface .'] 

 TOTAL BIGRAMS --> 10 



 ---- TRI-GRAMS ---- 

 ['Customers interact Eno', 'interact Eno asking', 'Eno asking questions', 'asking questions savings', 'questions savings others', 'savings others using', 'others using text', 'using text interface', 'text interface .'] 

 TOTAL TRIGRAMS --> 9 



 ---- NOUN PHRASES ---- 

 ['text interface'] 

 TOTAL NOUN PHRASES --> 1 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> ['Eno']
 TOTAL PERSON ENTITY --> 1 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['custom', 'interact', 'eno', 'ask', 'question', 'save', 'other', 'use', 'text', 'interfac', '.']

 TOTAL PORTER STEM WORDS ==> 11



 ---- SNOWBALL STEMMING ----

['custom', 'interact', 'eno', 'ask', 'question', 'save', 'other', 'use', 'text', 'interfac', '.']

 TOTAL SNOWBALL STEM WORDS ==> 11



 ---- LEMMATIZATION ----

['Customers', 'interact', 'Eno', 'asking', 'question', 'saving', 'others', 'using', 'text', 'interface', '.']

 TOTAL LEMMATIZE WORDS ==> 11

************************************************************************************************************************

318 --> Eno makes such an environment that it feels that a human is interacting. 


 ---- TOKENS ----

 ['Eno', 'makes', 'such', 'an', 'environment', 'that', 'it', 'feels', 'that', 'a', 'human', 'is', 'interacting', '.'] 

 TOTAL TOKENS ==> 14

 ---- POST ----

 [('Eno', 'NNP'), ('makes', 'VBZ'), ('such', 'PDT'), ('an', 'DT'), ('environment', 'NN'), ('that', 'IN'), ('it', 'PRP'), ('feels', 'VBZ'), ('that', 'IN'), ('a', 'DT'), ('human', 'JJ'), ('is', 'VBZ'), ('interacting', 'VBG'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Eno', 'makes', 'environment', 'feels', 'human', 'interacting', '.']

 TOTAL FILTERED TOKENS ==>  7

 ---- POST FOR FILTERED TOKENS ----

 [('Eno', 'NNP'), ('makes', 'VBZ'), ('environment', 'NN'), ('feels', 'NNS'), ('human', 'JJ'), ('interacting', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Eno makes', 'makes environment', 'environment feels', 'feels human', 'human interacting', 'interacting .'] 

 TOTAL BIGRAMS --> 6 



 ---- TRI-GRAMS ---- 

 ['Eno makes environment', 'makes environment feels', 'environment feels human', 'feels human interacting', 'human interacting .'] 

 TOTAL TRIGRAMS --> 5 



 ---- NOUN PHRASES ---- 

 ['environment', 'human interacting'] 

 TOTAL NOUN PHRASES --> 2 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> ['Eno']
 TOTAL PERSON ENTITY --> 1 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['eno', 'make', 'environ', 'feel', 'human', 'interact', '.']

 TOTAL PORTER STEM WORDS ==> 7



 ---- SNOWBALL STEMMING ----

['eno', 'make', 'environ', 'feel', 'human', 'interact', '.']

 TOTAL SNOWBALL STEM WORDS ==> 7



 ---- LEMMATIZATION ----

['Eno', 'make', 'environment', 'feel', 'human', 'interacting', '.']

 TOTAL LEMMATIZE WORDS ==> 7

************************************************************************************************************************

319 --> Ken  Dodelin, Capital One’s vice president of digital product development, said “We kind of  launched a chatbot and didn’t know it.”   This provides a different platform than other brands that launch chatbots like Facebook  Messenger and Skype. 


 ---- TOKENS ----

 ['Ken', 'Dodelin', ',', 'Capital', 'One', '’', 's', 'vice', 'president', 'of', 'digital', 'product', 'development', ',', 'said', '“', 'We', 'kind', 'of', 'launched', 'a', 'chatbot', 'and', 'didn', '’', 't', 'know', 'it.', '”', 'This', 'provides', 'a', 'different', 'platform', 'than', 'other', 'brands', 'that', 'launch', 'chatbots', 'like', 'Facebook', 'Messenger', 'and', 'Skype', '.'] 

 TOTAL TOKENS ==> 46

 ---- POST ----

 [('Ken', 'NNP'), ('Dodelin', 'NNP'), (',', ','), ('Capital', 'NNP'), ('One', 'NNP'), ('’', 'NNP'), ('s', 'JJ'), ('vice', 'NN'), ('president', 'NN'), ('of', 'IN'), ('digital', 'JJ'), ('product', 'NN'), ('development', 'NN'), (',', ','), ('said', 'VBD'), ('“', 'IN'), ('We', 'PRP'), ('kind', 'NN'), ('of', 'IN'), ('launched', 'VBD'), ('a', 'DT'), ('chatbot', 'NN'), ('and', 'CC'), ('didn', 'NN'), ('’', 'NNP'), ('t', 'NN'), ('know', 'VBP'), ('it.', 'RB'), ('”', 'VB'), ('This', 'DT'), ('provides', 'VBZ'), ('a', 'DT'), ('different', 'JJ'), ('platform', 'NN'), ('than', 'IN'), ('other', 'JJ'), ('brands', 'NNS'), ('that', 'IN'), ('launch', 'JJ'), ('chatbots', 'NNS'), ('like', 'IN'), ('Facebook', 'NNP'), ('Messenger', 'NNP'), ('and', 'CC'), ('Skype', 'NNP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Ken', 'Dodelin', ',', 'Capital', 'One', '’', 'vice', 'president', 'digital', 'product', 'development', ',', 'said', '“', 'kind', 'launched', 'chatbot', '’', 'know', 'it.', '”', 'provides', 'different', 'platform', 'brands', 'launch', 'chatbots', 'like', 'Facebook', 'Messenger', 'Skype', '.']

 TOTAL FILTERED TOKENS ==>  32

 ---- POST FOR FILTERED TOKENS ----

 [('Ken', 'NNP'), ('Dodelin', 'NNP'), (',', ','), ('Capital', 'NNP'), ('One', 'NNP'), ('’', 'NNP'), ('vice', 'NN'), ('president', 'NN'), ('digital', 'JJ'), ('product', 'NN'), ('development', 'NN'), (',', ','), ('said', 'VBD'), ('“', 'JJ'), ('kind', 'NN'), ('launched', 'VBN'), ('chatbot', 'NN'), ('’', 'NN'), ('know', 'VBP'), ('it.', 'JJ'), ('”', 'NNP'), ('provides', 'VBZ'), ('different', 'JJ'), ('platform', 'NN'), ('brands', 'NNS'), ('launch', 'JJ'), ('chatbots', 'NNS'), ('like', 'IN'), ('Facebook', 'NNP'), ('Messenger', 'NNP'), ('Skype', 'NNP'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Ken Dodelin', 'Dodelin ,', ', Capital', 'Capital One', 'One ’', '’ vice', 'vice president', 'president digital', 'digital product', 'product development', 'development ,', ', said', 'said “', '“ kind', 'kind launched', 'launched chatbot', 'chatbot ’', '’ know', 'know it.', 'it. ”', '” provides', 'provides different', 'different platform', 'platform brands', 'brands launch', 'launch chatbots', 'chatbots like', 'like Facebook', 'Facebook Messenger', 'Messenger Skype', 'Skype .'] 

 TOTAL BIGRAMS --> 31 



 ---- TRI-GRAMS ---- 

 ['Ken Dodelin ,', 'Dodelin , Capital', ', Capital One', 'Capital One ’', 'One ’ vice', '’ vice president', 'vice president digital', 'president digital product', 'digital product development', 'product development ,', 'development , said', ', said “', 'said “ kind', '“ kind launched', 'kind launched chatbot', 'launched chatbot ’', 'chatbot ’ know', '’ know it.', 'know it. ”', 'it. ” provides', '” provides different', 'provides different platform', 'different platform brands', 'platform brands launch', 'brands launch chatbots', 'launch chatbots like', 'chatbots like Facebook', 'like Facebook Messenger', 'Facebook Messenger Skype', 'Messenger Skype .'] 

 TOTAL TRIGRAMS --> 30 



 ---- NOUN PHRASES ---- 

 ['vice', 'president', 'digital product', 'development', '“ kind', 'chatbot', '’', 'different platform'] 

 TOTAL NOUN PHRASES --> 8 



 ---- NER ----

 
 ORGANIZATION ---> ['Dodelin', 'Capital One']
 TOTAL ORGANIZATION ENTITY --> 2 


 PERSON ---> ['Ken', 'Facebook Messenger Skype']
 TOTAL PERSON ENTITY --> 2 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['ken', 'dodelin', ',', 'capit', 'one', '’', 'vice', 'presid', 'digit', 'product', 'develop', ',', 'said', '“', 'kind', 'launch', 'chatbot', '’', 'know', 'it.', '”', 'provid', 'differ', 'platform', 'brand', 'launch', 'chatbot', 'like', 'facebook', 'messeng', 'skype', '.']

 TOTAL PORTER STEM WORDS ==> 32



 ---- SNOWBALL STEMMING ----

['ken', 'dodelin', ',', 'capit', 'one', '’', 'vice', 'presid', 'digit', 'product', 'develop', ',', 'said', '“', 'kind', 'launch', 'chatbot', '’', 'know', 'it.', '”', 'provid', 'differ', 'platform', 'brand', 'launch', 'chatbot', 'like', 'facebook', 'messeng', 'skype', '.']

 TOTAL SNOWBALL STEM WORDS ==> 32



 ---- LEMMATIZATION ----

['Ken', 'Dodelin', ',', 'Capital', 'One', '’', 'vice', 'president', 'digital', 'product', 'development', ',', 'said', '“', 'kind', 'launched', 'chatbot', '’', 'know', 'it.', '”', 'provides', 'different', 'platform', 'brand', 'launch', 'chatbots', 'like', 'Facebook', 'Messenger', 'Skype', '.']

 TOTAL LEMMATIZE WORDS ==> 32

************************************************************************************************************************

320 --> They believed that Facebook has too much access of private  information of a person, which could get them into trouble with privacy laws of U.S.  financial institutions work under. 


 ---- TOKENS ----

 ['They', 'believed', 'that', 'Facebook', 'has', 'too', 'much', 'access', 'of', 'private', 'information', 'of', 'a', 'person', ',', 'which', 'could', 'get', 'them', 'into', 'trouble', 'with', 'privacy', 'laws', 'of', 'U.S.', 'financial', 'institutions', 'work', 'under', '.'] 

 TOTAL TOKENS ==> 31

 ---- POST ----

 [('They', 'PRP'), ('believed', 'VBD'), ('that', 'DT'), ('Facebook', 'NNP'), ('has', 'VBZ'), ('too', 'RB'), ('much', 'JJ'), ('access', 'NN'), ('of', 'IN'), ('private', 'JJ'), ('information', 'NN'), ('of', 'IN'), ('a', 'DT'), ('person', 'NN'), (',', ','), ('which', 'WDT'), ('could', 'MD'), ('get', 'VB'), ('them', 'PRP'), ('into', 'IN'), ('trouble', 'NN'), ('with', 'IN'), ('privacy', 'NN'), ('laws', 'NNS'), ('of', 'IN'), ('U.S.', 'NNP'), ('financial', 'JJ'), ('institutions', 'NNS'), ('work', 'NN'), ('under', 'IN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['believed', 'Facebook', 'much', 'access', 'private', 'information', 'person', ',', 'could', 'get', 'trouble', 'privacy', 'laws', 'U.S.', 'financial', 'institutions', 'work', '.']

 TOTAL FILTERED TOKENS ==>  18

 ---- POST FOR FILTERED TOKENS ----

 [('believed', 'VBN'), ('Facebook', 'NNP'), ('much', 'JJ'), ('access', 'NN'), ('private', 'JJ'), ('information', 'NN'), ('person', 'NN'), (',', ','), ('could', 'MD'), ('get', 'VB'), ('trouble', 'NN'), ('privacy', 'NN'), ('laws', 'NNS'), ('U.S.', 'NNP'), ('financial', 'JJ'), ('institutions', 'NNS'), ('work', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['believed Facebook', 'Facebook much', 'much access', 'access private', 'private information', 'information person', 'person ,', ', could', 'could get', 'get trouble', 'trouble privacy', 'privacy laws', 'laws U.S.', 'U.S. financial', 'financial institutions', 'institutions work', 'work .'] 

 TOTAL BIGRAMS --> 17 



 ---- TRI-GRAMS ---- 

 ['believed Facebook much', 'Facebook much access', 'much access private', 'access private information', 'private information person', 'information person ,', 'person , could', ', could get', 'could get trouble', 'get trouble privacy', 'trouble privacy laws', 'privacy laws U.S.', 'laws U.S. financial', 'U.S. financial institutions', 'financial institutions work', 'institutions work .'] 

 TOTAL TRIGRAMS --> 16 



 ---- NOUN PHRASES ---- 

 ['much access', 'private information', 'person', 'trouble', 'privacy', 'work'] 

 TOTAL NOUN PHRASES --> 6 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> ['Facebook']
 TOTAL PERSON ENTITY --> 1 


 GPE ---> ['U.S.']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['believ', 'facebook', 'much', 'access', 'privat', 'inform', 'person', ',', 'could', 'get', 'troubl', 'privaci', 'law', 'u.s.', 'financi', 'institut', 'work', '.']

 TOTAL PORTER STEM WORDS ==> 18



 ---- SNOWBALL STEMMING ----

['believ', 'facebook', 'much', 'access', 'privat', 'inform', 'person', ',', 'could', 'get', 'troubl', 'privaci', 'law', 'u.s.', 'financi', 'institut', 'work', '.']

 TOTAL SNOWBALL STEM WORDS ==> 18



 ---- LEMMATIZATION ----

['believed', 'Facebook', 'much', 'access', 'private', 'information', 'person', ',', 'could', 'get', 'trouble', 'privacy', 'law', 'U.S.', 'financial', 'institution', 'work', '.']

 TOTAL LEMMATIZE WORDS ==> 18

************************************************************************************************************************

321 --> Like any Facebook Page admin can access full transcripts  of the bot’s conversations. 


 ---- TOKENS ----

 ['Like', 'any', 'Facebook', 'Page', 'admin', 'can', 'access', 'full', 'transcripts', 'of', 'the', 'bot', '’', 's', 'conversations', '.'] 

 TOTAL TOKENS ==> 16

 ---- POST ----

 [('Like', 'IN'), ('any', 'DT'), ('Facebook', 'NNP'), ('Page', 'NNP'), ('admin', 'NN'), ('can', 'MD'), ('access', 'NN'), ('full', 'JJ'), ('transcripts', 'NNS'), ('of', 'IN'), ('the', 'DT'), ('bot', 'NN'), ('’', 'NNP'), ('s', 'NN'), ('conversations', 'NNS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Like', 'Facebook', 'Page', 'admin', 'access', 'full', 'transcripts', 'bot', '’', 'conversations', '.']

 TOTAL FILTERED TOKENS ==>  11

 ---- POST FOR FILTERED TOKENS ----

 [('Like', 'IN'), ('Facebook', 'NNP'), ('Page', 'NNP'), ('admin', 'JJ'), ('access', 'NN'), ('full', 'JJ'), ('transcripts', 'NNS'), ('bot', 'VBP'), ('’', 'JJ'), ('conversations', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Like Facebook', 'Facebook Page', 'Page admin', 'admin access', 'access full', 'full transcripts', 'transcripts bot', 'bot ’', '’ conversations', 'conversations .'] 

 TOTAL BIGRAMS --> 10 



 ---- TRI-GRAMS ---- 

 ['Like Facebook Page', 'Facebook Page admin', 'Page admin access', 'admin access full', 'access full transcripts', 'full transcripts bot', 'transcripts bot ’', 'bot ’ conversations', '’ conversations .'] 

 TOTAL TRIGRAMS --> 9 



 ---- NOUN PHRASES ---- 

 ['admin access'] 

 TOTAL NOUN PHRASES --> 1 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> ['Facebook Page']
 TOTAL PERSON ENTITY --> 1 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['like', 'facebook', 'page', 'admin', 'access', 'full', 'transcript', 'bot', '’', 'convers', '.']

 TOTAL PORTER STEM WORDS ==> 11



 ---- SNOWBALL STEMMING ----

['like', 'facebook', 'page', 'admin', 'access', 'full', 'transcript', 'bot', '’', 'convers', '.']

 TOTAL SNOWBALL STEM WORDS ==> 11



 ---- LEMMATIZATION ----

['Like', 'Facebook', 'Page', 'admin', 'access', 'full', 'transcript', 'bot', '’', 'conversation', '.']

 TOTAL LEMMATIZE WORDS ==> 11

************************************************************************************************************************

322 --> If that would be the case then the admins could easily view the  personal banking information of customers with is not correct   LINK: https://www.macobserver.com/analysis/capital-one-natural-language-chatbot-eno/  8.3  Future of BI in Natural Language Processing [106]  Several companies in Bi spaces are trying to get with the trend and trying hard to ensure that  data becomes more friendly and easily accessible. 


 ---- TOKENS ----

 ['If', 'that', 'would', 'be', 'the', 'case', 'then', 'the', 'admins', 'could', 'easily', 'view', 'the', 'personal', 'banking', 'information', 'of', 'customers', 'with', 'is', 'not', 'correct', 'LINK', ':', 'https', ':', '//www.macobserver.com/analysis/capital-one-natural-language-chatbot-eno/', '8.3', 'Future', 'of', 'BI', 'in', 'Natural', 'Language', 'Processing', '[', '106', ']', 'Several', 'companies', 'in', 'Bi', 'spaces', 'are', 'trying', 'to', 'get', 'with', 'the', 'trend', 'and', 'trying', 'hard', 'to', 'ensure', 'that', 'data', 'becomes', 'more', 'friendly', 'and', 'easily', 'accessible', '.'] 

 TOTAL TOKENS ==> 64

 ---- POST ----

 [('If', 'IN'), ('that', 'DT'), ('would', 'MD'), ('be', 'VB'), ('the', 'DT'), ('case', 'NN'), ('then', 'RB'), ('the', 'DT'), ('admins', 'NNS'), ('could', 'MD'), ('easily', 'RB'), ('view', 'VB'), ('the', 'DT'), ('personal', 'JJ'), ('banking', 'NN'), ('information', 'NN'), ('of', 'IN'), ('customers', 'NNS'), ('with', 'IN'), ('is', 'VBZ'), ('not', 'RB'), ('correct', 'JJ'), ('LINK', 'NNP'), (':', ':'), ('https', 'NN'), (':', ':'), ('//www.macobserver.com/analysis/capital-one-natural-language-chatbot-eno/', 'JJ'), ('8.3', 'CD'), ('Future', 'NN'), ('of', 'IN'), ('BI', 'NNP'), ('in', 'IN'), ('Natural', 'NNP'), ('Language', 'NNP'), ('Processing', 'NNP'), ('[', 'VBZ'), ('106', 'CD'), (']', 'JJ'), ('Several', 'JJ'), ('companies', 'NNS'), ('in', 'IN'), ('Bi', 'NNP'), ('spaces', 'NNS'), ('are', 'VBP'), ('trying', 'VBG'), ('to', 'TO'), ('get', 'VB'), ('with', 'IN'), ('the', 'DT'), ('trend', 'NN'), ('and', 'CC'), ('trying', 'VBG'), ('hard', 'JJ'), ('to', 'TO'), ('ensure', 'VB'), ('that', 'IN'), ('data', 'NNS'), ('becomes', 'RB'), ('more', 'RBR'), ('friendly', 'JJ'), ('and', 'CC'), ('easily', 'RB'), ('accessible', 'JJ'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['would', 'case', 'admins', 'could', 'easily', 'view', 'personal', 'banking', 'information', 'customers', 'correct', 'LINK', ':', 'https', ':', '//www.macobserver.com/analysis/capital-one-natural-language-chatbot-eno/', '8.3', 'Future', 'BI', 'Natural', 'Language', 'Processing', '[', '106', ']', 'Several', 'companies', 'Bi', 'spaces', 'trying', 'get', 'trend', 'trying', 'hard', 'ensure', 'data', 'becomes', 'friendly', 'easily', 'accessible', '.']

 TOTAL FILTERED TOKENS ==>  41

 ---- POST FOR FILTERED TOKENS ----

 [('would', 'MD'), ('case', 'NN'), ('admins', 'NNS'), ('could', 'MD'), ('easily', 'RB'), ('view', 'VB'), ('personal', 'JJ'), ('banking', 'NN'), ('information', 'NN'), ('customers', 'NNS'), ('correct', 'VBP'), ('LINK', 'NNP'), (':', ':'), ('https', 'NN'), (':', ':'), ('//www.macobserver.com/analysis/capital-one-natural-language-chatbot-eno/', 'JJ'), ('8.3', 'CD'), ('Future', 'NNP'), ('BI', 'NNP'), ('Natural', 'NNP'), ('Language', 'NNP'), ('Processing', 'NNP'), ('[', 'VBZ'), ('106', 'CD'), (']', 'JJ'), ('Several', 'JJ'), ('companies', 'NNS'), ('Bi', 'NNP'), ('spaces', 'NNS'), ('trying', 'VBG'), ('get', 'VB'), ('trend', 'NN'), ('trying', 'VBG'), ('hard', 'JJ'), ('ensure', 'VB'), ('data', 'NNS'), ('becomes', 'NNS'), ('friendly', 'RB'), ('easily', 'RB'), ('accessible', 'JJ'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['would case', 'case admins', 'admins could', 'could easily', 'easily view', 'view personal', 'personal banking', 'banking information', 'information customers', 'customers correct', 'correct LINK', 'LINK :', ': https', 'https :', ': //www.macobserver.com/analysis/capital-one-natural-language-chatbot-eno/', '//www.macobserver.com/analysis/capital-one-natural-language-chatbot-eno/ 8.3', '8.3 Future', 'Future BI', 'BI Natural', 'Natural Language', 'Language Processing', 'Processing [', '[ 106', '106 ]', '] Several', 'Several companies', 'companies Bi', 'Bi spaces', 'spaces trying', 'trying get', 'get trend', 'trend trying', 'trying hard', 'hard ensure', 'ensure data', 'data becomes', 'becomes friendly', 'friendly easily', 'easily accessible', 'accessible .'] 

 TOTAL BIGRAMS --> 40 



 ---- TRI-GRAMS ---- 

 ['would case admins', 'case admins could', 'admins could easily', 'could easily view', 'easily view personal', 'view personal banking', 'personal banking information', 'banking information customers', 'information customers correct', 'customers correct LINK', 'correct LINK :', 'LINK : https', ': https :', 'https : //www.macobserver.com/analysis/capital-one-natural-language-chatbot-eno/', ': //www.macobserver.com/analysis/capital-one-natural-language-chatbot-eno/ 8.3', '//www.macobserver.com/analysis/capital-one-natural-language-chatbot-eno/ 8.3 Future', '8.3 Future BI', 'Future BI Natural', 'BI Natural Language', 'Natural Language Processing', 'Language Processing [', 'Processing [ 106', '[ 106 ]', '106 ] Several', '] Several companies', 'Several companies Bi', 'companies Bi spaces', 'Bi spaces trying', 'spaces trying get', 'trying get trend', 'get trend trying', 'trend trying hard', 'trying hard ensure', 'hard ensure data', 'ensure data becomes', 'data becomes friendly', 'becomes friendly easily', 'friendly easily accessible', 'easily accessible .'] 

 TOTAL TRIGRAMS --> 39 



 ---- NOUN PHRASES ---- 

 ['case', 'personal banking', 'information', 'https', 'trend'] 

 TOTAL NOUN PHRASES --> 5 



 ---- NER ----

 
 ORGANIZATION ---> ['LINK']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['would', 'case', 'admin', 'could', 'easili', 'view', 'person', 'bank', 'inform', 'custom', 'correct', 'link', ':', 'http', ':', '//www.macobserver.com/analysis/capital-one-natural-language-chatbot-eno/', '8.3', 'futur', 'bi', 'natur', 'languag', 'process', '[', '106', ']', 'sever', 'compani', 'bi', 'space', 'tri', 'get', 'trend', 'tri', 'hard', 'ensur', 'data', 'becom', 'friendli', 'easili', 'access', '.']

 TOTAL PORTER STEM WORDS ==> 41



 ---- SNOWBALL STEMMING ----

['would', 'case', 'admin', 'could', 'easili', 'view', 'person', 'bank', 'inform', 'custom', 'correct', 'link', ':', 'https', ':', '//www.macobserver.com/analysis/capital-one-natural-language-chatbot-eno/', '8.3', 'futur', 'bi', 'natur', 'languag', 'process', '[', '106', ']', 'sever', 'compani', 'bi', 'space', 'tri', 'get', 'trend', 'tri', 'hard', 'ensur', 'data', 'becom', 'friend', 'easili', 'access', '.']

 TOTAL SNOWBALL STEM WORDS ==> 41



 ---- LEMMATIZATION ----

['would', 'case', 'admins', 'could', 'easily', 'view', 'personal', 'banking', 'information', 'customer', 'correct', 'LINK', ':', 'http', ':', '//www.macobserver.com/analysis/capital-one-natural-language-chatbot-eno/', '8.3', 'Future', 'BI', 'Natural', 'Language', 'Processing', '[', '106', ']', 'Several', 'company', 'Bi', 'space', 'trying', 'get', 'trend', 'trying', 'hard', 'ensure', 'data', 'becomes', 'friendly', 'easily', 'accessible', '.']

 TOTAL LEMMATIZE WORDS ==> 41

************************************************************************************************************************

323 --> But still there is long way for this.BI will  also make it easier to access as GUI is not needed. 


 ---- TOKENS ----

 ['But', 'still', 'there', 'is', 'long', 'way', 'for', 'this.BI', 'will', 'also', 'make', 'it', 'easier', 'to', 'access', 'as', 'GUI', 'is', 'not', 'needed', '.'] 

 TOTAL TOKENS ==> 21

 ---- POST ----

 [('But', 'CC'), ('still', 'RB'), ('there', 'EX'), ('is', 'VBZ'), ('long', 'JJ'), ('way', 'NN'), ('for', 'IN'), ('this.BI', 'NN'), ('will', 'MD'), ('also', 'RB'), ('make', 'VB'), ('it', 'PRP'), ('easier', 'JJR'), ('to', 'TO'), ('access', 'NN'), ('as', 'IN'), ('GUI', 'NNP'), ('is', 'VBZ'), ('not', 'RB'), ('needed', 'VBN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['still', 'long', 'way', 'this.BI', 'also', 'make', 'easier', 'access', 'GUI', 'needed', '.']

 TOTAL FILTERED TOKENS ==>  11

 ---- POST FOR FILTERED TOKENS ----

 [('still', 'RB'), ('long', 'JJ'), ('way', 'NN'), ('this.BI', 'NN'), ('also', 'RB'), ('make', 'VBP'), ('easier', 'JJR'), ('access', 'NN'), ('GUI', 'NNP'), ('needed', 'VBD'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['still long', 'long way', 'way this.BI', 'this.BI also', 'also make', 'make easier', 'easier access', 'access GUI', 'GUI needed', 'needed .'] 

 TOTAL BIGRAMS --> 10 



 ---- TRI-GRAMS ---- 

 ['still long way', 'long way this.BI', 'way this.BI also', 'this.BI also make', 'also make easier', 'make easier access', 'easier access GUI', 'access GUI needed', 'GUI needed .'] 

 TOTAL TRIGRAMS --> 9 



 ---- NOUN PHRASES ---- 

 ['long way', 'this.BI', 'access'] 

 TOTAL NOUN PHRASES --> 3 



 ---- NER ----

 
 ORGANIZATION ---> ['GUI']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['still', 'long', 'way', 'this.bi', 'also', 'make', 'easier', 'access', 'gui', 'need', '.']

 TOTAL PORTER STEM WORDS ==> 11



 ---- SNOWBALL STEMMING ----

['still', 'long', 'way', 'this.bi', 'also', 'make', 'easier', 'access', 'gui', 'need', '.']

 TOTAL SNOWBALL STEM WORDS ==> 11



 ---- LEMMATIZATION ----

['still', 'long', 'way', 'this.BI', 'also', 'make', 'easier', 'access', 'GUI', 'needed', '.']

 TOTAL LEMMATIZE WORDS ==> 11

************************************************************************************************************************

324 --> Because now a days the queries are made  by text or voice command on smartphones.one of the most common example is Google might  tell you today what will be the tomorrows weather. 


 ---- TOKENS ----

 ['Because', 'now', 'a', 'days', 'the', 'queries', 'are', 'made', 'by', 'text', 'or', 'voice', 'command', 'on', 'smartphones.one', 'of', 'the', 'most', 'common', 'example', 'is', 'Google', 'might', 'tell', 'you', 'today', 'what', 'will', 'be', 'the', 'tomorrows', 'weather', '.'] 

 TOTAL TOKENS ==> 33

 ---- POST ----

 [('Because', 'IN'), ('now', 'RB'), ('a', 'DT'), ('days', 'NNS'), ('the', 'DT'), ('queries', 'NNS'), ('are', 'VBP'), ('made', 'VBN'), ('by', 'IN'), ('text', 'NN'), ('or', 'CC'), ('voice', 'NN'), ('command', 'NN'), ('on', 'IN'), ('smartphones.one', 'NN'), ('of', 'IN'), ('the', 'DT'), ('most', 'RBS'), ('common', 'JJ'), ('example', 'NN'), ('is', 'VBZ'), ('Google', 'NNP'), ('might', 'MD'), ('tell', 'VB'), ('you', 'PRP'), ('today', 'NN'), ('what', 'WP'), ('will', 'MD'), ('be', 'VB'), ('the', 'DT'), ('tomorrows', 'NNS'), ('weather', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['days', 'queries', 'made', 'text', 'voice', 'command', 'smartphones.one', 'common', 'example', 'Google', 'might', 'tell', 'today', 'tomorrows', 'weather', '.']

 TOTAL FILTERED TOKENS ==>  16

 ---- POST FOR FILTERED TOKENS ----

 [('days', 'NNS'), ('queries', 'NNS'), ('made', 'VBD'), ('text', 'JJ'), ('voice', 'NN'), ('command', 'NN'), ('smartphones.one', 'NN'), ('common', 'JJ'), ('example', 'NN'), ('Google', 'NNP'), ('might', 'MD'), ('tell', 'VB'), ('today', 'NN'), ('tomorrows', 'VBZ'), ('weather', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['days queries', 'queries made', 'made text', 'text voice', 'voice command', 'command smartphones.one', 'smartphones.one common', 'common example', 'example Google', 'Google might', 'might tell', 'tell today', 'today tomorrows', 'tomorrows weather', 'weather .'] 

 TOTAL BIGRAMS --> 15 



 ---- TRI-GRAMS ---- 

 ['days queries made', 'queries made text', 'made text voice', 'text voice command', 'voice command smartphones.one', 'command smartphones.one common', 'smartphones.one common example', 'common example Google', 'example Google might', 'Google might tell', 'might tell today', 'tell today tomorrows', 'today tomorrows weather', 'tomorrows weather .'] 

 TOTAL TRIGRAMS --> 14 



 ---- NOUN PHRASES ---- 

 ['text voice', 'command', 'smartphones.one', 'common example', 'today', 'weather'] 

 TOTAL NOUN PHRASES --> 6 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> ['Google']
 TOTAL PERSON ENTITY --> 1 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['day', 'queri', 'made', 'text', 'voic', 'command', 'smartphones.on', 'common', 'exampl', 'googl', 'might', 'tell', 'today', 'tomorrow', 'weather', '.']

 TOTAL PORTER STEM WORDS ==> 16



 ---- SNOWBALL STEMMING ----

['day', 'queri', 'made', 'text', 'voic', 'command', 'smartphones.on', 'common', 'exampl', 'googl', 'might', 'tell', 'today', 'tomorrow', 'weather', '.']

 TOTAL SNOWBALL STEM WORDS ==> 16



 ---- LEMMATIZATION ----

['day', 'query', 'made', 'text', 'voice', 'command', 'smartphones.one', 'common', 'example', 'Google', 'might', 'tell', 'today', 'tomorrow', 'weather', '.']

 TOTAL LEMMATIZE WORDS ==> 16

************************************************************************************************************************

325 --> But soon enough, we will be able to ask  our personal data chatbot about customer sentiment today, and how do we feel about their  brand next week; all while walking down the street. 


 ---- TOKENS ----

 ['But', 'soon', 'enough', ',', 'we', 'will', 'be', 'able', 'to', 'ask', 'our', 'personal', 'data', 'chatbot', 'about', 'customer', 'sentiment', 'today', ',', 'and', 'how', 'do', 'we', 'feel', 'about', 'their', 'brand', 'next', 'week', ';', 'all', 'while', 'walking', 'down', 'the', 'street', '.'] 

 TOTAL TOKENS ==> 37

 ---- POST ----

 [('But', 'CC'), ('soon', 'RB'), ('enough', 'RB'), (',', ','), ('we', 'PRP'), ('will', 'MD'), ('be', 'VB'), ('able', 'JJ'), ('to', 'TO'), ('ask', 'VB'), ('our', 'PRP$'), ('personal', 'JJ'), ('data', 'NNS'), ('chatbot', 'NN'), ('about', 'IN'), ('customer', 'NN'), ('sentiment', 'NN'), ('today', 'NN'), (',', ','), ('and', 'CC'), ('how', 'WRB'), ('do', 'VBP'), ('we', 'PRP'), ('feel', 'VB'), ('about', 'IN'), ('their', 'PRP$'), ('brand', 'NN'), ('next', 'IN'), ('week', 'NN'), (';', ':'), ('all', 'DT'), ('while', 'IN'), ('walking', 'VBG'), ('down', 'RP'), ('the', 'DT'), ('street', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['soon', 'enough', ',', 'able', 'ask', 'personal', 'data', 'chatbot', 'customer', 'sentiment', 'today', ',', 'feel', 'brand', 'next', 'week', ';', 'walking', 'street', '.']

 TOTAL FILTERED TOKENS ==>  20

 ---- POST FOR FILTERED TOKENS ----

 [('soon', 'RB'), ('enough', 'RB'), (',', ','), ('able', 'JJ'), ('ask', 'JJ'), ('personal', 'JJ'), ('data', 'NNS'), ('chatbot', 'NN'), ('customer', 'NN'), ('sentiment', 'NN'), ('today', 'NN'), (',', ','), ('feel', 'VB'), ('brand', 'NN'), ('next', 'IN'), ('week', 'NN'), (';', ':'), ('walking', 'VBG'), ('street', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['soon enough', 'enough ,', ', able', 'able ask', 'ask personal', 'personal data', 'data chatbot', 'chatbot customer', 'customer sentiment', 'sentiment today', 'today ,', ', feel', 'feel brand', 'brand next', 'next week', 'week ;', '; walking', 'walking street', 'street .'] 

 TOTAL BIGRAMS --> 19 



 ---- TRI-GRAMS ---- 

 ['soon enough ,', 'enough , able', ', able ask', 'able ask personal', 'ask personal data', 'personal data chatbot', 'data chatbot customer', 'chatbot customer sentiment', 'customer sentiment today', 'sentiment today ,', 'today , feel', ', feel brand', 'feel brand next', 'brand next week', 'next week ;', 'week ; walking', '; walking street', 'walking street .'] 

 TOTAL TRIGRAMS --> 18 



 ---- NOUN PHRASES ---- 

 ['chatbot', 'customer', 'sentiment', 'today', 'brand', 'week', 'street'] 

 TOTAL NOUN PHRASES --> 7 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['soon', 'enough', ',', 'abl', 'ask', 'person', 'data', 'chatbot', 'custom', 'sentiment', 'today', ',', 'feel', 'brand', 'next', 'week', ';', 'walk', 'street', '.']

 TOTAL PORTER STEM WORDS ==> 20



 ---- SNOWBALL STEMMING ----

['soon', 'enough', ',', 'abl', 'ask', 'person', 'data', 'chatbot', 'custom', 'sentiment', 'today', ',', 'feel', 'brand', 'next', 'week', ';', 'walk', 'street', '.']

 TOTAL SNOWBALL STEM WORDS ==> 20



 ---- LEMMATIZATION ----

['soon', 'enough', ',', 'able', 'ask', 'personal', 'data', 'chatbot', 'customer', 'sentiment', 'today', ',', 'feel', 'brand', 'next', 'week', ';', 'walking', 'street', '.']

 TOTAL LEMMATIZE WORDS ==> 20

************************************************************************************************************************

326 --> Today, NLP tends to be based on turning  natural language into machine language. 


 ---- TOKENS ----

 ['Today', ',', 'NLP', 'tends', 'to', 'be', 'based', 'on', 'turning', 'natural', 'language', 'into', 'machine', 'language', '.'] 

 TOTAL TOKENS ==> 15

 ---- POST ----

 [('Today', 'NN'), (',', ','), ('NLP', 'NNP'), ('tends', 'VBZ'), ('to', 'TO'), ('be', 'VB'), ('based', 'VBN'), ('on', 'IN'), ('turning', 'VBG'), ('natural', 'JJ'), ('language', 'NN'), ('into', 'IN'), ('machine', 'NN'), ('language', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Today', ',', 'NLP', 'tends', 'based', 'turning', 'natural', 'language', 'machine', 'language', '.']

 TOTAL FILTERED TOKENS ==>  11

 ---- POST FOR FILTERED TOKENS ----

 [('Today', 'NN'), (',', ','), ('NLP', 'NNP'), ('tends', 'VBZ'), ('based', 'VBN'), ('turning', 'VBG'), ('natural', 'JJ'), ('language', 'NN'), ('machine', 'NN'), ('language', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Today ,', ', NLP', 'NLP tends', 'tends based', 'based turning', 'turning natural', 'natural language', 'language machine', 'machine language', 'language .'] 

 TOTAL BIGRAMS --> 10 



 ---- TRI-GRAMS ---- 

 ['Today , NLP', ', NLP tends', 'NLP tends based', 'tends based turning', 'based turning natural', 'turning natural language', 'natural language machine', 'language machine language', 'machine language .'] 

 TOTAL TRIGRAMS --> 9 



 ---- NOUN PHRASES ---- 

 ['Today', 'natural language', 'machine', 'language'] 

 TOTAL NOUN PHRASES --> 4 



 ---- NER ----

 
 ORGANIZATION ---> ['NLP']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['today', ',', 'nlp', 'tend', 'base', 'turn', 'natur', 'languag', 'machin', 'languag', '.']

 TOTAL PORTER STEM WORDS ==> 11



 ---- SNOWBALL STEMMING ----

['today', ',', 'nlp', 'tend', 'base', 'turn', 'natur', 'languag', 'machin', 'languag', '.']

 TOTAL SNOWBALL STEM WORDS ==> 11



 ---- LEMMATIZATION ----

['Today', ',', 'NLP', 'tends', 'based', 'turning', 'natural', 'language', 'machine', 'language', '.']

 TOTAL LEMMATIZE WORDS ==> 11

************************************************************************************************************************

327 --> But with time the technology matures – especially  the AI component –the computer will get better at “understanding” the query and start to  deliver answers rather than search results. 


 ---- TOKENS ----

 ['But', 'with', 'time', 'the', 'technology', 'matures', '–', 'especially', 'the', 'AI', 'component', '–the', 'computer', 'will', 'get', 'better', 'at', '“', 'understanding', '”', 'the', 'query', 'and', 'start', 'to', 'deliver', 'answers', 'rather', 'than', 'search', 'results', '.'] 

 TOTAL TOKENS ==> 32

 ---- POST ----

 [('But', 'CC'), ('with', 'IN'), ('time', 'NN'), ('the', 'DT'), ('technology', 'NN'), ('matures', 'VBZ'), ('–', 'NNP'), ('especially', 'RB'), ('the', 'DT'), ('AI', 'NNP'), ('component', 'NN'), ('–the', 'NNP'), ('computer', 'NN'), ('will', 'MD'), ('get', 'VB'), ('better', 'JJR'), ('at', 'IN'), ('“', 'NNP'), ('understanding', 'VBG'), ('”', 'VBD'), ('the', 'DT'), ('query', 'NN'), ('and', 'CC'), ('start', 'VB'), ('to', 'TO'), ('deliver', 'VB'), ('answers', 'NNS'), ('rather', 'RB'), ('than', 'IN'), ('search', 'NN'), ('results', 'NNS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['time', 'technology', 'matures', '–', 'especially', 'AI', 'component', '–the', 'computer', 'get', 'better', '“', 'understanding', '”', 'query', 'start', 'deliver', 'answers', 'rather', 'search', 'results', '.']

 TOTAL FILTERED TOKENS ==>  22

 ---- POST FOR FILTERED TOKENS ----

 [('time', 'NN'), ('technology', 'NN'), ('matures', 'VBZ'), ('–', 'NNP'), ('especially', 'RB'), ('AI', 'NNP'), ('component', 'NN'), ('–the', 'NNP'), ('computer', 'NN'), ('get', 'VBP'), ('better', 'JJR'), ('“', 'NN'), ('understanding', 'VBG'), ('”', 'JJ'), ('query', 'JJ'), ('start', 'NN'), ('deliver', 'NN'), ('answers', 'NNS'), ('rather', 'RB'), ('search', 'JJ'), ('results', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['time technology', 'technology matures', 'matures –', '– especially', 'especially AI', 'AI component', 'component –the', '–the computer', 'computer get', 'get better', 'better “', '“ understanding', 'understanding ”', '” query', 'query start', 'start deliver', 'deliver answers', 'answers rather', 'rather search', 'search results', 'results .'] 

 TOTAL BIGRAMS --> 21 



 ---- TRI-GRAMS ---- 

 ['time technology matures', 'technology matures –', 'matures – especially', '– especially AI', 'especially AI component', 'AI component –the', 'component –the computer', '–the computer get', 'computer get better', 'get better “', 'better “ understanding', '“ understanding ”', 'understanding ” query', '” query start', 'query start deliver', 'start deliver answers', 'deliver answers rather', 'answers rather search', 'rather search results', 'search results .'] 

 TOTAL TRIGRAMS --> 20 



 ---- NOUN PHRASES ---- 

 ['time', 'technology', 'component', 'computer', '“', '” query start', 'deliver'] 

 TOTAL NOUN PHRASES --> 7 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['time', 'technolog', 'matur', '–', 'especi', 'ai', 'compon', '–the', 'comput', 'get', 'better', '“', 'understand', '”', 'queri', 'start', 'deliv', 'answer', 'rather', 'search', 'result', '.']

 TOTAL PORTER STEM WORDS ==> 22



 ---- SNOWBALL STEMMING ----

['time', 'technolog', 'matur', '–', 'especi', 'ai', 'compon', '–the', 'comput', 'get', 'better', '“', 'understand', '”', 'queri', 'start', 'deliv', 'answer', 'rather', 'search', 'result', '.']

 TOTAL SNOWBALL STEM WORDS ==> 22



 ---- LEMMATIZATION ----

['time', 'technology', 'matures', '–', 'especially', 'AI', 'component', '–the', 'computer', 'get', 'better', '“', 'understanding', '”', 'query', 'start', 'deliver', 'answer', 'rather', 'search', 'result', '.']

 TOTAL LEMMATIZE WORDS ==> 22

************************************************************************************************************************

328 --> Initially, the data chatbot will probably ask the question as how have revenues changed over  the last three-quarters?’ and then return pages of data for you to analyse. 


 ---- TOKENS ----

 ['Initially', ',', 'the', 'data', 'chatbot', 'will', 'probably', 'ask', 'the', 'question', 'as', 'how', 'have', 'revenues', 'changed', 'over', 'the', 'last', 'three-quarters', '?', '’', 'and', 'then', 'return', 'pages', 'of', 'data', 'for', 'you', 'to', 'analyse', '.'] 

 TOTAL TOKENS ==> 32

 ---- POST ----

 [('Initially', 'RB'), (',', ','), ('the', 'DT'), ('data', 'NN'), ('chatbot', 'NN'), ('will', 'MD'), ('probably', 'RB'), ('ask', 'VB'), ('the', 'DT'), ('question', 'NN'), ('as', 'IN'), ('how', 'WRB'), ('have', 'VBP'), ('revenues', 'NNS'), ('changed', 'VBN'), ('over', 'IN'), ('the', 'DT'), ('last', 'JJ'), ('three-quarters', 'NNS'), ('?', '.'), ('’', 'NN'), ('and', 'CC'), ('then', 'RB'), ('return', 'VB'), ('pages', 'NNS'), ('of', 'IN'), ('data', 'NNS'), ('for', 'IN'), ('you', 'PRP'), ('to', 'TO'), ('analyse', 'VB'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Initially', ',', 'data', 'chatbot', 'probably', 'ask', 'question', 'revenues', 'changed', 'last', 'three-quarters', '?', '’', 'return', 'pages', 'data', 'analyse', '.']

 TOTAL FILTERED TOKENS ==>  18

 ---- POST FOR FILTERED TOKENS ----

 [('Initially', 'RB'), (',', ','), ('data', 'NNS'), ('chatbot', 'NN'), ('probably', 'RB'), ('ask', 'JJ'), ('question', 'NN'), ('revenues', 'NNS'), ('changed', 'VBD'), ('last', 'JJ'), ('three-quarters', 'NNS'), ('?', '.'), ('’', 'JJ'), ('return', 'NN'), ('pages', 'NNS'), ('data', 'VBP'), ('analyse', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Initially ,', ', data', 'data chatbot', 'chatbot probably', 'probably ask', 'ask question', 'question revenues', 'revenues changed', 'changed last', 'last three-quarters', 'three-quarters ?', '? ’', '’ return', 'return pages', 'pages data', 'data analyse', 'analyse .'] 

 TOTAL BIGRAMS --> 17 



 ---- TRI-GRAMS ---- 

 ['Initially , data', ', data chatbot', 'data chatbot probably', 'chatbot probably ask', 'probably ask question', 'ask question revenues', 'question revenues changed', 'revenues changed last', 'changed last three-quarters', 'last three-quarters ?', 'three-quarters ? ’', '? ’ return', '’ return pages', 'return pages data', 'pages data analyse', 'data analyse .'] 

 TOTAL TRIGRAMS --> 16 



 ---- NOUN PHRASES ---- 

 ['chatbot', 'ask question', '’ return', 'analyse'] 

 TOTAL NOUN PHRASES --> 4 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['initi', ',', 'data', 'chatbot', 'probabl', 'ask', 'question', 'revenu', 'chang', 'last', 'three-quart', '?', '’', 'return', 'page', 'data', 'analys', '.']

 TOTAL PORTER STEM WORDS ==> 18



 ---- SNOWBALL STEMMING ----

['initi', ',', 'data', 'chatbot', 'probabl', 'ask', 'question', 'revenu', 'chang', 'last', 'three-quart', '?', '’', 'return', 'page', 'data', 'analys', '.']

 TOTAL SNOWBALL STEM WORDS ==> 18



 ---- LEMMATIZATION ----

['Initially', ',', 'data', 'chatbot', 'probably', 'ask', 'question', 'revenue', 'changed', 'last', 'three-quarters', '?', '’', 'return', 'page', 'data', 'analyse', '.']

 TOTAL LEMMATIZE WORDS ==> 18

************************************************************************************************************************

329 --> But once it learns  the semantic relations and inferences of the question, it will be able to automatically perform  the filtering and formulation necessary to provide an intelligible answer, rather than simply  showing you data. 


 ---- TOKENS ----

 ['But', 'once', 'it', 'learns', 'the', 'semantic', 'relations', 'and', 'inferences', 'of', 'the', 'question', ',', 'it', 'will', 'be', 'able', 'to', 'automatically', 'perform', 'the', 'filtering', 'and', 'formulation', 'necessary', 'to', 'provide', 'an', 'intelligible', 'answer', ',', 'rather', 'than', 'simply', 'showing', 'you', 'data', '.'] 

 TOTAL TOKENS ==> 38

 ---- POST ----

 [('But', 'CC'), ('once', 'IN'), ('it', 'PRP'), ('learns', 'VBZ'), ('the', 'DT'), ('semantic', 'JJ'), ('relations', 'NNS'), ('and', 'CC'), ('inferences', 'NNS'), ('of', 'IN'), ('the', 'DT'), ('question', 'NN'), (',', ','), ('it', 'PRP'), ('will', 'MD'), ('be', 'VB'), ('able', 'JJ'), ('to', 'TO'), ('automatically', 'RB'), ('perform', 'VB'), ('the', 'DT'), ('filtering', 'NN'), ('and', 'CC'), ('formulation', 'NN'), ('necessary', 'JJ'), ('to', 'TO'), ('provide', 'VB'), ('an', 'DT'), ('intelligible', 'JJ'), ('answer', 'NN'), (',', ','), ('rather', 'RB'), ('than', 'IN'), ('simply', 'RB'), ('showing', 'VBG'), ('you', 'PRP'), ('data', 'NNS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['learns', 'semantic', 'relations', 'inferences', 'question', ',', 'able', 'automatically', 'perform', 'filtering', 'formulation', 'necessary', 'provide', 'intelligible', 'answer', ',', 'rather', 'simply', 'showing', 'data', '.']

 TOTAL FILTERED TOKENS ==>  21

 ---- POST FOR FILTERED TOKENS ----

 [('learns', 'NNS'), ('semantic', 'JJ'), ('relations', 'NNS'), ('inferences', 'NNS'), ('question', 'NN'), (',', ','), ('able', 'JJ'), ('automatically', 'RB'), ('perform', 'VB'), ('filtering', 'VBG'), ('formulation', 'NN'), ('necessary', 'JJ'), ('provide', 'NN'), ('intelligible', 'JJ'), ('answer', 'NN'), (',', ','), ('rather', 'RB'), ('simply', 'RB'), ('showing', 'VBG'), ('data', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['learns semantic', 'semantic relations', 'relations inferences', 'inferences question', 'question ,', ', able', 'able automatically', 'automatically perform', 'perform filtering', 'filtering formulation', 'formulation necessary', 'necessary provide', 'provide intelligible', 'intelligible answer', 'answer ,', ', rather', 'rather simply', 'simply showing', 'showing data', 'data .'] 

 TOTAL BIGRAMS --> 20 



 ---- TRI-GRAMS ---- 

 ['learns semantic relations', 'semantic relations inferences', 'relations inferences question', 'inferences question ,', 'question , able', ', able automatically', 'able automatically perform', 'automatically perform filtering', 'perform filtering formulation', 'filtering formulation necessary', 'formulation necessary provide', 'necessary provide intelligible', 'provide intelligible answer', 'intelligible answer ,', 'answer , rather', ', rather simply', 'rather simply showing', 'simply showing data', 'showing data .'] 

 TOTAL TRIGRAMS --> 19 



 ---- NOUN PHRASES ---- 

 ['question', 'formulation', 'necessary provide', 'intelligible answer'] 

 TOTAL NOUN PHRASES --> 4 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['learn', 'semant', 'relat', 'infer', 'question', ',', 'abl', 'automat', 'perform', 'filter', 'formul', 'necessari', 'provid', 'intellig', 'answer', ',', 'rather', 'simpli', 'show', 'data', '.']

 TOTAL PORTER STEM WORDS ==> 21



 ---- SNOWBALL STEMMING ----

['learn', 'semant', 'relat', 'infer', 'question', ',', 'abl', 'automat', 'perform', 'filter', 'formul', 'necessari', 'provid', 'intellig', 'answer', ',', 'rather', 'simpli', 'show', 'data', '.']

 TOTAL SNOWBALL STEM WORDS ==> 21



 ---- LEMMATIZATION ----

['learns', 'semantic', 'relation', 'inference', 'question', ',', 'able', 'automatically', 'perform', 'filtering', 'formulation', 'necessary', 'provide', 'intelligible', 'answer', ',', 'rather', 'simply', 'showing', 'data', '.']

 TOTAL LEMMATIZE WORDS ==> 21

************************************************************************************************************************

330 --> Link: http://www.smartdatacollective.com/eran-levy/489410/here-s-why-natural-language-processing-future-bi  8.4 Using Natural Language Processing and Network Analysis to  Develop a Conceptual Framework for Medication Therapy  Management Research [107]  Natural Language Processing and Network Analysis to Develop a Conceptual Framework for  Medication Therapy Management Research describes a theory derivation process that is used  to develop conceptual framework for medication therapy management (MTM) research. 


 ---- TOKENS ----

 ['Link', ':', 'http', ':', '//www.smartdatacollective.com/eran-levy/489410/here-s-why-natural-language-processing-future-bi', '8.4', 'Using', 'Natural', 'Language', 'Processing', 'and', 'Network', 'Analysis', 'to', 'Develop', 'a', 'Conceptual', 'Framework', 'for', 'Medication', 'Therapy', 'Management', 'Research', '[', '107', ']', 'Natural', 'Language', 'Processing', 'and', 'Network', 'Analysis', 'to', 'Develop', 'a', 'Conceptual', 'Framework', 'for', 'Medication', 'Therapy', 'Management', 'Research', 'describes', 'a', 'theory', 'derivation', 'process', 'that', 'is', 'used', 'to', 'develop', 'conceptual', 'framework', 'for', 'medication', 'therapy', 'management', '(', 'MTM', ')', 'research', '.'] 

 TOTAL TOKENS ==> 63

 ---- POST ----

 [('Link', 'NN'), (':', ':'), ('http', 'NN'), (':', ':'), ('//www.smartdatacollective.com/eran-levy/489410/here-s-why-natural-language-processing-future-bi', 'JJ'), ('8.4', 'CD'), ('Using', 'NNP'), ('Natural', 'NNP'), ('Language', 'NNP'), ('Processing', 'NNP'), ('and', 'CC'), ('Network', 'NNP'), ('Analysis', 'NNP'), ('to', 'TO'), ('Develop', 'VB'), ('a', 'DT'), ('Conceptual', 'NNP'), ('Framework', 'NNP'), ('for', 'IN'), ('Medication', 'NNP'), ('Therapy', 'NNP'), ('Management', 'NNP'), ('Research', 'NNP'), ('[', 'VBZ'), ('107', 'CD'), (']', 'JJ'), ('Natural', 'NNP'), ('Language', 'NNP'), ('Processing', 'NNP'), ('and', 'CC'), ('Network', 'NNP'), ('Analysis', 'NNP'), ('to', 'TO'), ('Develop', 'VB'), ('a', 'DT'), ('Conceptual', 'NNP'), ('Framework', 'NNP'), ('for', 'IN'), ('Medication', 'NNP'), ('Therapy', 'NNP'), ('Management', 'NNP'), ('Research', 'NNP'), ('describes', 'VBZ'), ('a', 'DT'), ('theory', 'JJ'), ('derivation', 'NN'), ('process', 'NN'), ('that', 'WDT'), ('is', 'VBZ'), ('used', 'VBN'), ('to', 'TO'), ('develop', 'VB'), ('conceptual', 'JJ'), ('framework', 'NN'), ('for', 'IN'), ('medication', 'NN'), ('therapy', 'NN'), ('management', 'NN'), ('(', '('), ('MTM', 'NNP'), (')', ')'), ('research', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Link', ':', 'http', ':', '//www.smartdatacollective.com/eran-levy/489410/here-s-why-natural-language-processing-future-bi', '8.4', 'Using', 'Natural', 'Language', 'Processing', 'Network', 'Analysis', 'Develop', 'Conceptual', 'Framework', 'Medication', 'Therapy', 'Management', 'Research', '[', '107', ']', 'Natural', 'Language', 'Processing', 'Network', 'Analysis', 'Develop', 'Conceptual', 'Framework', 'Medication', 'Therapy', 'Management', 'Research', 'describes', 'theory', 'derivation', 'process', 'used', 'develop', 'conceptual', 'framework', 'medication', 'therapy', 'management', '(', 'MTM', ')', 'research', '.']

 TOTAL FILTERED TOKENS ==>  50

 ---- POST FOR FILTERED TOKENS ----

 [('Link', 'NN'), (':', ':'), ('http', 'NN'), (':', ':'), ('//www.smartdatacollective.com/eran-levy/489410/here-s-why-natural-language-processing-future-bi', 'JJ'), ('8.4', 'CD'), ('Using', 'NNP'), ('Natural', 'NNP'), ('Language', 'NNP'), ('Processing', 'NNP'), ('Network', 'NNP'), ('Analysis', 'NNP'), ('Develop', 'NNP'), ('Conceptual', 'NNP'), ('Framework', 'NNP'), ('Medication', 'NNP'), ('Therapy', 'NNP'), ('Management', 'NNP'), ('Research', 'NNP'), ('[', 'VBZ'), ('107', 'CD'), (']', 'JJ'), ('Natural', 'NNP'), ('Language', 'NNP'), ('Processing', 'NNP'), ('Network', 'NNP'), ('Analysis', 'NNP'), ('Develop', 'NNP'), ('Conceptual', 'NNP'), ('Framework', 'NNP'), ('Medication', 'NNP'), ('Therapy', 'NNP'), ('Management', 'NNP'), ('Research', 'NNP'), ('describes', 'VBZ'), ('theory', 'JJ'), ('derivation', 'NN'), ('process', 'NN'), ('used', 'VBN'), ('develop', 'VB'), ('conceptual', 'JJ'), ('framework', 'NN'), ('medication', 'NN'), ('therapy', 'NN'), ('management', 'NN'), ('(', '('), ('MTM', 'NNP'), (')', ')'), ('research', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Link :', ': http', 'http :', ': //www.smartdatacollective.com/eran-levy/489410/here-s-why-natural-language-processing-future-bi', '//www.smartdatacollective.com/eran-levy/489410/here-s-why-natural-language-processing-future-bi 8.4', '8.4 Using', 'Using Natural', 'Natural Language', 'Language Processing', 'Processing Network', 'Network Analysis', 'Analysis Develop', 'Develop Conceptual', 'Conceptual Framework', 'Framework Medication', 'Medication Therapy', 'Therapy Management', 'Management Research', 'Research [', '[ 107', '107 ]', '] Natural', 'Natural Language', 'Language Processing', 'Processing Network', 'Network Analysis', 'Analysis Develop', 'Develop Conceptual', 'Conceptual Framework', 'Framework Medication', 'Medication Therapy', 'Therapy Management', 'Management Research', 'Research describes', 'describes theory', 'theory derivation', 'derivation process', 'process used', 'used develop', 'develop conceptual', 'conceptual framework', 'framework medication', 'medication therapy', 'therapy management', 'management (', '( MTM', 'MTM )', ') research', 'research .'] 

 TOTAL BIGRAMS --> 49 



 ---- TRI-GRAMS ---- 

 ['Link : http', ': http :', 'http : //www.smartdatacollective.com/eran-levy/489410/here-s-why-natural-language-processing-future-bi', ': //www.smartdatacollective.com/eran-levy/489410/here-s-why-natural-language-processing-future-bi 8.4', '//www.smartdatacollective.com/eran-levy/489410/here-s-why-natural-language-processing-future-bi 8.4 Using', '8.4 Using Natural', 'Using Natural Language', 'Natural Language Processing', 'Language Processing Network', 'Processing Network Analysis', 'Network Analysis Develop', 'Analysis Develop Conceptual', 'Develop Conceptual Framework', 'Conceptual Framework Medication', 'Framework Medication Therapy', 'Medication Therapy Management', 'Therapy Management Research', 'Management Research [', 'Research [ 107', '[ 107 ]', '107 ] Natural', '] Natural Language', 'Natural Language Processing', 'Language Processing Network', 'Processing Network Analysis', 'Network Analysis Develop', 'Analysis Develop Conceptual', 'Develop Conceptual Framework', 'Conceptual Framework Medication', 'Framework Medication Therapy', 'Medication Therapy Management', 'Therapy Management Research', 'Management Research describes', 'Research describes theory', 'describes theory derivation', 'theory derivation process', 'derivation process used', 'process used develop', 'used develop conceptual', 'develop conceptual framework', 'conceptual framework medication', 'framework medication therapy', 'medication therapy management', 'therapy management (', 'management ( MTM', '( MTM )', 'MTM ) research', ') research .'] 

 TOTAL TRIGRAMS --> 48 



 ---- NOUN PHRASES ---- 

 ['Link', 'http', 'theory derivation', 'process', 'conceptual framework', 'medication', 'therapy', 'management', 'research'] 

 TOTAL NOUN PHRASES --> 9 



 ---- NER ----

 
 ORGANIZATION ---> ['Natural Language']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> ['Network Analysis Develop Conceptual Framework Medication Therapy', 'Network Analysis Develop Conceptual Framework Medication Therapy']
 TOTAL PERSON ENTITY --> 2 


 GPE ---> ['Link']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['link', ':', 'http', ':', '//www.smartdatacollective.com/eran-levy/489410/here-s-why-natural-language-processing-future-bi', '8.4', 'use', 'natur', 'languag', 'process', 'network', 'analysi', 'develop', 'conceptu', 'framework', 'medic', 'therapi', 'manag', 'research', '[', '107', ']', 'natur', 'languag', 'process', 'network', 'analysi', 'develop', 'conceptu', 'framework', 'medic', 'therapi', 'manag', 'research', 'describ', 'theori', 'deriv', 'process', 'use', 'develop', 'conceptu', 'framework', 'medic', 'therapi', 'manag', '(', 'mtm', ')', 'research', '.']

 TOTAL PORTER STEM WORDS ==> 50



 ---- SNOWBALL STEMMING ----

['link', ':', 'http', ':', '//www.smartdatacollective.com/eran-levy/489410/here-s-why-natural-language-processing-future-bi', '8.4', 'use', 'natur', 'languag', 'process', 'network', 'analysi', 'develop', 'conceptu', 'framework', 'medic', 'therapi', 'manag', 'research', '[', '107', ']', 'natur', 'languag', 'process', 'network', 'analysi', 'develop', 'conceptu', 'framework', 'medic', 'therapi', 'manag', 'research', 'describ', 'theori', 'deriv', 'process', 'use', 'develop', 'conceptu', 'framework', 'medic', 'therapi', 'manag', '(', 'mtm', ')', 'research', '.']

 TOTAL SNOWBALL STEM WORDS ==> 50



 ---- LEMMATIZATION ----

['Link', ':', 'http', ':', '//www.smartdatacollective.com/eran-levy/489410/here-s-why-natural-language-processing-future-bi', '8.4', 'Using', 'Natural', 'Language', 'Processing', 'Network', 'Analysis', 'Develop', 'Conceptual', 'Framework', 'Medication', 'Therapy', 'Management', 'Research', '[', '107', ']', 'Natural', 'Language', 'Processing', 'Network', 'Analysis', 'Develop', 'Conceptual', 'Framework', 'Medication', 'Therapy', 'Management', 'Research', 'describes', 'theory', 'derivation', 'process', 'used', 'develop', 'conceptual', 'framework', 'medication', 'therapy', 'management', '(', 'MTM', ')', 'research', '.']

 TOTAL LEMMATIZE WORDS ==> 50

************************************************************************************************************************

331 --> The  MTM service model and chronic care model are selected as parent theories. 


 ---- TOKENS ----

 ['The', 'MTM', 'service', 'model', 'and', 'chronic', 'care', 'model', 'are', 'selected', 'as', 'parent', 'theories', '.'] 

 TOTAL TOKENS ==> 14

 ---- POST ----

 [('The', 'DT'), ('MTM', 'NNP'), ('service', 'NN'), ('model', 'NN'), ('and', 'CC'), ('chronic', 'JJ'), ('care', 'NN'), ('model', 'NN'), ('are', 'VBP'), ('selected', 'VBN'), ('as', 'IN'), ('parent', 'NN'), ('theories', 'NNS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['MTM', 'service', 'model', 'chronic', 'care', 'model', 'selected', 'parent', 'theories', '.']

 TOTAL FILTERED TOKENS ==>  10

 ---- POST FOR FILTERED TOKENS ----

 [('MTM', 'NNP'), ('service', 'NN'), ('model', 'NN'), ('chronic', 'JJ'), ('care', 'NN'), ('model', 'NN'), ('selected', 'VBN'), ('parent', 'NN'), ('theories', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['MTM service', 'service model', 'model chronic', 'chronic care', 'care model', 'model selected', 'selected parent', 'parent theories', 'theories .'] 

 TOTAL BIGRAMS --> 9 



 ---- TRI-GRAMS ---- 

 ['MTM service model', 'service model chronic', 'model chronic care', 'chronic care model', 'care model selected', 'model selected parent', 'selected parent theories', 'parent theories .'] 

 TOTAL TRIGRAMS --> 8 



 ---- NOUN PHRASES ---- 

 ['service', 'model', 'chronic care', 'model', 'parent'] 

 TOTAL NOUN PHRASES --> 5 



 ---- NER ----

 
 ORGANIZATION ---> ['MTM']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['mtm', 'servic', 'model', 'chronic', 'care', 'model', 'select', 'parent', 'theori', '.']

 TOTAL PORTER STEM WORDS ==> 10



 ---- SNOWBALL STEMMING ----

['mtm', 'servic', 'model', 'chronic', 'care', 'model', 'select', 'parent', 'theori', '.']

 TOTAL SNOWBALL STEM WORDS ==> 10



 ---- LEMMATIZATION ----

['MTM', 'service', 'model', 'chronic', 'care', 'model', 'selected', 'parent', 'theory', '.']

 TOTAL LEMMATIZE WORDS ==> 10

************************************************************************************************************************

332 --> Review article  https://www.macobserver.com/analysis/capital-one-natural-language-chatbot-eno/ http://www.smartdatacollective.com/eran-levy/489410/here-s-why-natural-language-processing-future-bi  abstracts target medication therapy management in chronic disease care that were retrieved  from Ovid Medline (2000-2016). 


 ---- TOKENS ----

 ['Review', 'article', 'https', ':', '//www.macobserver.com/analysis/capital-one-natural-language-chatbot-eno/', 'http', ':', '//www.smartdatacollective.com/eran-levy/489410/here-s-why-natural-language-processing-future-bi', 'abstracts', 'target', 'medication', 'therapy', 'management', 'in', 'chronic', 'disease', 'care', 'that', 'were', 'retrieved', 'from', 'Ovid', 'Medline', '(', '2000-2016', ')', '.'] 

 TOTAL TOKENS ==> 27

 ---- POST ----

 [('Review', 'NNP'), ('article', 'NN'), ('https', 'NN'), (':', ':'), ('//www.macobserver.com/analysis/capital-one-natural-language-chatbot-eno/', 'JJ'), ('http', 'NN'), (':', ':'), ('//www.smartdatacollective.com/eran-levy/489410/here-s-why-natural-language-processing-future-bi', 'JJ'), ('abstracts', 'NNS'), ('target', 'VBP'), ('medication', 'NN'), ('therapy', 'NN'), ('management', 'NN'), ('in', 'IN'), ('chronic', 'JJ'), ('disease', 'NN'), ('care', 'NN'), ('that', 'WDT'), ('were', 'VBD'), ('retrieved', 'VBN'), ('from', 'IN'), ('Ovid', 'NNP'), ('Medline', 'NNP'), ('(', '('), ('2000-2016', 'JJ'), (')', ')'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Review', 'article', 'https', ':', '//www.macobserver.com/analysis/capital-one-natural-language-chatbot-eno/', 'http', ':', '//www.smartdatacollective.com/eran-levy/489410/here-s-why-natural-language-processing-future-bi', 'abstracts', 'target', 'medication', 'therapy', 'management', 'chronic', 'disease', 'care', 'retrieved', 'Ovid', 'Medline', '(', '2000-2016', ')', '.']

 TOTAL FILTERED TOKENS ==>  23

 ---- POST FOR FILTERED TOKENS ----

 [('Review', 'NNP'), ('article', 'NN'), ('https', 'NN'), (':', ':'), ('//www.macobserver.com/analysis/capital-one-natural-language-chatbot-eno/', 'JJ'), ('http', 'NN'), (':', ':'), ('//www.smartdatacollective.com/eran-levy/489410/here-s-why-natural-language-processing-future-bi', 'JJ'), ('abstracts', 'NNS'), ('target', 'VBP'), ('medication', 'NN'), ('therapy', 'NN'), ('management', 'NN'), ('chronic', 'JJ'), ('disease', 'NN'), ('care', 'NN'), ('retrieved', 'VBD'), ('Ovid', 'NNP'), ('Medline', 'NNP'), ('(', '('), ('2000-2016', 'JJ'), (')', ')'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Review article', 'article https', 'https :', ': //www.macobserver.com/analysis/capital-one-natural-language-chatbot-eno/', '//www.macobserver.com/analysis/capital-one-natural-language-chatbot-eno/ http', 'http :', ': //www.smartdatacollective.com/eran-levy/489410/here-s-why-natural-language-processing-future-bi', '//www.smartdatacollective.com/eran-levy/489410/here-s-why-natural-language-processing-future-bi abstracts', 'abstracts target', 'target medication', 'medication therapy', 'therapy management', 'management chronic', 'chronic disease', 'disease care', 'care retrieved', 'retrieved Ovid', 'Ovid Medline', 'Medline (', '( 2000-2016', '2000-2016 )', ') .'] 

 TOTAL BIGRAMS --> 22 



 ---- TRI-GRAMS ---- 

 ['Review article https', 'article https :', 'https : //www.macobserver.com/analysis/capital-one-natural-language-chatbot-eno/', ': //www.macobserver.com/analysis/capital-one-natural-language-chatbot-eno/ http', '//www.macobserver.com/analysis/capital-one-natural-language-chatbot-eno/ http :', 'http : //www.smartdatacollective.com/eran-levy/489410/here-s-why-natural-language-processing-future-bi', ': //www.smartdatacollective.com/eran-levy/489410/here-s-why-natural-language-processing-future-bi abstracts', '//www.smartdatacollective.com/eran-levy/489410/here-s-why-natural-language-processing-future-bi abstracts target', 'abstracts target medication', 'target medication therapy', 'medication therapy management', 'therapy management chronic', 'management chronic disease', 'chronic disease care', 'disease care retrieved', 'care retrieved Ovid', 'retrieved Ovid Medline', 'Ovid Medline (', 'Medline ( 2000-2016', '( 2000-2016 )', '2000-2016 ) .'] 

 TOTAL TRIGRAMS --> 21 



 ---- NOUN PHRASES ---- 

 ['article', 'https', ' http', 'medication', 'therapy', 'management', 'chronic disease', 'care'] 

 TOTAL NOUN PHRASES --> 8 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> ['Ovid Medline']
 TOTAL PERSON ENTITY --> 1 


 GPE ---> ['Review']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['review', 'articl', 'http', ':', '//www.macobserver.com/analysis/capital-one-natural-language-chatbot-eno/', 'http', ':', '//www.smartdatacollective.com/eran-levy/489410/here-s-why-natural-language-processing-future-bi', 'abstract', 'target', 'medic', 'therapi', 'manag', 'chronic', 'diseas', 'care', 'retriev', 'ovid', 'medlin', '(', '2000-2016', ')', '.']

 TOTAL PORTER STEM WORDS ==> 23



 ---- SNOWBALL STEMMING ----

['review', 'articl', 'https', ':', '//www.macobserver.com/analysis/capital-one-natural-language-chatbot-eno/', 'http', ':', '//www.smartdatacollective.com/eran-levy/489410/here-s-why-natural-language-processing-future-bi', 'abstract', 'target', 'medic', 'therapi', 'manag', 'chronic', 'diseas', 'care', 'retriev', 'ovid', 'medlin', '(', '2000-2016', ')', '.']

 TOTAL SNOWBALL STEM WORDS ==> 23



 ---- LEMMATIZATION ----

['Review', 'article', 'http', ':', '//www.macobserver.com/analysis/capital-one-natural-language-chatbot-eno/', 'http', ':', '//www.smartdatacollective.com/eran-levy/489410/here-s-why-natural-language-processing-future-bi', 'abstract', 'target', 'medication', 'therapy', 'management', 'chronic', 'disease', 'care', 'retrieved', 'Ovid', 'Medline', '(', '2000-2016', ')', '.']

 TOTAL LEMMATIZE WORDS ==> 23

************************************************************************************************************************

333 --> Unique concepts in each abstract are extracted using Meta Map and their pairwise  cooccurrence are determined. 


 ---- TOKENS ----

 ['Unique', 'concepts', 'in', 'each', 'abstract', 'are', 'extracted', 'using', 'Meta', 'Map', 'and', 'their', 'pairwise', 'cooccurrence', 'are', 'determined', '.'] 

 TOTAL TOKENS ==> 17

 ---- POST ----

 [('Unique', 'JJ'), ('concepts', 'NNS'), ('in', 'IN'), ('each', 'DT'), ('abstract', 'NN'), ('are', 'VBP'), ('extracted', 'VBN'), ('using', 'VBG'), ('Meta', 'NNP'), ('Map', 'NNP'), ('and', 'CC'), ('their', 'PRP$'), ('pairwise', 'NN'), ('cooccurrence', 'NN'), ('are', 'VBP'), ('determined', 'VBN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Unique', 'concepts', 'abstract', 'extracted', 'using', 'Meta', 'Map', 'pairwise', 'cooccurrence', 'determined', '.']

 TOTAL FILTERED TOKENS ==>  11

 ---- POST FOR FILTERED TOKENS ----

 [('Unique', 'JJ'), ('concepts', 'NNS'), ('abstract', 'VBP'), ('extracted', 'VBN'), ('using', 'VBG'), ('Meta', 'NNP'), ('Map', 'NNP'), ('pairwise', 'NN'), ('cooccurrence', 'NN'), ('determined', 'VBD'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Unique concepts', 'concepts abstract', 'abstract extracted', 'extracted using', 'using Meta', 'Meta Map', 'Map pairwise', 'pairwise cooccurrence', 'cooccurrence determined', 'determined .'] 

 TOTAL BIGRAMS --> 10 



 ---- TRI-GRAMS ---- 

 ['Unique concepts abstract', 'concepts abstract extracted', 'abstract extracted using', 'extracted using Meta', 'using Meta Map', 'Meta Map pairwise', 'Map pairwise cooccurrence', 'pairwise cooccurrence determined', 'cooccurrence determined .'] 

 TOTAL TRIGRAMS --> 9 



 ---- NOUN PHRASES ---- 

 ['pairwise', 'cooccurrence'] 

 TOTAL NOUN PHRASES --> 2 



 ---- NER ----

 
 ORGANIZATION ---> ['Meta Map']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> ['Unique']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['uniqu', 'concept', 'abstract', 'extract', 'use', 'meta', 'map', 'pairwis', 'cooccurr', 'determin', '.']

 TOTAL PORTER STEM WORDS ==> 11



 ---- SNOWBALL STEMMING ----

['uniqu', 'concept', 'abstract', 'extract', 'use', 'meta', 'map', 'pairwis', 'cooccurr', 'determin', '.']

 TOTAL SNOWBALL STEM WORDS ==> 11



 ---- LEMMATIZATION ----

['Unique', 'concept', 'abstract', 'extracted', 'using', 'Meta', 'Map', 'pairwise', 'cooccurrence', 'determined', '.']

 TOTAL LEMMATIZE WORDS ==> 11

************************************************************************************************************************

334 --> Then the information is used to construct a network graph of  concept co-occurrence that is further analysed to identify content for the new conceptual  model. 


 ---- TOKENS ----

 ['Then', 'the', 'information', 'is', 'used', 'to', 'construct', 'a', 'network', 'graph', 'of', 'concept', 'co-occurrence', 'that', 'is', 'further', 'analysed', 'to', 'identify', 'content', 'for', 'the', 'new', 'conceptual', 'model', '.'] 

 TOTAL TOKENS ==> 26

 ---- POST ----

 [('Then', 'RB'), ('the', 'DT'), ('information', 'NN'), ('is', 'VBZ'), ('used', 'VBN'), ('to', 'TO'), ('construct', 'VB'), ('a', 'DT'), ('network', 'NN'), ('graph', 'NN'), ('of', 'IN'), ('concept', 'NN'), ('co-occurrence', 'NN'), ('that', 'WDT'), ('is', 'VBZ'), ('further', 'RB'), ('analysed', 'VBN'), ('to', 'TO'), ('identify', 'VB'), ('content', 'NN'), ('for', 'IN'), ('the', 'DT'), ('new', 'JJ'), ('conceptual', 'JJ'), ('model', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['information', 'used', 'construct', 'network', 'graph', 'concept', 'co-occurrence', 'analysed', 'identify', 'content', 'new', 'conceptual', 'model', '.']

 TOTAL FILTERED TOKENS ==>  14

 ---- POST FOR FILTERED TOKENS ----

 [('information', 'NN'), ('used', 'VBN'), ('construct', 'NN'), ('network', 'NN'), ('graph', 'NN'), ('concept', 'VBD'), ('co-occurrence', 'NN'), ('analysed', 'JJ'), ('identify', 'VB'), ('content', 'JJ'), ('new', 'JJ'), ('conceptual', 'JJ'), ('model', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['information used', 'used construct', 'construct network', 'network graph', 'graph concept', 'concept co-occurrence', 'co-occurrence analysed', 'analysed identify', 'identify content', 'content new', 'new conceptual', 'conceptual model', 'model .'] 

 TOTAL BIGRAMS --> 13 



 ---- TRI-GRAMS ---- 

 ['information used construct', 'used construct network', 'construct network graph', 'network graph concept', 'graph concept co-occurrence', 'concept co-occurrence analysed', 'co-occurrence analysed identify', 'analysed identify content', 'identify content new', 'content new conceptual', 'new conceptual model', 'conceptual model .'] 

 TOTAL TRIGRAMS --> 12 



 ---- NOUN PHRASES ---- 

 ['information', 'construct', 'network', 'graph', 'co-occurrence', 'content new conceptual model'] 

 TOTAL NOUN PHRASES --> 6 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['inform', 'use', 'construct', 'network', 'graph', 'concept', 'co-occurr', 'analys', 'identifi', 'content', 'new', 'conceptu', 'model', '.']

 TOTAL PORTER STEM WORDS ==> 14



 ---- SNOWBALL STEMMING ----

['inform', 'use', 'construct', 'network', 'graph', 'concept', 'co-occurr', 'analys', 'identifi', 'content', 'new', 'conceptu', 'model', '.']

 TOTAL SNOWBALL STEM WORDS ==> 14



 ---- LEMMATIZATION ----

['information', 'used', 'construct', 'network', 'graph', 'concept', 'co-occurrence', 'analysed', 'identify', 'content', 'new', 'conceptual', 'model', '.']

 TOTAL LEMMATIZE WORDS ==> 14

************************************************************************************************************************

335 --> 142 abstracts are analysed. 


 ---- TOKENS ----

 ['142', 'abstracts', 'are', 'analysed', '.'] 

 TOTAL TOKENS ==> 5

 ---- POST ----

 [('142', 'CD'), ('abstracts', 'NNS'), ('are', 'VBP'), ('analysed', 'VBN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['142', 'abstracts', 'analysed', '.']

 TOTAL FILTERED TOKENS ==>  4

 ---- POST FOR FILTERED TOKENS ----

 [('142', 'CD'), ('abstracts', 'NNS'), ('analysed', 'VBN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['142 abstracts', 'abstracts analysed', 'analysed .'] 

 TOTAL BIGRAMS --> 3 



 ---- TRI-GRAMS ---- 

 ['142 abstracts analysed', 'abstracts analysed .'] 

 TOTAL TRIGRAMS --> 2 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['142', 'abstract', 'analys', '.']

 TOTAL PORTER STEM WORDS ==> 4



 ---- SNOWBALL STEMMING ----

['142', 'abstract', 'analys', '.']

 TOTAL SNOWBALL STEM WORDS ==> 4



 ---- LEMMATIZATION ----

['142', 'abstract', 'analysed', '.']

 TOTAL LEMMATIZE WORDS ==> 4

************************************************************************************************************************

336 --> Medication adherence is the most studied drug therapy  problem and co-occurred with concepts related to patient-centred interventions targeting self- management. 


 ---- TOKENS ----

 ['Medication', 'adherence', 'is', 'the', 'most', 'studied', 'drug', 'therapy', 'problem', 'and', 'co-occurred', 'with', 'concepts', 'related', 'to', 'patient-centred', 'interventions', 'targeting', 'self-', 'management', '.'] 

 TOTAL TOKENS ==> 21

 ---- POST ----

 [('Medication', 'NN'), ('adherence', 'NN'), ('is', 'VBZ'), ('the', 'DT'), ('most', 'RBS'), ('studied', 'JJ'), ('drug', 'NN'), ('therapy', 'NN'), ('problem', 'NN'), ('and', 'CC'), ('co-occurred', 'JJ'), ('with', 'IN'), ('concepts', 'NNS'), ('related', 'VBN'), ('to', 'TO'), ('patient-centred', 'JJ'), ('interventions', 'NNS'), ('targeting', 'VBG'), ('self-', 'JJ'), ('management', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Medication', 'adherence', 'studied', 'drug', 'therapy', 'problem', 'co-occurred', 'concepts', 'related', 'patient-centred', 'interventions', 'targeting', 'self-', 'management', '.']

 TOTAL FILTERED TOKENS ==>  15

 ---- POST FOR FILTERED TOKENS ----

 [('Medication', 'NNP'), ('adherence', 'NN'), ('studied', 'VBD'), ('drug', 'NN'), ('therapy', 'NN'), ('problem', 'NN'), ('co-occurred', 'JJ'), ('concepts', 'NNS'), ('related', 'VBN'), ('patient-centred', 'JJ'), ('interventions', 'NNS'), ('targeting', 'VBG'), ('self-', 'JJ'), ('management', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Medication adherence', 'adherence studied', 'studied drug', 'drug therapy', 'therapy problem', 'problem co-occurred', 'co-occurred concepts', 'concepts related', 'related patient-centred', 'patient-centred interventions', 'interventions targeting', 'targeting self-', 'self- management', 'management .'] 

 TOTAL BIGRAMS --> 14 



 ---- TRI-GRAMS ---- 

 ['Medication adherence studied', 'adherence studied drug', 'studied drug therapy', 'drug therapy problem', 'therapy problem co-occurred', 'problem co-occurred concepts', 'co-occurred concepts related', 'concepts related patient-centred', 'related patient-centred interventions', 'patient-centred interventions targeting', 'interventions targeting self-', 'targeting self- management', 'self- management .'] 

 TOTAL TRIGRAMS --> 13 



 ---- NOUN PHRASES ---- 

 ['adherence', 'drug', 'therapy', 'problem', 'self- management'] 

 TOTAL NOUN PHRASES --> 5 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> ['Medication']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['medic', 'adher', 'studi', 'drug', 'therapi', 'problem', 'co-occur', 'concept', 'relat', 'patient-centr', 'intervent', 'target', 'self-', 'manag', '.']

 TOTAL PORTER STEM WORDS ==> 15



 ---- SNOWBALL STEMMING ----

['medic', 'adher', 'studi', 'drug', 'therapi', 'problem', 'co-occur', 'concept', 'relat', 'patient-centr', 'intervent', 'target', 'self-', 'manag', '.']

 TOTAL SNOWBALL STEM WORDS ==> 15



 ---- LEMMATIZATION ----

['Medication', 'adherence', 'studied', 'drug', 'therapy', 'problem', 'co-occurred', 'concept', 'related', 'patient-centred', 'intervention', 'targeting', 'self-', 'management', '.']

 TOTAL LEMMATIZE WORDS ==> 15

************************************************************************************************************************

337 --> The enhanced model consists of 65 concepts clustered into 14 constructs. 


 ---- TOKENS ----

 ['The', 'enhanced', 'model', 'consists', 'of', '65', 'concepts', 'clustered', 'into', '14', 'constructs', '.'] 

 TOTAL TOKENS ==> 12

 ---- POST ----

 [('The', 'DT'), ('enhanced', 'JJ'), ('model', 'NN'), ('consists', 'VBZ'), ('of', 'IN'), ('65', 'CD'), ('concepts', 'NNS'), ('clustered', 'VBN'), ('into', 'IN'), ('14', 'CD'), ('constructs', 'NNS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['enhanced', 'model', 'consists', '65', 'concepts', 'clustered', '14', 'constructs', '.']

 TOTAL FILTERED TOKENS ==>  9

 ---- POST FOR FILTERED TOKENS ----

 [('enhanced', 'VBN'), ('model', 'NN'), ('consists', 'VBZ'), ('65', 'CD'), ('concepts', 'NNS'), ('clustered', 'VBD'), ('14', 'CD'), ('constructs', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['enhanced model', 'model consists', 'consists 65', '65 concepts', 'concepts clustered', 'clustered 14', '14 constructs', 'constructs .'] 

 TOTAL BIGRAMS --> 8 



 ---- TRI-GRAMS ---- 

 ['enhanced model consists', 'model consists 65', 'consists 65 concepts', '65 concepts clustered', 'concepts clustered 14', 'clustered 14 constructs', '14 constructs .'] 

 TOTAL TRIGRAMS --> 7 



 ---- NOUN PHRASES ---- 

 ['model'] 

 TOTAL NOUN PHRASES --> 1 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['enhanc', 'model', 'consist', '65', 'concept', 'cluster', '14', 'construct', '.']

 TOTAL PORTER STEM WORDS ==> 9



 ---- SNOWBALL STEMMING ----

['enhanc', 'model', 'consist', '65', 'concept', 'cluster', '14', 'construct', '.']

 TOTAL SNOWBALL STEM WORDS ==> 9



 ---- LEMMATIZATION ----

['enhanced', 'model', 'consists', '65', 'concept', 'clustered', '14', 'construct', '.']

 TOTAL LEMMATIZE WORDS ==> 9

************************************************************************************************************************

338 --> The  framework requires additional refinement and evaluation to determine its relevance and  applicability across a broad audience including underserved settings. 


 ---- TOKENS ----

 ['The', 'framework', 'requires', 'additional', 'refinement', 'and', 'evaluation', 'to', 'determine', 'its', 'relevance', 'and', 'applicability', 'across', 'a', 'broad', 'audience', 'including', 'underserved', 'settings', '.'] 

 TOTAL TOKENS ==> 21

 ---- POST ----

 [('The', 'DT'), ('framework', 'NN'), ('requires', 'VBZ'), ('additional', 'JJ'), ('refinement', 'NN'), ('and', 'CC'), ('evaluation', 'NN'), ('to', 'TO'), ('determine', 'VB'), ('its', 'PRP$'), ('relevance', 'NN'), ('and', 'CC'), ('applicability', 'NN'), ('across', 'IN'), ('a', 'DT'), ('broad', 'JJ'), ('audience', 'NN'), ('including', 'VBG'), ('underserved', 'JJ'), ('settings', 'NNS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['framework', 'requires', 'additional', 'refinement', 'evaluation', 'determine', 'relevance', 'applicability', 'across', 'broad', 'audience', 'including', 'underserved', 'settings', '.']

 TOTAL FILTERED TOKENS ==>  15

 ---- POST FOR FILTERED TOKENS ----

 [('framework', 'NN'), ('requires', 'VBZ'), ('additional', 'JJ'), ('refinement', 'NN'), ('evaluation', 'NN'), ('determine', 'NN'), ('relevance', 'NN'), ('applicability', 'NN'), ('across', 'IN'), ('broad', 'JJ'), ('audience', 'NN'), ('including', 'VBG'), ('underserved', 'JJ'), ('settings', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['framework requires', 'requires additional', 'additional refinement', 'refinement evaluation', 'evaluation determine', 'determine relevance', 'relevance applicability', 'applicability across', 'across broad', 'broad audience', 'audience including', 'including underserved', 'underserved settings', 'settings .'] 

 TOTAL BIGRAMS --> 14 



 ---- TRI-GRAMS ---- 

 ['framework requires additional', 'requires additional refinement', 'additional refinement evaluation', 'refinement evaluation determine', 'evaluation determine relevance', 'determine relevance applicability', 'relevance applicability across', 'applicability across broad', 'across broad audience', 'broad audience including', 'audience including underserved', 'including underserved settings', 'underserved settings .'] 

 TOTAL TRIGRAMS --> 13 



 ---- NOUN PHRASES ---- 

 ['framework', 'additional refinement', 'evaluation', 'determine', 'relevance', 'applicability', 'broad audience'] 

 TOTAL NOUN PHRASES --> 7 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['framework', 'requir', 'addit', 'refin', 'evalu', 'determin', 'relev', 'applic', 'across', 'broad', 'audienc', 'includ', 'underserv', 'set', '.']

 TOTAL PORTER STEM WORDS ==> 15



 ---- SNOWBALL STEMMING ----

['framework', 'requir', 'addit', 'refin', 'evalu', 'determin', 'relev', 'applic', 'across', 'broad', 'audienc', 'includ', 'underserv', 'set', '.']

 TOTAL SNOWBALL STEM WORDS ==> 15



 ---- LEMMATIZATION ----

['framework', 'requires', 'additional', 'refinement', 'evaluation', 'determine', 'relevance', 'applicability', 'across', 'broad', 'audience', 'including', 'underserved', 'setting', '.']

 TOTAL LEMMATIZE WORDS ==> 15

************************************************************************************************************************

339 --> Link: https://www.ncbi.nlm.nih.gov/pubmed/28269895?dopt=Abstract  8.5 Meet the Pilot, world’s first language translating earbuds [108]  The world’s first smart earpiece Pilot will soon be transcribed over 15 languages. 


 ---- TOKENS ----

 ['Link', ':', 'https', ':', '//www.ncbi.nlm.nih.gov/pubmed/28269895', '?', 'dopt=Abstract', '8.5', 'Meet', 'the', 'Pilot', ',', 'world', '’', 's', 'first', 'language', 'translating', 'earbuds', '[', '108', ']', 'The', 'world', '’', 's', 'first', 'smart', 'earpiece', 'Pilot', 'will', 'soon', 'be', 'transcribed', 'over', '15', 'languages', '.'] 

 TOTAL TOKENS ==> 38

 ---- POST ----

 [('Link', 'NN'), (':', ':'), ('https', 'NN'), (':', ':'), ('//www.ncbi.nlm.nih.gov/pubmed/28269895', 'NN'), ('?', '.'), ('dopt=Abstract', 'JJ'), ('8.5', 'CD'), ('Meet', 'NNP'), ('the', 'DT'), ('Pilot', 'NNP'), (',', ','), ('world', 'NN'), ('’', 'NN'), ('s', 'VBP'), ('first', 'JJ'), ('language', 'NN'), ('translating', 'VBG'), ('earbuds', 'JJ'), ('[', '$'), ('108', 'CD'), (']', 'IN'), ('The', 'DT'), ('world', 'NN'), ('’', 'NNP'), ('s', 'NN'), ('first', 'RB'), ('smart', 'JJ'), ('earpiece', 'NN'), ('Pilot', 'NNP'), ('will', 'MD'), ('soon', 'RB'), ('be', 'VB'), ('transcribed', 'VBN'), ('over', 'IN'), ('15', 'CD'), ('languages', 'NNS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Link', ':', 'https', ':', '//www.ncbi.nlm.nih.gov/pubmed/28269895', '?', 'dopt=Abstract', '8.5', 'Meet', 'Pilot', ',', 'world', '’', 'first', 'language', 'translating', 'earbuds', '[', '108', ']', 'world', '’', 'first', 'smart', 'earpiece', 'Pilot', 'soon', 'transcribed', '15', 'languages', '.']

 TOTAL FILTERED TOKENS ==>  31

 ---- POST FOR FILTERED TOKENS ----

 [('Link', 'NN'), (':', ':'), ('https', 'NN'), (':', ':'), ('//www.ncbi.nlm.nih.gov/pubmed/28269895', 'NN'), ('?', '.'), ('dopt=Abstract', 'JJ'), ('8.5', 'CD'), ('Meet', 'NNP'), ('Pilot', 'NNP'), (',', ','), ('world', 'NN'), ('’', 'NN'), ('first', 'JJ'), ('language', 'NN'), ('translating', 'VBG'), ('earbuds', 'JJ'), ('[', '$'), ('108', 'CD'), (']', 'NNP'), ('world', 'NN'), ('’', 'NN'), ('first', 'RB'), ('smart', 'JJ'), ('earpiece', 'NN'), ('Pilot', 'NNP'), ('soon', 'RB'), ('transcribed', 'VBD'), ('15', 'CD'), ('languages', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Link :', ': https', 'https :', ': //www.ncbi.nlm.nih.gov/pubmed/28269895', '//www.ncbi.nlm.nih.gov/pubmed/28269895 ?', '? dopt=Abstract', 'dopt=Abstract 8.5', '8.5 Meet', 'Meet Pilot', 'Pilot ,', ', world', 'world ’', '’ first', 'first language', 'language translating', 'translating earbuds', 'earbuds [', '[ 108', '108 ]', '] world', 'world ’', '’ first', 'first smart', 'smart earpiece', 'earpiece Pilot', 'Pilot soon', 'soon transcribed', 'transcribed 15', '15 languages', 'languages .'] 

 TOTAL BIGRAMS --> 30 



 ---- TRI-GRAMS ---- 

 ['Link : https', ': https :', 'https : //www.ncbi.nlm.nih.gov/pubmed/28269895', ': //www.ncbi.nlm.nih.gov/pubmed/28269895 ?', '//www.ncbi.nlm.nih.gov/pubmed/28269895 ? dopt=Abstract', '? dopt=Abstract 8.5', 'dopt=Abstract 8.5 Meet', '8.5 Meet Pilot', 'Meet Pilot ,', 'Pilot , world', ', world ’', 'world ’ first', '’ first language', 'first language translating', 'language translating earbuds', 'translating earbuds [', 'earbuds [ 108', '[ 108 ]', '108 ] world', '] world ’', 'world ’ first', '’ first smart', 'first smart earpiece', 'smart earpiece Pilot', 'earpiece Pilot soon', 'Pilot soon transcribed', 'soon transcribed 15', 'transcribed 15 languages', '15 languages .'] 

 TOTAL TRIGRAMS --> 29 



 ---- NOUN PHRASES ---- 

 ['Link', 'https', '', 'world', '’', 'first language', 'world', '’', 'smart earpiece'] 

 TOTAL NOUN PHRASES --> 9 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> ['Pilot']
 TOTAL PERSON ENTITY --> 1 


 GPE ---> ['Link']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['link', ':', 'http', ':', '//www.ncbi.nlm.nih.gov/pubmed/28269895', '?', 'dopt=abstract', '8.5', 'meet', 'pilot', ',', 'world', '’', 'first', 'languag', 'translat', 'earbud', '[', '108', ']', 'world', '’', 'first', 'smart', 'earpiec', 'pilot', 'soon', 'transcrib', '15', 'languag', '.']

 TOTAL PORTER STEM WORDS ==> 31



 ---- SNOWBALL STEMMING ----

['link', ':', 'https', ':', '//www.ncbi.nlm.nih.gov/pubmed/28269895', '?', 'dopt=abstract', '8.5', 'meet', 'pilot', ',', 'world', '’', 'first', 'languag', 'translat', 'earbud', '[', '108', ']', 'world', '’', 'first', 'smart', 'earpiec', 'pilot', 'soon', 'transcrib', '15', 'languag', '.']

 TOTAL SNOWBALL STEM WORDS ==> 31



 ---- LEMMATIZATION ----

['Link', ':', 'http', ':', '//www.ncbi.nlm.nih.gov/pubmed/28269895', '?', 'dopt=Abstract', '8.5', 'Meet', 'Pilot', ',', 'world', '’', 'first', 'language', 'translating', 'earbuds', '[', '108', ']', 'world', '’', 'first', 'smart', 'earpiece', 'Pilot', 'soon', 'transcribed', '15', 'language', '.']

 TOTAL LEMMATIZE WORDS ==> 31

************************************************************************************************************************

340 --> According  to Spring wise, Waverly Labs’ Pilot can already transliterate five spoken languages, English,  French, Italian, Portuguese and Spanish, and seven written affixed languages, German, Hindi,  Russian, Japanese, Arabic, Korean and Mandarin Chinese. 


 ---- TOKENS ----

 ['According', 'to', 'Spring', 'wise', ',', 'Waverly', 'Labs', '’', 'Pilot', 'can', 'already', 'transliterate', 'five', 'spoken', 'languages', ',', 'English', ',', 'French', ',', 'Italian', ',', 'Portuguese', 'and', 'Spanish', ',', 'and', 'seven', 'written', 'affixed', 'languages', ',', 'German', ',', 'Hindi', ',', 'Russian', ',', 'Japanese', ',', 'Arabic', ',', 'Korean', 'and', 'Mandarin', 'Chinese', '.'] 

 TOTAL TOKENS ==> 47

 ---- POST ----

 [('According', 'VBG'), ('to', 'TO'), ('Spring', 'NNP'), ('wise', 'NN'), (',', ','), ('Waverly', 'NNP'), ('Labs', 'NNP'), ('’', 'NNP'), ('Pilot', 'NNP'), ('can', 'MD'), ('already', 'RB'), ('transliterate', 'VB'), ('five', 'CD'), ('spoken', 'JJ'), ('languages', 'NNS'), (',', ','), ('English', 'NNP'), (',', ','), ('French', 'JJ'), (',', ','), ('Italian', 'JJ'), (',', ','), ('Portuguese', 'JJ'), ('and', 'CC'), ('Spanish', 'JJ'), (',', ','), ('and', 'CC'), ('seven', 'CD'), ('written', 'VBN'), ('affixed', 'JJ'), ('languages', 'NNS'), (',', ','), ('German', 'JJ'), (',', ','), ('Hindi', 'NNP'), (',', ','), ('Russian', 'NNP'), (',', ','), ('Japanese', 'NNP'), (',', ','), ('Arabic', 'NNP'), (',', ','), ('Korean', 'NNP'), ('and', 'CC'), ('Mandarin', 'NNP'), ('Chinese', 'NNP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['According', 'Spring', 'wise', ',', 'Waverly', 'Labs', '’', 'Pilot', 'already', 'transliterate', 'five', 'spoken', 'languages', ',', 'English', ',', 'French', ',', 'Italian', ',', 'Portuguese', 'Spanish', ',', 'seven', 'written', 'affixed', 'languages', ',', 'German', ',', 'Hindi', ',', 'Russian', ',', 'Japanese', ',', 'Arabic', ',', 'Korean', 'Mandarin', 'Chinese', '.']

 TOTAL FILTERED TOKENS ==>  42

 ---- POST FOR FILTERED TOKENS ----

 [('According', 'VBG'), ('Spring', 'NN'), ('wise', 'NN'), (',', ','), ('Waverly', 'NNP'), ('Labs', 'NNP'), ('’', 'NNP'), ('Pilot', 'NNP'), ('already', 'RB'), ('transliterate', 'VBP'), ('five', 'CD'), ('spoken', 'JJ'), ('languages', 'NNS'), (',', ','), ('English', 'NNP'), (',', ','), ('French', 'JJ'), (',', ','), ('Italian', 'JJ'), (',', ','), ('Portuguese', 'JJ'), ('Spanish', 'JJ'), (',', ','), ('seven', 'CD'), ('written', 'VBN'), ('affixed', 'JJ'), ('languages', 'NNS'), (',', ','), ('German', 'JJ'), (',', ','), ('Hindi', 'NNP'), (',', ','), ('Russian', 'NNP'), (',', ','), ('Japanese', 'NNP'), (',', ','), ('Arabic', 'NNP'), (',', ','), ('Korean', 'NNP'), ('Mandarin', 'NNP'), ('Chinese', 'NNP'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['According Spring', 'Spring wise', 'wise ,', ', Waverly', 'Waverly Labs', 'Labs ’', '’ Pilot', 'Pilot already', 'already transliterate', 'transliterate five', 'five spoken', 'spoken languages', 'languages ,', ', English', 'English ,', ', French', 'French ,', ', Italian', 'Italian ,', ', Portuguese', 'Portuguese Spanish', 'Spanish ,', ', seven', 'seven written', 'written affixed', 'affixed languages', 'languages ,', ', German', 'German ,', ', Hindi', 'Hindi ,', ', Russian', 'Russian ,', ', Japanese', 'Japanese ,', ', Arabic', 'Arabic ,', ', Korean', 'Korean Mandarin', 'Mandarin Chinese', 'Chinese .'] 

 TOTAL BIGRAMS --> 41 



 ---- TRI-GRAMS ---- 

 ['According Spring wise', 'Spring wise ,', 'wise , Waverly', ', Waverly Labs', 'Waverly Labs ’', 'Labs ’ Pilot', '’ Pilot already', 'Pilot already transliterate', 'already transliterate five', 'transliterate five spoken', 'five spoken languages', 'spoken languages ,', 'languages , English', ', English ,', 'English , French', ', French ,', 'French , Italian', ', Italian ,', 'Italian , Portuguese', ', Portuguese Spanish', 'Portuguese Spanish ,', 'Spanish , seven', ', seven written', 'seven written affixed', 'written affixed languages', 'affixed languages ,', 'languages , German', ', German ,', 'German , Hindi', ', Hindi ,', 'Hindi , Russian', ', Russian ,', 'Russian , Japanese', ', Japanese ,', 'Japanese , Arabic', ', Arabic ,', 'Arabic , Korean', ', Korean Mandarin', 'Korean Mandarin Chinese', 'Mandarin Chinese .'] 

 TOTAL TRIGRAMS --> 40 



 ---- NOUN PHRASES ---- 

 ['Spring', 'wise'] 

 TOTAL NOUN PHRASES --> 2 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> ['Waverly Labs', 'Hindi', 'Arabic', 'Korean Mandarin Chinese']
 TOTAL PERSON ENTITY --> 4 


 GPE ---> ['English', 'French', 'Italian', 'Portuguese', 'Spanish', 'German', 'Russian', 'Japanese']
 TOTAL GPE ENTITY --> 8 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['accord', 'spring', 'wise', ',', 'waverli', 'lab', '’', 'pilot', 'alreadi', 'transliter', 'five', 'spoken', 'languag', ',', 'english', ',', 'french', ',', 'italian', ',', 'portugues', 'spanish', ',', 'seven', 'written', 'affix', 'languag', ',', 'german', ',', 'hindi', ',', 'russian', ',', 'japanes', ',', 'arab', ',', 'korean', 'mandarin', 'chines', '.']

 TOTAL PORTER STEM WORDS ==> 42



 ---- SNOWBALL STEMMING ----

['accord', 'spring', 'wise', ',', 'waver', 'lab', '’', 'pilot', 'alreadi', 'transliter', 'five', 'spoken', 'languag', ',', 'english', ',', 'french', ',', 'italian', ',', 'portugues', 'spanish', ',', 'seven', 'written', 'affix', 'languag', ',', 'german', ',', 'hindi', ',', 'russian', ',', 'japanes', ',', 'arab', ',', 'korean', 'mandarin', 'chines', '.']

 TOTAL SNOWBALL STEM WORDS ==> 42



 ---- LEMMATIZATION ----

['According', 'Spring', 'wise', ',', 'Waverly', 'Labs', '’', 'Pilot', 'already', 'transliterate', 'five', 'spoken', 'language', ',', 'English', ',', 'French', ',', 'Italian', ',', 'Portuguese', 'Spanish', ',', 'seven', 'written', 'affixed', 'language', ',', 'German', ',', 'Hindi', ',', 'Russian', ',', 'Japanese', ',', 'Arabic', ',', 'Korean', 'Mandarin', 'Chinese', '.']

 TOTAL LEMMATIZE WORDS ==> 42

************************************************************************************************************************

341 --> The Pilot earpiece is connected  via Bluetooth to the Pilot speech translation app, which uses speech recognition, machine  translation and machine learning and speech synthesis technology. 


 ---- TOKENS ----

 ['The', 'Pilot', 'earpiece', 'is', 'connected', 'via', 'Bluetooth', 'to', 'the', 'Pilot', 'speech', 'translation', 'app', ',', 'which', 'uses', 'speech', 'recognition', ',', 'machine', 'translation', 'and', 'machine', 'learning', 'and', 'speech', 'synthesis', 'technology', '.'] 

 TOTAL TOKENS ==> 29

 ---- POST ----

 [('The', 'DT'), ('Pilot', 'NNP'), ('earpiece', 'NN'), ('is', 'VBZ'), ('connected', 'VBN'), ('via', 'IN'), ('Bluetooth', 'NNP'), ('to', 'TO'), ('the', 'DT'), ('Pilot', 'NNP'), ('speech', 'NN'), ('translation', 'NN'), ('app', 'NN'), (',', ','), ('which', 'WDT'), ('uses', 'VBZ'), ('speech', 'NN'), ('recognition', 'NN'), (',', ','), ('machine', 'NN'), ('translation', 'NN'), ('and', 'CC'), ('machine', 'NN'), ('learning', 'NN'), ('and', 'CC'), ('speech', 'JJ'), ('synthesis', 'NN'), ('technology', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Pilot', 'earpiece', 'connected', 'via', 'Bluetooth', 'Pilot', 'speech', 'translation', 'app', ',', 'uses', 'speech', 'recognition', ',', 'machine', 'translation', 'machine', 'learning', 'speech', 'synthesis', 'technology', '.']

 TOTAL FILTERED TOKENS ==>  22

 ---- POST FOR FILTERED TOKENS ----

 [('Pilot', 'NNP'), ('earpiece', 'CC'), ('connected', 'VBN'), ('via', 'IN'), ('Bluetooth', 'NNP'), ('Pilot', 'NNP'), ('speech', 'NN'), ('translation', 'NN'), ('app', 'NN'), (',', ','), ('uses', 'VBZ'), ('speech', 'JJ'), ('recognition', 'NN'), (',', ','), ('machine', 'NN'), ('translation', 'NN'), ('machine', 'NN'), ('learning', 'VBG'), ('speech', 'JJ'), ('synthesis', 'NN'), ('technology', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Pilot earpiece', 'earpiece connected', 'connected via', 'via Bluetooth', 'Bluetooth Pilot', 'Pilot speech', 'speech translation', 'translation app', 'app ,', ', uses', 'uses speech', 'speech recognition', 'recognition ,', ', machine', 'machine translation', 'translation machine', 'machine learning', 'learning speech', 'speech synthesis', 'synthesis technology', 'technology .'] 

 TOTAL BIGRAMS --> 21 



 ---- TRI-GRAMS ---- 

 ['Pilot earpiece connected', 'earpiece connected via', 'connected via Bluetooth', 'via Bluetooth Pilot', 'Bluetooth Pilot speech', 'Pilot speech translation', 'speech translation app', 'translation app ,', 'app , uses', ', uses speech', 'uses speech recognition', 'speech recognition ,', 'recognition , machine', ', machine translation', 'machine translation machine', 'translation machine learning', 'machine learning speech', 'learning speech synthesis', 'speech synthesis technology', 'synthesis technology .'] 

 TOTAL TRIGRAMS --> 20 



 ---- NOUN PHRASES ---- 

 ['speech', 'translation', 'app', 'speech recognition', 'machine', 'translation', 'machine', 'speech synthesis', 'technology'] 

 TOTAL NOUN PHRASES --> 9 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> ['Bluetooth Pilot']
 TOTAL PERSON ENTITY --> 1 


 GPE ---> ['Pilot']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['pilot', 'earpiec', 'connect', 'via', 'bluetooth', 'pilot', 'speech', 'translat', 'app', ',', 'use', 'speech', 'recognit', ',', 'machin', 'translat', 'machin', 'learn', 'speech', 'synthesi', 'technolog', '.']

 TOTAL PORTER STEM WORDS ==> 22



 ---- SNOWBALL STEMMING ----

['pilot', 'earpiec', 'connect', 'via', 'bluetooth', 'pilot', 'speech', 'translat', 'app', ',', 'use', 'speech', 'recognit', ',', 'machin', 'translat', 'machin', 'learn', 'speech', 'synthesi', 'technolog', '.']

 TOTAL SNOWBALL STEM WORDS ==> 22



 ---- LEMMATIZATION ----

['Pilot', 'earpiece', 'connected', 'via', 'Bluetooth', 'Pilot', 'speech', 'translation', 'app', ',', 'us', 'speech', 'recognition', ',', 'machine', 'translation', 'machine', 'learning', 'speech', 'synthesis', 'technology', '.']

 TOTAL LEMMATIZE WORDS ==> 22

************************************************************************************************************************

342 --> Simultaneously, the user will hear the translated version of the speech on the second earpiece. 


 ---- TOKENS ----

 ['Simultaneously', ',', 'the', 'user', 'will', 'hear', 'the', 'translated', 'version', 'of', 'the', 'speech', 'on', 'the', 'second', 'earpiece', '.'] 

 TOTAL TOKENS ==> 17

 ---- POST ----

 [('Simultaneously', 'RB'), (',', ','), ('the', 'DT'), ('user', 'NN'), ('will', 'MD'), ('hear', 'VB'), ('the', 'DT'), ('translated', 'JJ'), ('version', 'NN'), ('of', 'IN'), ('the', 'DT'), ('speech', 'NN'), ('on', 'IN'), ('the', 'DT'), ('second', 'JJ'), ('earpiece', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Simultaneously', ',', 'user', 'hear', 'translated', 'version', 'speech', 'second', 'earpiece', '.']

 TOTAL FILTERED TOKENS ==>  10

 ---- POST FOR FILTERED TOKENS ----

 [('Simultaneously', 'RB'), (',', ','), ('user', 'JJ'), ('hear', 'NN'), ('translated', 'VBN'), ('version', 'NN'), ('speech', 'NN'), ('second', 'JJ'), ('earpiece', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Simultaneously ,', ', user', 'user hear', 'hear translated', 'translated version', 'version speech', 'speech second', 'second earpiece', 'earpiece .'] 

 TOTAL BIGRAMS --> 9 



 ---- TRI-GRAMS ---- 

 ['Simultaneously , user', ', user hear', 'user hear translated', 'hear translated version', 'translated version speech', 'version speech second', 'speech second earpiece', 'second earpiece .'] 

 TOTAL TRIGRAMS --> 8 



 ---- NOUN PHRASES ---- 

 ['user hear', 'version', 'speech', 'second earpiece'] 

 TOTAL NOUN PHRASES --> 4 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['simultan', ',', 'user', 'hear', 'translat', 'version', 'speech', 'second', 'earpiec', '.']

 TOTAL PORTER STEM WORDS ==> 10



 ---- SNOWBALL STEMMING ----

['simultan', ',', 'user', 'hear', 'translat', 'version', 'speech', 'second', 'earpiec', '.']

 TOTAL SNOWBALL STEM WORDS ==> 10



 ---- LEMMATIZATION ----

['Simultaneously', ',', 'user', 'hear', 'translated', 'version', 'speech', 'second', 'earpiece', '.']

 TOTAL LEMMATIZE WORDS ==> 10

************************************************************************************************************************

343 --> Moreover, it is not necessary that conversation would be taking place between two people  only the users can join in and discuss as a group. 


 ---- TOKENS ----

 ['Moreover', ',', 'it', 'is', 'not', 'necessary', 'that', 'conversation', 'would', 'be', 'taking', 'place', 'between', 'two', 'people', 'only', 'the', 'users', 'can', 'join', 'in', 'and', 'discuss', 'as', 'a', 'group', '.'] 

 TOTAL TOKENS ==> 27

 ---- POST ----

 [('Moreover', 'RB'), (',', ','), ('it', 'PRP'), ('is', 'VBZ'), ('not', 'RB'), ('necessary', 'JJ'), ('that', 'IN'), ('conversation', 'NN'), ('would', 'MD'), ('be', 'VB'), ('taking', 'VBG'), ('place', 'NN'), ('between', 'IN'), ('two', 'CD'), ('people', 'NNS'), ('only', 'RB'), ('the', 'DT'), ('users', 'NNS'), ('can', 'MD'), ('join', 'VB'), ('in', 'IN'), ('and', 'CC'), ('discuss', 'VB'), ('as', 'IN'), ('a', 'DT'), ('group', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Moreover', ',', 'necessary', 'conversation', 'would', 'taking', 'place', 'two', 'people', 'users', 'join', 'discuss', 'group', '.']

 TOTAL FILTERED TOKENS ==>  14

 ---- POST FOR FILTERED TOKENS ----

 [('Moreover', 'RB'), (',', ','), ('necessary', 'JJ'), ('conversation', 'NN'), ('would', 'MD'), ('taking', 'VBG'), ('place', 'NN'), ('two', 'CD'), ('people', 'NNS'), ('users', 'NNS'), ('join', 'VBP'), ('discuss', 'JJ'), ('group', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Moreover ,', ', necessary', 'necessary conversation', 'conversation would', 'would taking', 'taking place', 'place two', 'two people', 'people users', 'users join', 'join discuss', 'discuss group', 'group .'] 

 TOTAL BIGRAMS --> 13 



 ---- TRI-GRAMS ---- 

 ['Moreover , necessary', ', necessary conversation', 'necessary conversation would', 'conversation would taking', 'would taking place', 'taking place two', 'place two people', 'two people users', 'people users join', 'users join discuss', 'join discuss group', 'discuss group .'] 

 TOTAL TRIGRAMS --> 12 



 ---- NOUN PHRASES ---- 

 ['necessary conversation', 'place', 'discuss group'] 

 TOTAL NOUN PHRASES --> 3 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['moreov', ',', 'necessari', 'convers', 'would', 'take', 'place', 'two', 'peopl', 'user', 'join', 'discuss', 'group', '.']

 TOTAL PORTER STEM WORDS ==> 14



 ---- SNOWBALL STEMMING ----

['moreov', ',', 'necessari', 'convers', 'would', 'take', 'place', 'two', 'peopl', 'user', 'join', 'discuss', 'group', '.']

 TOTAL SNOWBALL STEM WORDS ==> 14



 ---- LEMMATIZATION ----

['Moreover', ',', 'necessary', 'conversation', 'would', 'taking', 'place', 'two', 'people', 'user', 'join', 'discus', 'group', '.']

 TOTAL LEMMATIZE WORDS ==> 14

************************************************************************************************************************

344 --> As if now the user may experience a few  second lag interpolated the speech and translation, which Waverly Labs pursue to reduce. 


 ---- TOKENS ----

 ['As', 'if', 'now', 'the', 'user', 'may', 'experience', 'a', 'few', 'second', 'lag', 'interpolated', 'the', 'speech', 'and', 'translation', ',', 'which', 'Waverly', 'Labs', 'pursue', 'to', 'reduce', '.'] 

 TOTAL TOKENS ==> 24

 ---- POST ----

 [('As', 'IN'), ('if', 'IN'), ('now', 'RB'), ('the', 'DT'), ('user', 'NN'), ('may', 'MD'), ('experience', 'VB'), ('a', 'DT'), ('few', 'JJ'), ('second', 'JJ'), ('lag', 'NN'), ('interpolated', 'VBD'), ('the', 'DT'), ('speech', 'NN'), ('and', 'CC'), ('translation', 'NN'), (',', ','), ('which', 'WDT'), ('Waverly', 'NNP'), ('Labs', 'NNP'), ('pursue', 'NN'), ('to', 'TO'), ('reduce', 'VB'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['user', 'may', 'experience', 'second', 'lag', 'interpolated', 'speech', 'translation', ',', 'Waverly', 'Labs', 'pursue', 'reduce', '.']

 TOTAL FILTERED TOKENS ==>  14

 ---- POST FOR FILTERED TOKENS ----

 [('user', 'NN'), ('may', 'MD'), ('experience', 'VB'), ('second', 'JJ'), ('lag', 'NN'), ('interpolated', 'VBD'), ('speech', 'JJ'), ('translation', 'NN'), (',', ','), ('Waverly', 'NNP'), ('Labs', 'NNP'), ('pursue', 'NN'), ('reduce', 'VB'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['user may', 'may experience', 'experience second', 'second lag', 'lag interpolated', 'interpolated speech', 'speech translation', 'translation ,', ', Waverly', 'Waverly Labs', 'Labs pursue', 'pursue reduce', 'reduce .'] 

 TOTAL BIGRAMS --> 13 



 ---- TRI-GRAMS ---- 

 ['user may experience', 'may experience second', 'experience second lag', 'second lag interpolated', 'lag interpolated speech', 'interpolated speech translation', 'speech translation ,', 'translation , Waverly', ', Waverly Labs', 'Waverly Labs pursue', 'Labs pursue reduce', 'pursue reduce .'] 

 TOTAL TRIGRAMS --> 12 



 ---- NOUN PHRASES ---- 

 ['user', 'second lag', 'speech translation', 'pursue'] 

 TOTAL NOUN PHRASES --> 4 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> ['Waverly Labs']
 TOTAL PERSON ENTITY --> 1 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['user', 'may', 'experi', 'second', 'lag', 'interpol', 'speech', 'translat', ',', 'waverli', 'lab', 'pursu', 'reduc', '.']

 TOTAL PORTER STEM WORDS ==> 14



 ---- SNOWBALL STEMMING ----

['user', 'may', 'experi', 'second', 'lag', 'interpol', 'speech', 'translat', ',', 'waver', 'lab', 'pursu', 'reduc', '.']

 TOTAL SNOWBALL STEM WORDS ==> 14



 ---- LEMMATIZATION ----

['user', 'may', 'experience', 'second', 'lag', 'interpolated', 'speech', 'translation', ',', 'Waverly', 'Labs', 'pursue', 'reduce', '.']

 TOTAL LEMMATIZE WORDS ==> 14

************************************************************************************************************************

345 --> The Pilot earpiece will be available from September, but can be pre-ordered now for $249. 


 ---- TOKENS ----

 ['The', 'Pilot', 'earpiece', 'will', 'be', 'available', 'from', 'September', ',', 'but', 'can', 'be', 'pre-ordered', 'now', 'for', '$', '249', '.'] 

 TOTAL TOKENS ==> 18

 ---- POST ----

 [('The', 'DT'), ('Pilot', 'NNP'), ('earpiece', 'NN'), ('will', 'MD'), ('be', 'VB'), ('available', 'JJ'), ('from', 'IN'), ('September', 'NNP'), (',', ','), ('but', 'CC'), ('can', 'MD'), ('be', 'VB'), ('pre-ordered', 'JJ'), ('now', 'RB'), ('for', 'IN'), ('$', '$'), ('249', 'CD'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Pilot', 'earpiece', 'available', 'September', ',', 'pre-ordered', '$', '249', '.']

 TOTAL FILTERED TOKENS ==>  9

 ---- POST FOR FILTERED TOKENS ----

 [('Pilot', 'NNP'), ('earpiece', 'CC'), ('available', 'JJ'), ('September', 'NNP'), (',', ','), ('pre-ordered', 'JJ'), ('$', '$'), ('249', 'CD'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Pilot earpiece', 'earpiece available', 'available September', 'September ,', ', pre-ordered', 'pre-ordered $', '$ 249', '249 .'] 

 TOTAL BIGRAMS --> 8 



 ---- TRI-GRAMS ---- 

 ['Pilot earpiece available', 'earpiece available September', 'available September ,', 'September , pre-ordered', ', pre-ordered $', 'pre-ordered $ 249', '$ 249 .'] 

 TOTAL TRIGRAMS --> 7 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> ['Pilot']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['pilot', 'earpiec', 'avail', 'septemb', ',', 'pre-ord', '$', '249', '.']

 TOTAL PORTER STEM WORDS ==> 9



 ---- SNOWBALL STEMMING ----

['pilot', 'earpiec', 'avail', 'septemb', ',', 'pre-ord', '$', '249', '.']

 TOTAL SNOWBALL STEM WORDS ==> 9



 ---- LEMMATIZATION ----

['Pilot', 'earpiece', 'available', 'September', ',', 'pre-ordered', '$', '249', '.']

 TOTAL LEMMATIZE WORDS ==> 9

************************************************************************************************************************

346 --> The earpieces can also be used for streaming music, answering voice calls and getting audio  notifications. 


 ---- TOKENS ----

 ['The', 'earpieces', 'can', 'also', 'be', 'used', 'for', 'streaming', 'music', ',', 'answering', 'voice', 'calls', 'and', 'getting', 'audio', 'notifications', '.'] 

 TOTAL TOKENS ==> 18

 ---- POST ----

 [('The', 'DT'), ('earpieces', 'NNS'), ('can', 'MD'), ('also', 'RB'), ('be', 'VB'), ('used', 'VBN'), ('for', 'IN'), ('streaming', 'VBG'), ('music', 'NN'), (',', ','), ('answering', 'VBG'), ('voice', 'NN'), ('calls', 'NNS'), ('and', 'CC'), ('getting', 'VBG'), ('audio', 'JJ'), ('notifications', 'NNS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['earpieces', 'also', 'used', 'streaming', 'music', ',', 'answering', 'voice', 'calls', 'getting', 'audio', 'notifications', '.']

 TOTAL FILTERED TOKENS ==>  13

 ---- POST FOR FILTERED TOKENS ----

 [('earpieces', 'NNS'), ('also', 'RB'), ('used', 'VBD'), ('streaming', 'VBG'), ('music', 'NN'), (',', ','), ('answering', 'VBG'), ('voice', 'NN'), ('calls', 'VBZ'), ('getting', 'VBG'), ('audio', 'JJ'), ('notifications', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['earpieces also', 'also used', 'used streaming', 'streaming music', 'music ,', ', answering', 'answering voice', 'voice calls', 'calls getting', 'getting audio', 'audio notifications', 'notifications .'] 

 TOTAL BIGRAMS --> 12 



 ---- TRI-GRAMS ---- 

 ['earpieces also used', 'also used streaming', 'used streaming music', 'streaming music ,', 'music , answering', ', answering voice', 'answering voice calls', 'voice calls getting', 'calls getting audio', 'getting audio notifications', 'audio notifications .'] 

 TOTAL TRIGRAMS --> 11 



 ---- NOUN PHRASES ---- 

 ['music', 'voice'] 

 TOTAL NOUN PHRASES --> 2 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['earpiec', 'also', 'use', 'stream', 'music', ',', 'answer', 'voic', 'call', 'get', 'audio', 'notif', '.']

 TOTAL PORTER STEM WORDS ==> 13



 ---- SNOWBALL STEMMING ----

['earpiec', 'also', 'use', 'stream', 'music', ',', 'answer', 'voic', 'call', 'get', 'audio', 'notif', '.']

 TOTAL SNOWBALL STEM WORDS ==> 13



 ---- LEMMATIZATION ----

['earpiece', 'also', 'used', 'streaming', 'music', ',', 'answering', 'voice', 'call', 'getting', 'audio', 'notification', '.']

 TOTAL LEMMATIZE WORDS ==> 13

************************************************************************************************************************

347 --> Link:https://www.indiegogo.com/projects/meet-the-pilot-smart-earpiece-language-translator- headphones-travel#/    REFRENCES  [1] Chomsky, Noam, 1965, Aspects of the Theory of Syntax, Cambridge, Massachusetts:  MIT Press. 


 ---- TOKENS ----

 ['Link', ':', 'https', ':', '//www.indiegogo.com/projects/meet-the-pilot-smart-earpiece-language-translator-', 'headphones-travel', '#', '/', 'REFRENCES', '[', '1', ']', 'Chomsky', ',', 'Noam', ',', '1965', ',', 'Aspects', 'of', 'the', 'Theory', 'of', 'Syntax', ',', 'Cambridge', ',', 'Massachusetts', ':', 'MIT', 'Press', '.'] 

 TOTAL TOKENS ==> 32

 ---- POST ----

 [('Link', 'NN'), (':', ':'), ('https', 'NN'), (':', ':'), ('//www.indiegogo.com/projects/meet-the-pilot-smart-earpiece-language-translator-', 'JJ'), ('headphones-travel', 'JJ'), ('#', '#'), ('/', 'JJ'), ('REFRENCES', 'NNP'), ('[', 'NNP'), ('1', 'CD'), (']', 'NNP'), ('Chomsky', 'NNP'), (',', ','), ('Noam', 'NNP'), (',', ','), ('1965', 'CD'), (',', ','), ('Aspects', 'NNS'), ('of', 'IN'), ('the', 'DT'), ('Theory', 'NNP'), ('of', 'IN'), ('Syntax', 'NNP'), (',', ','), ('Cambridge', 'NNP'), (',', ','), ('Massachusetts', 'NNP'), (':', ':'), ('MIT', 'NNP'), ('Press', 'NNP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Link', ':', 'https', ':', '//www.indiegogo.com/projects/meet-the-pilot-smart-earpiece-language-translator-', 'headphones-travel', '#', '/', 'REFRENCES', '[', '1', ']', 'Chomsky', ',', 'Noam', ',', '1965', ',', 'Aspects', 'Theory', 'Syntax', ',', 'Cambridge', ',', 'Massachusetts', ':', 'MIT', 'Press', '.']

 TOTAL FILTERED TOKENS ==>  29

 ---- POST FOR FILTERED TOKENS ----

 [('Link', 'NN'), (':', ':'), ('https', 'NN'), (':', ':'), ('//www.indiegogo.com/projects/meet-the-pilot-smart-earpiece-language-translator-', 'JJ'), ('headphones-travel', 'JJ'), ('#', '#'), ('/', 'JJ'), ('REFRENCES', 'NNP'), ('[', 'NNP'), ('1', 'CD'), (']', 'NNP'), ('Chomsky', 'NNP'), (',', ','), ('Noam', 'NNP'), (',', ','), ('1965', 'CD'), (',', ','), ('Aspects', 'NNP'), ('Theory', 'NNP'), ('Syntax', 'NNP'), (',', ','), ('Cambridge', 'NNP'), (',', ','), ('Massachusetts', 'NNP'), (':', ':'), ('MIT', 'NNP'), ('Press', 'NNP'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Link :', ': https', 'https :', ': //www.indiegogo.com/projects/meet-the-pilot-smart-earpiece-language-translator-', '//www.indiegogo.com/projects/meet-the-pilot-smart-earpiece-language-translator- headphones-travel', 'headphones-travel #', '# /', '/ REFRENCES', 'REFRENCES [', '[ 1', '1 ]', '] Chomsky', 'Chomsky ,', ', Noam', 'Noam ,', ', 1965', '1965 ,', ', Aspects', 'Aspects Theory', 'Theory Syntax', 'Syntax ,', ', Cambridge', 'Cambridge ,', ', Massachusetts', 'Massachusetts :', ': MIT', 'MIT Press', 'Press .'] 

 TOTAL BIGRAMS --> 28 



 ---- TRI-GRAMS ---- 

 ['Link : https', ': https :', 'https : //www.indiegogo.com/projects/meet-the-pilot-smart-earpiece-language-translator-', ': //www.indiegogo.com/projects/meet-the-pilot-smart-earpiece-language-translator- headphones-travel', '//www.indiegogo.com/projects/meet-the-pilot-smart-earpiece-language-translator- headphones-travel #', 'headphones-travel # /', '# / REFRENCES', '/ REFRENCES [', 'REFRENCES [ 1', '[ 1 ]', '1 ] Chomsky', '] Chomsky ,', 'Chomsky , Noam', ', Noam ,', 'Noam , 1965', ', 1965 ,', '1965 , Aspects', ', Aspects Theory', 'Aspects Theory Syntax', 'Theory Syntax ,', 'Syntax , Cambridge', ', Cambridge ,', 'Cambridge , Massachusetts', ', Massachusetts :', 'Massachusetts : MIT', ': MIT Press', 'MIT Press .'] 

 TOTAL TRIGRAMS --> 27 



 ---- NOUN PHRASES ---- 

 ['Link', 'https'] 

 TOTAL NOUN PHRASES --> 2 



 ---- NER ----

 
 ORGANIZATION ---> ['REFRENCES', 'MIT']
 TOTAL ORGANIZATION ENTITY --> 2 


 PERSON ---> ['Noam', 'Aspects Theory Syntax']
 TOTAL PERSON ENTITY --> 2 


 GPE ---> ['Link', 'Cambridge', 'Massachusetts']
 TOTAL GPE ENTITY --> 3 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['link', ':', 'http', ':', '//www.indiegogo.com/projects/meet-the-pilot-smart-earpiece-language-translator-', 'headphones-travel', '#', '/', 'refrenc', '[', '1', ']', 'chomski', ',', 'noam', ',', '1965', ',', 'aspect', 'theori', 'syntax', ',', 'cambridg', ',', 'massachusett', ':', 'mit', 'press', '.']

 TOTAL PORTER STEM WORDS ==> 29



 ---- SNOWBALL STEMMING ----

['link', ':', 'https', ':', '//www.indiegogo.com/projects/meet-the-pilot-smart-earpiece-language-translator-', 'headphones-travel', '#', '/', 'refrenc', '[', '1', ']', 'chomski', ',', 'noam', ',', '1965', ',', 'aspect', 'theori', 'syntax', ',', 'cambridg', ',', 'massachusett', ':', 'mit', 'press', '.']

 TOTAL SNOWBALL STEM WORDS ==> 29



 ---- LEMMATIZATION ----

['Link', ':', 'http', ':', '//www.indiegogo.com/projects/meet-the-pilot-smart-earpiece-language-translator-', 'headphones-travel', '#', '/', 'REFRENCES', '[', '1', ']', 'Chomsky', ',', 'Noam', ',', '1965', ',', 'Aspects', 'Theory', 'Syntax', ',', 'Cambridge', ',', 'Massachusetts', ':', 'MIT', 'Press', '.']

 TOTAL LEMMATIZE WORDS ==> 29

************************************************************************************************************************

348 --> [2] Rospocher, M., van Erp, M., Vossen, P., Fokkens, A., Aldabe,I., Rigau, G., Soroa, A.,  Ploeger, T., and Bogaard, T.(2016). 


 ---- TOKENS ----

 ['[', '2', ']', 'Rospocher', ',', 'M.', ',', 'van', 'Erp', ',', 'M.', ',', 'Vossen', ',', 'P.', ',', 'Fokkens', ',', 'A.', ',', 'Aldabe', ',', 'I.', ',', 'Rigau', ',', 'G.', ',', 'Soroa', ',', 'A.', ',', 'Ploeger', ',', 'T.', ',', 'and', 'Bogaard', ',', 'T.', '(', '2016', ')', '.'] 

 TOTAL TOKENS ==> 44

 ---- POST ----

 [('[', 'RB'), ('2', 'CD'), (']', 'NN'), ('Rospocher', 'NNP'), (',', ','), ('M.', 'NNP'), (',', ','), ('van', 'NN'), ('Erp', 'NNP'), (',', ','), ('M.', 'NNP'), (',', ','), ('Vossen', 'NNP'), (',', ','), ('P.', 'NNP'), (',', ','), ('Fokkens', 'NNP'), (',', ','), ('A.', 'NNP'), (',', ','), ('Aldabe', 'NNP'), (',', ','), ('I.', 'NNP'), (',', ','), ('Rigau', 'NNP'), (',', ','), ('G.', 'NNP'), (',', ','), ('Soroa', 'NNP'), (',', ','), ('A.', 'NNP'), (',', ','), ('Ploeger', 'NNP'), (',', ','), ('T.', 'NNP'), (',', ','), ('and', 'CC'), ('Bogaard', 'NNP'), (',', ','), ('T.', 'NNP'), ('(', '('), ('2016', 'CD'), (')', ')'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['[', '2', ']', 'Rospocher', ',', 'M.', ',', 'van', 'Erp', ',', 'M.', ',', 'Vossen', ',', 'P.', ',', 'Fokkens', ',', 'A.', ',', 'Aldabe', ',', 'I.', ',', 'Rigau', ',', 'G.', ',', 'Soroa', ',', 'A.', ',', 'Ploeger', ',', 'T.', ',', 'Bogaard', ',', 'T.', '(', '2016', ')', '.']

 TOTAL FILTERED TOKENS ==>  43

 ---- POST FOR FILTERED TOKENS ----

 [('[', 'RB'), ('2', 'CD'), (']', 'NN'), ('Rospocher', 'NNP'), (',', ','), ('M.', 'NNP'), (',', ','), ('van', 'NN'), ('Erp', 'NNP'), (',', ','), ('M.', 'NNP'), (',', ','), ('Vossen', 'NNP'), (',', ','), ('P.', 'NNP'), (',', ','), ('Fokkens', 'NNP'), (',', ','), ('A.', 'NNP'), (',', ','), ('Aldabe', 'NNP'), (',', ','), ('I.', 'NNP'), (',', ','), ('Rigau', 'NNP'), (',', ','), ('G.', 'NNP'), (',', ','), ('Soroa', 'NNP'), (',', ','), ('A.', 'NNP'), (',', ','), ('Ploeger', 'NNP'), (',', ','), ('T.', 'NNP'), (',', ','), ('Bogaard', 'NNP'), (',', ','), ('T.', 'NNP'), ('(', '('), ('2016', 'CD'), (')', ')'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['[ 2', '2 ]', '] Rospocher', 'Rospocher ,', ', M.', 'M. ,', ', van', 'van Erp', 'Erp ,', ', M.', 'M. ,', ', Vossen', 'Vossen ,', ', P.', 'P. ,', ', Fokkens', 'Fokkens ,', ', A.', 'A. ,', ', Aldabe', 'Aldabe ,', ', I.', 'I. ,', ', Rigau', 'Rigau ,', ', G.', 'G. ,', ', Soroa', 'Soroa ,', ', A.', 'A. ,', ', Ploeger', 'Ploeger ,', ', T.', 'T. ,', ', Bogaard', 'Bogaard ,', ', T.', 'T. (', '( 2016', '2016 )', ') .'] 

 TOTAL BIGRAMS --> 42 



 ---- TRI-GRAMS ---- 

 ['[ 2 ]', '2 ] Rospocher', '] Rospocher ,', 'Rospocher , M.', ', M. ,', 'M. , van', ', van Erp', 'van Erp ,', 'Erp , M.', ', M. ,', 'M. , Vossen', ', Vossen ,', 'Vossen , P.', ', P. ,', 'P. , Fokkens', ', Fokkens ,', 'Fokkens , A.', ', A. ,', 'A. , Aldabe', ', Aldabe ,', 'Aldabe , I.', ', I. ,', 'I. , Rigau', ', Rigau ,', 'Rigau , G.', ', G. ,', 'G. , Soroa', ', Soroa ,', 'Soroa , A.', ', A. ,', 'A. , Ploeger', ', Ploeger ,', 'Ploeger , T.', ', T. ,', 'T. , Bogaard', ', Bogaard ,', 'Bogaard , T.', ', T. (', 'T. ( 2016', '( 2016 )', '2016 ) .'] 

 TOTAL TRIGRAMS --> 41 



 ---- NOUN PHRASES ---- 

 [']', 'van'] 

 TOTAL NOUN PHRASES --> 2 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> ['Rospocher', 'Erp', 'Aldabe', 'Rigau', 'Soroa', 'Ploeger', 'Bogaard']
 TOTAL PERSON ENTITY --> 7 


 GPE ---> ['Vossen', 'Fokkens']
 TOTAL GPE ENTITY --> 2 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['[', '2', ']', 'rospoch', ',', 'm.', ',', 'van', 'erp', ',', 'm.', ',', 'vossen', ',', 'p.', ',', 'fokken', ',', 'a.', ',', 'aldab', ',', 'i.', ',', 'rigau', ',', 'g.', ',', 'soroa', ',', 'a.', ',', 'ploeger', ',', 't.', ',', 'bogaard', ',', 't.', '(', '2016', ')', '.']

 TOTAL PORTER STEM WORDS ==> 43



 ---- SNOWBALL STEMMING ----

['[', '2', ']', 'rospoch', ',', 'm.', ',', 'van', 'erp', ',', 'm.', ',', 'vossen', ',', 'p.', ',', 'fokken', ',', 'a.', ',', 'aldab', ',', 'i.', ',', 'rigau', ',', 'g.', ',', 'soroa', ',', 'a.', ',', 'ploeger', ',', 't.', ',', 'bogaard', ',', 't.', '(', '2016', ')', '.']

 TOTAL SNOWBALL STEM WORDS ==> 43



 ---- LEMMATIZATION ----

['[', '2', ']', 'Rospocher', ',', 'M.', ',', 'van', 'Erp', ',', 'M.', ',', 'Vossen', ',', 'P.', ',', 'Fokkens', ',', 'A.', ',', 'Aldabe', ',', 'I.', ',', 'Rigau', ',', 'G.', ',', 'Soroa', ',', 'A.', ',', 'Ploeger', ',', 'T.', ',', 'Bogaard', ',', 'T.', '(', '2016', ')', '.']

 TOTAL LEMMATIZE WORDS ==> 43

************************************************************************************************************************

349 --> Building event-centric knowledge graphs from news. 


 ---- TOKENS ----

 ['Building', 'event-centric', 'knowledge', 'graphs', 'from', 'news', '.'] 

 TOTAL TOKENS ==> 7

 ---- POST ----

 [('Building', 'VBG'), ('event-centric', 'JJ'), ('knowledge', 'NN'), ('graphs', 'NN'), ('from', 'IN'), ('news', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Building', 'event-centric', 'knowledge', 'graphs', 'news', '.']

 TOTAL FILTERED TOKENS ==>  6

 ---- POST FOR FILTERED TOKENS ----

 [('Building', 'VBG'), ('event-centric', 'JJ'), ('knowledge', 'NN'), ('graphs', 'NN'), ('news', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Building event-centric', 'event-centric knowledge', 'knowledge graphs', 'graphs news', 'news .'] 

 TOTAL BIGRAMS --> 5 



 ---- TRI-GRAMS ---- 

 ['Building event-centric knowledge', 'event-centric knowledge graphs', 'knowledge graphs news', 'graphs news .'] 

 TOTAL TRIGRAMS --> 4 



 ---- NOUN PHRASES ---- 

 ['event-centric knowledge', 'graphs', 'news'] 

 TOTAL NOUN PHRASES --> 3 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['build', 'event-centr', 'knowledg', 'graph', 'news', '.']

 TOTAL PORTER STEM WORDS ==> 6



 ---- SNOWBALL STEMMING ----

['build', 'event-centr', 'knowledg', 'graph', 'news', '.']

 TOTAL SNOWBALL STEM WORDS ==> 6



 ---- LEMMATIZATION ----

['Building', 'event-centric', 'knowledge', 'graph', 'news', '.']

 TOTAL LEMMATIZE WORDS ==> 6

************************************************************************************************************************

350 --> Web Semantics: Science, Services and Agents on the World Wide Web, In Press. 


 ---- TOKENS ----

 ['Web', 'Semantics', ':', 'Science', ',', 'Services', 'and', 'Agents', 'on', 'the', 'World', 'Wide', 'Web', ',', 'In', 'Press', '.'] 

 TOTAL TOKENS ==> 17

 ---- POST ----

 [('Web', 'JJ'), ('Semantics', 'NNS'), (':', ':'), ('Science', 'NN'), (',', ','), ('Services', 'NNPS'), ('and', 'CC'), ('Agents', 'NNS'), ('on', 'IN'), ('the', 'DT'), ('World', 'NNP'), ('Wide', 'NNP'), ('Web', 'NNP'), (',', ','), ('In', 'IN'), ('Press', 'NNP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Web', 'Semantics', ':', 'Science', ',', 'Services', 'Agents', 'World', 'Wide', 'Web', ',', 'Press', '.']

 TOTAL FILTERED TOKENS ==>  13

 ---- POST FOR FILTERED TOKENS ----

 [('Web', 'JJ'), ('Semantics', 'NNS'), (':', ':'), ('Science', 'NN'), (',', ','), ('Services', 'NNPS'), ('Agents', 'NNP'), ('World', 'NNP'), ('Wide', 'NNP'), ('Web', 'NNP'), (',', ','), ('Press', 'NNP'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Web Semantics', 'Semantics :', ': Science', 'Science ,', ', Services', 'Services Agents', 'Agents World', 'World Wide', 'Wide Web', 'Web ,', ', Press', 'Press .'] 

 TOTAL BIGRAMS --> 12 



 ---- TRI-GRAMS ---- 

 ['Web Semantics :', 'Semantics : Science', ': Science ,', 'Science , Services', ', Services Agents', 'Services Agents World', 'Agents World Wide', 'World Wide Web', 'Wide Web ,', 'Web , Press', ', Press .'] 

 TOTAL TRIGRAMS --> 11 



 ---- NOUN PHRASES ---- 

 ['Science'] 

 TOTAL NOUN PHRASES --> 1 



 ---- NER ----

 
 ORGANIZATION ---> ['Services Agents']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['web', 'semant', ':', 'scienc', ',', 'servic', 'agent', 'world', 'wide', 'web', ',', 'press', '.']

 TOTAL PORTER STEM WORDS ==> 13



 ---- SNOWBALL STEMMING ----

['web', 'semant', ':', 'scienc', ',', 'servic', 'agent', 'world', 'wide', 'web', ',', 'press', '.']

 TOTAL SNOWBALL STEM WORDS ==> 13



 ---- LEMMATIZATION ----

['Web', 'Semantics', ':', 'Science', ',', 'Services', 'Agents', 'World', 'Wide', 'Web', ',', 'Press', '.']

 TOTAL LEMMATIZE WORDS ==> 13

************************************************************************************************************************

351 --> [3] Shemtov, H. (1997). 


 ---- TOKENS ----

 ['[', '3', ']', 'Shemtov', ',', 'H.', '(', '1997', ')', '.'] 

 TOTAL TOKENS ==> 10

 ---- POST ----

 [('[', 'RB'), ('3', 'CD'), (']', 'JJ'), ('Shemtov', 'NNP'), (',', ','), ('H.', 'NNP'), ('(', '('), ('1997', 'CD'), (')', ')'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['[', '3', ']', 'Shemtov', ',', 'H.', '(', '1997', ')', '.']

 TOTAL FILTERED TOKENS ==>  10

 ---- POST FOR FILTERED TOKENS ----

 [('[', 'RB'), ('3', 'CD'), (']', 'JJ'), ('Shemtov', 'NNP'), (',', ','), ('H.', 'NNP'), ('(', '('), ('1997', 'CD'), (')', ')'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['[ 3', '3 ]', '] Shemtov', 'Shemtov ,', ', H.', 'H. (', '( 1997', '1997 )', ') .'] 

 TOTAL BIGRAMS --> 9 



 ---- TRI-GRAMS ---- 

 ['[ 3 ]', '3 ] Shemtov', '] Shemtov ,', 'Shemtov , H.', ', H. (', 'H. ( 1997', '( 1997 )', '1997 ) .'] 

 TOTAL TRIGRAMS --> 8 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> ['Shemtov']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['[', '3', ']', 'shemtov', ',', 'h.', '(', '1997', ')', '.']

 TOTAL PORTER STEM WORDS ==> 10



 ---- SNOWBALL STEMMING ----

['[', '3', ']', 'shemtov', ',', 'h.', '(', '1997', ')', '.']

 TOTAL SNOWBALL STEM WORDS ==> 10



 ---- LEMMATIZATION ----

['[', '3', ']', 'Shemtov', ',', 'H.', '(', '1997', ')', '.']

 TOTAL LEMMATIZE WORDS ==> 10

************************************************************************************************************************

352 --> Ambiguity management in natural language generation. 


 ---- TOKENS ----

 ['Ambiguity', 'management', 'in', 'natural', 'language', 'generation', '.'] 

 TOTAL TOKENS ==> 7

 ---- POST ----

 [('Ambiguity', 'NNP'), ('management', 'NN'), ('in', 'IN'), ('natural', 'JJ'), ('language', 'NN'), ('generation', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Ambiguity', 'management', 'natural', 'language', 'generation', '.']

 TOTAL FILTERED TOKENS ==>  6

 ---- POST FOR FILTERED TOKENS ----

 [('Ambiguity', 'NNP'), ('management', 'NN'), ('natural', 'JJ'), ('language', 'NN'), ('generation', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Ambiguity management', 'management natural', 'natural language', 'language generation', 'generation .'] 

 TOTAL BIGRAMS --> 5 



 ---- TRI-GRAMS ---- 

 ['Ambiguity management natural', 'management natural language', 'natural language generation', 'language generation .'] 

 TOTAL TRIGRAMS --> 4 



 ---- NOUN PHRASES ---- 

 ['management', 'natural language', 'generation'] 

 TOTAL NOUN PHRASES --> 3 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> ['Ambiguity']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['ambigu', 'manag', 'natur', 'languag', 'gener', '.']

 TOTAL PORTER STEM WORDS ==> 6



 ---- SNOWBALL STEMMING ----

['ambigu', 'manag', 'natur', 'languag', 'generat', '.']

 TOTAL SNOWBALL STEM WORDS ==> 6



 ---- LEMMATIZATION ----

['Ambiguity', 'management', 'natural', 'language', 'generation', '.']

 TOTAL LEMMATIZE WORDS ==> 6

************************************************************************************************************************

353 --> Stanford  University. 


 ---- TOKENS ----

 ['Stanford', 'University', '.'] 

 TOTAL TOKENS ==> 3

 ---- POST ----

 [('Stanford', 'NNP'), ('University', 'NNP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Stanford', 'University', '.']

 TOTAL FILTERED TOKENS ==>  3

 ---- POST FOR FILTERED TOKENS ----

 [('Stanford', 'NNP'), ('University', 'NNP'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Stanford University', 'University .'] 

 TOTAL BIGRAMS --> 2 



 ---- TRI-GRAMS ---- 

 ['Stanford University .'] 

 TOTAL TRIGRAMS --> 1 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> ['University']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> ['Stanford']
 TOTAL PERSON ENTITY --> 1 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['stanford', 'univers', '.']

 TOTAL PORTER STEM WORDS ==> 3



 ---- SNOWBALL STEMMING ----

['stanford', 'univers', '.']

 TOTAL SNOWBALL STEM WORDS ==> 3



 ---- LEMMATIZATION ----

['Stanford', 'University', '.']

 TOTAL LEMMATIZE WORDS ==> 3

************************************************************************************************************************

354 --> [4] Emele, M. C., & Dorna, M. (1998, August). 


 ---- TOKENS ----

 ['[', '4', ']', 'Emele', ',', 'M.', 'C.', ',', '&', 'Dorna', ',', 'M.', '(', '1998', ',', 'August', ')', '.'] 

 TOTAL TOKENS ==> 18

 ---- POST ----

 [('[', 'RB'), ('4', 'CD'), (']', 'JJ'), ('Emele', 'NNP'), (',', ','), ('M.', 'NNP'), ('C.', 'NNP'), (',', ','), ('&', 'CC'), ('Dorna', 'NNP'), (',', ','), ('M.', 'NNP'), ('(', '('), ('1998', 'CD'), (',', ','), ('August', 'NNP'), (')', ')'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['[', '4', ']', 'Emele', ',', 'M.', 'C.', ',', '&', 'Dorna', ',', 'M.', '(', '1998', ',', 'August', ')', '.']

 TOTAL FILTERED TOKENS ==>  18

 ---- POST FOR FILTERED TOKENS ----

 [('[', 'RB'), ('4', 'CD'), (']', 'JJ'), ('Emele', 'NNP'), (',', ','), ('M.', 'NNP'), ('C.', 'NNP'), (',', ','), ('&', 'CC'), ('Dorna', 'NNP'), (',', ','), ('M.', 'NNP'), ('(', '('), ('1998', 'CD'), (',', ','), ('August', 'NNP'), (')', ')'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['[ 4', '4 ]', '] Emele', 'Emele ,', ', M.', 'M. C.', 'C. ,', ', &', '& Dorna', 'Dorna ,', ', M.', 'M. (', '( 1998', '1998 ,', ', August', 'August )', ') .'] 

 TOTAL BIGRAMS --> 17 



 ---- TRI-GRAMS ---- 

 ['[ 4 ]', '4 ] Emele', '] Emele ,', 'Emele , M.', ', M. C.', 'M. C. ,', 'C. , &', ', & Dorna', '& Dorna ,', 'Dorna , M.', ', M. (', 'M. ( 1998', '( 1998 ,', '1998 , August', ', August )', 'August ) .'] 

 TOTAL TRIGRAMS --> 16 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> ['Emele', 'Dorna']
 TOTAL PERSON ENTITY --> 2 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['[', '4', ']', 'emel', ',', 'm.', 'c.', ',', '&', 'dorna', ',', 'm.', '(', '1998', ',', 'august', ')', '.']

 TOTAL PORTER STEM WORDS ==> 18



 ---- SNOWBALL STEMMING ----

['[', '4', ']', 'emel', ',', 'm.', 'c.', ',', '&', 'dorna', ',', 'm.', '(', '1998', ',', 'august', ')', '.']

 TOTAL SNOWBALL STEM WORDS ==> 18



 ---- LEMMATIZATION ----

['[', '4', ']', 'Emele', ',', 'M.', 'C.', ',', '&', 'Dorna', ',', 'M.', '(', '1998', ',', 'August', ')', '.']

 TOTAL LEMMATIZE WORDS ==> 18

************************************************************************************************************************

355 --> Ambiguity preserving machine translation  using packed representations. 


 ---- TOKENS ----

 ['Ambiguity', 'preserving', 'machine', 'translation', 'using', 'packed', 'representations', '.'] 

 TOTAL TOKENS ==> 8

 ---- POST ----

 [('Ambiguity', 'NNP'), ('preserving', 'VBG'), ('machine', 'NN'), ('translation', 'NN'), ('using', 'VBG'), ('packed', 'JJ'), ('representations', 'NNS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Ambiguity', 'preserving', 'machine', 'translation', 'using', 'packed', 'representations', '.']

 TOTAL FILTERED TOKENS ==>  8

 ---- POST FOR FILTERED TOKENS ----

 [('Ambiguity', 'NNP'), ('preserving', 'VBG'), ('machine', 'NN'), ('translation', 'NN'), ('using', 'VBG'), ('packed', 'JJ'), ('representations', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Ambiguity preserving', 'preserving machine', 'machine translation', 'translation using', 'using packed', 'packed representations', 'representations .'] 

 TOTAL BIGRAMS --> 7 



 ---- TRI-GRAMS ---- 

 ['Ambiguity preserving machine', 'preserving machine translation', 'machine translation using', 'translation using packed', 'using packed representations', 'packed representations .'] 

 TOTAL TRIGRAMS --> 6 



 ---- NOUN PHRASES ---- 

 ['machine', 'translation'] 

 TOTAL NOUN PHRASES --> 2 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['ambigu', 'preserv', 'machin', 'translat', 'use', 'pack', 'represent', '.']

 TOTAL PORTER STEM WORDS ==> 8



 ---- SNOWBALL STEMMING ----

['ambigu', 'preserv', 'machin', 'translat', 'use', 'pack', 'represent', '.']

 TOTAL SNOWBALL STEM WORDS ==> 8



 ---- LEMMATIZATION ----

['Ambiguity', 'preserving', 'machine', 'translation', 'using', 'packed', 'representation', '.']

 TOTAL LEMMATIZE WORDS ==> 8

************************************************************************************************************************

356 --> In Proceedings of the 36th Annual Meeting of the Association  https://www.ncbi.nlm.nih.gov/pubmed/28269895?dopt=Abstract https://www.indiegogo.com/projects/meet-the-pilot-smart-earpiece-language-translator-headphones-travel#/ https://www.indiegogo.com/projects/meet-the-pilot-smart-earpiece-language-translator-headphones-travel#/  for Computational Linguistics and 17th International Conference on Computational  Linguistics-Volume 1 (pp. 


 ---- TOKENS ----

 ['In', 'Proceedings', 'of', 'the', '36th', 'Annual', 'Meeting', 'of', 'the', 'Association', 'https', ':', '//www.ncbi.nlm.nih.gov/pubmed/28269895', '?', 'dopt=Abstract', 'https', ':', '//www.indiegogo.com/projects/meet-the-pilot-smart-earpiece-language-translator-headphones-travel', '#', '/', 'https', ':', '//www.indiegogo.com/projects/meet-the-pilot-smart-earpiece-language-translator-headphones-travel', '#', '/', 'for', 'Computational', 'Linguistics', 'and', '17th', 'International', 'Conference', 'on', 'Computational', 'Linguistics-Volume', '1', '(', 'pp', '.'] 

 TOTAL TOKENS ==> 39

 ---- POST ----

 [('In', 'IN'), ('Proceedings', 'NNP'), ('of', 'IN'), ('the', 'DT'), ('36th', 'CD'), ('Annual', 'JJ'), ('Meeting', 'NN'), ('of', 'IN'), ('the', 'DT'), ('Association', 'NNP'), ('https', 'NN'), (':', ':'), ('//www.ncbi.nlm.nih.gov/pubmed/28269895', 'NN'), ('?', '.'), ('dopt=Abstract', 'JJ'), ('https', 'NN'), (':', ':'), ('//www.indiegogo.com/projects/meet-the-pilot-smart-earpiece-language-translator-headphones-travel', 'JJ'), ('#', '#'), ('/', 'JJ'), ('https', 'NN'), (':', ':'), ('//www.indiegogo.com/projects/meet-the-pilot-smart-earpiece-language-translator-headphones-travel', 'JJ'), ('#', '#'), ('/', 'NN'), ('for', 'IN'), ('Computational', 'NNP'), ('Linguistics', 'NNP'), ('and', 'CC'), ('17th', 'CD'), ('International', 'NNP'), ('Conference', 'NNP'), ('on', 'IN'), ('Computational', 'NNP'), ('Linguistics-Volume', 'NNP'), ('1', 'CD'), ('(', '('), ('pp', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Proceedings', '36th', 'Annual', 'Meeting', 'Association', 'https', ':', '//www.ncbi.nlm.nih.gov/pubmed/28269895', '?', 'dopt=Abstract', 'https', ':', '//www.indiegogo.com/projects/meet-the-pilot-smart-earpiece-language-translator-headphones-travel', '#', '/', 'https', ':', '//www.indiegogo.com/projects/meet-the-pilot-smart-earpiece-language-translator-headphones-travel', '#', '/', 'Computational', 'Linguistics', '17th', 'International', 'Conference', 'Computational', 'Linguistics-Volume', '1', '(', 'pp', '.']

 TOTAL FILTERED TOKENS ==>  31

 ---- POST FOR FILTERED TOKENS ----

 [('Proceedings', 'NNS'), ('36th', 'CD'), ('Annual', 'NNP'), ('Meeting', 'NNP'), ('Association', 'NNP'), ('https', 'NN'), (':', ':'), ('//www.ncbi.nlm.nih.gov/pubmed/28269895', 'NN'), ('?', '.'), ('dopt=Abstract', 'JJ'), ('https', 'NN'), (':', ':'), ('//www.indiegogo.com/projects/meet-the-pilot-smart-earpiece-language-translator-headphones-travel', 'JJ'), ('#', '#'), ('/', 'JJ'), ('https', 'NN'), (':', ':'), ('//www.indiegogo.com/projects/meet-the-pilot-smart-earpiece-language-translator-headphones-travel', 'JJ'), ('#', '#'), ('/', 'JJ'), ('Computational', 'NNP'), ('Linguistics', 'NNP'), ('17th', 'CD'), ('International', 'NNP'), ('Conference', 'NNP'), ('Computational', 'NNP'), ('Linguistics-Volume', 'NNP'), ('1', 'CD'), ('(', '('), ('pp', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Proceedings 36th', '36th Annual', 'Annual Meeting', 'Meeting Association', 'Association https', 'https :', ': //www.ncbi.nlm.nih.gov/pubmed/28269895', '//www.ncbi.nlm.nih.gov/pubmed/28269895 ?', '? dopt=Abstract', 'dopt=Abstract https', 'https :', ': //www.indiegogo.com/projects/meet-the-pilot-smart-earpiece-language-translator-headphones-travel', '//www.indiegogo.com/projects/meet-the-pilot-smart-earpiece-language-translator-headphones-travel #', '# /', '/ https', 'https :', ': //www.indiegogo.com/projects/meet-the-pilot-smart-earpiece-language-translator-headphones-travel', '//www.indiegogo.com/projects/meet-the-pilot-smart-earpiece-language-translator-headphones-travel #', '# /', '/ Computational', 'Computational Linguistics', 'Linguistics 17th', '17th International', 'International Conference', 'Conference Computational', 'Computational Linguistics-Volume', 'Linguistics-Volume 1', '1 (', '( pp', 'pp .'] 

 TOTAL BIGRAMS --> 30 



 ---- TRI-GRAMS ---- 

 ['Proceedings 36th Annual', '36th Annual Meeting', 'Annual Meeting Association', 'Meeting Association https', 'Association https :', 'https : //www.ncbi.nlm.nih.gov/pubmed/28269895', ': //www.ncbi.nlm.nih.gov/pubmed/28269895 ?', '//www.ncbi.nlm.nih.gov/pubmed/28269895 ? dopt=Abstract', '? dopt=Abstract https', 'dopt=Abstract https :', 'https : //www.indiegogo.com/projects/meet-the-pilot-smart-earpiece-language-translator-headphones-travel', ': //www.indiegogo.com/projects/meet-the-pilot-smart-earpiece-language-translator-headphones-travel #', '//www.indiegogo.com/projects/meet-the-pilot-smart-earpiece-language-translator-headphones-travel # /', '# / https', '/ https :', 'https : //www.indiegogo.com/projects/meet-the-pilot-smart-earpiece-language-translator-headphones-travel', ': //www.indiegogo.com/projects/meet-the-pilot-smart-earpiece-language-translator-headphones-travel #', '//www.indiegogo.com/projects/meet-the-pilot-smart-earpiece-language-translator-headphones-travel # /', '# / Computational', '/ Computational Linguistics', 'Computational Linguistics 17th', 'Linguistics 17th International', '17th International Conference', 'International Conference Computational', 'Conference Computational Linguistics-Volume', 'Computational Linguistics-Volume 1', 'Linguistics-Volume 1 (', '1 ( pp', '( pp .'] 

 TOTAL TRIGRAMS --> 29 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 []

 ---- PORTER STEMMING ----

['proceed', '36th', 'annual', 'meet', 'associ', 'http', ':', '//www.ncbi.nlm.nih.gov/pubmed/28269895', '?', 'dopt=abstract', 'http', ':', '//www.indiegogo.com/projects/meet-the-pilot-smart-earpiece-language-translator-headphones-travel', '#', '/', 'http', ':', '//www.indiegogo.com/projects/meet-the-pilot-smart-earpiece-language-translator-headphones-travel', '#', '/', 'comput', 'linguist', '17th', 'intern', 'confer', 'comput', 'linguistics-volum', '1', '(', 'pp', '.']

 TOTAL PORTER STEM WORDS ==> 31



 ---- SNOWBALL STEMMING ----

['proceed', '36th', 'annual', 'meet', 'associ', 'https', ':', '//www.ncbi.nlm.nih.gov/pubmed/28269895', '?', 'dopt=abstract', 'https', ':', '//www.indiegogo.com/projects/meet-the-pilot-smart-earpiece-language-translator-headphones-travel', '#', '/', 'https', ':', '//www.indiegogo.com/projects/meet-the-pilot-smart-earpiece-language-translator-headphones-travel', '#', '/', 'comput', 'linguist', '17th', 'intern', 'confer', 'comput', 'linguistics-volum', '1', '(', 'pp', '.']

 TOTAL SNOWBALL STEM WORDS ==> 31



 ---- LEMMATIZATION ----

['Proceedings', '36th', 'Annual', 'Meeting', 'Association', 'http', ':', '//www.ncbi.nlm.nih.gov/pubmed/28269895', '?', 'dopt=Abstract', 'http', ':', '//www.indiegogo.com/projects/meet-the-pilot-smart-earpiece-language-translator-headphones-travel', '#', '/', 'http', ':', '//www.indiegogo.com/projects/meet-the-pilot-smart-earpiece-language-translator-headphones-travel', '#', '/', 'Computational', 'Linguistics', '17th', 'International', 'Conference', 'Computational', 'Linguistics-Volume', '1', '(', 'pp', '.']

 TOTAL LEMMATIZE WORDS ==> 31

************************************************************************************************************************

357 --> 365-371). 


 ---- TOKENS ----

 ['365-371', ')', '.'] 

 TOTAL TOKENS ==> 3

 ---- POST ----

 [('365-371', 'JJ'), (')', ')'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['365-371', ')', '.']

 TOTAL FILTERED TOKENS ==>  3

 ---- POST FOR FILTERED TOKENS ----

 [('365-371', 'JJ'), (')', ')'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['365-371 )', ') .'] 

 TOTAL BIGRAMS --> 2 



 ---- TRI-GRAMS ---- 

 ['365-371 ) .'] 

 TOTAL TRIGRAMS --> 1 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 []

 ---- PORTER STEMMING ----

['365-371', ')', '.']

 TOTAL PORTER STEM WORDS ==> 3



 ---- SNOWBALL STEMMING ----

['365-371', ')', '.']

 TOTAL SNOWBALL STEM WORDS ==> 3



 ---- LEMMATIZATION ----

['365-371', ')', '.']

 TOTAL LEMMATIZE WORDS ==> 3

************************************************************************************************************************

358 --> Association for Computational Linguistics. 


 ---- TOKENS ----

 ['Association', 'for', 'Computational', 'Linguistics', '.'] 

 TOTAL TOKENS ==> 5

 ---- POST ----

 [('Association', 'NNP'), ('for', 'IN'), ('Computational', 'NNP'), ('Linguistics', 'NNP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Association', 'Computational', 'Linguistics', '.']

 TOTAL FILTERED TOKENS ==>  4

 ---- POST FOR FILTERED TOKENS ----

 [('Association', 'NNP'), ('Computational', 'NNP'), ('Linguistics', 'NNP'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Association Computational', 'Computational Linguistics', 'Linguistics .'] 

 TOTAL BIGRAMS --> 3 



 ---- TRI-GRAMS ---- 

 ['Association Computational Linguistics', 'Computational Linguistics .'] 

 TOTAL TRIGRAMS --> 2 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['associ', 'comput', 'linguist', '.']

 TOTAL PORTER STEM WORDS ==> 4



 ---- SNOWBALL STEMMING ----

['associ', 'comput', 'linguist', '.']

 TOTAL SNOWBALL STEM WORDS ==> 4



 ---- LEMMATIZATION ----

['Association', 'Computational', 'Linguistics', '.']

 TOTAL LEMMATIZE WORDS ==> 4

************************************************************************************************************************

359 --> [5] Knight, K., & Langkilde, I. 


 ---- TOKENS ----

 ['[', '5', ']', 'Knight', ',', 'K.', ',', '&', 'Langkilde', ',', 'I', '.'] 

 TOTAL TOKENS ==> 12

 ---- POST ----

 [('[', 'RB'), ('5', 'CD'), (']', 'JJ'), ('Knight', 'NNP'), (',', ','), ('K.', 'NNP'), (',', ','), ('&', 'CC'), ('Langkilde', 'NNP'), (',', ','), ('I', 'PRP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['[', '5', ']', 'Knight', ',', 'K.', ',', '&', 'Langkilde', ',', '.']

 TOTAL FILTERED TOKENS ==>  11

 ---- POST FOR FILTERED TOKENS ----

 [('[', 'RB'), ('5', 'CD'), (']', 'JJ'), ('Knight', 'NNP'), (',', ','), ('K.', 'NNP'), (',', ','), ('&', 'CC'), ('Langkilde', 'NNP'), (',', ','), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['[ 5', '5 ]', '] Knight', 'Knight ,', ', K.', 'K. ,', ', &', '& Langkilde', 'Langkilde ,', ', .'] 

 TOTAL BIGRAMS --> 10 



 ---- TRI-GRAMS ---- 

 ['[ 5 ]', '5 ] Knight', '] Knight ,', 'Knight , K.', ', K. ,', 'K. , &', ', & Langkilde', '& Langkilde ,', 'Langkilde , .'] 

 TOTAL TRIGRAMS --> 9 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> ['Langkilde']
 TOTAL PERSON ENTITY --> 1 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['[', '5', ']', 'knight', ',', 'k.', ',', '&', 'langkild', ',', '.']

 TOTAL PORTER STEM WORDS ==> 11



 ---- SNOWBALL STEMMING ----

['[', '5', ']', 'knight', ',', 'k.', ',', '&', 'langkild', ',', '.']

 TOTAL SNOWBALL STEM WORDS ==> 11



 ---- LEMMATIZATION ----

['[', '5', ']', 'Knight', ',', 'K.', ',', '&', 'Langkilde', ',', '.']

 TOTAL LEMMATIZE WORDS ==> 11

************************************************************************************************************************

360 --> (2000, July). 


 ---- TOKENS ----

 ['(', '2000', ',', 'July', ')', '.'] 

 TOTAL TOKENS ==> 6

 ---- POST ----

 [('(', '('), ('2000', 'CD'), (',', ','), ('July', 'NNP'), (')', ')'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['(', '2000', ',', 'July', ')', '.']

 TOTAL FILTERED TOKENS ==>  6

 ---- POST FOR FILTERED TOKENS ----

 [('(', '('), ('2000', 'CD'), (',', ','), ('July', 'NNP'), (')', ')'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['( 2000', '2000 ,', ', July', 'July )', ') .'] 

 TOTAL BIGRAMS --> 5 



 ---- TRI-GRAMS ---- 

 ['( 2000 ,', '2000 , July', ', July )', 'July ) .'] 

 TOTAL TRIGRAMS --> 4 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['(', '2000', ',', 'juli', ')', '.']

 TOTAL PORTER STEM WORDS ==> 6



 ---- SNOWBALL STEMMING ----

['(', '2000', ',', 'juli', ')', '.']

 TOTAL SNOWBALL STEM WORDS ==> 6



 ---- LEMMATIZATION ----

['(', '2000', ',', 'July', ')', '.']

 TOTAL LEMMATIZE WORDS ==> 6

************************************************************************************************************************

361 --> Preserving ambiguities in generation via  automata intersection. 


 ---- TOKENS ----

 ['Preserving', 'ambiguities', 'in', 'generation', 'via', 'automata', 'intersection', '.'] 

 TOTAL TOKENS ==> 8

 ---- POST ----

 [('Preserving', 'VBG'), ('ambiguities', 'NNS'), ('in', 'IN'), ('generation', 'NN'), ('via', 'IN'), ('automata', 'JJ'), ('intersection', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Preserving', 'ambiguities', 'generation', 'via', 'automata', 'intersection', '.']

 TOTAL FILTERED TOKENS ==>  7

 ---- POST FOR FILTERED TOKENS ----

 [('Preserving', 'VBG'), ('ambiguities', 'NNS'), ('generation', 'NN'), ('via', 'IN'), ('automata', 'JJ'), ('intersection', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Preserving ambiguities', 'ambiguities generation', 'generation via', 'via automata', 'automata intersection', 'intersection .'] 

 TOTAL BIGRAMS --> 6 



 ---- TRI-GRAMS ---- 

 ['Preserving ambiguities generation', 'ambiguities generation via', 'generation via automata', 'via automata intersection', 'automata intersection .'] 

 TOTAL TRIGRAMS --> 5 



 ---- NOUN PHRASES ---- 

 ['generation', 'automata intersection'] 

 TOTAL NOUN PHRASES --> 2 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['preserv', 'ambigu', 'gener', 'via', 'automata', 'intersect', '.']

 TOTAL PORTER STEM WORDS ==> 7



 ---- SNOWBALL STEMMING ----

['preserv', 'ambigu', 'generat', 'via', 'automata', 'intersect', '.']

 TOTAL SNOWBALL STEM WORDS ==> 7



 ---- LEMMATIZATION ----

['Preserving', 'ambiguity', 'generation', 'via', 'automaton', 'intersection', '.']

 TOTAL LEMMATIZE WORDS ==> 7

************************************************************************************************************************

362 --> In AAAI/IAAI (pp. 


 ---- TOKENS ----

 ['In', 'AAAI/IAAI', '(', 'pp', '.'] 

 TOTAL TOKENS ==> 5

 ---- POST ----

 [('In', 'IN'), ('AAAI/IAAI', 'NNP'), ('(', '('), ('pp', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['AAAI/IAAI', '(', 'pp', '.']

 TOTAL FILTERED TOKENS ==>  4

 ---- POST FOR FILTERED TOKENS ----

 [('AAAI/IAAI', 'NNP'), ('(', '('), ('pp', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['AAAI/IAAI (', '( pp', 'pp .'] 

 TOTAL BIGRAMS --> 3 



 ---- TRI-GRAMS ---- 

 ['AAAI/IAAI ( pp', '( pp .'] 

 TOTAL TRIGRAMS --> 2 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 []

 ---- PORTER STEMMING ----

['aaai/iaai', '(', 'pp', '.']

 TOTAL PORTER STEM WORDS ==> 4



 ---- SNOWBALL STEMMING ----

['aaai/iaai', '(', 'pp', '.']

 TOTAL SNOWBALL STEM WORDS ==> 4



 ---- LEMMATIZATION ----

['AAAI/IAAI', '(', 'pp', '.']

 TOTAL LEMMATIZE WORDS ==> 4

************************************************************************************************************************

363 --> 697-702). 


 ---- TOKENS ----

 ['697-702', ')', '.'] 

 TOTAL TOKENS ==> 3

 ---- POST ----

 [('697-702', 'JJ'), (')', ')'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['697-702', ')', '.']

 TOTAL FILTERED TOKENS ==>  3

 ---- POST FOR FILTERED TOKENS ----

 [('697-702', 'JJ'), (')', ')'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['697-702 )', ') .'] 

 TOTAL BIGRAMS --> 2 



 ---- TRI-GRAMS ---- 

 ['697-702 ) .'] 

 TOTAL TRIGRAMS --> 1 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 []

 ---- PORTER STEMMING ----

['697-702', ')', '.']

 TOTAL PORTER STEM WORDS ==> 3



 ---- SNOWBALL STEMMING ----

['697-702', ')', '.']

 TOTAL SNOWBALL STEM WORDS ==> 3



 ---- LEMMATIZATION ----

['697-702', ')', '.']

 TOTAL LEMMATIZE WORDS ==> 3

************************************************************************************************************************

364 --> [6] Nation, K., Snowling, M. J., & Clarke, P. (2007). 


 ---- TOKENS ----

 ['[', '6', ']', 'Nation', ',', 'K.', ',', 'Snowling', ',', 'M.', 'J.', ',', '&', 'Clarke', ',', 'P.', '(', '2007', ')', '.'] 

 TOTAL TOKENS ==> 20

 ---- POST ----

 [('[', 'RB'), ('6', 'CD'), (']', 'JJ'), ('Nation', 'NN'), (',', ','), ('K.', 'NNP'), (',', ','), ('Snowling', 'NNP'), (',', ','), ('M.', 'NNP'), ('J.', 'NNP'), (',', ','), ('&', 'CC'), ('Clarke', 'NNP'), (',', ','), ('P.', 'NNP'), ('(', '('), ('2007', 'CD'), (')', ')'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['[', '6', ']', 'Nation', ',', 'K.', ',', 'Snowling', ',', 'M.', 'J.', ',', '&', 'Clarke', ',', 'P.', '(', '2007', ')', '.']

 TOTAL FILTERED TOKENS ==>  20

 ---- POST FOR FILTERED TOKENS ----

 [('[', 'RB'), ('6', 'CD'), (']', 'JJ'), ('Nation', 'NN'), (',', ','), ('K.', 'NNP'), (',', ','), ('Snowling', 'NNP'), (',', ','), ('M.', 'NNP'), ('J.', 'NNP'), (',', ','), ('&', 'CC'), ('Clarke', 'NNP'), (',', ','), ('P.', 'NNP'), ('(', '('), ('2007', 'CD'), (')', ')'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['[ 6', '6 ]', '] Nation', 'Nation ,', ', K.', 'K. ,', ', Snowling', 'Snowling ,', ', M.', 'M. J.', 'J. ,', ', &', '& Clarke', 'Clarke ,', ', P.', 'P. (', '( 2007', '2007 )', ') .'] 

 TOTAL BIGRAMS --> 19 



 ---- TRI-GRAMS ---- 

 ['[ 6 ]', '6 ] Nation', '] Nation ,', 'Nation , K.', ', K. ,', 'K. , Snowling', ', Snowling ,', 'Snowling , M.', ', M. J.', 'M. J. ,', 'J. , &', ', & Clarke', '& Clarke ,', 'Clarke , P.', ', P. (', 'P. ( 2007', '( 2007 )', '2007 ) .'] 

 TOTAL TRIGRAMS --> 18 



 ---- NOUN PHRASES ---- 

 ['] Nation'] 

 TOTAL NOUN PHRASES --> 1 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> ['Clarke']
 TOTAL PERSON ENTITY --> 1 


 GPE ---> ['Snowling']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['[', '6', ']', 'nation', ',', 'k.', ',', 'snowl', ',', 'm.', 'j.', ',', '&', 'clark', ',', 'p.', '(', '2007', ')', '.']

 TOTAL PORTER STEM WORDS ==> 20



 ---- SNOWBALL STEMMING ----

['[', '6', ']', 'nation', ',', 'k.', ',', 'snowl', ',', 'm.', 'j.', ',', '&', 'clark', ',', 'p.', '(', '2007', ')', '.']

 TOTAL SNOWBALL STEM WORDS ==> 20



 ---- LEMMATIZATION ----

['[', '6', ']', 'Nation', ',', 'K.', ',', 'Snowling', ',', 'M.', 'J.', ',', '&', 'Clarke', ',', 'P.', '(', '2007', ')', '.']

 TOTAL LEMMATIZE WORDS ==> 20

************************************************************************************************************************

365 --> Dissecting the relationship between  language skills and learning to read: Semantic and phonological contributions to new  vocabulary learning in children with poor reading comprehension. 


 ---- TOKENS ----

 ['Dissecting', 'the', 'relationship', 'between', 'language', 'skills', 'and', 'learning', 'to', 'read', ':', 'Semantic', 'and', 'phonological', 'contributions', 'to', 'new', 'vocabulary', 'learning', 'in', 'children', 'with', 'poor', 'reading', 'comprehension', '.'] 

 TOTAL TOKENS ==> 26

 ---- POST ----

 [('Dissecting', 'VBG'), ('the', 'DT'), ('relationship', 'NN'), ('between', 'IN'), ('language', 'NN'), ('skills', 'NNS'), ('and', 'CC'), ('learning', 'VBG'), ('to', 'TO'), ('read', 'VB'), (':', ':'), ('Semantic', 'JJ'), ('and', 'CC'), ('phonological', 'JJ'), ('contributions', 'NNS'), ('to', 'TO'), ('new', 'JJ'), ('vocabulary', 'JJ'), ('learning', 'NN'), ('in', 'IN'), ('children', 'NNS'), ('with', 'IN'), ('poor', 'JJ'), ('reading', 'VBG'), ('comprehension', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Dissecting', 'relationship', 'language', 'skills', 'learning', 'read', ':', 'Semantic', 'phonological', 'contributions', 'new', 'vocabulary', 'learning', 'children', 'poor', 'reading', 'comprehension', '.']

 TOTAL FILTERED TOKENS ==>  18

 ---- POST FOR FILTERED TOKENS ----

 [('Dissecting', 'VBG'), ('relationship', 'NN'), ('language', 'NN'), ('skills', 'NNS'), ('learning', 'VBG'), ('read', 'NN'), (':', ':'), ('Semantic', 'JJ'), ('phonological', 'JJ'), ('contributions', 'NNS'), ('new', 'JJ'), ('vocabulary', 'JJ'), ('learning', 'NN'), ('children', 'NNS'), ('poor', 'JJ'), ('reading', 'VBG'), ('comprehension', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Dissecting relationship', 'relationship language', 'language skills', 'skills learning', 'learning read', 'read :', ': Semantic', 'Semantic phonological', 'phonological contributions', 'contributions new', 'new vocabulary', 'vocabulary learning', 'learning children', 'children poor', 'poor reading', 'reading comprehension', 'comprehension .'] 

 TOTAL BIGRAMS --> 17 



 ---- TRI-GRAMS ---- 

 ['Dissecting relationship language', 'relationship language skills', 'language skills learning', 'skills learning read', 'learning read :', 'read : Semantic', ': Semantic phonological', 'Semantic phonological contributions', 'phonological contributions new', 'contributions new vocabulary', 'new vocabulary learning', 'vocabulary learning children', 'learning children poor', 'children poor reading', 'poor reading comprehension', 'reading comprehension .'] 

 TOTAL TRIGRAMS --> 16 



 ---- NOUN PHRASES ---- 

 ['relationship', 'language', 'read', 'new vocabulary learning', 'comprehension'] 

 TOTAL NOUN PHRASES --> 5 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['dissect', 'relationship', 'languag', 'skill', 'learn', 'read', ':', 'semant', 'phonolog', 'contribut', 'new', 'vocabulari', 'learn', 'children', 'poor', 'read', 'comprehens', '.']

 TOTAL PORTER STEM WORDS ==> 18



 ---- SNOWBALL STEMMING ----

['dissect', 'relationship', 'languag', 'skill', 'learn', 'read', ':', 'semant', 'phonolog', 'contribut', 'new', 'vocabulari', 'learn', 'children', 'poor', 'read', 'comprehens', '.']

 TOTAL SNOWBALL STEM WORDS ==> 18



 ---- LEMMATIZATION ----

['Dissecting', 'relationship', 'language', 'skill', 'learning', 'read', ':', 'Semantic', 'phonological', 'contribution', 'new', 'vocabulary', 'learning', 'child', 'poor', 'reading', 'comprehension', '.']

 TOTAL LEMMATIZE WORDS ==> 18

************************************************************************************************************************

366 --> Advances in Speech  Language Pathology, 9(2), 131-139. 


 ---- TOKENS ----

 ['Advances', 'in', 'Speech', 'Language', 'Pathology', ',', '9', '(', '2', ')', ',', '131-139', '.'] 

 TOTAL TOKENS ==> 13

 ---- POST ----

 [('Advances', 'NNS'), ('in', 'IN'), ('Speech', 'NNP'), ('Language', 'NNP'), ('Pathology', 'NNP'), (',', ','), ('9', 'CD'), ('(', '('), ('2', 'CD'), (')', ')'), (',', ','), ('131-139', 'JJ'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Advances', 'Speech', 'Language', 'Pathology', ',', '9', '(', '2', ')', ',', '131-139', '.']

 TOTAL FILTERED TOKENS ==>  12

 ---- POST FOR FILTERED TOKENS ----

 [('Advances', 'NNS'), ('Speech', 'NNP'), ('Language', 'NNP'), ('Pathology', 'NNP'), (',', ','), ('9', 'CD'), ('(', '('), ('2', 'CD'), (')', ')'), (',', ','), ('131-139', 'JJ'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Advances Speech', 'Speech Language', 'Language Pathology', 'Pathology ,', ', 9', '9 (', '( 2', '2 )', ') ,', ', 131-139', '131-139 .'] 

 TOTAL BIGRAMS --> 11 



 ---- TRI-GRAMS ---- 

 ['Advances Speech Language', 'Speech Language Pathology', 'Language Pathology ,', 'Pathology , 9', ', 9 (', '9 ( 2', '( 2 )', '2 ) ,', ') , 131-139', ', 131-139 .'] 

 TOTAL TRIGRAMS --> 10 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> ['Speech Language Pathology']
 TOTAL PERSON ENTITY --> 1 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['advanc', 'speech', 'languag', 'patholog', ',', '9', '(', '2', ')', ',', '131-139', '.']

 TOTAL PORTER STEM WORDS ==> 12



 ---- SNOWBALL STEMMING ----

['advanc', 'speech', 'languag', 'patholog', ',', '9', '(', '2', ')', ',', '131-139', '.']

 TOTAL SNOWBALL STEM WORDS ==> 12



 ---- LEMMATIZATION ----

['Advances', 'Speech', 'Language', 'Pathology', ',', '9', '(', '2', ')', ',', '131-139', '.']

 TOTAL LEMMATIZE WORDS ==> 12

************************************************************************************************************************

367 --> [7] Liddy, E. D. (2001). 


 ---- TOKENS ----

 ['[', '7', ']', 'Liddy', ',', 'E.', 'D.', '(', '2001', ')', '.'] 

 TOTAL TOKENS ==> 11

 ---- POST ----

 [('[', 'RB'), ('7', 'CD'), (']', 'NN'), ('Liddy', 'NNP'), (',', ','), ('E.', 'NNP'), ('D.', 'NNP'), ('(', '('), ('2001', 'CD'), (')', ')'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['[', '7', ']', 'Liddy', ',', 'E.', 'D.', '(', '2001', ')', '.']

 TOTAL FILTERED TOKENS ==>  11

 ---- POST FOR FILTERED TOKENS ----

 [('[', 'RB'), ('7', 'CD'), (']', 'NN'), ('Liddy', 'NNP'), (',', ','), ('E.', 'NNP'), ('D.', 'NNP'), ('(', '('), ('2001', 'CD'), (')', ')'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['[ 7', '7 ]', '] Liddy', 'Liddy ,', ', E.', 'E. D.', 'D. (', '( 2001', '2001 )', ') .'] 

 TOTAL BIGRAMS --> 10 



 ---- TRI-GRAMS ---- 

 ['[ 7 ]', '7 ] Liddy', '] Liddy ,', 'Liddy , E.', ', E. D.', 'E. D. (', 'D. ( 2001', '( 2001 )', '2001 ) .'] 

 TOTAL TRIGRAMS --> 9 



 ---- NOUN PHRASES ---- 

 [']'] 

 TOTAL NOUN PHRASES --> 1 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> ['Liddy']
 TOTAL PERSON ENTITY --> 1 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['[', '7', ']', 'liddi', ',', 'e.', 'd.', '(', '2001', ')', '.']

 TOTAL PORTER STEM WORDS ==> 11



 ---- SNOWBALL STEMMING ----

['[', '7', ']', 'liddi', ',', 'e.', 'd.', '(', '2001', ')', '.']

 TOTAL SNOWBALL STEM WORDS ==> 11



 ---- LEMMATIZATION ----

['[', '7', ']', 'Liddy', ',', 'E.', 'D.', '(', '2001', ')', '.']

 TOTAL LEMMATIZE WORDS ==> 11

************************************************************************************************************************

368 --> Natural language processing. 


 ---- TOKENS ----

 ['Natural', 'language', 'processing', '.'] 

 TOTAL TOKENS ==> 4

 ---- POST ----

 [('Natural', 'JJ'), ('language', 'NN'), ('processing', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Natural', 'language', 'processing', '.']

 TOTAL FILTERED TOKENS ==>  4

 ---- POST FOR FILTERED TOKENS ----

 [('Natural', 'JJ'), ('language', 'NN'), ('processing', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Natural language', 'language processing', 'processing .'] 

 TOTAL BIGRAMS --> 3 



 ---- TRI-GRAMS ---- 

 ['Natural language processing', 'language processing .'] 

 TOTAL TRIGRAMS --> 2 



 ---- NOUN PHRASES ---- 

 ['Natural language', 'processing'] 

 TOTAL NOUN PHRASES --> 2 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['natur', 'languag', 'process', '.']

 TOTAL PORTER STEM WORDS ==> 4



 ---- SNOWBALL STEMMING ----

['natur', 'languag', 'process', '.']

 TOTAL SNOWBALL STEM WORDS ==> 4



 ---- LEMMATIZATION ----

['Natural', 'language', 'processing', '.']

 TOTAL LEMMATIZE WORDS ==> 4

************************************************************************************************************************

369 --> [8] Feldman, S. (1999). 


 ---- TOKENS ----

 ['[', '8', ']', 'Feldman', ',', 'S.', '(', '1999', ')', '.'] 

 TOTAL TOKENS ==> 10

 ---- POST ----

 [('[', 'RB'), ('8', 'CD'), (']', 'JJ'), ('Feldman', 'NNP'), (',', ','), ('S.', 'NNP'), ('(', '('), ('1999', 'CD'), (')', ')'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['[', '8', ']', 'Feldman', ',', 'S.', '(', '1999', ')', '.']

 TOTAL FILTERED TOKENS ==>  10

 ---- POST FOR FILTERED TOKENS ----

 [('[', 'RB'), ('8', 'CD'), (']', 'JJ'), ('Feldman', 'NNP'), (',', ','), ('S.', 'NNP'), ('(', '('), ('1999', 'CD'), (')', ')'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['[ 8', '8 ]', '] Feldman', 'Feldman ,', ', S.', 'S. (', '( 1999', '1999 )', ') .'] 

 TOTAL BIGRAMS --> 9 



 ---- TRI-GRAMS ---- 

 ['[ 8 ]', '8 ] Feldman', '] Feldman ,', 'Feldman , S.', ', S. (', 'S. ( 1999', '( 1999 )', '1999 ) .'] 

 TOTAL TRIGRAMS --> 8 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> ['Feldman']
 TOTAL PERSON ENTITY --> 1 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['[', '8', ']', 'feldman', ',', 's.', '(', '1999', ')', '.']

 TOTAL PORTER STEM WORDS ==> 10



 ---- SNOWBALL STEMMING ----

['[', '8', ']', 'feldman', ',', 's.', '(', '1999', ')', '.']

 TOTAL SNOWBALL STEM WORDS ==> 10



 ---- LEMMATIZATION ----

['[', '8', ']', 'Feldman', ',', 'S.', '(', '1999', ')', '.']

 TOTAL LEMMATIZE WORDS ==> 10

************************************************************************************************************************

370 --> NLP Meets the Jabberwocky: Natural Language Processing in  Information Retrieval. 


 ---- TOKENS ----

 ['NLP', 'Meets', 'the', 'Jabberwocky', ':', 'Natural', 'Language', 'Processing', 'in', 'Information', 'Retrieval', '.'] 

 TOTAL TOKENS ==> 12

 ---- POST ----

 [('NLP', 'NNP'), ('Meets', 'VBZ'), ('the', 'DT'), ('Jabberwocky', 'NNP'), (':', ':'), ('Natural', 'JJ'), ('Language', 'NNP'), ('Processing', 'NNP'), ('in', 'IN'), ('Information', 'NNP'), ('Retrieval', 'NNP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['NLP', 'Meets', 'Jabberwocky', ':', 'Natural', 'Language', 'Processing', 'Information', 'Retrieval', '.']

 TOTAL FILTERED TOKENS ==>  10

 ---- POST FOR FILTERED TOKENS ----

 [('NLP', 'NNP'), ('Meets', 'NNP'), ('Jabberwocky', 'NNP'), (':', ':'), ('Natural', 'JJ'), ('Language', 'NNP'), ('Processing', 'NNP'), ('Information', 'NNP'), ('Retrieval', 'NNP'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['NLP Meets', 'Meets Jabberwocky', 'Jabberwocky :', ': Natural', 'Natural Language', 'Language Processing', 'Processing Information', 'Information Retrieval', 'Retrieval .'] 

 TOTAL BIGRAMS --> 9 



 ---- TRI-GRAMS ---- 

 ['NLP Meets Jabberwocky', 'Meets Jabberwocky :', 'Jabberwocky : Natural', ': Natural Language', 'Natural Language Processing', 'Language Processing Information', 'Processing Information Retrieval', 'Information Retrieval .'] 

 TOTAL TRIGRAMS --> 8 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> ['NLP']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> ['Meets Jabberwocky']
 TOTAL PERSON ENTITY --> 1 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['nlp', 'meet', 'jabberwocki', ':', 'natur', 'languag', 'process', 'inform', 'retriev', '.']

 TOTAL PORTER STEM WORDS ==> 10



 ---- SNOWBALL STEMMING ----

['nlp', 'meet', 'jabberwocki', ':', 'natur', 'languag', 'process', 'inform', 'retriev', '.']

 TOTAL SNOWBALL STEM WORDS ==> 10



 ---- LEMMATIZATION ----

['NLP', 'Meets', 'Jabberwocky', ':', 'Natural', 'Language', 'Processing', 'Information', 'Retrieval', '.']

 TOTAL LEMMATIZE WORDS ==> 10

************************************************************************************************************************

371 --> ONLINE-WESTON THEN WILTON-, 23, 62-73. 


 ---- TOKENS ----

 ['ONLINE-WESTON', 'THEN', 'WILTON-', ',', '23', ',', '62-73', '.'] 

 TOTAL TOKENS ==> 8

 ---- POST ----

 [('ONLINE-WESTON', 'JJ'), ('THEN', 'NNP'), ('WILTON-', 'NNP'), (',', ','), ('23', 'CD'), (',', ','), ('62-73', 'JJ'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['ONLINE-WESTON', 'WILTON-', ',', '23', ',', '62-73', '.']

 TOTAL FILTERED TOKENS ==>  7

 ---- POST FOR FILTERED TOKENS ----

 [('ONLINE-WESTON', 'JJ'), ('WILTON-', 'NNP'), (',', ','), ('23', 'CD'), (',', ','), ('62-73', 'JJ'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['ONLINE-WESTON WILTON-', 'WILTON- ,', ', 23', '23 ,', ', 62-73', '62-73 .'] 

 TOTAL BIGRAMS --> 6 



 ---- TRI-GRAMS ---- 

 ['ONLINE-WESTON WILTON- ,', 'WILTON- , 23', ', 23 ,', '23 , 62-73', ', 62-73 .'] 

 TOTAL TRIGRAMS --> 5 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['online-weston', 'wilton-', ',', '23', ',', '62-73', '.']

 TOTAL PORTER STEM WORDS ==> 7



 ---- SNOWBALL STEMMING ----

['online-weston', 'wilton-', ',', '23', ',', '62-73', '.']

 TOTAL SNOWBALL STEM WORDS ==> 7



 ---- LEMMATIZATION ----

['ONLINE-WESTON', 'WILTON-', ',', '23', ',', '62-73', '.']

 TOTAL LEMMATIZE WORDS ==> 7

************************************************************************************************************************

372 --> [9] "Natural Language Processing." 


 ---- TOKENS ----

 ['[', '9', ']', '``', 'Natural', 'Language', 'Processing', '.', "''"] 

 TOTAL TOKENS ==> 9

 ---- POST ----

 [('[', 'RB'), ('9', 'CD'), (']', 'NN'), ('``', '``'), ('Natural', 'JJ'), ('Language', 'NN'), ('Processing', 'NN'), ('.', '.'), ("''", "''")] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['[', '9', ']', '``', 'Natural', 'Language', 'Processing', '.', "''"]

 TOTAL FILTERED TOKENS ==>  9

 ---- POST FOR FILTERED TOKENS ----

 [('[', 'RB'), ('9', 'CD'), (']', 'NN'), ('``', '``'), ('Natural', 'JJ'), ('Language', 'NN'), ('Processing', 'NN'), ('.', '.'), ("''", "''")] 



 ---- BI-GRAMS ---- 

 ['[ 9', '9 ]', '] ``', '`` Natural', 'Natural Language', 'Language Processing', 'Processing .', ". ''"] 

 TOTAL BIGRAMS --> 8 



 ---- TRI-GRAMS ---- 

 ['[ 9 ]', '9 ] ``', '] `` Natural', '`` Natural Language', 'Natural Language Processing', 'Language Processing .', "Processing . ''"] 

 TOTAL TRIGRAMS --> 7 



 ---- NOUN PHRASES ---- 

 [']', 'Natural Language', 'Processing'] 

 TOTAL NOUN PHRASES --> 3 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['[', '9', ']', '``', 'natur', 'languag', 'process', '.', "''"]

 TOTAL PORTER STEM WORDS ==> 9



 ---- SNOWBALL STEMMING ----

['[', '9', ']', '``', 'natur', 'languag', 'process', '.', "''"]

 TOTAL SNOWBALL STEM WORDS ==> 9



 ---- LEMMATIZATION ----

['[', '9', ']', '``', 'Natural', 'Language', 'Processing', '.', "''"]

 TOTAL LEMMATIZE WORDS ==> 9

************************************************************************************************************************

373 --> Natural Language Processing RSS. 


 ---- TOKENS ----

 ['Natural', 'Language', 'Processing', 'RSS', '.'] 

 TOTAL TOKENS ==> 5

 ---- POST ----

 [('Natural', 'JJ'), ('Language', 'NNP'), ('Processing', 'NNP'), ('RSS', 'NNP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Natural', 'Language', 'Processing', 'RSS', '.']

 TOTAL FILTERED TOKENS ==>  5

 ---- POST FOR FILTERED TOKENS ----

 [('Natural', 'JJ'), ('Language', 'NNP'), ('Processing', 'NNP'), ('RSS', 'NNP'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Natural Language', 'Language Processing', 'Processing RSS', 'RSS .'] 

 TOTAL BIGRAMS --> 4 



 ---- TRI-GRAMS ---- 

 ['Natural Language Processing', 'Language Processing RSS', 'Processing RSS .'] 

 TOTAL TRIGRAMS --> 3 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['natur', 'languag', 'process', 'rss', '.']

 TOTAL PORTER STEM WORDS ==> 5



 ---- SNOWBALL STEMMING ----

['natur', 'languag', 'process', 'rss', '.']

 TOTAL SNOWBALL STEM WORDS ==> 5



 ---- LEMMATIZATION ----

['Natural', 'Language', 'Processing', 'RSS', '.']

 TOTAL LEMMATIZE WORDS ==> 5

************************************************************************************************************************

374 --> N.p., n.d. 


 ---- TOKENS ----

 ['N.p.', ',', 'n.d', '.'] 

 TOTAL TOKENS ==> 4

 ---- POST ----

 [('N.p.', 'NNP'), (',', ','), ('n.d', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['N.p.', ',', 'n.d', '.']

 TOTAL FILTERED TOKENS ==>  4

 ---- POST FOR FILTERED TOKENS ----

 [('N.p.', 'NNP'), (',', ','), ('n.d', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['N.p. ,', ', n.d', 'n.d .'] 

 TOTAL BIGRAMS --> 3 



 ---- TRI-GRAMS ---- 

 ['N.p. , n.d', ', n.d .'] 

 TOTAL TRIGRAMS --> 2 



 ---- NOUN PHRASES ---- 

 ['n.d'] 

 TOTAL NOUN PHRASES --> 1 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['n.p.', ',', 'n.d', '.']

 TOTAL PORTER STEM WORDS ==> 4



 ---- SNOWBALL STEMMING ----

['n.p.', ',', 'n.d', '.']

 TOTAL SNOWBALL STEM WORDS ==> 4



 ---- LEMMATIZATION ----

['N.p.', ',', 'n.d', '.']

 TOTAL LEMMATIZE WORDS ==> 4

************************************************************************************************************************

375 --> Web. 


 ---- TOKENS ----

 ['Web', '.'] 

 TOTAL TOKENS ==> 2

 ---- POST ----

 [('Web', 'NNP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Web', '.']

 TOTAL FILTERED TOKENS ==>  2

 ---- POST FOR FILTERED TOKENS ----

 [('Web', 'NNP'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Web .'] 

 TOTAL BIGRAMS --> 1 



 ---- TRI-GRAMS ---- 

 [] 

 TOTAL TRIGRAMS --> 0 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['web', '.']

 TOTAL PORTER STEM WORDS ==> 2



 ---- SNOWBALL STEMMING ----

['web', '.']

 TOTAL SNOWBALL STEM WORDS ==> 2



 ---- LEMMATIZATION ----

['Web', '.']

 TOTAL LEMMATIZE WORDS ==> 2

************************************************************************************************************************

376 --> 25  Mar. 


 ---- TOKENS ----

 ['25', 'Mar', '.'] 

 TOTAL TOKENS ==> 3

 ---- POST ----

 [('25', 'CD'), ('Mar', 'NNP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['25', 'Mar', '.']

 TOTAL FILTERED TOKENS ==>  3

 ---- POST FOR FILTERED TOKENS ----

 [('25', 'CD'), ('Mar', 'NNP'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['25 Mar', 'Mar .'] 

 TOTAL BIGRAMS --> 2 



 ---- TRI-GRAMS ---- 

 ['25 Mar .'] 

 TOTAL TRIGRAMS --> 1 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['25', 'mar', '.']

 TOTAL PORTER STEM WORDS ==> 3



 ---- SNOWBALL STEMMING ----

['25', 'mar', '.']

 TOTAL SNOWBALL STEM WORDS ==> 3



 ---- LEMMATIZATION ----

['25', 'Mar', '.']

 TOTAL LEMMATIZE WORDS ==> 3

************************************************************************************************************************

377 --> 2017  [10] Hutchins, W. J. 


 ---- TOKENS ----

 ['2017', '[', '10', ']', 'Hutchins', ',', 'W.', 'J', '.'] 

 TOTAL TOKENS ==> 9

 ---- POST ----

 [('2017', 'CD'), ('[', '$'), ('10', 'CD'), (']', 'NN'), ('Hutchins', 'NNP'), (',', ','), ('W.', 'NNP'), ('J', 'NNP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['2017', '[', '10', ']', 'Hutchins', ',', 'W.', 'J', '.']

 TOTAL FILTERED TOKENS ==>  9

 ---- POST FOR FILTERED TOKENS ----

 [('2017', 'CD'), ('[', '$'), ('10', 'CD'), (']', 'NN'), ('Hutchins', 'NNP'), (',', ','), ('W.', 'NNP'), ('J', 'NNP'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['2017 [', '[ 10', '10 ]', '] Hutchins', 'Hutchins ,', ', W.', 'W. J', 'J .'] 

 TOTAL BIGRAMS --> 8 



 ---- TRI-GRAMS ---- 

 ['2017 [ 10', '[ 10 ]', '10 ] Hutchins', '] Hutchins ,', 'Hutchins , W.', ', W. J', 'W. J .'] 

 TOTAL TRIGRAMS --> 7 



 ---- NOUN PHRASES ---- 

 [']'] 

 TOTAL NOUN PHRASES --> 1 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> ['Hutchins']
 TOTAL PERSON ENTITY --> 1 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['2017', '[', '10', ']', 'hutchin', ',', 'w.', 'j', '.']

 TOTAL PORTER STEM WORDS ==> 9



 ---- SNOWBALL STEMMING ----

['2017', '[', '10', ']', 'hutchin', ',', 'w.', 'j', '.']

 TOTAL SNOWBALL STEM WORDS ==> 9



 ---- LEMMATIZATION ----

['2017', '[', '10', ']', 'Hutchins', ',', 'W.', 'J', '.']

 TOTAL LEMMATIZE WORDS ==> 9

************************************************************************************************************************

378 --> (1986). 


 ---- TOKENS ----

 ['(', '1986', ')', '.'] 

 TOTAL TOKENS ==> 4

 ---- POST ----

 [('(', '('), ('1986', 'CD'), (')', ')'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['(', '1986', ')', '.']

 TOTAL FILTERED TOKENS ==>  4

 ---- POST FOR FILTERED TOKENS ----

 [('(', '('), ('1986', 'CD'), (')', ')'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['( 1986', '1986 )', ') .'] 

 TOTAL BIGRAMS --> 3 



 ---- TRI-GRAMS ---- 

 ['( 1986 )', '1986 ) .'] 

 TOTAL TRIGRAMS --> 2 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['(', '1986', ')', '.']

 TOTAL PORTER STEM WORDS ==> 4



 ---- SNOWBALL STEMMING ----

['(', '1986', ')', '.']

 TOTAL SNOWBALL STEM WORDS ==> 4



 ---- LEMMATIZATION ----

['(', '1986', ')', '.']

 TOTAL LEMMATIZE WORDS ==> 4

************************************************************************************************************************

379 --> Machine translation: past, present, future (p. 66). 


 ---- TOKENS ----

 ['Machine', 'translation', ':', 'past', ',', 'present', ',', 'future', '(', 'p.', '66', ')', '.'] 

 TOTAL TOKENS ==> 13

 ---- POST ----

 [('Machine', 'NN'), ('translation', 'NN'), (':', ':'), ('past', 'NN'), (',', ','), ('present', 'NN'), (',', ','), ('future', 'NN'), ('(', '('), ('p.', 'JJ'), ('66', 'CD'), (')', ')'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Machine', 'translation', ':', 'past', ',', 'present', ',', 'future', '(', 'p.', '66', ')', '.']

 TOTAL FILTERED TOKENS ==>  13

 ---- POST FOR FILTERED TOKENS ----

 [('Machine', 'NN'), ('translation', 'NN'), (':', ':'), ('past', 'NN'), (',', ','), ('present', 'NN'), (',', ','), ('future', 'NN'), ('(', '('), ('p.', 'JJ'), ('66', 'CD'), (')', ')'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Machine translation', 'translation :', ': past', 'past ,', ', present', 'present ,', ', future', 'future (', '( p.', 'p. 66', '66 )', ') .'] 

 TOTAL BIGRAMS --> 12 



 ---- TRI-GRAMS ---- 

 ['Machine translation :', 'translation : past', ': past ,', 'past , present', ', present ,', 'present , future', ', future (', 'future ( p.', '( p. 66', 'p. 66 )', '66 ) .'] 

 TOTAL TRIGRAMS --> 11 



 ---- NOUN PHRASES ---- 

 ['Machine', 'translation', 'past', 'present', 'future'] 

 TOTAL NOUN PHRASES --> 5 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> ['Machine']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['machin', 'translat', ':', 'past', ',', 'present', ',', 'futur', '(', 'p.', '66', ')', '.']

 TOTAL PORTER STEM WORDS ==> 13



 ---- SNOWBALL STEMMING ----

['machin', 'translat', ':', 'past', ',', 'present', ',', 'futur', '(', 'p.', '66', ')', '.']

 TOTAL SNOWBALL STEM WORDS ==> 13



 ---- LEMMATIZATION ----

['Machine', 'translation', ':', 'past', ',', 'present', ',', 'future', '(', 'p.', '66', ')', '.']

 TOTAL LEMMATIZE WORDS ==> 13

************************************************************************************************************************

380 --> Chichester:  Ellis Horwood. 


 ---- TOKENS ----

 ['Chichester', ':', 'Ellis', 'Horwood', '.'] 

 TOTAL TOKENS ==> 5

 ---- POST ----

 [('Chichester', 'NN'), (':', ':'), ('Ellis', 'NNP'), ('Horwood', 'NNP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Chichester', ':', 'Ellis', 'Horwood', '.']

 TOTAL FILTERED TOKENS ==>  5

 ---- POST FOR FILTERED TOKENS ----

 [('Chichester', 'NN'), (':', ':'), ('Ellis', 'NNP'), ('Horwood', 'NNP'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Chichester :', ': Ellis', 'Ellis Horwood', 'Horwood .'] 

 TOTAL BIGRAMS --> 4 



 ---- TRI-GRAMS ---- 

 ['Chichester : Ellis', ': Ellis Horwood', 'Ellis Horwood .'] 

 TOTAL TRIGRAMS --> 3 



 ---- NOUN PHRASES ---- 

 ['Chichester'] 

 TOTAL NOUN PHRASES --> 1 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> ['Ellis Horwood']
 TOTAL PERSON ENTITY --> 1 


 GPE ---> ['Chichester']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['chichest', ':', 'elli', 'horwood', '.']

 TOTAL PORTER STEM WORDS ==> 5



 ---- SNOWBALL STEMMING ----

['chichest', ':', 'elli', 'horwood', '.']

 TOTAL SNOWBALL STEM WORDS ==> 5



 ---- LEMMATIZATION ----

['Chichester', ':', 'Ellis', 'Horwood', '.']

 TOTAL LEMMATIZE WORDS ==> 5

************************************************************************************************************************

381 --> [11] Hutchins, W. J. 


 ---- TOKENS ----

 ['[', '11', ']', 'Hutchins', ',', 'W.', 'J', '.'] 

 TOTAL TOKENS ==> 8

 ---- POST ----

 [('[', 'RB'), ('11', 'CD'), (']', 'NN'), ('Hutchins', 'NNP'), (',', ','), ('W.', 'NNP'), ('J', 'NNP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['[', '11', ']', 'Hutchins', ',', 'W.', 'J', '.']

 TOTAL FILTERED TOKENS ==>  8

 ---- POST FOR FILTERED TOKENS ----

 [('[', 'RB'), ('11', 'CD'), (']', 'NN'), ('Hutchins', 'NNP'), (',', ','), ('W.', 'NNP'), ('J', 'NNP'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['[ 11', '11 ]', '] Hutchins', 'Hutchins ,', ', W.', 'W. J', 'J .'] 

 TOTAL BIGRAMS --> 7 



 ---- TRI-GRAMS ---- 

 ['[ 11 ]', '11 ] Hutchins', '] Hutchins ,', 'Hutchins , W.', ', W. J', 'W. J .'] 

 TOTAL TRIGRAMS --> 6 



 ---- NOUN PHRASES ---- 

 [']'] 

 TOTAL NOUN PHRASES --> 1 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> ['Hutchins']
 TOTAL PERSON ENTITY --> 1 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['[', '11', ']', 'hutchin', ',', 'w.', 'j', '.']

 TOTAL PORTER STEM WORDS ==> 8



 ---- SNOWBALL STEMMING ----

['[', '11', ']', 'hutchin', ',', 'w.', 'j', '.']

 TOTAL SNOWBALL STEM WORDS ==> 8



 ---- LEMMATIZATION ----

['[', '11', ']', 'Hutchins', ',', 'W.', 'J', '.']

 TOTAL LEMMATIZE WORDS ==> 8

************************************************************************************************************************

382 --> (Ed.). 


 ---- TOKENS ----

 ['(', 'Ed', '.', ')', '.'] 

 TOTAL TOKENS ==> 5

 ---- POST ----

 [('(', '('), ('Ed', 'NNP'), ('.', '.'), (')', ')'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['(', 'Ed', '.', ')', '.']

 TOTAL FILTERED TOKENS ==>  5

 ---- POST FOR FILTERED TOKENS ----

 [('(', '('), ('Ed', 'NNP'), ('.', '.'), (')', ')'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['( Ed', 'Ed .', '. )', ') .'] 

 TOTAL BIGRAMS --> 4 



 ---- TRI-GRAMS ---- 

 ['( Ed .', 'Ed . )', '. ) .'] 

 TOTAL TRIGRAMS --> 3 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['(', 'ed', '.', ')', '.']

 TOTAL PORTER STEM WORDS ==> 5



 ---- SNOWBALL STEMMING ----

['(', 'ed', '.', ')', '.']

 TOTAL SNOWBALL STEM WORDS ==> 5



 ---- LEMMATIZATION ----

['(', 'Ed', '.', ')', '.']

 TOTAL LEMMATIZE WORDS ==> 5

************************************************************************************************************************

383 --> (2000). 


 ---- TOKENS ----

 ['(', '2000', ')', '.'] 

 TOTAL TOKENS ==> 4

 ---- POST ----

 [('(', '('), ('2000', 'CD'), (')', ')'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['(', '2000', ')', '.']

 TOTAL FILTERED TOKENS ==>  4

 ---- POST FOR FILTERED TOKENS ----

 [('(', '('), ('2000', 'CD'), (')', ')'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['( 2000', '2000 )', ') .'] 

 TOTAL BIGRAMS --> 3 



 ---- TRI-GRAMS ---- 

 ['( 2000 )', '2000 ) .'] 

 TOTAL TRIGRAMS --> 2 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['(', '2000', ')', '.']

 TOTAL PORTER STEM WORDS ==> 4



 ---- SNOWBALL STEMMING ----

['(', '2000', ')', '.']

 TOTAL SNOWBALL STEM WORDS ==> 4



 ---- LEMMATIZATION ----

['(', '2000', ')', '.']

 TOTAL LEMMATIZE WORDS ==> 4

************************************************************************************************************************

384 --> Early years in machine translation: memoirs and  biographies of pioneers (Vol. 


 ---- TOKENS ----

 ['Early', 'years', 'in', 'machine', 'translation', ':', 'memoirs', 'and', 'biographies', 'of', 'pioneers', '(', 'Vol', '.'] 

 TOTAL TOKENS ==> 14

 ---- POST ----

 [('Early', 'JJ'), ('years', 'NNS'), ('in', 'IN'), ('machine', 'NN'), ('translation', 'NN'), (':', ':'), ('memoirs', 'NNS'), ('and', 'CC'), ('biographies', 'NNS'), ('of', 'IN'), ('pioneers', 'NNS'), ('(', '('), ('Vol', 'NNP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Early', 'years', 'machine', 'translation', ':', 'memoirs', 'biographies', 'pioneers', '(', 'Vol', '.']

 TOTAL FILTERED TOKENS ==>  11

 ---- POST FOR FILTERED TOKENS ----

 [('Early', 'JJ'), ('years', 'NNS'), ('machine', 'NN'), ('translation', 'NN'), (':', ':'), ('memoirs', 'NN'), ('biographies', 'NNS'), ('pioneers', 'NNS'), ('(', '('), ('Vol', 'NNP'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Early years', 'years machine', 'machine translation', 'translation :', ': memoirs', 'memoirs biographies', 'biographies pioneers', 'pioneers (', '( Vol', 'Vol .'] 

 TOTAL BIGRAMS --> 10 



 ---- TRI-GRAMS ---- 

 ['Early years machine', 'years machine translation', 'machine translation :', 'translation : memoirs', ': memoirs biographies', 'memoirs biographies pioneers', 'biographies pioneers (', 'pioneers ( Vol', '( Vol .'] 

 TOTAL TRIGRAMS --> 9 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 []

 ---- PORTER STEMMING ----

['earli', 'year', 'machin', 'translat', ':', 'memoir', 'biographi', 'pioneer', '(', 'vol', '.']

 TOTAL PORTER STEM WORDS ==> 11



 ---- SNOWBALL STEMMING ----

['earli', 'year', 'machin', 'translat', ':', 'memoir', 'biographi', 'pioneer', '(', 'vol', '.']

 TOTAL SNOWBALL STEM WORDS ==> 11



 ---- LEMMATIZATION ----

['Early', 'year', 'machine', 'translation', ':', 'memoir', 'biography', 'pioneer', '(', 'Vol', '.']

 TOTAL LEMMATIZE WORDS ==> 11

************************************************************************************************************************

385 --> 97). 


 ---- TOKENS ----

 ['97', ')', '.'] 

 TOTAL TOKENS ==> 3

 ---- POST ----

 [('97', 'CD'), (')', ')'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['97', ')', '.']

 TOTAL FILTERED TOKENS ==>  3

 ---- POST FOR FILTERED TOKENS ----

 [('97', 'CD'), (')', ')'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['97 )', ') .'] 

 TOTAL BIGRAMS --> 2 



 ---- TRI-GRAMS ---- 

 ['97 ) .'] 

 TOTAL TRIGRAMS --> 1 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 []

 ---- PORTER STEMMING ----

['97', ')', '.']

 TOTAL PORTER STEM WORDS ==> 3



 ---- SNOWBALL STEMMING ----

['97', ')', '.']

 TOTAL SNOWBALL STEM WORDS ==> 3



 ---- LEMMATIZATION ----

['97', ')', '.']

 TOTAL LEMMATIZE WORDS ==> 3

************************************************************************************************************************

386 --> John Benjamins Publishing. 


 ---- TOKENS ----

 ['John', 'Benjamins', 'Publishing', '.'] 

 TOTAL TOKENS ==> 4

 ---- POST ----

 [('John', 'NNP'), ('Benjamins', 'NNP'), ('Publishing', 'NNP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['John', 'Benjamins', 'Publishing', '.']

 TOTAL FILTERED TOKENS ==>  4

 ---- POST FOR FILTERED TOKENS ----

 [('John', 'NNP'), ('Benjamins', 'NNP'), ('Publishing', 'NNP'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['John Benjamins', 'Benjamins Publishing', 'Publishing .'] 

 TOTAL BIGRAMS --> 3 



 ---- TRI-GRAMS ---- 

 ['John Benjamins Publishing', 'Benjamins Publishing .'] 

 TOTAL TRIGRAMS --> 2 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> ['John', 'Benjamins Publishing']
 TOTAL PERSON ENTITY --> 2 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['john', 'benjamin', 'publish', '.']

 TOTAL PORTER STEM WORDS ==> 4



 ---- SNOWBALL STEMMING ----

['john', 'benjamin', 'publish', '.']

 TOTAL SNOWBALL STEM WORDS ==> 4



 ---- LEMMATIZATION ----

['John', 'Benjamins', 'Publishing', '.']

 TOTAL LEMMATIZE WORDS ==> 4

************************************************************************************************************************

387 --> [12] Green Jr, B. F., Wolf, A. K., Chomsky, C., & Laughery, K. (1961, May). 


 ---- TOKENS ----

 ['[', '12', ']', 'Green', 'Jr', ',', 'B.', 'F.', ',', 'Wolf', ',', 'A.', 'K.', ',', 'Chomsky', ',', 'C.', ',', '&', 'Laughery', ',', 'K.', '(', '1961', ',', 'May', ')', '.'] 

 TOTAL TOKENS ==> 28

 ---- POST ----

 [('[', 'RB'), ('12', 'CD'), (']', 'NNS'), ('Green', 'NNP'), ('Jr', 'NNP'), (',', ','), ('B.', 'NNP'), ('F.', 'NNP'), (',', ','), ('Wolf', 'NNP'), (',', ','), ('A.', 'NNP'), ('K.', 'NNP'), (',', ','), ('Chomsky', 'NNP'), (',', ','), ('C.', 'NNP'), (',', ','), ('&', 'CC'), ('Laughery', 'NNP'), (',', ','), ('K.', 'NNP'), ('(', '('), ('1961', 'CD'), (',', ','), ('May', 'NNP'), (')', ')'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['[', '12', ']', 'Green', 'Jr', ',', 'B.', 'F.', ',', 'Wolf', ',', 'A.', 'K.', ',', 'Chomsky', ',', 'C.', ',', '&', 'Laughery', ',', 'K.', '(', '1961', ',', 'May', ')', '.']

 TOTAL FILTERED TOKENS ==>  28

 ---- POST FOR FILTERED TOKENS ----

 [('[', 'RB'), ('12', 'CD'), (']', 'NNS'), ('Green', 'NNP'), ('Jr', 'NNP'), (',', ','), ('B.', 'NNP'), ('F.', 'NNP'), (',', ','), ('Wolf', 'NNP'), (',', ','), ('A.', 'NNP'), ('K.', 'NNP'), (',', ','), ('Chomsky', 'NNP'), (',', ','), ('C.', 'NNP'), (',', ','), ('&', 'CC'), ('Laughery', 'NNP'), (',', ','), ('K.', 'NNP'), ('(', '('), ('1961', 'CD'), (',', ','), ('May', 'NNP'), (')', ')'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['[ 12', '12 ]', '] Green', 'Green Jr', 'Jr ,', ', B.', 'B. F.', 'F. ,', ', Wolf', 'Wolf ,', ', A.', 'A. K.', 'K. ,', ', Chomsky', 'Chomsky ,', ', C.', 'C. ,', ', &', '& Laughery', 'Laughery ,', ', K.', 'K. (', '( 1961', '1961 ,', ', May', 'May )', ') .'] 

 TOTAL BIGRAMS --> 27 



 ---- TRI-GRAMS ---- 

 ['[ 12 ]', '12 ] Green', '] Green Jr', 'Green Jr ,', 'Jr , B.', ', B. F.', 'B. F. ,', 'F. , Wolf', ', Wolf ,', 'Wolf , A.', ', A. K.', 'A. K. ,', 'K. , Chomsky', ', Chomsky ,', 'Chomsky , C.', ', C. ,', 'C. , &', ', & Laughery', '& Laughery ,', 'Laughery , K.', ', K. (', 'K. ( 1961', '( 1961 ,', '1961 , May', ', May )', 'May ) .'] 

 TOTAL TRIGRAMS --> 26 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> ['Green Jr', 'Wolf', 'Chomsky', 'Laughery']
 TOTAL PERSON ENTITY --> 4 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['[', '12', ']', 'green', 'jr', ',', 'b.', 'f.', ',', 'wolf', ',', 'a.', 'k.', ',', 'chomski', ',', 'c.', ',', '&', 'laugheri', ',', 'k.', '(', '1961', ',', 'may', ')', '.']

 TOTAL PORTER STEM WORDS ==> 28



 ---- SNOWBALL STEMMING ----

['[', '12', ']', 'green', 'jr', ',', 'b.', 'f.', ',', 'wolf', ',', 'a.', 'k.', ',', 'chomski', ',', 'c.', ',', '&', 'laugheri', ',', 'k.', '(', '1961', ',', 'may', ')', '.']

 TOTAL SNOWBALL STEM WORDS ==> 28



 ---- LEMMATIZATION ----

['[', '12', ']', 'Green', 'Jr', ',', 'B.', 'F.', ',', 'Wolf', ',', 'A.', 'K.', ',', 'Chomsky', ',', 'C.', ',', '&', 'Laughery', ',', 'K.', '(', '1961', ',', 'May', ')', '.']

 TOTAL LEMMATIZE WORDS ==> 28

************************************************************************************************************************

388 --> Baseball: an  automatic question-answerer. 


 ---- TOKENS ----

 ['Baseball', ':', 'an', 'automatic', 'question-answerer', '.'] 

 TOTAL TOKENS ==> 6

 ---- POST ----

 [('Baseball', 'NN'), (':', ':'), ('an', 'DT'), ('automatic', 'JJ'), ('question-answerer', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Baseball', ':', 'automatic', 'question-answerer', '.']

 TOTAL FILTERED TOKENS ==>  5

 ---- POST FOR FILTERED TOKENS ----

 [('Baseball', 'NN'), (':', ':'), ('automatic', 'JJ'), ('question-answerer', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Baseball :', ': automatic', 'automatic question-answerer', 'question-answerer .'] 

 TOTAL BIGRAMS --> 4 



 ---- TRI-GRAMS ---- 

 ['Baseball : automatic', ': automatic question-answerer', 'automatic question-answerer .'] 

 TOTAL TRIGRAMS --> 3 



 ---- NOUN PHRASES ---- 

 ['Baseball', 'automatic question-answerer'] 

 TOTAL NOUN PHRASES --> 2 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> ['Baseball']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['basebal', ':', 'automat', 'question-answer', '.']

 TOTAL PORTER STEM WORDS ==> 5



 ---- SNOWBALL STEMMING ----

['basebal', ':', 'automat', 'question-answer', '.']

 TOTAL SNOWBALL STEM WORDS ==> 5



 ---- LEMMATIZATION ----

['Baseball', ':', 'automatic', 'question-answerer', '.']

 TOTAL LEMMATIZE WORDS ==> 5

************************************************************************************************************************

389 --> In Papers presented at the May 9-11, 1961, western joint IRE- AIEE-ACM computer conference (pp. 


 ---- TOKENS ----

 ['In', 'Papers', 'presented', 'at', 'the', 'May', '9-11', ',', '1961', ',', 'western', 'joint', 'IRE-', 'AIEE-ACM', 'computer', 'conference', '(', 'pp', '.'] 

 TOTAL TOKENS ==> 19

 ---- POST ----

 [('In', 'IN'), ('Papers', 'NNP'), ('presented', 'VBD'), ('at', 'IN'), ('the', 'DT'), ('May', 'NNP'), ('9-11', 'CD'), (',', ','), ('1961', 'CD'), (',', ','), ('western', 'JJ'), ('joint', 'JJ'), ('IRE-', 'NNP'), ('AIEE-ACM', 'NNP'), ('computer', 'NN'), ('conference', 'NN'), ('(', '('), ('pp', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Papers', 'presented', 'May', '9-11', ',', '1961', ',', 'western', 'joint', 'IRE-', 'AIEE-ACM', 'computer', 'conference', '(', 'pp', '.']

 TOTAL FILTERED TOKENS ==>  16

 ---- POST FOR FILTERED TOKENS ----

 [('Papers', 'NNS'), ('presented', 'VBD'), ('May', 'NNP'), ('9-11', 'CD'), (',', ','), ('1961', 'CD'), (',', ','), ('western', 'JJ'), ('joint', 'JJ'), ('IRE-', 'NNP'), ('AIEE-ACM', 'NNP'), ('computer', 'NN'), ('conference', 'NN'), ('(', '('), ('pp', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Papers presented', 'presented May', 'May 9-11', '9-11 ,', ', 1961', '1961 ,', ', western', 'western joint', 'joint IRE-', 'IRE- AIEE-ACM', 'AIEE-ACM computer', 'computer conference', 'conference (', '( pp', 'pp .'] 

 TOTAL BIGRAMS --> 15 



 ---- TRI-GRAMS ---- 

 ['Papers presented May', 'presented May 9-11', 'May 9-11 ,', '9-11 , 1961', ', 1961 ,', '1961 , western', ', western joint', 'western joint IRE-', 'joint IRE- AIEE-ACM', 'IRE- AIEE-ACM computer', 'AIEE-ACM computer conference', 'computer conference (', 'conference ( pp', '( pp .'] 

 TOTAL TRIGRAMS --> 14 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 []

 ---- PORTER STEMMING ----

['paper', 'present', 'may', '9-11', ',', '1961', ',', 'western', 'joint', 'ire-', 'aiee-acm', 'comput', 'confer', '(', 'pp', '.']

 TOTAL PORTER STEM WORDS ==> 16



 ---- SNOWBALL STEMMING ----

['paper', 'present', 'may', '9-11', ',', '1961', ',', 'western', 'joint', 'ire-', 'aiee-acm', 'comput', 'confer', '(', 'pp', '.']

 TOTAL SNOWBALL STEM WORDS ==> 16



 ---- LEMMATIZATION ----

['Papers', 'presented', 'May', '9-11', ',', '1961', ',', 'western', 'joint', 'IRE-', 'AIEE-ACM', 'computer', 'conference', '(', 'pp', '.']

 TOTAL LEMMATIZE WORDS ==> 16

************************************************************************************************************************

390 --> 219-224). 


 ---- TOKENS ----

 ['219-224', ')', '.'] 

 TOTAL TOKENS ==> 3

 ---- POST ----

 [('219-224', 'JJ'), (')', ')'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['219-224', ')', '.']

 TOTAL FILTERED TOKENS ==>  3

 ---- POST FOR FILTERED TOKENS ----

 [('219-224', 'JJ'), (')', ')'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['219-224 )', ') .'] 

 TOTAL BIGRAMS --> 2 



 ---- TRI-GRAMS ---- 

 ['219-224 ) .'] 

 TOTAL TRIGRAMS --> 1 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 []

 ---- PORTER STEMMING ----

['219-224', ')', '.']

 TOTAL PORTER STEM WORDS ==> 3



 ---- SNOWBALL STEMMING ----

['219-224', ')', '.']

 TOTAL SNOWBALL STEM WORDS ==> 3



 ---- LEMMATIZATION ----

['219-224', ')', '.']

 TOTAL LEMMATIZE WORDS ==> 3

************************************************************************************************************************

391 --> ACM. 


 ---- TOKENS ----

 ['ACM', '.'] 

 TOTAL TOKENS ==> 2

 ---- POST ----

 [('ACM', 'NNP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['ACM', '.']

 TOTAL FILTERED TOKENS ==>  2

 ---- POST FOR FILTERED TOKENS ----

 [('ACM', 'NNP'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['ACM .'] 

 TOTAL BIGRAMS --> 1 



 ---- TRI-GRAMS ---- 

 [] 

 TOTAL TRIGRAMS --> 0 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['acm', '.']

 TOTAL PORTER STEM WORDS ==> 2



 ---- SNOWBALL STEMMING ----

['acm', '.']

 TOTAL SNOWBALL STEM WORDS ==> 2



 ---- LEMMATIZATION ----

['ACM', '.']

 TOTAL LEMMATIZE WORDS ==> 2

************************************************************************************************************************

392 --> [13] Woods, W. A. 


 ---- TOKENS ----

 ['[', '13', ']', 'Woods', ',', 'W.', 'A', '.'] 

 TOTAL TOKENS ==> 8

 ---- POST ----

 [('[', 'RB'), ('13', 'CD'), (']', 'JJ'), ('Woods', 'NNP'), (',', ','), ('W.', 'NNP'), ('A', 'NNP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['[', '13', ']', 'Woods', ',', 'W.', '.']

 TOTAL FILTERED TOKENS ==>  7

 ---- POST FOR FILTERED TOKENS ----

 [('[', 'RB'), ('13', 'CD'), (']', 'JJ'), ('Woods', 'NNP'), (',', ','), ('W.', 'NNP'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['[ 13', '13 ]', '] Woods', 'Woods ,', ', W.', 'W. .'] 

 TOTAL BIGRAMS --> 6 



 ---- TRI-GRAMS ---- 

 ['[ 13 ]', '13 ] Woods', '] Woods ,', 'Woods , W.', ', W. .'] 

 TOTAL TRIGRAMS --> 5 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> ['Woods']
 TOTAL PERSON ENTITY --> 1 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['[', '13', ']', 'wood', ',', 'w.', '.']

 TOTAL PORTER STEM WORDS ==> 7



 ---- SNOWBALL STEMMING ----

['[', '13', ']', 'wood', ',', 'w.', '.']

 TOTAL SNOWBALL STEM WORDS ==> 7



 ---- LEMMATIZATION ----

['[', '13', ']', 'Woods', ',', 'W.', '.']

 TOTAL LEMMATIZE WORDS ==> 7

************************************************************************************************************************

393 --> (1978). 


 ---- TOKENS ----

 ['(', '1978', ')', '.'] 

 TOTAL TOKENS ==> 4

 ---- POST ----

 [('(', '('), ('1978', 'CD'), (')', ')'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['(', '1978', ')', '.']

 TOTAL FILTERED TOKENS ==>  4

 ---- POST FOR FILTERED TOKENS ----

 [('(', '('), ('1978', 'CD'), (')', ')'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['( 1978', '1978 )', ') .'] 

 TOTAL BIGRAMS --> 3 



 ---- TRI-GRAMS ---- 

 ['( 1978 )', '1978 ) .'] 

 TOTAL TRIGRAMS --> 2 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['(', '1978', ')', '.']

 TOTAL PORTER STEM WORDS ==> 4



 ---- SNOWBALL STEMMING ----

['(', '1978', ')', '.']

 TOTAL SNOWBALL STEM WORDS ==> 4



 ---- LEMMATIZATION ----

['(', '1978', ')', '.']

 TOTAL LEMMATIZE WORDS ==> 4

************************************************************************************************************************

394 --> Semantics and quantification in natural language question  answering. 


 ---- TOKENS ----

 ['Semantics', 'and', 'quantification', 'in', 'natural', 'language', 'question', 'answering', '.'] 

 TOTAL TOKENS ==> 9

 ---- POST ----

 [('Semantics', 'NNS'), ('and', 'CC'), ('quantification', 'NN'), ('in', 'IN'), ('natural', 'JJ'), ('language', 'NN'), ('question', 'NN'), ('answering', 'VBG'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Semantics', 'quantification', 'natural', 'language', 'question', 'answering', '.']

 TOTAL FILTERED TOKENS ==>  7

 ---- POST FOR FILTERED TOKENS ----

 [('Semantics', 'NNS'), ('quantification', 'VBP'), ('natural', 'JJ'), ('language', 'NN'), ('question', 'NN'), ('answering', 'VBG'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Semantics quantification', 'quantification natural', 'natural language', 'language question', 'question answering', 'answering .'] 

 TOTAL BIGRAMS --> 6 



 ---- TRI-GRAMS ---- 

 ['Semantics quantification natural', 'quantification natural language', 'natural language question', 'language question answering', 'question answering .'] 

 TOTAL TRIGRAMS --> 5 



 ---- NOUN PHRASES ---- 

 ['natural language', 'question'] 

 TOTAL NOUN PHRASES --> 2 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['semant', 'quantif', 'natur', 'languag', 'question', 'answer', '.']

 TOTAL PORTER STEM WORDS ==> 7



 ---- SNOWBALL STEMMING ----

['semant', 'quantif', 'natur', 'languag', 'question', 'answer', '.']

 TOTAL SNOWBALL STEM WORDS ==> 7



 ---- LEMMATIZATION ----

['Semantics', 'quantification', 'natural', 'language', 'question', 'answering', '.']

 TOTAL LEMMATIZE WORDS ==> 7

************************************************************************************************************************

395 --> Advances in computers, 17, 1-87. 


 ---- TOKENS ----

 ['Advances', 'in', 'computers', ',', '17', ',', '1-87', '.'] 

 TOTAL TOKENS ==> 8

 ---- POST ----

 [('Advances', 'NNS'), ('in', 'IN'), ('computers', 'NNS'), (',', ','), ('17', 'CD'), (',', ','), ('1-87', 'CD'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Advances', 'computers', ',', '17', ',', '1-87', '.']

 TOTAL FILTERED TOKENS ==>  7

 ---- POST FOR FILTERED TOKENS ----

 [('Advances', 'NNS'), ('computers', 'NNS'), (',', ','), ('17', 'CD'), (',', ','), ('1-87', 'CD'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Advances computers', 'computers ,', ', 17', '17 ,', ', 1-87', '1-87 .'] 

 TOTAL BIGRAMS --> 6 



 ---- TRI-GRAMS ---- 

 ['Advances computers ,', 'computers , 17', ', 17 ,', '17 , 1-87', ', 1-87 .'] 

 TOTAL TRIGRAMS --> 5 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['advanc', 'comput', ',', '17', ',', '1-87', '.']

 TOTAL PORTER STEM WORDS ==> 7



 ---- SNOWBALL STEMMING ----

['advanc', 'comput', ',', '17', ',', '1-87', '.']

 TOTAL SNOWBALL STEM WORDS ==> 7



 ---- LEMMATIZATION ----

['Advances', 'computer', ',', '17', ',', '1-87', '.']

 TOTAL LEMMATIZE WORDS ==> 7

************************************************************************************************************************

396 --> [14] Hendrix, G. G., Sacerdoti, E. D., Sagalowicz, D., & Slocum, J. 


 ---- TOKENS ----

 ['[', '14', ']', 'Hendrix', ',', 'G.', 'G.', ',', 'Sacerdoti', ',', 'E.', 'D.', ',', 'Sagalowicz', ',', 'D.', ',', '&', 'Slocum', ',', 'J', '.'] 

 TOTAL TOKENS ==> 22

 ---- POST ----

 [('[', 'RB'), ('14', 'CD'), (']', 'JJ'), ('Hendrix', 'NNP'), (',', ','), ('G.', 'NNP'), ('G.', 'NNP'), (',', ','), ('Sacerdoti', 'NNP'), (',', ','), ('E.', 'NNP'), ('D.', 'NNP'), (',', ','), ('Sagalowicz', 'NNP'), (',', ','), ('D.', 'NNP'), (',', ','), ('&', 'CC'), ('Slocum', 'NNP'), (',', ','), ('J', 'NNP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['[', '14', ']', 'Hendrix', ',', 'G.', 'G.', ',', 'Sacerdoti', ',', 'E.', 'D.', ',', 'Sagalowicz', ',', 'D.', ',', '&', 'Slocum', ',', 'J', '.']

 TOTAL FILTERED TOKENS ==>  22

 ---- POST FOR FILTERED TOKENS ----

 [('[', 'RB'), ('14', 'CD'), (']', 'JJ'), ('Hendrix', 'NNP'), (',', ','), ('G.', 'NNP'), ('G.', 'NNP'), (',', ','), ('Sacerdoti', 'NNP'), (',', ','), ('E.', 'NNP'), ('D.', 'NNP'), (',', ','), ('Sagalowicz', 'NNP'), (',', ','), ('D.', 'NNP'), (',', ','), ('&', 'CC'), ('Slocum', 'NNP'), (',', ','), ('J', 'NNP'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['[ 14', '14 ]', '] Hendrix', 'Hendrix ,', ', G.', 'G. G.', 'G. ,', ', Sacerdoti', 'Sacerdoti ,', ', E.', 'E. D.', 'D. ,', ', Sagalowicz', 'Sagalowicz ,', ', D.', 'D. ,', ', &', '& Slocum', 'Slocum ,', ', J', 'J .'] 

 TOTAL BIGRAMS --> 21 



 ---- TRI-GRAMS ---- 

 ['[ 14 ]', '14 ] Hendrix', '] Hendrix ,', 'Hendrix , G.', ', G. G.', 'G. G. ,', 'G. , Sacerdoti', ', Sacerdoti ,', 'Sacerdoti , E.', ', E. D.', 'E. D. ,', 'D. , Sagalowicz', ', Sagalowicz ,', 'Sagalowicz , D.', ', D. ,', 'D. , &', ', & Slocum', '& Slocum ,', 'Slocum , J', ', J .'] 

 TOTAL TRIGRAMS --> 20 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> ['Hendrix', 'Sacerdoti', 'Slocum']
 TOTAL PERSON ENTITY --> 3 


 GPE ---> ['Sagalowicz']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['[', '14', ']', 'hendrix', ',', 'g.', 'g.', ',', 'sacerdoti', ',', 'e.', 'd.', ',', 'sagalowicz', ',', 'd.', ',', '&', 'slocum', ',', 'j', '.']

 TOTAL PORTER STEM WORDS ==> 22



 ---- SNOWBALL STEMMING ----

['[', '14', ']', 'hendrix', ',', 'g.', 'g.', ',', 'sacerdoti', ',', 'e.', 'd.', ',', 'sagalowicz', ',', 'd.', ',', '&', 'slocum', ',', 'j', '.']

 TOTAL SNOWBALL STEM WORDS ==> 22



 ---- LEMMATIZATION ----

['[', '14', ']', 'Hendrix', ',', 'G.', 'G.', ',', 'Sacerdoti', ',', 'E.', 'D.', ',', 'Sagalowicz', ',', 'D.', ',', '&', 'Slocum', ',', 'J', '.']

 TOTAL LEMMATIZE WORDS ==> 22

************************************************************************************************************************

397 --> (1978). 


 ---- TOKENS ----

 ['(', '1978', ')', '.'] 

 TOTAL TOKENS ==> 4

 ---- POST ----

 [('(', '('), ('1978', 'CD'), (')', ')'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['(', '1978', ')', '.']

 TOTAL FILTERED TOKENS ==>  4

 ---- POST FOR FILTERED TOKENS ----

 [('(', '('), ('1978', 'CD'), (')', ')'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['( 1978', '1978 )', ') .'] 

 TOTAL BIGRAMS --> 3 



 ---- TRI-GRAMS ---- 

 ['( 1978 )', '1978 ) .'] 

 TOTAL TRIGRAMS --> 2 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['(', '1978', ')', '.']

 TOTAL PORTER STEM WORDS ==> 4



 ---- SNOWBALL STEMMING ----

['(', '1978', ')', '.']

 TOTAL SNOWBALL STEM WORDS ==> 4



 ---- LEMMATIZATION ----

['(', '1978', ')', '.']

 TOTAL LEMMATIZE WORDS ==> 4

************************************************************************************************************************

398 --> Developing a  natural language interface to complex data. 


 ---- TOKENS ----

 ['Developing', 'a', 'natural', 'language', 'interface', 'to', 'complex', 'data', '.'] 

 TOTAL TOKENS ==> 9

 ---- POST ----

 [('Developing', 'VBG'), ('a', 'DT'), ('natural', 'JJ'), ('language', 'NN'), ('interface', 'NN'), ('to', 'TO'), ('complex', 'JJ'), ('data', 'NNS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Developing', 'natural', 'language', 'interface', 'complex', 'data', '.']

 TOTAL FILTERED TOKENS ==>  7

 ---- POST FOR FILTERED TOKENS ----

 [('Developing', 'VBG'), ('natural', 'JJ'), ('language', 'NN'), ('interface', 'NN'), ('complex', 'JJ'), ('data', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Developing natural', 'natural language', 'language interface', 'interface complex', 'complex data', 'data .'] 

 TOTAL BIGRAMS --> 6 



 ---- TRI-GRAMS ---- 

 ['Developing natural language', 'natural language interface', 'language interface complex', 'interface complex data', 'complex data .'] 

 TOTAL TRIGRAMS --> 5 



 ---- NOUN PHRASES ---- 

 ['natural language', 'interface'] 

 TOTAL NOUN PHRASES --> 2 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['develop', 'natur', 'languag', 'interfac', 'complex', 'data', '.']

 TOTAL PORTER STEM WORDS ==> 7



 ---- SNOWBALL STEMMING ----

['develop', 'natur', 'languag', 'interfac', 'complex', 'data', '.']

 TOTAL SNOWBALL STEM WORDS ==> 7



 ---- LEMMATIZATION ----

['Developing', 'natural', 'language', 'interface', 'complex', 'data', '.']

 TOTAL LEMMATIZE WORDS ==> 7

************************************************************************************************************************

399 --> ACM Transactions on Database Systems  (TODS), 3(2), 105-147. 


 ---- TOKENS ----

 ['ACM', 'Transactions', 'on', 'Database', 'Systems', '(', 'TODS', ')', ',', '3', '(', '2', ')', ',', '105-147', '.'] 

 TOTAL TOKENS ==> 16

 ---- POST ----

 [('ACM', 'NNP'), ('Transactions', 'NNP'), ('on', 'IN'), ('Database', 'NNP'), ('Systems', 'NNP'), ('(', '('), ('TODS', 'NNP'), (')', ')'), (',', ','), ('3', 'CD'), ('(', '('), ('2', 'CD'), (')', ')'), (',', ','), ('105-147', 'JJ'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['ACM', 'Transactions', 'Database', 'Systems', '(', 'TODS', ')', ',', '3', '(', '2', ')', ',', '105-147', '.']

 TOTAL FILTERED TOKENS ==>  15

 ---- POST FOR FILTERED TOKENS ----

 [('ACM', 'NNP'), ('Transactions', 'NNP'), ('Database', 'NNP'), ('Systems', 'NNP'), ('(', '('), ('TODS', 'NNP'), (')', ')'), (',', ','), ('3', 'CD'), ('(', '('), ('2', 'CD'), (')', ')'), (',', ','), ('105-147', 'JJ'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['ACM Transactions', 'Transactions Database', 'Database Systems', 'Systems (', '( TODS', 'TODS )', ') ,', ', 3', '3 (', '( 2', '2 )', ') ,', ', 105-147', '105-147 .'] 

 TOTAL BIGRAMS --> 14 



 ---- TRI-GRAMS ---- 

 ['ACM Transactions Database', 'Transactions Database Systems', 'Database Systems (', 'Systems ( TODS', '( TODS )', 'TODS ) ,', ') , 3', ', 3 (', '3 ( 2', '( 2 )', '2 ) ,', ') , 105-147', ', 105-147 .'] 

 TOTAL TRIGRAMS --> 13 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> ['ACM Transactions Database Systems']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['acm', 'transact', 'databas', 'system', '(', 'tod', ')', ',', '3', '(', '2', ')', ',', '105-147', '.']

 TOTAL PORTER STEM WORDS ==> 15



 ---- SNOWBALL STEMMING ----

['acm', 'transact', 'databas', 'system', '(', 'tod', ')', ',', '3', '(', '2', ')', ',', '105-147', '.']

 TOTAL SNOWBALL STEM WORDS ==> 15



 ---- LEMMATIZATION ----

['ACM', 'Transactions', 'Database', 'Systems', '(', 'TODS', ')', ',', '3', '(', '2', ')', ',', '105-147', '.']

 TOTAL LEMMATIZE WORDS ==> 15

************************************************************************************************************************

400 --> [15] Alshawi, H. (1992). 


 ---- TOKENS ----

 ['[', '15', ']', 'Alshawi', ',', 'H.', '(', '1992', ')', '.'] 

 TOTAL TOKENS ==> 10

 ---- POST ----

 [('[', 'RB'), ('15', 'CD'), (']', 'JJ'), ('Alshawi', 'NNP'), (',', ','), ('H.', 'NNP'), ('(', '('), ('1992', 'CD'), (')', ')'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['[', '15', ']', 'Alshawi', ',', 'H.', '(', '1992', ')', '.']

 TOTAL FILTERED TOKENS ==>  10

 ---- POST FOR FILTERED TOKENS ----

 [('[', 'RB'), ('15', 'CD'), (']', 'JJ'), ('Alshawi', 'NNP'), (',', ','), ('H.', 'NNP'), ('(', '('), ('1992', 'CD'), (')', ')'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['[ 15', '15 ]', '] Alshawi', 'Alshawi ,', ', H.', 'H. (', '( 1992', '1992 )', ') .'] 

 TOTAL BIGRAMS --> 9 



 ---- TRI-GRAMS ---- 

 ['[ 15 ]', '15 ] Alshawi', '] Alshawi ,', 'Alshawi , H.', ', H. (', 'H. ( 1992', '( 1992 )', '1992 ) .'] 

 TOTAL TRIGRAMS --> 8 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['[', '15', ']', 'alshawi', ',', 'h.', '(', '1992', ')', '.']

 TOTAL PORTER STEM WORDS ==> 10



 ---- SNOWBALL STEMMING ----

['[', '15', ']', 'alshawi', ',', 'h.', '(', '1992', ')', '.']

 TOTAL SNOWBALL STEM WORDS ==> 10



 ---- LEMMATIZATION ----

['[', '15', ']', 'Alshawi', ',', 'H.', '(', '1992', ')', '.']

 TOTAL LEMMATIZE WORDS ==> 10

************************************************************************************************************************

401 --> The core language engine. 


 ---- TOKENS ----

 ['The', 'core', 'language', 'engine', '.'] 

 TOTAL TOKENS ==> 5

 ---- POST ----

 [('The', 'DT'), ('core', 'NN'), ('language', 'NN'), ('engine', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['core', 'language', 'engine', '.']

 TOTAL FILTERED TOKENS ==>  4

 ---- POST FOR FILTERED TOKENS ----

 [('core', 'NN'), ('language', 'NN'), ('engine', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['core language', 'language engine', 'engine .'] 

 TOTAL BIGRAMS --> 3 



 ---- TRI-GRAMS ---- 

 ['core language engine', 'language engine .'] 

 TOTAL TRIGRAMS --> 2 



 ---- NOUN PHRASES ---- 

 ['core', 'language', 'engine'] 

 TOTAL NOUN PHRASES --> 3 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['core', 'languag', 'engin', '.']

 TOTAL PORTER STEM WORDS ==> 4



 ---- SNOWBALL STEMMING ----

['core', 'languag', 'engin', '.']

 TOTAL SNOWBALL STEM WORDS ==> 4



 ---- LEMMATIZATION ----

['core', 'language', 'engine', '.']

 TOTAL LEMMATIZE WORDS ==> 4

************************************************************************************************************************

402 --> MIT press. 


 ---- TOKENS ----

 ['MIT', 'press', '.'] 

 TOTAL TOKENS ==> 3

 ---- POST ----

 [('MIT', 'NNP'), ('press', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['MIT', 'press', '.']

 TOTAL FILTERED TOKENS ==>  3

 ---- POST FOR FILTERED TOKENS ----

 [('MIT', 'NNP'), ('press', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['MIT press', 'press .'] 

 TOTAL BIGRAMS --> 2 



 ---- TRI-GRAMS ---- 

 ['MIT press .'] 

 TOTAL TRIGRAMS --> 1 



 ---- NOUN PHRASES ---- 

 ['press'] 

 TOTAL NOUN PHRASES --> 1 



 ---- NER ----

 
 ORGANIZATION ---> ['MIT']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['mit', 'press', '.']

 TOTAL PORTER STEM WORDS ==> 3



 ---- SNOWBALL STEMMING ----

['mit', 'press', '.']

 TOTAL SNOWBALL STEM WORDS ==> 3



 ---- LEMMATIZATION ----

['MIT', 'press', '.']

 TOTAL LEMMATIZE WORDS ==> 3

************************************************************************************************************************

403 --> [16] Kamp, H., & Reyle, U. 


 ---- TOKENS ----

 ['[', '16', ']', 'Kamp', ',', 'H.', ',', '&', 'Reyle', ',', 'U', '.'] 

 TOTAL TOKENS ==> 12

 ---- POST ----

 [('[', 'RB'), ('16', 'CD'), (']', 'JJ'), ('Kamp', 'NNP'), (',', ','), ('H.', 'NNP'), (',', ','), ('&', 'CC'), ('Reyle', 'NNP'), (',', ','), ('U', 'NNP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['[', '16', ']', 'Kamp', ',', 'H.', ',', '&', 'Reyle', ',', 'U', '.']

 TOTAL FILTERED TOKENS ==>  12

 ---- POST FOR FILTERED TOKENS ----

 [('[', 'RB'), ('16', 'CD'), (']', 'JJ'), ('Kamp', 'NNP'), (',', ','), ('H.', 'NNP'), (',', ','), ('&', 'CC'), ('Reyle', 'NNP'), (',', ','), ('U', 'NNP'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['[ 16', '16 ]', '] Kamp', 'Kamp ,', ', H.', 'H. ,', ', &', '& Reyle', 'Reyle ,', ', U', 'U .'] 

 TOTAL BIGRAMS --> 11 



 ---- TRI-GRAMS ---- 

 ['[ 16 ]', '16 ] Kamp', '] Kamp ,', 'Kamp , H.', ', H. ,', 'H. , &', ', & Reyle', '& Reyle ,', 'Reyle , U', ', U .'] 

 TOTAL TRIGRAMS --> 10 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> ['Reyle']
 TOTAL PERSON ENTITY --> 1 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['[', '16', ']', 'kamp', ',', 'h.', ',', '&', 'reyl', ',', 'u', '.']

 TOTAL PORTER STEM WORDS ==> 12



 ---- SNOWBALL STEMMING ----

['[', '16', ']', 'kamp', ',', 'h.', ',', '&', 'reyl', ',', 'u', '.']

 TOTAL SNOWBALL STEM WORDS ==> 12



 ---- LEMMATIZATION ----

['[', '16', ']', 'Kamp', ',', 'H.', ',', '&', 'Reyle', ',', 'U', '.']

 TOTAL LEMMATIZE WORDS ==> 12

************************************************************************************************************************

404 --> (1993). 


 ---- TOKENS ----

 ['(', '1993', ')', '.'] 

 TOTAL TOKENS ==> 4

 ---- POST ----

 [('(', '('), ('1993', 'CD'), (')', ')'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['(', '1993', ')', '.']

 TOTAL FILTERED TOKENS ==>  4

 ---- POST FOR FILTERED TOKENS ----

 [('(', '('), ('1993', 'CD'), (')', ')'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['( 1993', '1993 )', ') .'] 

 TOTAL BIGRAMS --> 3 



 ---- TRI-GRAMS ---- 

 ['( 1993 )', '1993 ) .'] 

 TOTAL TRIGRAMS --> 2 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['(', '1993', ')', '.']

 TOTAL PORTER STEM WORDS ==> 4



 ---- SNOWBALL STEMMING ----

['(', '1993', ')', '.']

 TOTAL SNOWBALL STEM WORDS ==> 4



 ---- LEMMATIZATION ----

['(', '1993', ')', '.']

 TOTAL LEMMATIZE WORDS ==> 4

************************************************************************************************************************

405 --> Tense and Aspect. 


 ---- TOKENS ----

 ['Tense', 'and', 'Aspect', '.'] 

 TOTAL TOKENS ==> 4

 ---- POST ----

 [('Tense', 'NN'), ('and', 'CC'), ('Aspect', 'NNP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Tense', 'Aspect', '.']

 TOTAL FILTERED TOKENS ==>  3

 ---- POST FOR FILTERED TOKENS ----

 [('Tense', 'NNP'), ('Aspect', 'NNP'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Tense Aspect', 'Aspect .'] 

 TOTAL BIGRAMS --> 2 



 ---- TRI-GRAMS ---- 

 ['Tense Aspect .'] 

 TOTAL TRIGRAMS --> 1 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> ['Aspect']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> ['Tense']
 TOTAL PERSON ENTITY --> 1 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['tens', 'aspect', '.']

 TOTAL PORTER STEM WORDS ==> 3



 ---- SNOWBALL STEMMING ----

['tens', 'aspect', '.']

 TOTAL SNOWBALL STEM WORDS ==> 3



 ---- LEMMATIZATION ----

['Tense', 'Aspect', '.']

 TOTAL LEMMATIZE WORDS ==> 3

************************************************************************************************************************

406 --> In From Discourse to Logic (pp. 


 ---- TOKENS ----

 ['In', 'From', 'Discourse', 'to', 'Logic', '(', 'pp', '.'] 

 TOTAL TOKENS ==> 8

 ---- POST ----

 [('In', 'IN'), ('From', 'NNP'), ('Discourse', 'NNP'), ('to', 'TO'), ('Logic', 'NNP'), ('(', '('), ('pp', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Discourse', 'Logic', '(', 'pp', '.']

 TOTAL FILTERED TOKENS ==>  5

 ---- POST FOR FILTERED TOKENS ----

 [('Discourse', 'NNP'), ('Logic', 'NNP'), ('(', '('), ('pp', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Discourse Logic', 'Logic (', '( pp', 'pp .'] 

 TOTAL BIGRAMS --> 4 



 ---- TRI-GRAMS ---- 

 ['Discourse Logic (', 'Logic ( pp', '( pp .'] 

 TOTAL TRIGRAMS --> 3 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 []

 ---- PORTER STEMMING ----

['discours', 'logic', '(', 'pp', '.']

 TOTAL PORTER STEM WORDS ==> 5



 ---- SNOWBALL STEMMING ----

['discours', 'logic', '(', 'pp', '.']

 TOTAL SNOWBALL STEM WORDS ==> 5



 ---- LEMMATIZATION ----

['Discourse', 'Logic', '(', 'pp', '.']

 TOTAL LEMMATIZE WORDS ==> 5

************************************************************************************************************************

407 --> 483- 689). 


 ---- TOKENS ----

 ['483-', '689', ')', '.'] 

 TOTAL TOKENS ==> 4

 ---- POST ----

 [('483-', 'JJ'), ('689', 'CD'), (')', ')'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['483-', '689', ')', '.']

 TOTAL FILTERED TOKENS ==>  4

 ---- POST FOR FILTERED TOKENS ----

 [('483-', 'JJ'), ('689', 'CD'), (')', ')'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['483- 689', '689 )', ') .'] 

 TOTAL BIGRAMS --> 3 



 ---- TRI-GRAMS ---- 

 ['483- 689 )', '689 ) .'] 

 TOTAL TRIGRAMS --> 2 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 []

 ---- PORTER STEMMING ----

['483-', '689', ')', '.']

 TOTAL PORTER STEM WORDS ==> 4



 ---- SNOWBALL STEMMING ----

['483-', '689', ')', '.']

 TOTAL SNOWBALL STEM WORDS ==> 4



 ---- LEMMATIZATION ----

['483-', '689', ')', '.']

 TOTAL LEMMATIZE WORDS ==> 4

************************************************************************************************************************

408 --> Springer Netherlands. 


 ---- TOKENS ----

 ['Springer', 'Netherlands', '.'] 

 TOTAL TOKENS ==> 3

 ---- POST ----

 [('Springer', 'NNP'), ('Netherlands', 'NNP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Springer', 'Netherlands', '.']

 TOTAL FILTERED TOKENS ==>  3

 ---- POST FOR FILTERED TOKENS ----

 [('Springer', 'NNP'), ('Netherlands', 'NNP'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Springer Netherlands', 'Netherlands .'] 

 TOTAL BIGRAMS --> 2 



 ---- TRI-GRAMS ---- 

 ['Springer Netherlands .'] 

 TOTAL TRIGRAMS --> 1 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> ['Netherlands']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> ['Springer']
 TOTAL PERSON ENTITY --> 1 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['springer', 'netherland', '.']

 TOTAL PORTER STEM WORDS ==> 3



 ---- SNOWBALL STEMMING ----

['springer', 'netherland', '.']

 TOTAL SNOWBALL STEM WORDS ==> 3



 ---- LEMMATIZATION ----

['Springer', 'Netherlands', '.']

 TOTAL LEMMATIZE WORDS ==> 3

************************************************************************************************************************

409 --> [17] Lea , W.A Trends in speech recognition , Englewoods Cliffs , NJ: Prentice Hall , 1980. 


 ---- TOKENS ----

 ['[', '17', ']', 'Lea', ',', 'W.A', 'Trends', 'in', 'speech', 'recognition', ',', 'Englewoods', 'Cliffs', ',', 'NJ', ':', 'Prentice', 'Hall', ',', '1980', '.'] 

 TOTAL TOKENS ==> 21

 ---- POST ----

 [('[', 'RB'), ('17', 'CD'), (']', 'JJ'), ('Lea', 'NNP'), (',', ','), ('W.A', 'NNP'), ('Trends', 'NNP'), ('in', 'IN'), ('speech', 'JJ'), ('recognition', 'NN'), (',', ','), ('Englewoods', 'NNP'), ('Cliffs', 'NNP'), (',', ','), ('NJ', 'NNP'), (':', ':'), ('Prentice', 'NNP'), ('Hall', 'NNP'), (',', ','), ('1980', 'CD'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['[', '17', ']', 'Lea', ',', 'W.A', 'Trends', 'speech', 'recognition', ',', 'Englewoods', 'Cliffs', ',', 'NJ', ':', 'Prentice', 'Hall', ',', '1980', '.']

 TOTAL FILTERED TOKENS ==>  20

 ---- POST FOR FILTERED TOKENS ----

 [('[', 'RB'), ('17', 'CD'), (']', 'JJ'), ('Lea', 'NNP'), (',', ','), ('W.A', 'NNP'), ('Trends', 'NNP'), ('speech', 'NN'), ('recognition', 'NN'), (',', ','), ('Englewoods', 'NNP'), ('Cliffs', 'NNP'), (',', ','), ('NJ', 'NNP'), (':', ':'), ('Prentice', 'NNP'), ('Hall', 'NNP'), (',', ','), ('1980', 'CD'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['[ 17', '17 ]', '] Lea', 'Lea ,', ', W.A', 'W.A Trends', 'Trends speech', 'speech recognition', 'recognition ,', ', Englewoods', 'Englewoods Cliffs', 'Cliffs ,', ', NJ', 'NJ :', ': Prentice', 'Prentice Hall', 'Hall ,', ', 1980', '1980 .'] 

 TOTAL BIGRAMS --> 19 



 ---- TRI-GRAMS ---- 

 ['[ 17 ]', '17 ] Lea', '] Lea ,', 'Lea , W.A', ', W.A Trends', 'W.A Trends speech', 'Trends speech recognition', 'speech recognition ,', 'recognition , Englewoods', ', Englewoods Cliffs', 'Englewoods Cliffs ,', 'Cliffs , NJ', ', NJ :', 'NJ : Prentice', ': Prentice Hall', 'Prentice Hall ,', 'Hall , 1980', ', 1980 .'] 

 TOTAL TRIGRAMS --> 18 



 ---- NOUN PHRASES ---- 

 ['speech', 'recognition'] 

 TOTAL NOUN PHRASES --> 2 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> ['Englewoods Cliffs']
 TOTAL PERSON ENTITY --> 1 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['[', '17', ']', 'lea', ',', 'w.a', 'trend', 'speech', 'recognit', ',', 'englewood', 'cliff', ',', 'nj', ':', 'prentic', 'hall', ',', '1980', '.']

 TOTAL PORTER STEM WORDS ==> 20



 ---- SNOWBALL STEMMING ----

['[', '17', ']', 'lea', ',', 'w.a', 'trend', 'speech', 'recognit', ',', 'englewood', 'cliff', ',', 'nj', ':', 'prentic', 'hall', ',', '1980', '.']

 TOTAL SNOWBALL STEM WORDS ==> 20



 ---- LEMMATIZATION ----

['[', '17', ']', 'Lea', ',', 'W.A', 'Trends', 'speech', 'recognition', ',', 'Englewoods', 'Cliffs', ',', 'NJ', ':', 'Prentice', 'Hall', ',', '1980', '.']

 TOTAL LEMMATIZE WORDS ==> 20

************************************************************************************************************************

410 --> [18] Young, S. J., & Chase, L. L. (1998). 


 ---- TOKENS ----

 ['[', '18', ']', 'Young', ',', 'S.', 'J.', ',', '&', 'Chase', ',', 'L.', 'L.', '(', '1998', ')', '.'] 

 TOTAL TOKENS ==> 17

 ---- POST ----

 [('[', 'RB'), ('18', 'CD'), (']', 'JJ'), ('Young', 'NNP'), (',', ','), ('S.', 'NNP'), ('J.', 'NNP'), (',', ','), ('&', 'CC'), ('Chase', 'NNP'), (',', ','), ('L.', 'NNP'), ('L.', 'NNP'), ('(', '('), ('1998', 'CD'), (')', ')'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['[', '18', ']', 'Young', ',', 'S.', 'J.', ',', '&', 'Chase', ',', 'L.', 'L.', '(', '1998', ')', '.']

 TOTAL FILTERED TOKENS ==>  17

 ---- POST FOR FILTERED TOKENS ----

 [('[', 'RB'), ('18', 'CD'), (']', 'JJ'), ('Young', 'NNP'), (',', ','), ('S.', 'NNP'), ('J.', 'NNP'), (',', ','), ('&', 'CC'), ('Chase', 'NNP'), (',', ','), ('L.', 'NNP'), ('L.', 'NNP'), ('(', '('), ('1998', 'CD'), (')', ')'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['[ 18', '18 ]', '] Young', 'Young ,', ', S.', 'S. J.', 'J. ,', ', &', '& Chase', 'Chase ,', ', L.', 'L. L.', 'L. (', '( 1998', '1998 )', ') .'] 

 TOTAL BIGRAMS --> 16 



 ---- TRI-GRAMS ---- 

 ['[ 18 ]', '18 ] Young', '] Young ,', 'Young , S.', ', S. J.', 'S. J. ,', 'J. , &', ', & Chase', '& Chase ,', 'Chase , L.', ', L. L.', 'L. L. (', 'L. ( 1998', '( 1998 )', '1998 ) .'] 

 TOTAL TRIGRAMS --> 15 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> ['Chase']
 TOTAL PERSON ENTITY --> 1 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['[', '18', ']', 'young', ',', 's.', 'j.', ',', '&', 'chase', ',', 'l.', 'l.', '(', '1998', ')', '.']

 TOTAL PORTER STEM WORDS ==> 17



 ---- SNOWBALL STEMMING ----

['[', '18', ']', 'young', ',', 's.', 'j.', ',', '&', 'chase', ',', 'l.', 'l.', '(', '1998', ')', '.']

 TOTAL SNOWBALL STEM WORDS ==> 17



 ---- LEMMATIZATION ----

['[', '18', ']', 'Young', ',', 'S.', 'J.', ',', '&', 'Chase', ',', 'L.', 'L.', '(', '1998', ')', '.']

 TOTAL LEMMATIZE WORDS ==> 17

************************************************************************************************************************

411 --> Speech recognition evaluation: a review of the US  CSR and LVCSR programmes. 


 ---- TOKENS ----

 ['Speech', 'recognition', 'evaluation', ':', 'a', 'review', 'of', 'the', 'US', 'CSR', 'and', 'LVCSR', 'programmes', '.'] 

 TOTAL TOKENS ==> 14

 ---- POST ----

 [('Speech', 'NNP'), ('recognition', 'NN'), ('evaluation', 'NN'), (':', ':'), ('a', 'DT'), ('review', 'NN'), ('of', 'IN'), ('the', 'DT'), ('US', 'NNP'), ('CSR', 'NNP'), ('and', 'CC'), ('LVCSR', 'NNP'), ('programmes', 'NNS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Speech', 'recognition', 'evaluation', ':', 'review', 'US', 'CSR', 'LVCSR', 'programmes', '.']

 TOTAL FILTERED TOKENS ==>  10

 ---- POST FOR FILTERED TOKENS ----

 [('Speech', 'NNP'), ('recognition', 'NN'), ('evaluation', 'NN'), (':', ':'), ('review', 'VB'), ('US', 'NNP'), ('CSR', 'NNP'), ('LVCSR', 'NNP'), ('programmes', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Speech recognition', 'recognition evaluation', 'evaluation :', ': review', 'review US', 'US CSR', 'CSR LVCSR', 'LVCSR programmes', 'programmes .'] 

 TOTAL BIGRAMS --> 9 



 ---- TRI-GRAMS ---- 

 ['Speech recognition evaluation', 'recognition evaluation :', 'evaluation : review', ': review US', 'review US CSR', 'US CSR LVCSR', 'CSR LVCSR programmes', 'LVCSR programmes .'] 

 TOTAL TRIGRAMS --> 8 



 ---- NOUN PHRASES ---- 

 ['recognition', 'evaluation'] 

 TOTAL NOUN PHRASES --> 2 



 ---- NER ----

 
 ORGANIZATION ---> ['US']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> ['Speech']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['speech', 'recognit', 'evalu', ':', 'review', 'us', 'csr', 'lvcsr', 'programm', '.']

 TOTAL PORTER STEM WORDS ==> 10



 ---- SNOWBALL STEMMING ----

['speech', 'recognit', 'evalu', ':', 'review', 'us', 'csr', 'lvcsr', 'programm', '.']

 TOTAL SNOWBALL STEM WORDS ==> 10



 ---- LEMMATIZATION ----

['Speech', 'recognition', 'evaluation', ':', 'review', 'US', 'CSR', 'LVCSR', 'programme', '.']

 TOTAL LEMMATIZE WORDS ==> 10

************************************************************************************************************************

412 --> Computer Speech & Language, 12(4), 263-279. 


 ---- TOKENS ----

 ['Computer', 'Speech', '&', 'Language', ',', '12', '(', '4', ')', ',', '263-279', '.'] 

 TOTAL TOKENS ==> 12

 ---- POST ----

 [('Computer', 'NNP'), ('Speech', 'NNP'), ('&', 'CC'), ('Language', 'NNP'), (',', ','), ('12', 'CD'), ('(', '('), ('4', 'CD'), (')', ')'), (',', ','), ('263-279', 'JJ'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Computer', 'Speech', '&', 'Language', ',', '12', '(', '4', ')', ',', '263-279', '.']

 TOTAL FILTERED TOKENS ==>  12

 ---- POST FOR FILTERED TOKENS ----

 [('Computer', 'NNP'), ('Speech', 'NNP'), ('&', 'CC'), ('Language', 'NNP'), (',', ','), ('12', 'CD'), ('(', '('), ('4', 'CD'), (')', ')'), (',', ','), ('263-279', 'JJ'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Computer Speech', 'Speech &', '& Language', 'Language ,', ', 12', '12 (', '( 4', '4 )', ') ,', ', 263-279', '263-279 .'] 

 TOTAL BIGRAMS --> 11 



 ---- TRI-GRAMS ---- 

 ['Computer Speech &', 'Speech & Language', '& Language ,', 'Language , 12', ', 12 (', '12 ( 4', '( 4 )', '4 ) ,', ') , 263-279', ', 263-279 .'] 

 TOTAL TRIGRAMS --> 10 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> ['Computer Speech']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> ['Language']
 TOTAL PERSON ENTITY --> 1 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['comput', 'speech', '&', 'languag', ',', '12', '(', '4', ')', ',', '263-279', '.']

 TOTAL PORTER STEM WORDS ==> 12



 ---- SNOWBALL STEMMING ----

['comput', 'speech', '&', 'languag', ',', '12', '(', '4', ')', ',', '263-279', '.']

 TOTAL SNOWBALL STEM WORDS ==> 12



 ---- LEMMATIZATION ----

['Computer', 'Speech', '&', 'Language', ',', '12', '(', '4', ')', ',', '263-279', '.']

 TOTAL LEMMATIZE WORDS ==> 12

************************************************************************************************************************

413 --> [19] Sundheim, B. M., & Chinchor, N. A. 


 ---- TOKENS ----

 ['[', '19', ']', 'Sundheim', ',', 'B.', 'M.', ',', '&', 'Chinchor', ',', 'N.', 'A', '.'] 

 TOTAL TOKENS ==> 14

 ---- POST ----

 [('[', 'RB'), ('19', 'CD'), (']', 'JJ'), ('Sundheim', 'NNP'), (',', ','), ('B.', 'NNP'), ('M.', 'NNP'), (',', ','), ('&', 'CC'), ('Chinchor', 'NNP'), (',', ','), ('N.', 'NNP'), ('A', 'NNP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['[', '19', ']', 'Sundheim', ',', 'B.', 'M.', ',', '&', 'Chinchor', ',', 'N.', '.']

 TOTAL FILTERED TOKENS ==>  13

 ---- POST FOR FILTERED TOKENS ----

 [('[', 'RB'), ('19', 'CD'), (']', 'JJ'), ('Sundheim', 'NNP'), (',', ','), ('B.', 'NNP'), ('M.', 'NNP'), (',', ','), ('&', 'CC'), ('Chinchor', 'NNP'), (',', ','), ('N.', 'NNP'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['[ 19', '19 ]', '] Sundheim', 'Sundheim ,', ', B.', 'B. M.', 'M. ,', ', &', '& Chinchor', 'Chinchor ,', ', N.', 'N. .'] 

 TOTAL BIGRAMS --> 12 



 ---- TRI-GRAMS ---- 

 ['[ 19 ]', '19 ] Sundheim', '] Sundheim ,', 'Sundheim , B.', ', B. M.', 'B. M. ,', 'M. , &', ', & Chinchor', '& Chinchor ,', 'Chinchor , N.', ', N. .'] 

 TOTAL TRIGRAMS --> 11 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> ['Sundheim']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> ['Chinchor']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['[', '19', ']', 'sundheim', ',', 'b.', 'm.', ',', '&', 'chinchor', ',', 'n.', '.']

 TOTAL PORTER STEM WORDS ==> 13



 ---- SNOWBALL STEMMING ----

['[', '19', ']', 'sundheim', ',', 'b.', 'm.', ',', '&', 'chinchor', ',', 'n.', '.']

 TOTAL SNOWBALL STEM WORDS ==> 13



 ---- LEMMATIZATION ----

['[', '19', ']', 'Sundheim', ',', 'B.', 'M.', ',', '&', 'Chinchor', ',', 'N.', '.']

 TOTAL LEMMATIZE WORDS ==> 13

************************************************************************************************************************

414 --> (1993, March). 


 ---- TOKENS ----

 ['(', '1993', ',', 'March', ')', '.'] 

 TOTAL TOKENS ==> 6

 ---- POST ----

 [('(', '('), ('1993', 'CD'), (',', ','), ('March', 'NNP'), (')', ')'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['(', '1993', ',', 'March', ')', '.']

 TOTAL FILTERED TOKENS ==>  6

 ---- POST FOR FILTERED TOKENS ----

 [('(', '('), ('1993', 'CD'), (',', ','), ('March', 'NNP'), (')', ')'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['( 1993', '1993 ,', ', March', 'March )', ') .'] 

 TOTAL BIGRAMS --> 5 



 ---- TRI-GRAMS ---- 

 ['( 1993 ,', '1993 , March', ', March )', 'March ) .'] 

 TOTAL TRIGRAMS --> 4 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['(', '1993', ',', 'march', ')', '.']

 TOTAL PORTER STEM WORDS ==> 6



 ---- SNOWBALL STEMMING ----

['(', '1993', ',', 'march', ')', '.']

 TOTAL SNOWBALL STEM WORDS ==> 6



 ---- LEMMATIZATION ----

['(', '1993', ',', 'March', ')', '.']

 TOTAL LEMMATIZE WORDS ==> 6

************************************************************************************************************************

415 --> Survey of the message  understanding conferences. 


 ---- TOKENS ----

 ['Survey', 'of', 'the', 'message', 'understanding', 'conferences', '.'] 

 TOTAL TOKENS ==> 7

 ---- POST ----

 [('Survey', 'NNP'), ('of', 'IN'), ('the', 'DT'), ('message', 'NN'), ('understanding', 'JJ'), ('conferences', 'NNS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Survey', 'message', 'understanding', 'conferences', '.']

 TOTAL FILTERED TOKENS ==>  5

 ---- POST FOR FILTERED TOKENS ----

 [('Survey', 'NNP'), ('message', 'NN'), ('understanding', 'JJ'), ('conferences', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Survey message', 'message understanding', 'understanding conferences', 'conferences .'] 

 TOTAL BIGRAMS --> 4 



 ---- TRI-GRAMS ---- 

 ['Survey message understanding', 'message understanding conferences', 'understanding conferences .'] 

 TOTAL TRIGRAMS --> 3 



 ---- NOUN PHRASES ---- 

 ['message'] 

 TOTAL NOUN PHRASES --> 1 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> ['Survey']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['survey', 'messag', 'understand', 'confer', '.']

 TOTAL PORTER STEM WORDS ==> 5



 ---- SNOWBALL STEMMING ----

['survey', 'messag', 'understand', 'confer', '.']

 TOTAL SNOWBALL STEM WORDS ==> 5



 ---- LEMMATIZATION ----

['Survey', 'message', 'understanding', 'conference', '.']

 TOTAL LEMMATIZE WORDS ==> 5

************************************************************************************************************************

416 --> In Proceedings of the workshop on Human Language  Technology (pp. 


 ---- TOKENS ----

 ['In', 'Proceedings', 'of', 'the', 'workshop', 'on', 'Human', 'Language', 'Technology', '(', 'pp', '.'] 

 TOTAL TOKENS ==> 12

 ---- POST ----

 [('In', 'IN'), ('Proceedings', 'NNP'), ('of', 'IN'), ('the', 'DT'), ('workshop', 'NN'), ('on', 'IN'), ('Human', 'NNP'), ('Language', 'NNP'), ('Technology', 'NNP'), ('(', '('), ('pp', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Proceedings', 'workshop', 'Human', 'Language', 'Technology', '(', 'pp', '.']

 TOTAL FILTERED TOKENS ==>  8

 ---- POST FOR FILTERED TOKENS ----

 [('Proceedings', 'NNS'), ('workshop', 'VBP'), ('Human', 'NNP'), ('Language', 'NNP'), ('Technology', 'NNP'), ('(', '('), ('pp', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Proceedings workshop', 'workshop Human', 'Human Language', 'Language Technology', 'Technology (', '( pp', 'pp .'] 

 TOTAL BIGRAMS --> 7 



 ---- TRI-GRAMS ---- 

 ['Proceedings workshop Human', 'workshop Human Language', 'Human Language Technology', 'Language Technology (', 'Technology ( pp', '( pp .'] 

 TOTAL TRIGRAMS --> 6 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 []

 ---- PORTER STEMMING ----

['proceed', 'workshop', 'human', 'languag', 'technolog', '(', 'pp', '.']

 TOTAL PORTER STEM WORDS ==> 8



 ---- SNOWBALL STEMMING ----

['proceed', 'workshop', 'human', 'languag', 'technolog', '(', 'pp', '.']

 TOTAL SNOWBALL STEM WORDS ==> 8



 ---- LEMMATIZATION ----

['Proceedings', 'workshop', 'Human', 'Language', 'Technology', '(', 'pp', '.']

 TOTAL LEMMATIZE WORDS ==> 8

************************************************************************************************************************

417 --> 56-60). 


 ---- TOKENS ----

 ['56-60', ')', '.'] 

 TOTAL TOKENS ==> 3

 ---- POST ----

 [('56-60', 'JJ'), (')', ')'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['56-60', ')', '.']

 TOTAL FILTERED TOKENS ==>  3

 ---- POST FOR FILTERED TOKENS ----

 [('56-60', 'JJ'), (')', ')'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['56-60 )', ') .'] 

 TOTAL BIGRAMS --> 2 



 ---- TRI-GRAMS ---- 

 ['56-60 ) .'] 

 TOTAL TRIGRAMS --> 1 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 []

 ---- PORTER STEMMING ----

['56-60', ')', '.']

 TOTAL PORTER STEM WORDS ==> 3



 ---- SNOWBALL STEMMING ----

['56-60', ')', '.']

 TOTAL SNOWBALL STEM WORDS ==> 3



 ---- LEMMATIZATION ----

['56-60', ')', '.']

 TOTAL LEMMATIZE WORDS ==> 3

************************************************************************************************************************

418 --> Association for Computational Linguistics. 


 ---- TOKENS ----

 ['Association', 'for', 'Computational', 'Linguistics', '.'] 

 TOTAL TOKENS ==> 5

 ---- POST ----

 [('Association', 'NNP'), ('for', 'IN'), ('Computational', 'NNP'), ('Linguistics', 'NNP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Association', 'Computational', 'Linguistics', '.']

 TOTAL FILTERED TOKENS ==>  4

 ---- POST FOR FILTERED TOKENS ----

 [('Association', 'NNP'), ('Computational', 'NNP'), ('Linguistics', 'NNP'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Association Computational', 'Computational Linguistics', 'Linguistics .'] 

 TOTAL BIGRAMS --> 3 



 ---- TRI-GRAMS ---- 

 ['Association Computational Linguistics', 'Computational Linguistics .'] 

 TOTAL TRIGRAMS --> 2 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['associ', 'comput', 'linguist', '.']

 TOTAL PORTER STEM WORDS ==> 4



 ---- SNOWBALL STEMMING ----

['associ', 'comput', 'linguist', '.']

 TOTAL SNOWBALL STEM WORDS ==> 4



 ---- LEMMATIZATION ----

['Association', 'Computational', 'Linguistics', '.']

 TOTAL LEMMATIZE WORDS ==> 4

************************************************************************************************************************

419 --> [20] Wahlster, W., & Kobsa, A. 


 ---- TOKENS ----

 ['[', '20', ']', 'Wahlster', ',', 'W.', ',', '&', 'Kobsa', ',', 'A', '.'] 

 TOTAL TOKENS ==> 12

 ---- POST ----

 [('[', 'RB'), ('20', 'CD'), (']', 'JJ'), ('Wahlster', 'NNP'), (',', ','), ('W.', 'NNP'), (',', ','), ('&', 'CC'), ('Kobsa', 'NNP'), (',', ','), ('A', 'NNP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['[', '20', ']', 'Wahlster', ',', 'W.', ',', '&', 'Kobsa', ',', '.']

 TOTAL FILTERED TOKENS ==>  11

 ---- POST FOR FILTERED TOKENS ----

 [('[', 'RB'), ('20', 'CD'), (']', 'JJ'), ('Wahlster', 'NNP'), (',', ','), ('W.', 'NNP'), (',', ','), ('&', 'CC'), ('Kobsa', 'NNP'), (',', ','), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['[ 20', '20 ]', '] Wahlster', 'Wahlster ,', ', W.', 'W. ,', ', &', '& Kobsa', 'Kobsa ,', ', .'] 

 TOTAL BIGRAMS --> 10 



 ---- TRI-GRAMS ---- 

 ['[ 20 ]', '20 ] Wahlster', '] Wahlster ,', 'Wahlster , W.', ', W. ,', 'W. , &', ', & Kobsa', '& Kobsa ,', 'Kobsa , .'] 

 TOTAL TRIGRAMS --> 9 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> ['Kobsa']
 TOTAL PERSON ENTITY --> 1 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['[', '20', ']', 'wahlster', ',', 'w.', ',', '&', 'kobsa', ',', '.']

 TOTAL PORTER STEM WORDS ==> 11



 ---- SNOWBALL STEMMING ----

['[', '20', ']', 'wahlster', ',', 'w.', ',', '&', 'kobsa', ',', '.']

 TOTAL SNOWBALL STEM WORDS ==> 11



 ---- LEMMATIZATION ----

['[', '20', ']', 'Wahlster', ',', 'W.', ',', '&', 'Kobsa', ',', '.']

 TOTAL LEMMATIZE WORDS ==> 11

************************************************************************************************************************

420 --> (1989). 


 ---- TOKENS ----

 ['(', '1989', ')', '.'] 

 TOTAL TOKENS ==> 4

 ---- POST ----

 [('(', '('), ('1989', 'CD'), (')', ')'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['(', '1989', ')', '.']

 TOTAL FILTERED TOKENS ==>  4

 ---- POST FOR FILTERED TOKENS ----

 [('(', '('), ('1989', 'CD'), (')', ')'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['( 1989', '1989 )', ') .'] 

 TOTAL BIGRAMS --> 3 



 ---- TRI-GRAMS ---- 

 ['( 1989 )', '1989 ) .'] 

 TOTAL TRIGRAMS --> 2 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['(', '1989', ')', '.']

 TOTAL PORTER STEM WORDS ==> 4



 ---- SNOWBALL STEMMING ----

['(', '1989', ')', '.']

 TOTAL SNOWBALL STEM WORDS ==> 4



 ---- LEMMATIZATION ----

['(', '1989', ')', '.']

 TOTAL LEMMATIZE WORDS ==> 4

************************************************************************************************************************

421 --> User models in dialog systems. 


 ---- TOKENS ----

 ['User', 'models', 'in', 'dialog', 'systems', '.'] 

 TOTAL TOKENS ==> 6

 ---- POST ----

 [('User', 'NNP'), ('models', 'NNS'), ('in', 'IN'), ('dialog', 'NN'), ('systems', 'NNS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['User', 'models', 'dialog', 'systems', '.']

 TOTAL FILTERED TOKENS ==>  5

 ---- POST FOR FILTERED TOKENS ----

 [('User', 'NNP'), ('models', 'NNS'), ('dialog', 'VBP'), ('systems', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['User models', 'models dialog', 'dialog systems', 'systems .'] 

 TOTAL BIGRAMS --> 4 



 ---- TRI-GRAMS ---- 

 ['User models dialog', 'models dialog systems', 'dialog systems .'] 

 TOTAL TRIGRAMS --> 3 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> ['User']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['user', 'model', 'dialog', 'system', '.']

 TOTAL PORTER STEM WORDS ==> 5



 ---- SNOWBALL STEMMING ----

['user', 'model', 'dialog', 'system', '.']

 TOTAL SNOWBALL STEM WORDS ==> 5



 ---- LEMMATIZATION ----

['User', 'model', 'dialog', 'system', '.']

 TOTAL LEMMATIZE WORDS ==> 5

************************************************************************************************************************

422 --> In User models in  dialog systems (pp. 


 ---- TOKENS ----

 ['In', 'User', 'models', 'in', 'dialog', 'systems', '(', 'pp', '.'] 

 TOTAL TOKENS ==> 9

 ---- POST ----

 [('In', 'IN'), ('User', 'NNP'), ('models', 'NNS'), ('in', 'IN'), ('dialog', 'NN'), ('systems', 'NNS'), ('(', '('), ('pp', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['User', 'models', 'dialog', 'systems', '(', 'pp', '.']

 TOTAL FILTERED TOKENS ==>  7

 ---- POST FOR FILTERED TOKENS ----

 [('User', 'NNP'), ('models', 'NNS'), ('dialog', 'VBP'), ('systems', 'NNS'), ('(', '('), ('pp', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['User models', 'models dialog', 'dialog systems', 'systems (', '( pp', 'pp .'] 

 TOTAL BIGRAMS --> 6 



 ---- TRI-GRAMS ---- 

 ['User models dialog', 'models dialog systems', 'dialog systems (', 'systems ( pp', '( pp .'] 

 TOTAL TRIGRAMS --> 5 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 []

 ---- PORTER STEMMING ----

['user', 'model', 'dialog', 'system', '(', 'pp', '.']

 TOTAL PORTER STEM WORDS ==> 7



 ---- SNOWBALL STEMMING ----

['user', 'model', 'dialog', 'system', '(', 'pp', '.']

 TOTAL SNOWBALL STEM WORDS ==> 7



 ---- LEMMATIZATION ----

['User', 'model', 'dialog', 'system', '(', 'pp', '.']

 TOTAL LEMMATIZE WORDS ==> 7

************************************************************************************************************************

423 --> 4-34). 


 ---- TOKENS ----

 ['4-34', ')', '.'] 

 TOTAL TOKENS ==> 3

 ---- POST ----

 [('4-34', 'JJ'), (')', ')'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['4-34', ')', '.']

 TOTAL FILTERED TOKENS ==>  3

 ---- POST FOR FILTERED TOKENS ----

 [('4-34', 'JJ'), (')', ')'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['4-34 )', ') .'] 

 TOTAL BIGRAMS --> 2 



 ---- TRI-GRAMS ---- 

 ['4-34 ) .'] 

 TOTAL TRIGRAMS --> 1 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 []

 ---- PORTER STEMMING ----

['4-34', ')', '.']

 TOTAL PORTER STEM WORDS ==> 3



 ---- SNOWBALL STEMMING ----

['4-34', ')', '.']

 TOTAL SNOWBALL STEM WORDS ==> 3



 ---- LEMMATIZATION ----

['4-34', ')', '.']

 TOTAL LEMMATIZE WORDS ==> 3

************************************************************************************************************************

424 --> Springer Berlin Heidelberg. 


 ---- TOKENS ----

 ['Springer', 'Berlin', 'Heidelberg', '.'] 

 TOTAL TOKENS ==> 4

 ---- POST ----

 [('Springer', 'NNP'), ('Berlin', 'NNP'), ('Heidelberg', 'NNP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Springer', 'Berlin', 'Heidelberg', '.']

 TOTAL FILTERED TOKENS ==>  4

 ---- POST FOR FILTERED TOKENS ----

 [('Springer', 'NNP'), ('Berlin', 'NNP'), ('Heidelberg', 'NNP'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Springer Berlin', 'Berlin Heidelberg', 'Heidelberg .'] 

 TOTAL BIGRAMS --> 3 



 ---- TRI-GRAMS ---- 

 ['Springer Berlin Heidelberg', 'Berlin Heidelberg .'] 

 TOTAL TRIGRAMS --> 2 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> ['Springer', 'Berlin Heidelberg']
 TOTAL PERSON ENTITY --> 2 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['springer', 'berlin', 'heidelberg', '.']

 TOTAL PORTER STEM WORDS ==> 4



 ---- SNOWBALL STEMMING ----

['springer', 'berlin', 'heidelberg', '.']

 TOTAL SNOWBALL STEM WORDS ==> 4



 ---- LEMMATIZATION ----

['Springer', 'Berlin', 'Heidelberg', '.']

 TOTAL LEMMATIZE WORDS ==> 4

************************************************************************************************************************

425 --> [21] McKeown, K.R. 


 ---- TOKENS ----

 ['[', '21', ']', 'McKeown', ',', 'K.R', '.'] 

 TOTAL TOKENS ==> 7

 ---- POST ----

 [('[', 'RB'), ('21', 'CD'), (']', 'JJ'), ('McKeown', 'NNP'), (',', ','), ('K.R', 'NNP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['[', '21', ']', 'McKeown', ',', 'K.R', '.']

 TOTAL FILTERED TOKENS ==>  7

 ---- POST FOR FILTERED TOKENS ----

 [('[', 'RB'), ('21', 'CD'), (']', 'JJ'), ('McKeown', 'NNP'), (',', ','), ('K.R', 'NNP'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['[ 21', '21 ]', '] McKeown', 'McKeown ,', ', K.R', 'K.R .'] 

 TOTAL BIGRAMS --> 6 



 ---- TRI-GRAMS ---- 

 ['[ 21 ]', '21 ] McKeown', '] McKeown ,', 'McKeown , K.R', ', K.R .'] 

 TOTAL TRIGRAMS --> 5 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> ['McKeown']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['[', '21', ']', 'mckeown', ',', 'k.r', '.']

 TOTAL PORTER STEM WORDS ==> 7



 ---- SNOWBALL STEMMING ----

['[', '21', ']', 'mckeown', ',', 'k.r', '.']

 TOTAL SNOWBALL STEM WORDS ==> 7



 ---- LEMMATIZATION ----

['[', '21', ']', 'McKeown', ',', 'K.R', '.']

 TOTAL LEMMATIZE WORDS ==> 7

************************************************************************************************************************

426 --> Text generation , Cambridge: Cambridge University Press , 1985. 


 ---- TOKENS ----

 ['Text', 'generation', ',', 'Cambridge', ':', 'Cambridge', 'University', 'Press', ',', '1985', '.'] 

 TOTAL TOKENS ==> 11

 ---- POST ----

 [('Text', 'NNP'), ('generation', 'NN'), (',', ','), ('Cambridge', 'NNP'), (':', ':'), ('Cambridge', 'NNP'), ('University', 'NNP'), ('Press', 'NNP'), (',', ','), ('1985', 'CD'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Text', 'generation', ',', 'Cambridge', ':', 'Cambridge', 'University', 'Press', ',', '1985', '.']

 TOTAL FILTERED TOKENS ==>  11

 ---- POST FOR FILTERED TOKENS ----

 [('Text', 'NNP'), ('generation', 'NN'), (',', ','), ('Cambridge', 'NNP'), (':', ':'), ('Cambridge', 'NNP'), ('University', 'NNP'), ('Press', 'NNP'), (',', ','), ('1985', 'CD'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Text generation', 'generation ,', ', Cambridge', 'Cambridge :', ': Cambridge', 'Cambridge University', 'University Press', 'Press ,', ', 1985', '1985 .'] 

 TOTAL BIGRAMS --> 10 



 ---- TRI-GRAMS ---- 

 ['Text generation ,', 'generation , Cambridge', ', Cambridge :', 'Cambridge : Cambridge', ': Cambridge University', 'Cambridge University Press', 'University Press ,', 'Press , 1985', ', 1985 .'] 

 TOTAL TRIGRAMS --> 9 



 ---- NOUN PHRASES ---- 

 ['generation'] 

 TOTAL NOUN PHRASES --> 1 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> ['Cambridge', 'Cambridge University Press']
 TOTAL PERSON ENTITY --> 2 


 GPE ---> ['Text']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['text', 'gener', ',', 'cambridg', ':', 'cambridg', 'univers', 'press', ',', '1985', '.']

 TOTAL PORTER STEM WORDS ==> 11



 ---- SNOWBALL STEMMING ----

['text', 'generat', ',', 'cambridg', ':', 'cambridg', 'univers', 'press', ',', '1985', '.']

 TOTAL SNOWBALL STEM WORDS ==> 11



 ---- LEMMATIZATION ----

['Text', 'generation', ',', 'Cambridge', ':', 'Cambridge', 'University', 'Press', ',', '1985', '.']

 TOTAL LEMMATIZE WORDS ==> 11

************************************************************************************************************************

427 --> [22] Small S.L., Cortell G.W., and Tanenhaus , M.K. 


 ---- TOKENS ----

 ['[', '22', ']', 'Small', 'S.L.', ',', 'Cortell', 'G.W.', ',', 'and', 'Tanenhaus', ',', 'M.K', '.'] 

 TOTAL TOKENS ==> 14

 ---- POST ----

 [('[', 'RB'), ('22', 'CD'), (']', 'NNP'), ('Small', 'NNP'), ('S.L.', 'NNP'), (',', ','), ('Cortell', 'NNP'), ('G.W.', 'NNP'), (',', ','), ('and', 'CC'), ('Tanenhaus', 'NNP'), (',', ','), ('M.K', 'NNP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['[', '22', ']', 'Small', 'S.L.', ',', 'Cortell', 'G.W.', ',', 'Tanenhaus', ',', 'M.K', '.']

 TOTAL FILTERED TOKENS ==>  13

 ---- POST FOR FILTERED TOKENS ----

 [('[', 'RB'), ('22', 'CD'), (']', 'NNP'), ('Small', 'NNP'), ('S.L.', 'NNP'), (',', ','), ('Cortell', 'NNP'), ('G.W.', 'NNP'), (',', ','), ('Tanenhaus', 'NNP'), (',', ','), ('M.K', 'NNP'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['[ 22', '22 ]', '] Small', 'Small S.L.', 'S.L. ,', ', Cortell', 'Cortell G.W.', 'G.W. ,', ', Tanenhaus', 'Tanenhaus ,', ', M.K', 'M.K .'] 

 TOTAL BIGRAMS --> 12 



 ---- TRI-GRAMS ---- 

 ['[ 22 ]', '22 ] Small', '] Small S.L.', 'Small S.L. ,', 'S.L. , Cortell', ', Cortell G.W.', 'Cortell G.W. ,', 'G.W. , Tanenhaus', ', Tanenhaus ,', 'Tanenhaus , M.K', ', M.K .'] 

 TOTAL TRIGRAMS --> 11 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> ['Cortell']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> ['Tanenhaus']
 TOTAL PERSON ENTITY --> 1 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['[', '22', ']', 'small', 's.l.', ',', 'cortel', 'g.w.', ',', 'tanenhau', ',', 'm.k', '.']

 TOTAL PORTER STEM WORDS ==> 13



 ---- SNOWBALL STEMMING ----

['[', '22', ']', 'small', 's.l.', ',', 'cortel', 'g.w.', ',', 'tanenhaus', ',', 'm.k', '.']

 TOTAL SNOWBALL STEM WORDS ==> 13



 ---- LEMMATIZATION ----

['[', '22', ']', 'Small', 'S.L.', ',', 'Cortell', 'G.W.', ',', 'Tanenhaus', ',', 'M.K', '.']

 TOTAL LEMMATIZE WORDS ==> 13

************************************************************************************************************************

428 --> Lexical Ambiguity Resolutions , San  Mateo , CA : Morgan Kauffman, 1988. 


 ---- TOKENS ----

 ['Lexical', 'Ambiguity', 'Resolutions', ',', 'San', 'Mateo', ',', 'CA', ':', 'Morgan', 'Kauffman', ',', '1988', '.'] 

 TOTAL TOKENS ==> 14

 ---- POST ----

 [('Lexical', 'JJ'), ('Ambiguity', 'NNP'), ('Resolutions', 'NNP'), (',', ','), ('San', 'NNP'), ('Mateo', 'NNP'), (',', ','), ('CA', 'NNP'), (':', ':'), ('Morgan', 'NNP'), ('Kauffman', 'NNP'), (',', ','), ('1988', 'CD'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Lexical', 'Ambiguity', 'Resolutions', ',', 'San', 'Mateo', ',', 'CA', ':', 'Morgan', 'Kauffman', ',', '1988', '.']

 TOTAL FILTERED TOKENS ==>  14

 ---- POST FOR FILTERED TOKENS ----

 [('Lexical', 'JJ'), ('Ambiguity', 'NNP'), ('Resolutions', 'NNP'), (',', ','), ('San', 'NNP'), ('Mateo', 'NNP'), (',', ','), ('CA', 'NNP'), (':', ':'), ('Morgan', 'NNP'), ('Kauffman', 'NNP'), (',', ','), ('1988', 'CD'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Lexical Ambiguity', 'Ambiguity Resolutions', 'Resolutions ,', ', San', 'San Mateo', 'Mateo ,', ', CA', 'CA :', ': Morgan', 'Morgan Kauffman', 'Kauffman ,', ', 1988', '1988 .'] 

 TOTAL BIGRAMS --> 13 



 ---- TRI-GRAMS ---- 

 ['Lexical Ambiguity Resolutions', 'Ambiguity Resolutions ,', 'Resolutions , San', ', San Mateo', 'San Mateo ,', 'Mateo , CA', ', CA :', 'CA : Morgan', ': Morgan Kauffman', 'Morgan Kauffman ,', 'Kauffman , 1988', ', 1988 .'] 

 TOTAL TRIGRAMS --> 12 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> ['Ambiguity Resolutions']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> ['Lexical', 'San Mateo', 'Morgan Kauffman']
 TOTAL PERSON ENTITY --> 3 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['lexic', 'ambigu', 'resolut', ',', 'san', 'mateo', ',', 'ca', ':', 'morgan', 'kauffman', ',', '1988', '.']

 TOTAL PORTER STEM WORDS ==> 14



 ---- SNOWBALL STEMMING ----

['lexic', 'ambigu', 'resolut', ',', 'san', 'mateo', ',', 'ca', ':', 'morgan', 'kauffman', ',', '1988', '.']

 TOTAL SNOWBALL STEM WORDS ==> 14



 ---- LEMMATIZATION ----

['Lexical', 'Ambiguity', 'Resolutions', ',', 'San', 'Mateo', ',', 'CA', ':', 'Morgan', 'Kauffman', ',', '1988', '.']

 TOTAL LEMMATIZE WORDS ==> 14

************************************************************************************************************************

429 --> [23] Manning, C. D., & Schütze, H. (1999). 


 ---- TOKENS ----

 ['[', '23', ']', 'Manning', ',', 'C.', 'D.', ',', '&', 'Schütze', ',', 'H.', '(', '1999', ')', '.'] 

 TOTAL TOKENS ==> 16

 ---- POST ----

 [('[', 'RB'), ('23', 'CD'), (']', 'JJ'), ('Manning', 'NNP'), (',', ','), ('C.', 'NNP'), ('D.', 'NNP'), (',', ','), ('&', 'CC'), ('Schütze', 'NNP'), (',', ','), ('H.', 'NNP'), ('(', '('), ('1999', 'CD'), (')', ')'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['[', '23', ']', 'Manning', ',', 'C.', 'D.', ',', '&', 'Schütze', ',', 'H.', '(', '1999', ')', '.']

 TOTAL FILTERED TOKENS ==>  16

 ---- POST FOR FILTERED TOKENS ----

 [('[', 'RB'), ('23', 'CD'), (']', 'JJ'), ('Manning', 'NNP'), (',', ','), ('C.', 'NNP'), ('D.', 'NNP'), (',', ','), ('&', 'CC'), ('Schütze', 'NNP'), (',', ','), ('H.', 'NNP'), ('(', '('), ('1999', 'CD'), (')', ')'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['[ 23', '23 ]', '] Manning', 'Manning ,', ', C.', 'C. D.', 'D. ,', ', &', '& Schütze', 'Schütze ,', ', H.', 'H. (', '( 1999', '1999 )', ') .'] 

 TOTAL BIGRAMS --> 15 



 ---- TRI-GRAMS ---- 

 ['[ 23 ]', '23 ] Manning', '] Manning ,', 'Manning , C.', ', C. D.', 'C. D. ,', 'D. , &', ', & Schütze', '& Schütze ,', 'Schütze , H.', ', H. (', 'H. ( 1999', '( 1999 )', '1999 ) .'] 

 TOTAL TRIGRAMS --> 14 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> ['Schütze']
 TOTAL PERSON ENTITY --> 1 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['[', '23', ']', 'man', ',', 'c.', 'd.', ',', '&', 'schütze', ',', 'h.', '(', '1999', ')', '.']

 TOTAL PORTER STEM WORDS ==> 16



 ---- SNOWBALL STEMMING ----

['[', '23', ']', 'man', ',', 'c.', 'd.', ',', '&', 'schütze', ',', 'h.', '(', '1999', ')', '.']

 TOTAL SNOWBALL STEM WORDS ==> 16



 ---- LEMMATIZATION ----

['[', '23', ']', 'Manning', ',', 'C.', 'D.', ',', '&', 'Schütze', ',', 'H.', '(', '1999', ')', '.']

 TOTAL LEMMATIZE WORDS ==> 16

************************************************************************************************************************

430 --> Foundations of statistical natural language  processing (Vol. 


 ---- TOKENS ----

 ['Foundations', 'of', 'statistical', 'natural', 'language', 'processing', '(', 'Vol', '.'] 

 TOTAL TOKENS ==> 9

 ---- POST ----

 [('Foundations', 'NNS'), ('of', 'IN'), ('statistical', 'JJ'), ('natural', 'JJ'), ('language', 'NN'), ('processing', 'NN'), ('(', '('), ('Vol', 'NNP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Foundations', 'statistical', 'natural', 'language', 'processing', '(', 'Vol', '.']

 TOTAL FILTERED TOKENS ==>  8

 ---- POST FOR FILTERED TOKENS ----

 [('Foundations', 'NNS'), ('statistical', 'JJ'), ('natural', 'JJ'), ('language', 'NN'), ('processing', 'NN'), ('(', '('), ('Vol', 'NNP'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Foundations statistical', 'statistical natural', 'natural language', 'language processing', 'processing (', '( Vol', 'Vol .'] 

 TOTAL BIGRAMS --> 7 



 ---- TRI-GRAMS ---- 

 ['Foundations statistical natural', 'statistical natural language', 'natural language processing', 'language processing (', 'processing ( Vol', '( Vol .'] 

 TOTAL TRIGRAMS --> 6 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 []

 ---- PORTER STEMMING ----

['foundat', 'statist', 'natur', 'languag', 'process', '(', 'vol', '.']

 TOTAL PORTER STEM WORDS ==> 8



 ---- SNOWBALL STEMMING ----

['foundat', 'statist', 'natur', 'languag', 'process', '(', 'vol', '.']

 TOTAL SNOWBALL STEM WORDS ==> 8



 ---- LEMMATIZATION ----

['Foundations', 'statistical', 'natural', 'language', 'processing', '(', 'Vol', '.']

 TOTAL LEMMATIZE WORDS ==> 8

************************************************************************************************************************

431 --> 999). 


 ---- TOKENS ----

 ['999', ')', '.'] 

 TOTAL TOKENS ==> 3

 ---- POST ----

 [('999', 'CD'), (')', ')'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['999', ')', '.']

 TOTAL FILTERED TOKENS ==>  3

 ---- POST FOR FILTERED TOKENS ----

 [('999', 'CD'), (')', ')'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['999 )', ') .'] 

 TOTAL BIGRAMS --> 2 



 ---- TRI-GRAMS ---- 

 ['999 ) .'] 

 TOTAL TRIGRAMS --> 1 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 []

 ---- PORTER STEMMING ----

['999', ')', '.']

 TOTAL PORTER STEM WORDS ==> 3



 ---- SNOWBALL STEMMING ----

['999', ')', '.']

 TOTAL SNOWBALL STEM WORDS ==> 3



 ---- LEMMATIZATION ----

['999', ')', '.']

 TOTAL LEMMATIZE WORDS ==> 3

************************************************************************************************************************

432 --> Cambridge: MIT press. 


 ---- TOKENS ----

 ['Cambridge', ':', 'MIT', 'press', '.'] 

 TOTAL TOKENS ==> 5

 ---- POST ----

 [('Cambridge', 'NN'), (':', ':'), ('MIT', 'NNP'), ('press', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Cambridge', ':', 'MIT', 'press', '.']

 TOTAL FILTERED TOKENS ==>  5

 ---- POST FOR FILTERED TOKENS ----

 [('Cambridge', 'NN'), (':', ':'), ('MIT', 'NNP'), ('press', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Cambridge :', ': MIT', 'MIT press', 'press .'] 

 TOTAL BIGRAMS --> 4 



 ---- TRI-GRAMS ---- 

 ['Cambridge : MIT', ': MIT press', 'MIT press .'] 

 TOTAL TRIGRAMS --> 3 



 ---- NOUN PHRASES ---- 

 ['Cambridge', 'press'] 

 TOTAL NOUN PHRASES --> 2 



 ---- NER ----

 
 ORGANIZATION ---> ['MIT']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> ['Cambridge']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['cambridg', ':', 'mit', 'press', '.']

 TOTAL PORTER STEM WORDS ==> 5



 ---- SNOWBALL STEMMING ----

['cambridg', ':', 'mit', 'press', '.']

 TOTAL SNOWBALL STEM WORDS ==> 5



 ---- LEMMATIZATION ----

['Cambridge', ':', 'MIT', 'press', '.']

 TOTAL LEMMATIZE WORDS ==> 5

************************************************************************************************************************

433 --> [24] Mani, I., & Maybury, M. T. 


 ---- TOKENS ----

 ['[', '24', ']', 'Mani', ',', 'I.', ',', '&', 'Maybury', ',', 'M.', 'T', '.'] 

 TOTAL TOKENS ==> 13

 ---- POST ----

 [('[', 'RB'), ('24', 'CD'), (']', 'JJ'), ('Mani', 'NNP'), (',', ','), ('I.', 'NNP'), (',', ','), ('&', 'CC'), ('Maybury', 'NNP'), (',', ','), ('M.', 'NNP'), ('T', 'NNP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['[', '24', ']', 'Mani', ',', 'I.', ',', '&', 'Maybury', ',', 'M.', '.']

 TOTAL FILTERED TOKENS ==>  12

 ---- POST FOR FILTERED TOKENS ----

 [('[', 'RB'), ('24', 'CD'), (']', 'JJ'), ('Mani', 'NNP'), (',', ','), ('I.', 'NNP'), (',', ','), ('&', 'CC'), ('Maybury', 'NNP'), (',', ','), ('M.', 'NNP'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['[ 24', '24 ]', '] Mani', 'Mani ,', ', I.', 'I. ,', ', &', '& Maybury', 'Maybury ,', ', M.', 'M. .'] 

 TOTAL BIGRAMS --> 11 



 ---- TRI-GRAMS ---- 

 ['[ 24 ]', '24 ] Mani', '] Mani ,', 'Mani , I.', ', I. ,', 'I. , &', ', & Maybury', '& Maybury ,', 'Maybury , M.', ', M. .'] 

 TOTAL TRIGRAMS --> 10 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> ['Maybury']
 TOTAL PERSON ENTITY --> 1 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['[', '24', ']', 'mani', ',', 'i.', ',', '&', 'mayburi', ',', 'm.', '.']

 TOTAL PORTER STEM WORDS ==> 12



 ---- SNOWBALL STEMMING ----

['[', '24', ']', 'mani', ',', 'i.', ',', '&', 'mayburi', ',', 'm.', '.']

 TOTAL SNOWBALL STEM WORDS ==> 12



 ---- LEMMATIZATION ----

['[', '24', ']', 'Mani', ',', 'I.', ',', '&', 'Maybury', ',', 'M.', '.']

 TOTAL LEMMATIZE WORDS ==> 12

************************************************************************************************************************

434 --> (Eds.). 


 ---- TOKENS ----

 ['(', 'Eds', '.', ')', '.'] 

 TOTAL TOKENS ==> 5

 ---- POST ----

 [('(', '('), ('Eds', 'NNP'), ('.', '.'), (')', ')'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['(', 'Eds', '.', ')', '.']

 TOTAL FILTERED TOKENS ==>  5

 ---- POST FOR FILTERED TOKENS ----

 [('(', '('), ('Eds', 'NNP'), ('.', '.'), (')', ')'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['( Eds', 'Eds .', '. )', ') .'] 

 TOTAL BIGRAMS --> 4 



 ---- TRI-GRAMS ---- 

 ['( Eds .', 'Eds . )', '. ) .'] 

 TOTAL TRIGRAMS --> 3 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['(', 'ed', '.', ')', '.']

 TOTAL PORTER STEM WORDS ==> 5



 ---- SNOWBALL STEMMING ----

['(', 'ed', '.', ')', '.']

 TOTAL SNOWBALL STEM WORDS ==> 5



 ---- LEMMATIZATION ----

['(', 'Eds', '.', ')', '.']

 TOTAL LEMMATIZE WORDS ==> 5

************************************************************************************************************************

435 --> (1999). 


 ---- TOKENS ----

 ['(', '1999', ')', '.'] 

 TOTAL TOKENS ==> 4

 ---- POST ----

 [('(', '('), ('1999', 'CD'), (')', ')'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['(', '1999', ')', '.']

 TOTAL FILTERED TOKENS ==>  4

 ---- POST FOR FILTERED TOKENS ----

 [('(', '('), ('1999', 'CD'), (')', ')'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['( 1999', '1999 )', ') .'] 

 TOTAL BIGRAMS --> 3 



 ---- TRI-GRAMS ---- 

 ['( 1999 )', '1999 ) .'] 

 TOTAL TRIGRAMS --> 2 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['(', '1999', ')', '.']

 TOTAL PORTER STEM WORDS ==> 4



 ---- SNOWBALL STEMMING ----

['(', '1999', ')', '.']

 TOTAL SNOWBALL STEM WORDS ==> 4



 ---- LEMMATIZATION ----

['(', '1999', ')', '.']

 TOTAL LEMMATIZE WORDS ==> 4

************************************************************************************************************************

436 --> Advances in automatic text  summarization (Vol. 


 ---- TOKENS ----

 ['Advances', 'in', 'automatic', 'text', 'summarization', '(', 'Vol', '.'] 

 TOTAL TOKENS ==> 8

 ---- POST ----

 [('Advances', 'NNS'), ('in', 'IN'), ('automatic', 'JJ'), ('text', 'NN'), ('summarization', 'NN'), ('(', '('), ('Vol', 'NNP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Advances', 'automatic', 'text', 'summarization', '(', 'Vol', '.']

 TOTAL FILTERED TOKENS ==>  7

 ---- POST FOR FILTERED TOKENS ----

 [('Advances', 'NNS'), ('automatic', 'JJ'), ('text', 'JJ'), ('summarization', 'NN'), ('(', '('), ('Vol', 'NNP'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Advances automatic', 'automatic text', 'text summarization', 'summarization (', '( Vol', 'Vol .'] 

 TOTAL BIGRAMS --> 6 



 ---- TRI-GRAMS ---- 

 ['Advances automatic text', 'automatic text summarization', 'text summarization (', 'summarization ( Vol', '( Vol .'] 

 TOTAL TRIGRAMS --> 5 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 []

 ---- PORTER STEMMING ----

['advanc', 'automat', 'text', 'summar', '(', 'vol', '.']

 TOTAL PORTER STEM WORDS ==> 7



 ---- SNOWBALL STEMMING ----

['advanc', 'automat', 'text', 'summar', '(', 'vol', '.']

 TOTAL SNOWBALL STEM WORDS ==> 7



 ---- LEMMATIZATION ----

['Advances', 'automatic', 'text', 'summarization', '(', 'Vol', '.']

 TOTAL LEMMATIZE WORDS ==> 7

************************************************************************************************************************

437 --> 293). 


 ---- TOKENS ----

 ['293', ')', '.'] 

 TOTAL TOKENS ==> 3

 ---- POST ----

 [('293', 'CD'), (')', ')'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['293', ')', '.']

 TOTAL FILTERED TOKENS ==>  3

 ---- POST FOR FILTERED TOKENS ----

 [('293', 'CD'), (')', ')'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['293 )', ') .'] 

 TOTAL BIGRAMS --> 2 



 ---- TRI-GRAMS ---- 

 ['293 ) .'] 

 TOTAL TRIGRAMS --> 1 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 []

 ---- PORTER STEMMING ----

['293', ')', '.']

 TOTAL PORTER STEM WORDS ==> 3



 ---- SNOWBALL STEMMING ----

['293', ')', '.']

 TOTAL SNOWBALL STEM WORDS ==> 3



 ---- LEMMATIZATION ----

['293', ')', '.']

 TOTAL LEMMATIZE WORDS ==> 3

************************************************************************************************************************

438 --> Cambridge, MA: MIT press. 


 ---- TOKENS ----

 ['Cambridge', ',', 'MA', ':', 'MIT', 'press', '.'] 

 TOTAL TOKENS ==> 7

 ---- POST ----

 [('Cambridge', 'NNP'), (',', ','), ('MA', 'NNP'), (':', ':'), ('MIT', 'NNP'), ('press', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Cambridge', ',', ':', 'MIT', 'press', '.']

 TOTAL FILTERED TOKENS ==>  6

 ---- POST FOR FILTERED TOKENS ----

 [('Cambridge', 'NNP'), (',', ','), (':', ':'), ('MIT', 'NNP'), ('press', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Cambridge ,', ', :', ': MIT', 'MIT press', 'press .'] 

 TOTAL BIGRAMS --> 5 



 ---- TRI-GRAMS ---- 

 ['Cambridge , :', ', : MIT', ': MIT press', 'MIT press .'] 

 TOTAL TRIGRAMS --> 4 



 ---- NOUN PHRASES ---- 

 ['press'] 

 TOTAL NOUN PHRASES --> 1 



 ---- NER ----

 
 ORGANIZATION ---> ['MIT']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> ['Cambridge']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['cambridg', ',', ':', 'mit', 'press', '.']

 TOTAL PORTER STEM WORDS ==> 6



 ---- SNOWBALL STEMMING ----

['cambridg', ',', ':', 'mit', 'press', '.']

 TOTAL SNOWBALL STEM WORDS ==> 6



 ---- LEMMATIZATION ----

['Cambridge', ',', ':', 'MIT', 'press', '.']

 TOTAL LEMMATIZE WORDS ==> 6

************************************************************************************************************************

439 --> [25] Yi, J., Nasukawa, T., Bunescu, R., & Niblack, W. (2003, November). 


 ---- TOKENS ----

 ['[', '25', ']', 'Yi', ',', 'J.', ',', 'Nasukawa', ',', 'T.', ',', 'Bunescu', ',', 'R.', ',', '&', 'Niblack', ',', 'W.', '(', '2003', ',', 'November', ')', '.'] 

 TOTAL TOKENS ==> 25

 ---- POST ----

 [('[', 'RB'), ('25', 'CD'), (']', 'JJ'), ('Yi', 'NNP'), (',', ','), ('J.', 'NNP'), (',', ','), ('Nasukawa', 'NNP'), (',', ','), ('T.', 'NNP'), (',', ','), ('Bunescu', 'NNP'), (',', ','), ('R.', 'NNP'), (',', ','), ('&', 'CC'), ('Niblack', 'NNP'), (',', ','), ('W.', 'NNP'), ('(', '('), ('2003', 'CD'), (',', ','), ('November', 'NNP'), (')', ')'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['[', '25', ']', 'Yi', ',', 'J.', ',', 'Nasukawa', ',', 'T.', ',', 'Bunescu', ',', 'R.', ',', '&', 'Niblack', ',', 'W.', '(', '2003', ',', 'November', ')', '.']

 TOTAL FILTERED TOKENS ==>  25

 ---- POST FOR FILTERED TOKENS ----

 [('[', 'RB'), ('25', 'CD'), (']', 'JJ'), ('Yi', 'NNP'), (',', ','), ('J.', 'NNP'), (',', ','), ('Nasukawa', 'NNP'), (',', ','), ('T.', 'NNP'), (',', ','), ('Bunescu', 'NNP'), (',', ','), ('R.', 'NNP'), (',', ','), ('&', 'CC'), ('Niblack', 'NNP'), (',', ','), ('W.', 'NNP'), ('(', '('), ('2003', 'CD'), (',', ','), ('November', 'NNP'), (')', ')'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['[ 25', '25 ]', '] Yi', 'Yi ,', ', J.', 'J. ,', ', Nasukawa', 'Nasukawa ,', ', T.', 'T. ,', ', Bunescu', 'Bunescu ,', ', R.', 'R. ,', ', &', '& Niblack', 'Niblack ,', ', W.', 'W. (', '( 2003', '2003 ,', ', November', 'November )', ') .'] 

 TOTAL BIGRAMS --> 24 



 ---- TRI-GRAMS ---- 

 ['[ 25 ]', '25 ] Yi', '] Yi ,', 'Yi , J.', ', J. ,', 'J. , Nasukawa', ', Nasukawa ,', 'Nasukawa , T.', ', T. ,', 'T. , Bunescu', ', Bunescu ,', 'Bunescu , R.', ', R. ,', 'R. , &', ', & Niblack', '& Niblack ,', 'Niblack , W.', ', W. (', 'W. ( 2003', '( 2003 ,', '2003 , November', ', November )', 'November ) .'] 

 TOTAL TRIGRAMS --> 23 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> ['Bunescu', 'Niblack']
 TOTAL PERSON ENTITY --> 2 


 GPE ---> ['Nasukawa']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['[', '25', ']', 'yi', ',', 'j.', ',', 'nasukawa', ',', 't.', ',', 'bunescu', ',', 'r.', ',', '&', 'niblack', ',', 'w.', '(', '2003', ',', 'novemb', ')', '.']

 TOTAL PORTER STEM WORDS ==> 25



 ---- SNOWBALL STEMMING ----

['[', '25', ']', 'yi', ',', 'j.', ',', 'nasukawa', ',', 't.', ',', 'bunescu', ',', 'r.', ',', '&', 'niblack', ',', 'w.', '(', '2003', ',', 'novemb', ')', '.']

 TOTAL SNOWBALL STEM WORDS ==> 25



 ---- LEMMATIZATION ----

['[', '25', ']', 'Yi', ',', 'J.', ',', 'Nasukawa', ',', 'T.', ',', 'Bunescu', ',', 'R.', ',', '&', 'Niblack', ',', 'W.', '(', '2003', ',', 'November', ')', '.']

 TOTAL LEMMATIZE WORDS ==> 25

************************************************************************************************************************

440 --> Sentiment  analyzer: Extracting sentiments about a given topic using natural language processing  techniques. 


 ---- TOKENS ----

 ['Sentiment', 'analyzer', ':', 'Extracting', 'sentiments', 'about', 'a', 'given', 'topic', 'using', 'natural', 'language', 'processing', 'techniques', '.'] 

 TOTAL TOKENS ==> 15

 ---- POST ----

 [('Sentiment', 'JJ'), ('analyzer', 'NN'), (':', ':'), ('Extracting', 'JJ'), ('sentiments', 'NNS'), ('about', 'IN'), ('a', 'DT'), ('given', 'VBN'), ('topic', 'NN'), ('using', 'VBG'), ('natural', 'JJ'), ('language', 'NN'), ('processing', 'NN'), ('techniques', 'NNS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Sentiment', 'analyzer', ':', 'Extracting', 'sentiments', 'given', 'topic', 'using', 'natural', 'language', 'processing', 'techniques', '.']

 TOTAL FILTERED TOKENS ==>  13

 ---- POST FOR FILTERED TOKENS ----

 [('Sentiment', 'JJ'), ('analyzer', 'NN'), (':', ':'), ('Extracting', 'JJ'), ('sentiments', 'NNS'), ('given', 'VBN'), ('topic', 'RP'), ('using', 'VBG'), ('natural', 'JJ'), ('language', 'NN'), ('processing', 'NN'), ('techniques', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Sentiment analyzer', 'analyzer :', ': Extracting', 'Extracting sentiments', 'sentiments given', 'given topic', 'topic using', 'using natural', 'natural language', 'language processing', 'processing techniques', 'techniques .'] 

 TOTAL BIGRAMS --> 12 



 ---- TRI-GRAMS ---- 

 ['Sentiment analyzer :', 'analyzer : Extracting', ': Extracting sentiments', 'Extracting sentiments given', 'sentiments given topic', 'given topic using', 'topic using natural', 'using natural language', 'natural language processing', 'language processing techniques', 'processing techniques .'] 

 TOTAL TRIGRAMS --> 11 



 ---- NOUN PHRASES ---- 

 ['Sentiment analyzer', 'natural language', 'processing'] 

 TOTAL NOUN PHRASES --> 3 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['sentiment', 'analyz', ':', 'extract', 'sentiment', 'given', 'topic', 'use', 'natur', 'languag', 'process', 'techniqu', '.']

 TOTAL PORTER STEM WORDS ==> 13



 ---- SNOWBALL STEMMING ----

['sentiment', 'analyz', ':', 'extract', 'sentiment', 'given', 'topic', 'use', 'natur', 'languag', 'process', 'techniqu', '.']

 TOTAL SNOWBALL STEM WORDS ==> 13



 ---- LEMMATIZATION ----

['Sentiment', 'analyzer', ':', 'Extracting', 'sentiment', 'given', 'topic', 'using', 'natural', 'language', 'processing', 'technique', '.']

 TOTAL LEMMATIZE WORDS ==> 13

************************************************************************************************************************

441 --> In Data Mining, 2003. 


 ---- TOKENS ----

 ['In', 'Data', 'Mining', ',', '2003', '.'] 

 TOTAL TOKENS ==> 6

 ---- POST ----

 [('In', 'IN'), ('Data', 'NNP'), ('Mining', 'NNP'), (',', ','), ('2003', 'CD'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Data', 'Mining', ',', '2003', '.']

 TOTAL FILTERED TOKENS ==>  5

 ---- POST FOR FILTERED TOKENS ----

 [('Data', 'NNP'), ('Mining', 'NNP'), (',', ','), ('2003', 'CD'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Data Mining', 'Mining ,', ', 2003', '2003 .'] 

 TOTAL BIGRAMS --> 4 



 ---- TRI-GRAMS ---- 

 ['Data Mining ,', 'Mining , 2003', ', 2003 .'] 

 TOTAL TRIGRAMS --> 3 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> ['Mining']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> ['Data']
 TOTAL PERSON ENTITY --> 1 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['data', 'mine', ',', '2003', '.']

 TOTAL PORTER STEM WORDS ==> 5



 ---- SNOWBALL STEMMING ----

['data', 'mine', ',', '2003', '.']

 TOTAL SNOWBALL STEM WORDS ==> 5



 ---- LEMMATIZATION ----

['Data', 'Mining', ',', '2003', '.']

 TOTAL LEMMATIZE WORDS ==> 5

************************************************************************************************************************

442 --> ICDM 2003. 


 ---- TOKENS ----

 ['ICDM', '2003', '.'] 

 TOTAL TOKENS ==> 3

 ---- POST ----

 [('ICDM', 'JJ'), ('2003', 'CD'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['ICDM', '2003', '.']

 TOTAL FILTERED TOKENS ==>  3

 ---- POST FOR FILTERED TOKENS ----

 [('ICDM', 'JJ'), ('2003', 'CD'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['ICDM 2003', '2003 .'] 

 TOTAL BIGRAMS --> 2 



 ---- TRI-GRAMS ---- 

 ['ICDM 2003 .'] 

 TOTAL TRIGRAMS --> 1 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['icdm', '2003', '.']

 TOTAL PORTER STEM WORDS ==> 3



 ---- SNOWBALL STEMMING ----

['icdm', '2003', '.']

 TOTAL SNOWBALL STEM WORDS ==> 3



 ---- LEMMATIZATION ----

['ICDM', '2003', '.']

 TOTAL LEMMATIZE WORDS ==> 3

************************************************************************************************************************

443 --> Third IEEE International Conference on (pp. 


 ---- TOKENS ----

 ['Third', 'IEEE', 'International', 'Conference', 'on', '(', 'pp', '.'] 

 TOTAL TOKENS ==> 8

 ---- POST ----

 [('Third', 'NNP'), ('IEEE', 'NNP'), ('International', 'NNP'), ('Conference', 'NNP'), ('on', 'IN'), ('(', '('), ('pp', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Third', 'IEEE', 'International', 'Conference', '(', 'pp', '.']

 TOTAL FILTERED TOKENS ==>  7

 ---- POST FOR FILTERED TOKENS ----

 [('Third', 'NNP'), ('IEEE', 'NNP'), ('International', 'NNP'), ('Conference', 'NNP'), ('(', '('), ('pp', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Third IEEE', 'IEEE International', 'International Conference', 'Conference (', '( pp', 'pp .'] 

 TOTAL BIGRAMS --> 6 



 ---- TRI-GRAMS ---- 

 ['Third IEEE International', 'IEEE International Conference', 'International Conference (', 'Conference ( pp', '( pp .'] 

 TOTAL TRIGRAMS --> 5 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 []

 ---- PORTER STEMMING ----

['third', 'ieee', 'intern', 'confer', '(', 'pp', '.']

 TOTAL PORTER STEM WORDS ==> 7



 ---- SNOWBALL STEMMING ----

['third', 'ieee', 'intern', 'confer', '(', 'pp', '.']

 TOTAL SNOWBALL STEM WORDS ==> 7



 ---- LEMMATIZATION ----

['Third', 'IEEE', 'International', 'Conference', '(', 'pp', '.']

 TOTAL LEMMATIZE WORDS ==> 7

************************************************************************************************************************

444 --> 427-434). 


 ---- TOKENS ----

 ['427-434', ')', '.'] 

 TOTAL TOKENS ==> 3

 ---- POST ----

 [('427-434', 'JJ'), (')', ')'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['427-434', ')', '.']

 TOTAL FILTERED TOKENS ==>  3

 ---- POST FOR FILTERED TOKENS ----

 [('427-434', 'JJ'), (')', ')'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['427-434 )', ') .'] 

 TOTAL BIGRAMS --> 2 



 ---- TRI-GRAMS ---- 

 ['427-434 ) .'] 

 TOTAL TRIGRAMS --> 1 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 []

 ---- PORTER STEMMING ----

['427-434', ')', '.']

 TOTAL PORTER STEM WORDS ==> 3



 ---- SNOWBALL STEMMING ----

['427-434', ')', '.']

 TOTAL SNOWBALL STEM WORDS ==> 3



 ---- LEMMATIZATION ----

['427-434', ')', '.']

 TOTAL LEMMATIZE WORDS ==> 3

************************************************************************************************************************

445 --> IEEE. 


 ---- TOKENS ----

 ['IEEE', '.'] 

 TOTAL TOKENS ==> 2

 ---- POST ----

 [('IEEE', 'NNP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['IEEE', '.']

 TOTAL FILTERED TOKENS ==>  2

 ---- POST FOR FILTERED TOKENS ----

 [('IEEE', 'NNP'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['IEEE .'] 

 TOTAL BIGRAMS --> 1 



 ---- TRI-GRAMS ---- 

 [] 

 TOTAL TRIGRAMS --> 0 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['ieee', '.']

 TOTAL PORTER STEM WORDS ==> 2



 ---- SNOWBALL STEMMING ----

['ieee', '.']

 TOTAL SNOWBALL STEM WORDS ==> 2



 ---- LEMMATIZATION ----

['IEEE', '.']

 TOTAL LEMMATIZE WORDS ==> 2

************************************************************************************************************************

446 --> [26] Yi, J., Nasukawa, T., Bunescu, R., & Niblack, W. (2003, November). 


 ---- TOKENS ----

 ['[', '26', ']', 'Yi', ',', 'J.', ',', 'Nasukawa', ',', 'T.', ',', 'Bunescu', ',', 'R.', ',', '&', 'Niblack', ',', 'W.', '(', '2003', ',', 'November', ')', '.'] 

 TOTAL TOKENS ==> 25

 ---- POST ----

 [('[', 'RB'), ('26', 'CD'), (']', 'JJ'), ('Yi', 'NNP'), (',', ','), ('J.', 'NNP'), (',', ','), ('Nasukawa', 'NNP'), (',', ','), ('T.', 'NNP'), (',', ','), ('Bunescu', 'NNP'), (',', ','), ('R.', 'NNP'), (',', ','), ('&', 'CC'), ('Niblack', 'NNP'), (',', ','), ('W.', 'NNP'), ('(', '('), ('2003', 'CD'), (',', ','), ('November', 'NNP'), (')', ')'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['[', '26', ']', 'Yi', ',', 'J.', ',', 'Nasukawa', ',', 'T.', ',', 'Bunescu', ',', 'R.', ',', '&', 'Niblack', ',', 'W.', '(', '2003', ',', 'November', ')', '.']

 TOTAL FILTERED TOKENS ==>  25

 ---- POST FOR FILTERED TOKENS ----

 [('[', 'RB'), ('26', 'CD'), (']', 'JJ'), ('Yi', 'NNP'), (',', ','), ('J.', 'NNP'), (',', ','), ('Nasukawa', 'NNP'), (',', ','), ('T.', 'NNP'), (',', ','), ('Bunescu', 'NNP'), (',', ','), ('R.', 'NNP'), (',', ','), ('&', 'CC'), ('Niblack', 'NNP'), (',', ','), ('W.', 'NNP'), ('(', '('), ('2003', 'CD'), (',', ','), ('November', 'NNP'), (')', ')'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['[ 26', '26 ]', '] Yi', 'Yi ,', ', J.', 'J. ,', ', Nasukawa', 'Nasukawa ,', ', T.', 'T. ,', ', Bunescu', 'Bunescu ,', ', R.', 'R. ,', ', &', '& Niblack', 'Niblack ,', ', W.', 'W. (', '( 2003', '2003 ,', ', November', 'November )', ') .'] 

 TOTAL BIGRAMS --> 24 



 ---- TRI-GRAMS ---- 

 ['[ 26 ]', '26 ] Yi', '] Yi ,', 'Yi , J.', ', J. ,', 'J. , Nasukawa', ', Nasukawa ,', 'Nasukawa , T.', ', T. ,', 'T. , Bunescu', ', Bunescu ,', 'Bunescu , R.', ', R. ,', 'R. , &', ', & Niblack', '& Niblack ,', 'Niblack , W.', ', W. (', 'W. ( 2003', '( 2003 ,', '2003 , November', ', November )', 'November ) .'] 

 TOTAL TRIGRAMS --> 23 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> ['Bunescu', 'Niblack']
 TOTAL PERSON ENTITY --> 2 


 GPE ---> ['Nasukawa']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['[', '26', ']', 'yi', ',', 'j.', ',', 'nasukawa', ',', 't.', ',', 'bunescu', ',', 'r.', ',', '&', 'niblack', ',', 'w.', '(', '2003', ',', 'novemb', ')', '.']

 TOTAL PORTER STEM WORDS ==> 25



 ---- SNOWBALL STEMMING ----

['[', '26', ']', 'yi', ',', 'j.', ',', 'nasukawa', ',', 't.', ',', 'bunescu', ',', 'r.', ',', '&', 'niblack', ',', 'w.', '(', '2003', ',', 'novemb', ')', '.']

 TOTAL SNOWBALL STEM WORDS ==> 25



 ---- LEMMATIZATION ----

['[', '26', ']', 'Yi', ',', 'J.', ',', 'Nasukawa', ',', 'T.', ',', 'Bunescu', ',', 'R.', ',', '&', 'Niblack', ',', 'W.', '(', '2003', ',', 'November', ')', '.']

 TOTAL LEMMATIZE WORDS ==> 25

************************************************************************************************************************

447 --> Sentiment  analyzer: Extracting sentiments about a given topic using natural language processing  techniques. 


 ---- TOKENS ----

 ['Sentiment', 'analyzer', ':', 'Extracting', 'sentiments', 'about', 'a', 'given', 'topic', 'using', 'natural', 'language', 'processing', 'techniques', '.'] 

 TOTAL TOKENS ==> 15

 ---- POST ----

 [('Sentiment', 'JJ'), ('analyzer', 'NN'), (':', ':'), ('Extracting', 'JJ'), ('sentiments', 'NNS'), ('about', 'IN'), ('a', 'DT'), ('given', 'VBN'), ('topic', 'NN'), ('using', 'VBG'), ('natural', 'JJ'), ('language', 'NN'), ('processing', 'NN'), ('techniques', 'NNS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Sentiment', 'analyzer', ':', 'Extracting', 'sentiments', 'given', 'topic', 'using', 'natural', 'language', 'processing', 'techniques', '.']

 TOTAL FILTERED TOKENS ==>  13

 ---- POST FOR FILTERED TOKENS ----

 [('Sentiment', 'JJ'), ('analyzer', 'NN'), (':', ':'), ('Extracting', 'JJ'), ('sentiments', 'NNS'), ('given', 'VBN'), ('topic', 'RP'), ('using', 'VBG'), ('natural', 'JJ'), ('language', 'NN'), ('processing', 'NN'), ('techniques', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Sentiment analyzer', 'analyzer :', ': Extracting', 'Extracting sentiments', 'sentiments given', 'given topic', 'topic using', 'using natural', 'natural language', 'language processing', 'processing techniques', 'techniques .'] 

 TOTAL BIGRAMS --> 12 



 ---- TRI-GRAMS ---- 

 ['Sentiment analyzer :', 'analyzer : Extracting', ': Extracting sentiments', 'Extracting sentiments given', 'sentiments given topic', 'given topic using', 'topic using natural', 'using natural language', 'natural language processing', 'language processing techniques', 'processing techniques .'] 

 TOTAL TRIGRAMS --> 11 



 ---- NOUN PHRASES ---- 

 ['Sentiment analyzer', 'natural language', 'processing'] 

 TOTAL NOUN PHRASES --> 3 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['sentiment', 'analyz', ':', 'extract', 'sentiment', 'given', 'topic', 'use', 'natur', 'languag', 'process', 'techniqu', '.']

 TOTAL PORTER STEM WORDS ==> 13



 ---- SNOWBALL STEMMING ----

['sentiment', 'analyz', ':', 'extract', 'sentiment', 'given', 'topic', 'use', 'natur', 'languag', 'process', 'techniqu', '.']

 TOTAL SNOWBALL STEM WORDS ==> 13



 ---- LEMMATIZATION ----

['Sentiment', 'analyzer', ':', 'Extracting', 'sentiment', 'given', 'topic', 'using', 'natural', 'language', 'processing', 'technique', '.']

 TOTAL LEMMATIZE WORDS ==> 13

************************************************************************************************************************

448 --> In Data Mining, 2003. 


 ---- TOKENS ----

 ['In', 'Data', 'Mining', ',', '2003', '.'] 

 TOTAL TOKENS ==> 6

 ---- POST ----

 [('In', 'IN'), ('Data', 'NNP'), ('Mining', 'NNP'), (',', ','), ('2003', 'CD'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Data', 'Mining', ',', '2003', '.']

 TOTAL FILTERED TOKENS ==>  5

 ---- POST FOR FILTERED TOKENS ----

 [('Data', 'NNP'), ('Mining', 'NNP'), (',', ','), ('2003', 'CD'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Data Mining', 'Mining ,', ', 2003', '2003 .'] 

 TOTAL BIGRAMS --> 4 



 ---- TRI-GRAMS ---- 

 ['Data Mining ,', 'Mining , 2003', ', 2003 .'] 

 TOTAL TRIGRAMS --> 3 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> ['Mining']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> ['Data']
 TOTAL PERSON ENTITY --> 1 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['data', 'mine', ',', '2003', '.']

 TOTAL PORTER STEM WORDS ==> 5



 ---- SNOWBALL STEMMING ----

['data', 'mine', ',', '2003', '.']

 TOTAL SNOWBALL STEM WORDS ==> 5



 ---- LEMMATIZATION ----

['Data', 'Mining', ',', '2003', '.']

 TOTAL LEMMATIZE WORDS ==> 5

************************************************************************************************************************

449 --> ICDM 2003. 


 ---- TOKENS ----

 ['ICDM', '2003', '.'] 

 TOTAL TOKENS ==> 3

 ---- POST ----

 [('ICDM', 'JJ'), ('2003', 'CD'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['ICDM', '2003', '.']

 TOTAL FILTERED TOKENS ==>  3

 ---- POST FOR FILTERED TOKENS ----

 [('ICDM', 'JJ'), ('2003', 'CD'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['ICDM 2003', '2003 .'] 

 TOTAL BIGRAMS --> 2 



 ---- TRI-GRAMS ---- 

 ['ICDM 2003 .'] 

 TOTAL TRIGRAMS --> 1 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['icdm', '2003', '.']

 TOTAL PORTER STEM WORDS ==> 3



 ---- SNOWBALL STEMMING ----

['icdm', '2003', '.']

 TOTAL SNOWBALL STEM WORDS ==> 3



 ---- LEMMATIZATION ----

['ICDM', '2003', '.']

 TOTAL LEMMATIZE WORDS ==> 3

************************************************************************************************************************

450 --> Third IEEE International Conference on (pp. 


 ---- TOKENS ----

 ['Third', 'IEEE', 'International', 'Conference', 'on', '(', 'pp', '.'] 

 TOTAL TOKENS ==> 8

 ---- POST ----

 [('Third', 'NNP'), ('IEEE', 'NNP'), ('International', 'NNP'), ('Conference', 'NNP'), ('on', 'IN'), ('(', '('), ('pp', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Third', 'IEEE', 'International', 'Conference', '(', 'pp', '.']

 TOTAL FILTERED TOKENS ==>  7

 ---- POST FOR FILTERED TOKENS ----

 [('Third', 'NNP'), ('IEEE', 'NNP'), ('International', 'NNP'), ('Conference', 'NNP'), ('(', '('), ('pp', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Third IEEE', 'IEEE International', 'International Conference', 'Conference (', '( pp', 'pp .'] 

 TOTAL BIGRAMS --> 6 



 ---- TRI-GRAMS ---- 

 ['Third IEEE International', 'IEEE International Conference', 'International Conference (', 'Conference ( pp', '( pp .'] 

 TOTAL TRIGRAMS --> 5 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 []

 ---- PORTER STEMMING ----

['third', 'ieee', 'intern', 'confer', '(', 'pp', '.']

 TOTAL PORTER STEM WORDS ==> 7



 ---- SNOWBALL STEMMING ----

['third', 'ieee', 'intern', 'confer', '(', 'pp', '.']

 TOTAL SNOWBALL STEM WORDS ==> 7



 ---- LEMMATIZATION ----

['Third', 'IEEE', 'International', 'Conference', '(', 'pp', '.']

 TOTAL LEMMATIZE WORDS ==> 7

************************************************************************************************************************

451 --> 427-434). 


 ---- TOKENS ----

 ['427-434', ')', '.'] 

 TOTAL TOKENS ==> 3

 ---- POST ----

 [('427-434', 'JJ'), (')', ')'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['427-434', ')', '.']

 TOTAL FILTERED TOKENS ==>  3

 ---- POST FOR FILTERED TOKENS ----

 [('427-434', 'JJ'), (')', ')'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['427-434 )', ') .'] 

 TOTAL BIGRAMS --> 2 



 ---- TRI-GRAMS ---- 

 ['427-434 ) .'] 

 TOTAL TRIGRAMS --> 1 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 []

 ---- PORTER STEMMING ----

['427-434', ')', '.']

 TOTAL PORTER STEM WORDS ==> 3



 ---- SNOWBALL STEMMING ----

['427-434', ')', '.']

 TOTAL SNOWBALL STEM WORDS ==> 3



 ---- LEMMATIZATION ----

['427-434', ')', '.']

 TOTAL LEMMATIZE WORDS ==> 3

************************************************************************************************************************

452 --> IEEE. 


 ---- TOKENS ----

 ['IEEE', '.'] 

 TOTAL TOKENS ==> 2

 ---- POST ----

 [('IEEE', 'NNP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['IEEE', '.']

 TOTAL FILTERED TOKENS ==>  2

 ---- POST FOR FILTERED TOKENS ----

 [('IEEE', 'NNP'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['IEEE .'] 

 TOTAL BIGRAMS --> 1 



 ---- TRI-GRAMS ---- 

 [] 

 TOTAL TRIGRAMS --> 0 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['ieee', '.']

 TOTAL PORTER STEM WORDS ==> 2



 ---- SNOWBALL STEMMING ----

['ieee', '.']

 TOTAL SNOWBALL STEM WORDS ==> 2



 ---- LEMMATIZATION ----

['IEEE', '.']

 TOTAL LEMMATIZE WORDS ==> 2

************************************************************************************************************************

453 --> [27] Tapaswi, N., & Jain, S. (2012, September). 


 ---- TOKENS ----

 ['[', '27', ']', 'Tapaswi', ',', 'N.', ',', '&', 'Jain', ',', 'S.', '(', '2012', ',', 'September', ')', '.'] 

 TOTAL TOKENS ==> 17

 ---- POST ----

 [('[', 'RB'), ('27', 'CD'), (']', 'JJ'), ('Tapaswi', 'NNP'), (',', ','), ('N.', 'NNP'), (',', ','), ('&', 'CC'), ('Jain', 'NNP'), (',', ','), ('S.', 'NNP'), ('(', '('), ('2012', 'CD'), (',', ','), ('September', 'NNP'), (')', ')'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['[', '27', ']', 'Tapaswi', ',', 'N.', ',', '&', 'Jain', ',', 'S.', '(', '2012', ',', 'September', ')', '.']

 TOTAL FILTERED TOKENS ==>  17

 ---- POST FOR FILTERED TOKENS ----

 [('[', 'RB'), ('27', 'CD'), (']', 'JJ'), ('Tapaswi', 'NNP'), (',', ','), ('N.', 'NNP'), (',', ','), ('&', 'CC'), ('Jain', 'NNP'), (',', ','), ('S.', 'NNP'), ('(', '('), ('2012', 'CD'), (',', ','), ('September', 'NNP'), (')', ')'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['[ 27', '27 ]', '] Tapaswi', 'Tapaswi ,', ', N.', 'N. ,', ', &', '& Jain', 'Jain ,', ', S.', 'S. (', '( 2012', '2012 ,', ', September', 'September )', ') .'] 

 TOTAL BIGRAMS --> 16 



 ---- TRI-GRAMS ---- 

 ['[ 27 ]', '27 ] Tapaswi', '] Tapaswi ,', 'Tapaswi , N.', ', N. ,', 'N. , &', ', & Jain', '& Jain ,', 'Jain , S.', ', S. (', 'S. ( 2012', '( 2012 ,', '2012 , September', ', September )', 'September ) .'] 

 TOTAL TRIGRAMS --> 15 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> ['Jain']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['[', '27', ']', 'tapaswi', ',', 'n.', ',', '&', 'jain', ',', 's.', '(', '2012', ',', 'septemb', ')', '.']

 TOTAL PORTER STEM WORDS ==> 17



 ---- SNOWBALL STEMMING ----

['[', '27', ']', 'tapaswi', ',', 'n.', ',', '&', 'jain', ',', 's.', '(', '2012', ',', 'septemb', ')', '.']

 TOTAL SNOWBALL STEM WORDS ==> 17



 ---- LEMMATIZATION ----

['[', '27', ']', 'Tapaswi', ',', 'N.', ',', '&', 'Jain', ',', 'S.', '(', '2012', ',', 'September', ')', '.']

 TOTAL LEMMATIZE WORDS ==> 17

************************************************************************************************************************

454 --> Treebank based deep grammar acquisition  and Part-Of-Speech Tagging for Sanskrit sentences. 


 ---- TOKENS ----

 ['Treebank', 'based', 'deep', 'grammar', 'acquisition', 'and', 'Part-Of-Speech', 'Tagging', 'for', 'Sanskrit', 'sentences', '.'] 

 TOTAL TOKENS ==> 12

 ---- POST ----

 [('Treebank', 'NNP'), ('based', 'VBN'), ('deep', 'JJ'), ('grammar', 'NN'), ('acquisition', 'NN'), ('and', 'CC'), ('Part-Of-Speech', 'NNP'), ('Tagging', 'NNP'), ('for', 'IN'), ('Sanskrit', 'NNP'), ('sentences', 'NNS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Treebank', 'based', 'deep', 'grammar', 'acquisition', 'Part-Of-Speech', 'Tagging', 'Sanskrit', 'sentences', '.']

 TOTAL FILTERED TOKENS ==>  10

 ---- POST FOR FILTERED TOKENS ----

 [('Treebank', 'NNP'), ('based', 'VBN'), ('deep', 'JJ'), ('grammar', 'NN'), ('acquisition', 'NN'), ('Part-Of-Speech', 'NNP'), ('Tagging', 'NNP'), ('Sanskrit', 'NNP'), ('sentences', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Treebank based', 'based deep', 'deep grammar', 'grammar acquisition', 'acquisition Part-Of-Speech', 'Part-Of-Speech Tagging', 'Tagging Sanskrit', 'Sanskrit sentences', 'sentences .'] 

 TOTAL BIGRAMS --> 9 



 ---- TRI-GRAMS ---- 

 ['Treebank based deep', 'based deep grammar', 'deep grammar acquisition', 'grammar acquisition Part-Of-Speech', 'acquisition Part-Of-Speech Tagging', 'Part-Of-Speech Tagging Sanskrit', 'Tagging Sanskrit sentences', 'Sanskrit sentences .'] 

 TOTAL TRIGRAMS --> 8 



 ---- NOUN PHRASES ---- 

 ['deep grammar', 'acquisition'] 

 TOTAL NOUN PHRASES --> 2 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> ['Treebank']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['treebank', 'base', 'deep', 'grammar', 'acquisit', 'part-of-speech', 'tag', 'sanskrit', 'sentenc', '.']

 TOTAL PORTER STEM WORDS ==> 10



 ---- SNOWBALL STEMMING ----

['treebank', 'base', 'deep', 'grammar', 'acquisit', 'part-of-speech', 'tag', 'sanskrit', 'sentenc', '.']

 TOTAL SNOWBALL STEM WORDS ==> 10



 ---- LEMMATIZATION ----

['Treebank', 'based', 'deep', 'grammar', 'acquisition', 'Part-Of-Speech', 'Tagging', 'Sanskrit', 'sentence', '.']

 TOTAL LEMMATIZE WORDS ==> 10

************************************************************************************************************************

455 --> In Software Engineering (CONSEG),  2012 CSI Sixth International Conference on (pp. 


 ---- TOKENS ----

 ['In', 'Software', 'Engineering', '(', 'CONSEG', ')', ',', '2012', 'CSI', 'Sixth', 'International', 'Conference', 'on', '(', 'pp', '.'] 

 TOTAL TOKENS ==> 16

 ---- POST ----

 [('In', 'IN'), ('Software', 'NNP'), ('Engineering', 'NNP'), ('(', '('), ('CONSEG', 'NNP'), (')', ')'), (',', ','), ('2012', 'CD'), ('CSI', 'NNP'), ('Sixth', 'NNP'), ('International', 'NNP'), ('Conference', 'NNP'), ('on', 'IN'), ('(', '('), ('pp', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Software', 'Engineering', '(', 'CONSEG', ')', ',', '2012', 'CSI', 'Sixth', 'International', 'Conference', '(', 'pp', '.']

 TOTAL FILTERED TOKENS ==>  14

 ---- POST FOR FILTERED TOKENS ----

 [('Software', 'NNP'), ('Engineering', 'NNP'), ('(', '('), ('CONSEG', 'NNP'), (')', ')'), (',', ','), ('2012', 'CD'), ('CSI', 'NNP'), ('Sixth', 'NNP'), ('International', 'NNP'), ('Conference', 'NNP'), ('(', '('), ('pp', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Software Engineering', 'Engineering (', '( CONSEG', 'CONSEG )', ') ,', ', 2012', '2012 CSI', 'CSI Sixth', 'Sixth International', 'International Conference', 'Conference (', '( pp', 'pp .'] 

 TOTAL BIGRAMS --> 13 



 ---- TRI-GRAMS ---- 

 ['Software Engineering (', 'Engineering ( CONSEG', '( CONSEG )', 'CONSEG ) ,', ') , 2012', ', 2012 CSI', '2012 CSI Sixth', 'CSI Sixth International', 'Sixth International Conference', 'International Conference (', 'Conference ( pp', '( pp .'] 

 TOTAL TRIGRAMS --> 12 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 []

 ---- PORTER STEMMING ----

['softwar', 'engin', '(', 'conseg', ')', ',', '2012', 'csi', 'sixth', 'intern', 'confer', '(', 'pp', '.']

 TOTAL PORTER STEM WORDS ==> 14



 ---- SNOWBALL STEMMING ----

['softwar', 'engin', '(', 'conseg', ')', ',', '2012', 'csi', 'sixth', 'intern', 'confer', '(', 'pp', '.']

 TOTAL SNOWBALL STEM WORDS ==> 14



 ---- LEMMATIZATION ----

['Software', 'Engineering', '(', 'CONSEG', ')', ',', '2012', 'CSI', 'Sixth', 'International', 'Conference', '(', 'pp', '.']

 TOTAL LEMMATIZE WORDS ==> 14

************************************************************************************************************************

456 --> 1-4). 


 ---- TOKENS ----

 ['1-4', ')', '.'] 

 TOTAL TOKENS ==> 3

 ---- POST ----

 [('1-4', 'JJ'), (')', ')'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['1-4', ')', '.']

 TOTAL FILTERED TOKENS ==>  3

 ---- POST FOR FILTERED TOKENS ----

 [('1-4', 'JJ'), (')', ')'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['1-4 )', ') .'] 

 TOTAL BIGRAMS --> 2 



 ---- TRI-GRAMS ---- 

 ['1-4 ) .'] 

 TOTAL TRIGRAMS --> 1 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 []

 ---- PORTER STEMMING ----

['1-4', ')', '.']

 TOTAL PORTER STEM WORDS ==> 3



 ---- SNOWBALL STEMMING ----

['1-4', ')', '.']

 TOTAL SNOWBALL STEM WORDS ==> 3



 ---- LEMMATIZATION ----

['1-4', ')', '.']

 TOTAL LEMMATIZE WORDS ==> 3

************************************************************************************************************************

457 --> IEEE. 


 ---- TOKENS ----

 ['IEEE', '.'] 

 TOTAL TOKENS ==> 2

 ---- POST ----

 [('IEEE', 'NNP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['IEEE', '.']

 TOTAL FILTERED TOKENS ==>  2

 ---- POST FOR FILTERED TOKENS ----

 [('IEEE', 'NNP'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['IEEE .'] 

 TOTAL BIGRAMS --> 1 



 ---- TRI-GRAMS ---- 

 [] 

 TOTAL TRIGRAMS --> 0 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['ieee', '.']

 TOTAL PORTER STEM WORDS ==> 2



 ---- SNOWBALL STEMMING ----

['ieee', '.']

 TOTAL SNOWBALL STEM WORDS ==> 2



 ---- LEMMATIZATION ----

['IEEE', '.']

 TOTAL LEMMATIZE WORDS ==> 2

************************************************************************************************************************

458 --> [28] Ranjan, P., & Basu, H. V. S. S. A. 


 ---- TOKENS ----

 ['[', '28', ']', 'Ranjan', ',', 'P.', ',', '&', 'Basu', ',', 'H.', 'V.', 'S.', 'S.', 'A', '.'] 

 TOTAL TOKENS ==> 16

 ---- POST ----

 [('[', 'RB'), ('28', 'CD'), (']', 'JJ'), ('Ranjan', 'NNP'), (',', ','), ('P.', 'NNP'), (',', ','), ('&', 'CC'), ('Basu', 'NNP'), (',', ','), ('H.', 'NNP'), ('V.', 'NNP'), ('S.', 'NNP'), ('S.', 'NNP'), ('A', 'NNP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['[', '28', ']', 'Ranjan', ',', 'P.', ',', '&', 'Basu', ',', 'H.', 'V.', 'S.', 'S.', '.']

 TOTAL FILTERED TOKENS ==>  15

 ---- POST FOR FILTERED TOKENS ----

 [('[', 'RB'), ('28', 'CD'), (']', 'JJ'), ('Ranjan', 'NNP'), (',', ','), ('P.', 'NNP'), (',', ','), ('&', 'CC'), ('Basu', 'NNP'), (',', ','), ('H.', 'NNP'), ('V.', 'NNP'), ('S.', 'NNP'), ('S.', 'NNP'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['[ 28', '28 ]', '] Ranjan', 'Ranjan ,', ', P.', 'P. ,', ', &', '& Basu', 'Basu ,', ', H.', 'H. V.', 'V. S.', 'S. S.', 'S. .'] 

 TOTAL BIGRAMS --> 14 



 ---- TRI-GRAMS ---- 

 ['[ 28 ]', '28 ] Ranjan', '] Ranjan ,', 'Ranjan , P.', ', P. ,', 'P. , &', ', & Basu', '& Basu ,', 'Basu , H.', ', H. V.', 'H. V. S.', 'V. S. S.', 'S. S. .'] 

 TOTAL TRIGRAMS --> 13 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> ['Basu']
 TOTAL PERSON ENTITY --> 1 


 GPE ---> ['Ranjan']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['[', '28', ']', 'ranjan', ',', 'p.', ',', '&', 'basu', ',', 'h.', 'v.', 's.', 's.', '.']

 TOTAL PORTER STEM WORDS ==> 15



 ---- SNOWBALL STEMMING ----

['[', '28', ']', 'ranjan', ',', 'p.', ',', '&', 'basu', ',', 'h.', 'v.', 's.', 's.', '.']

 TOTAL SNOWBALL STEM WORDS ==> 15



 ---- LEMMATIZATION ----

['[', '28', ']', 'Ranjan', ',', 'P.', ',', '&', 'Basu', ',', 'H.', 'V.', 'S.', 'S.', '.']

 TOTAL LEMMATIZE WORDS ==> 15

************************************************************************************************************************

459 --> (2003). 


 ---- TOKENS ----

 ['(', '2003', ')', '.'] 

 TOTAL TOKENS ==> 4

 ---- POST ----

 [('(', '('), ('2003', 'CD'), (')', ')'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['(', '2003', ')', '.']

 TOTAL FILTERED TOKENS ==>  4

 ---- POST FOR FILTERED TOKENS ----

 [('(', '('), ('2003', 'CD'), (')', ')'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['( 2003', '2003 )', ') .'] 

 TOTAL BIGRAMS --> 3 



 ---- TRI-GRAMS ---- 

 ['( 2003 )', '2003 ) .'] 

 TOTAL TRIGRAMS --> 2 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['(', '2003', ')', '.']

 TOTAL PORTER STEM WORDS ==> 4



 ---- SNOWBALL STEMMING ----

['(', '2003', ')', '.']

 TOTAL SNOWBALL STEM WORDS ==> 4



 ---- LEMMATIZATION ----

['(', '2003', ')', '.']

 TOTAL LEMMATIZE WORDS ==> 4

************************************************************************************************************************

460 --> Part of speech tagging and local word  grouping techniques for natural language parsing in Hindi. 


 ---- TOKENS ----

 ['Part', 'of', 'speech', 'tagging', 'and', 'local', 'word', 'grouping', 'techniques', 'for', 'natural', 'language', 'parsing', 'in', 'Hindi', '.'] 

 TOTAL TOKENS ==> 16

 ---- POST ----

 [('Part', 'NN'), ('of', 'IN'), ('speech', 'NN'), ('tagging', 'NN'), ('and', 'CC'), ('local', 'JJ'), ('word', 'NN'), ('grouping', 'VBG'), ('techniques', 'NNS'), ('for', 'IN'), ('natural', 'JJ'), ('language', 'NN'), ('parsing', 'NN'), ('in', 'IN'), ('Hindi', 'NNP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Part', 'speech', 'tagging', 'local', 'word', 'grouping', 'techniques', 'natural', 'language', 'parsing', 'Hindi', '.']

 TOTAL FILTERED TOKENS ==>  12

 ---- POST FOR FILTERED TOKENS ----

 [('Part', 'NN'), ('speech', 'NN'), ('tagging', 'VBG'), ('local', 'JJ'), ('word', 'NN'), ('grouping', 'VBG'), ('techniques', 'NNS'), ('natural', 'JJ'), ('language', 'NN'), ('parsing', 'VBG'), ('Hindi', 'NNP'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Part speech', 'speech tagging', 'tagging local', 'local word', 'word grouping', 'grouping techniques', 'techniques natural', 'natural language', 'language parsing', 'parsing Hindi', 'Hindi .'] 

 TOTAL BIGRAMS --> 11 



 ---- TRI-GRAMS ---- 

 ['Part speech tagging', 'speech tagging local', 'tagging local word', 'local word grouping', 'word grouping techniques', 'grouping techniques natural', 'techniques natural language', 'natural language parsing', 'language parsing Hindi', 'parsing Hindi .'] 

 TOTAL TRIGRAMS --> 10 



 ---- NOUN PHRASES ---- 

 ['Part', 'speech', 'local word', 'natural language'] 

 TOTAL NOUN PHRASES --> 4 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> ['Hindi']
 TOTAL PERSON ENTITY --> 1 


 GPE ---> ['Part']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['part', 'speech', 'tag', 'local', 'word', 'group', 'techniqu', 'natur', 'languag', 'pars', 'hindi', '.']

 TOTAL PORTER STEM WORDS ==> 12



 ---- SNOWBALL STEMMING ----

['part', 'speech', 'tag', 'local', 'word', 'group', 'techniqu', 'natur', 'languag', 'pars', 'hindi', '.']

 TOTAL SNOWBALL STEM WORDS ==> 12



 ---- LEMMATIZATION ----

['Part', 'speech', 'tagging', 'local', 'word', 'grouping', 'technique', 'natural', 'language', 'parsing', 'Hindi', '.']

 TOTAL LEMMATIZE WORDS ==> 12

************************************************************************************************************************

461 --> In Proceedings of the 1st  International Conference on Natural Language Processing (ICON 2003). 


 ---- TOKENS ----

 ['In', 'Proceedings', 'of', 'the', '1st', 'International', 'Conference', 'on', 'Natural', 'Language', 'Processing', '(', 'ICON', '2003', ')', '.'] 

 TOTAL TOKENS ==> 16

 ---- POST ----

 [('In', 'IN'), ('Proceedings', 'NNP'), ('of', 'IN'), ('the', 'DT'), ('1st', 'CD'), ('International', 'NNP'), ('Conference', 'NNP'), ('on', 'IN'), ('Natural', 'NNP'), ('Language', 'NNP'), ('Processing', 'NNP'), ('(', '('), ('ICON', 'NNP'), ('2003', 'CD'), (')', ')'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Proceedings', '1st', 'International', 'Conference', 'Natural', 'Language', 'Processing', '(', 'ICON', '2003', ')', '.']

 TOTAL FILTERED TOKENS ==>  12

 ---- POST FOR FILTERED TOKENS ----

 [('Proceedings', 'NNS'), ('1st', 'CD'), ('International', 'NNP'), ('Conference', 'NNP'), ('Natural', 'NNP'), ('Language', 'NNP'), ('Processing', 'NNP'), ('(', '('), ('ICON', 'NNP'), ('2003', 'CD'), (')', ')'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Proceedings 1st', '1st International', 'International Conference', 'Conference Natural', 'Natural Language', 'Language Processing', 'Processing (', '( ICON', 'ICON 2003', '2003 )', ') .'] 

 TOTAL BIGRAMS --> 11 



 ---- TRI-GRAMS ---- 

 ['Proceedings 1st International', '1st International Conference', 'International Conference Natural', 'Conference Natural Language', 'Natural Language Processing', 'Language Processing (', 'Processing ( ICON', '( ICON 2003', 'ICON 2003 )', '2003 ) .'] 

 TOTAL TRIGRAMS --> 10 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> ['International Conference Natural Language']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['proceed', '1st', 'intern', 'confer', 'natur', 'languag', 'process', '(', 'icon', '2003', ')', '.']

 TOTAL PORTER STEM WORDS ==> 12



 ---- SNOWBALL STEMMING ----

['proceed', '1st', 'intern', 'confer', 'natur', 'languag', 'process', '(', 'icon', '2003', ')', '.']

 TOTAL SNOWBALL STEM WORDS ==> 12



 ---- LEMMATIZATION ----

['Proceedings', '1st', 'International', 'Conference', 'Natural', 'Language', 'Processing', '(', 'ICON', '2003', ')', '.']

 TOTAL LEMMATIZE WORDS ==> 12

************************************************************************************************************************

462 --> [29] Diab, M., Hacioglu, K., & Jurafsky, D. (2004, May). 


 ---- TOKENS ----

 ['[', '29', ']', 'Diab', ',', 'M.', ',', 'Hacioglu', ',', 'K.', ',', '&', 'Jurafsky', ',', 'D.', '(', '2004', ',', 'May', ')', '.'] 

 TOTAL TOKENS ==> 21

 ---- POST ----

 [('[', 'RB'), ('29', 'CD'), (']', 'JJ'), ('Diab', 'NNP'), (',', ','), ('M.', 'NNP'), (',', ','), ('Hacioglu', 'NNP'), (',', ','), ('K.', 'NNP'), (',', ','), ('&', 'CC'), ('Jurafsky', 'NNP'), (',', ','), ('D.', 'NNP'), ('(', '('), ('2004', 'CD'), (',', ','), ('May', 'NNP'), (')', ')'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['[', '29', ']', 'Diab', ',', 'M.', ',', 'Hacioglu', ',', 'K.', ',', '&', 'Jurafsky', ',', 'D.', '(', '2004', ',', 'May', ')', '.']

 TOTAL FILTERED TOKENS ==>  21

 ---- POST FOR FILTERED TOKENS ----

 [('[', 'RB'), ('29', 'CD'), (']', 'JJ'), ('Diab', 'NNP'), (',', ','), ('M.', 'NNP'), (',', ','), ('Hacioglu', 'NNP'), (',', ','), ('K.', 'NNP'), (',', ','), ('&', 'CC'), ('Jurafsky', 'NNP'), (',', ','), ('D.', 'NNP'), ('(', '('), ('2004', 'CD'), (',', ','), ('May', 'NNP'), (')', ')'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['[ 29', '29 ]', '] Diab', 'Diab ,', ', M.', 'M. ,', ', Hacioglu', 'Hacioglu ,', ', K.', 'K. ,', ', &', '& Jurafsky', 'Jurafsky ,', ', D.', 'D. (', '( 2004', '2004 ,', ', May', 'May )', ') .'] 

 TOTAL BIGRAMS --> 20 



 ---- TRI-GRAMS ---- 

 ['[ 29 ]', '29 ] Diab', '] Diab ,', 'Diab , M.', ', M. ,', 'M. , Hacioglu', ', Hacioglu ,', 'Hacioglu , K.', ', K. ,', 'K. , &', ', & Jurafsky', '& Jurafsky ,', 'Jurafsky , D.', ', D. (', 'D. ( 2004', '( 2004 ,', '2004 , May', ', May )', 'May ) .'] 

 TOTAL TRIGRAMS --> 19 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> ['Diab', 'Jurafsky']
 TOTAL PERSON ENTITY --> 2 


 GPE ---> ['Hacioglu']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['[', '29', ']', 'diab', ',', 'm.', ',', 'hacioglu', ',', 'k.', ',', '&', 'jurafski', ',', 'd.', '(', '2004', ',', 'may', ')', '.']

 TOTAL PORTER STEM WORDS ==> 21



 ---- SNOWBALL STEMMING ----

['[', '29', ']', 'diab', ',', 'm.', ',', 'hacioglu', ',', 'k.', ',', '&', 'jurafski', ',', 'd.', '(', '2004', ',', 'may', ')', '.']

 TOTAL SNOWBALL STEM WORDS ==> 21



 ---- LEMMATIZATION ----

['[', '29', ']', 'Diab', ',', 'M.', ',', 'Hacioglu', ',', 'K.', ',', '&', 'Jurafsky', ',', 'D.', '(', '2004', ',', 'May', ')', '.']

 TOTAL LEMMATIZE WORDS ==> 21

************************************************************************************************************************

463 --> Automatic tagging of Arabic text:  From raw text to base phrase chunks. 


 ---- TOKENS ----

 ['Automatic', 'tagging', 'of', 'Arabic', 'text', ':', 'From', 'raw', 'text', 'to', 'base', 'phrase', 'chunks', '.'] 

 TOTAL TOKENS ==> 14

 ---- POST ----

 [('Automatic', 'JJ'), ('tagging', 'NN'), ('of', 'IN'), ('Arabic', 'NNP'), ('text', 'NN'), (':', ':'), ('From', 'IN'), ('raw', 'JJ'), ('text', 'NN'), ('to', 'TO'), ('base', 'VB'), ('phrase', 'JJ'), ('chunks', 'NNS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Automatic', 'tagging', 'Arabic', 'text', ':', 'raw', 'text', 'base', 'phrase', 'chunks', '.']

 TOTAL FILTERED TOKENS ==>  11

 ---- POST FOR FILTERED TOKENS ----

 [('Automatic', 'JJ'), ('tagging', 'VBG'), ('Arabic', 'NNP'), ('text', 'NN'), (':', ':'), ('raw', 'JJ'), ('text', 'NN'), ('base', 'NN'), ('phrase', 'NN'), ('chunks', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Automatic tagging', 'tagging Arabic', 'Arabic text', 'text :', ': raw', 'raw text', 'text base', 'base phrase', 'phrase chunks', 'chunks .'] 

 TOTAL BIGRAMS --> 10 



 ---- TRI-GRAMS ---- 

 ['Automatic tagging Arabic', 'tagging Arabic text', 'Arabic text :', 'text : raw', ': raw text', 'raw text base', 'text base phrase', 'base phrase chunks', 'phrase chunks .'] 

 TOTAL TRIGRAMS --> 9 



 ---- NOUN PHRASES ---- 

 ['text', 'raw text', 'base', 'phrase'] 

 TOTAL NOUN PHRASES --> 4 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> ['Arabic']
 TOTAL PERSON ENTITY --> 1 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['automat', 'tag', 'arab', 'text', ':', 'raw', 'text', 'base', 'phrase', 'chunk', '.']

 TOTAL PORTER STEM WORDS ==> 11



 ---- SNOWBALL STEMMING ----

['automat', 'tag', 'arab', 'text', ':', 'raw', 'text', 'base', 'phrase', 'chunk', '.']

 TOTAL SNOWBALL STEM WORDS ==> 11



 ---- LEMMATIZATION ----

['Automatic', 'tagging', 'Arabic', 'text', ':', 'raw', 'text', 'base', 'phrase', 'chunk', '.']

 TOTAL LEMMATIZE WORDS ==> 11

************************************************************************************************************************

464 --> In Proceedings of HLT-NAACL 2004: Short  papers (pp. 


 ---- TOKENS ----

 ['In', 'Proceedings', 'of', 'HLT-NAACL', '2004', ':', 'Short', 'papers', '(', 'pp', '.'] 

 TOTAL TOKENS ==> 11

 ---- POST ----

 [('In', 'IN'), ('Proceedings', 'NNP'), ('of', 'IN'), ('HLT-NAACL', 'NNP'), ('2004', 'CD'), (':', ':'), ('Short', 'JJ'), ('papers', 'NNS'), ('(', '('), ('pp', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Proceedings', 'HLT-NAACL', '2004', ':', 'Short', 'papers', '(', 'pp', '.']

 TOTAL FILTERED TOKENS ==>  9

 ---- POST FOR FILTERED TOKENS ----

 [('Proceedings', 'NNS'), ('HLT-NAACL', 'JJ'), ('2004', 'CD'), (':', ':'), ('Short', 'JJ'), ('papers', 'NNS'), ('(', '('), ('pp', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Proceedings HLT-NAACL', 'HLT-NAACL 2004', '2004 :', ': Short', 'Short papers', 'papers (', '( pp', 'pp .'] 

 TOTAL BIGRAMS --> 8 



 ---- TRI-GRAMS ---- 

 ['Proceedings HLT-NAACL 2004', 'HLT-NAACL 2004 :', '2004 : Short', ': Short papers', 'Short papers (', 'papers ( pp', '( pp .'] 

 TOTAL TRIGRAMS --> 7 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 []

 ---- PORTER STEMMING ----

['proceed', 'hlt-naacl', '2004', ':', 'short', 'paper', '(', 'pp', '.']

 TOTAL PORTER STEM WORDS ==> 9



 ---- SNOWBALL STEMMING ----

['proceed', 'hlt-naacl', '2004', ':', 'short', 'paper', '(', 'pp', '.']

 TOTAL SNOWBALL STEM WORDS ==> 9



 ---- LEMMATIZATION ----

['Proceedings', 'HLT-NAACL', '2004', ':', 'Short', 'paper', '(', 'pp', '.']

 TOTAL LEMMATIZE WORDS ==> 9

************************************************************************************************************************

465 --> 149-152). 


 ---- TOKENS ----

 ['149-152', ')', '.'] 

 TOTAL TOKENS ==> 3

 ---- POST ----

 [('149-152', 'JJ'), (')', ')'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['149-152', ')', '.']

 TOTAL FILTERED TOKENS ==>  3

 ---- POST FOR FILTERED TOKENS ----

 [('149-152', 'JJ'), (')', ')'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['149-152 )', ') .'] 

 TOTAL BIGRAMS --> 2 



 ---- TRI-GRAMS ---- 

 ['149-152 ) .'] 

 TOTAL TRIGRAMS --> 1 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 []

 ---- PORTER STEMMING ----

['149-152', ')', '.']

 TOTAL PORTER STEM WORDS ==> 3



 ---- SNOWBALL STEMMING ----

['149-152', ')', '.']

 TOTAL SNOWBALL STEM WORDS ==> 3



 ---- LEMMATIZATION ----

['149-152', ')', '.']

 TOTAL LEMMATIZE WORDS ==> 3

************************************************************************************************************************

466 --> Association for Computational Linguistics. 


 ---- TOKENS ----

 ['Association', 'for', 'Computational', 'Linguistics', '.'] 

 TOTAL TOKENS ==> 5

 ---- POST ----

 [('Association', 'NNP'), ('for', 'IN'), ('Computational', 'NNP'), ('Linguistics', 'NNP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Association', 'Computational', 'Linguistics', '.']

 TOTAL FILTERED TOKENS ==>  4

 ---- POST FOR FILTERED TOKENS ----

 [('Association', 'NNP'), ('Computational', 'NNP'), ('Linguistics', 'NNP'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Association Computational', 'Computational Linguistics', 'Linguistics .'] 

 TOTAL BIGRAMS --> 3 



 ---- TRI-GRAMS ---- 

 ['Association Computational Linguistics', 'Computational Linguistics .'] 

 TOTAL TRIGRAMS --> 2 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['associ', 'comput', 'linguist', '.']

 TOTAL PORTER STEM WORDS ==> 4



 ---- SNOWBALL STEMMING ----

['associ', 'comput', 'linguist', '.']

 TOTAL SNOWBALL STEM WORDS ==> 4



 ---- LEMMATIZATION ----

['Association', 'Computational', 'Linguistics', '.']

 TOTAL LEMMATIZE WORDS ==> 4

************************************************************************************************************************

467 --> [30] Sha, F., & Pereira, F. (2003, May). 


 ---- TOKENS ----

 ['[', '30', ']', 'Sha', ',', 'F.', ',', '&', 'Pereira', ',', 'F.', '(', '2003', ',', 'May', ')', '.'] 

 TOTAL TOKENS ==> 17

 ---- POST ----

 [('[', 'RB'), ('30', 'CD'), (']', 'JJ'), ('Sha', 'NNP'), (',', ','), ('F.', 'NNP'), (',', ','), ('&', 'CC'), ('Pereira', 'NNP'), (',', ','), ('F.', 'NNP'), ('(', '('), ('2003', 'CD'), (',', ','), ('May', 'NNP'), (')', ')'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['[', '30', ']', 'Sha', ',', 'F.', ',', '&', 'Pereira', ',', 'F.', '(', '2003', ',', 'May', ')', '.']

 TOTAL FILTERED TOKENS ==>  17

 ---- POST FOR FILTERED TOKENS ----

 [('[', 'RB'), ('30', 'CD'), (']', 'JJ'), ('Sha', 'NNP'), (',', ','), ('F.', 'NNP'), (',', ','), ('&', 'CC'), ('Pereira', 'NNP'), (',', ','), ('F.', 'NNP'), ('(', '('), ('2003', 'CD'), (',', ','), ('May', 'NNP'), (')', ')'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['[ 30', '30 ]', '] Sha', 'Sha ,', ', F.', 'F. ,', ', &', '& Pereira', 'Pereira ,', ', F.', 'F. (', '( 2003', '2003 ,', ', May', 'May )', ') .'] 

 TOTAL BIGRAMS --> 16 



 ---- TRI-GRAMS ---- 

 ['[ 30 ]', '30 ] Sha', '] Sha ,', 'Sha , F.', ', F. ,', 'F. , &', ', & Pereira', '& Pereira ,', 'Pereira , F.', ', F. (', 'F. ( 2003', '( 2003 ,', '2003 , May', ', May )', 'May ) .'] 

 TOTAL TRIGRAMS --> 15 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> ['Pereira']
 TOTAL PERSON ENTITY --> 1 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['[', '30', ']', 'sha', ',', 'f.', ',', '&', 'pereira', ',', 'f.', '(', '2003', ',', 'may', ')', '.']

 TOTAL PORTER STEM WORDS ==> 17



 ---- SNOWBALL STEMMING ----

['[', '30', ']', 'sha', ',', 'f.', ',', '&', 'pereira', ',', 'f.', '(', '2003', ',', 'may', ')', '.']

 TOTAL SNOWBALL STEM WORDS ==> 17



 ---- LEMMATIZATION ----

['[', '30', ']', 'Sha', ',', 'F.', ',', '&', 'Pereira', ',', 'F.', '(', '2003', ',', 'May', ')', '.']

 TOTAL LEMMATIZE WORDS ==> 17

************************************************************************************************************************

468 --> Shallow parsing with conditional random fields. 


 ---- TOKENS ----

 ['Shallow', 'parsing', 'with', 'conditional', 'random', 'fields', '.'] 

 TOTAL TOKENS ==> 7

 ---- POST ----

 [('Shallow', 'JJ'), ('parsing', 'VBG'), ('with', 'IN'), ('conditional', 'JJ'), ('random', 'NN'), ('fields', 'NNS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Shallow', 'parsing', 'conditional', 'random', 'fields', '.']

 TOTAL FILTERED TOKENS ==>  6

 ---- POST FOR FILTERED TOKENS ----

 [('Shallow', 'NNP'), ('parsing', 'VBG'), ('conditional', 'JJ'), ('random', 'NN'), ('fields', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Shallow parsing', 'parsing conditional', 'conditional random', 'random fields', 'fields .'] 

 TOTAL BIGRAMS --> 5 



 ---- TRI-GRAMS ---- 

 ['Shallow parsing conditional', 'parsing conditional random', 'conditional random fields', 'random fields .'] 

 TOTAL TRIGRAMS --> 4 



 ---- NOUN PHRASES ---- 

 ['conditional random'] 

 TOTAL NOUN PHRASES --> 1 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> ['Shallow']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['shallow', 'pars', 'condit', 'random', 'field', '.']

 TOTAL PORTER STEM WORDS ==> 6



 ---- SNOWBALL STEMMING ----

['shallow', 'pars', 'condit', 'random', 'field', '.']

 TOTAL SNOWBALL STEM WORDS ==> 6



 ---- LEMMATIZATION ----

['Shallow', 'parsing', 'conditional', 'random', 'field', '.']

 TOTAL LEMMATIZE WORDS ==> 6

************************************************************************************************************************

469 --> In Proceedings of the 2003 Conference of the North American Chapter of the Association for  Computational Linguistics on Human Language Technology-Volume 1 (pp. 


 ---- TOKENS ----

 ['In', 'Proceedings', 'of', 'the', '2003', 'Conference', 'of', 'the', 'North', 'American', 'Chapter', 'of', 'the', 'Association', 'for', 'Computational', 'Linguistics', 'on', 'Human', 'Language', 'Technology-Volume', '1', '(', 'pp', '.'] 

 TOTAL TOKENS ==> 25

 ---- POST ----

 [('In', 'IN'), ('Proceedings', 'NNP'), ('of', 'IN'), ('the', 'DT'), ('2003', 'CD'), ('Conference', 'NN'), ('of', 'IN'), ('the', 'DT'), ('North', 'JJ'), ('American', 'JJ'), ('Chapter', 'NN'), ('of', 'IN'), ('the', 'DT'), ('Association', 'NNP'), ('for', 'IN'), ('Computational', 'NNP'), ('Linguistics', 'NNP'), ('on', 'IN'), ('Human', 'NNP'), ('Language', 'NNP'), ('Technology-Volume', 'NNP'), ('1', 'CD'), ('(', '('), ('pp', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Proceedings', '2003', 'Conference', 'North', 'American', 'Chapter', 'Association', 'Computational', 'Linguistics', 'Human', 'Language', 'Technology-Volume', '1', '(', 'pp', '.']

 TOTAL FILTERED TOKENS ==>  16

 ---- POST FOR FILTERED TOKENS ----

 [('Proceedings', 'NNS'), ('2003', 'CD'), ('Conference', 'NNP'), ('North', 'NNP'), ('American', 'NNP'), ('Chapter', 'NNP'), ('Association', 'NNP'), ('Computational', 'NNP'), ('Linguistics', 'NNP'), ('Human', 'NNP'), ('Language', 'NNP'), ('Technology-Volume', 'NNP'), ('1', 'CD'), ('(', '('), ('pp', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Proceedings 2003', '2003 Conference', 'Conference North', 'North American', 'American Chapter', 'Chapter Association', 'Association Computational', 'Computational Linguistics', 'Linguistics Human', 'Human Language', 'Language Technology-Volume', 'Technology-Volume 1', '1 (', '( pp', 'pp .'] 

 TOTAL BIGRAMS --> 15 



 ---- TRI-GRAMS ---- 

 ['Proceedings 2003 Conference', '2003 Conference North', 'Conference North American', 'North American Chapter', 'American Chapter Association', 'Chapter Association Computational', 'Association Computational Linguistics', 'Computational Linguistics Human', 'Linguistics Human Language', 'Human Language Technology-Volume', 'Language Technology-Volume 1', 'Technology-Volume 1 (', '1 ( pp', '( pp .'] 

 TOTAL TRIGRAMS --> 14 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 []

 ---- PORTER STEMMING ----

['proceed', '2003', 'confer', 'north', 'american', 'chapter', 'associ', 'comput', 'linguist', 'human', 'languag', 'technology-volum', '1', '(', 'pp', '.']

 TOTAL PORTER STEM WORDS ==> 16



 ---- SNOWBALL STEMMING ----

['proceed', '2003', 'confer', 'north', 'american', 'chapter', 'associ', 'comput', 'linguist', 'human', 'languag', 'technology-volum', '1', '(', 'pp', '.']

 TOTAL SNOWBALL STEM WORDS ==> 16



 ---- LEMMATIZATION ----

['Proceedings', '2003', 'Conference', 'North', 'American', 'Chapter', 'Association', 'Computational', 'Linguistics', 'Human', 'Language', 'Technology-Volume', '1', '(', 'pp', '.']

 TOTAL LEMMATIZE WORDS ==> 16

************************************************************************************************************************

470 --> 134-141). 


 ---- TOKENS ----

 ['134-141', ')', '.'] 

 TOTAL TOKENS ==> 3

 ---- POST ----

 [('134-141', 'JJ'), (')', ')'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['134-141', ')', '.']

 TOTAL FILTERED TOKENS ==>  3

 ---- POST FOR FILTERED TOKENS ----

 [('134-141', 'JJ'), (')', ')'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['134-141 )', ') .'] 

 TOTAL BIGRAMS --> 2 



 ---- TRI-GRAMS ---- 

 ['134-141 ) .'] 

 TOTAL TRIGRAMS --> 1 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 []

 ---- PORTER STEMMING ----

['134-141', ')', '.']

 TOTAL PORTER STEM WORDS ==> 3



 ---- SNOWBALL STEMMING ----

['134-141', ')', '.']

 TOTAL SNOWBALL STEM WORDS ==> 3



 ---- LEMMATIZATION ----

['134-141', ')', '.']

 TOTAL LEMMATIZE WORDS ==> 3

************************************************************************************************************************

471 --> Association for Computational Linguistics. 


 ---- TOKENS ----

 ['Association', 'for', 'Computational', 'Linguistics', '.'] 

 TOTAL TOKENS ==> 5

 ---- POST ----

 [('Association', 'NNP'), ('for', 'IN'), ('Computational', 'NNP'), ('Linguistics', 'NNP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Association', 'Computational', 'Linguistics', '.']

 TOTAL FILTERED TOKENS ==>  4

 ---- POST FOR FILTERED TOKENS ----

 [('Association', 'NNP'), ('Computational', 'NNP'), ('Linguistics', 'NNP'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Association Computational', 'Computational Linguistics', 'Linguistics .'] 

 TOTAL BIGRAMS --> 3 



 ---- TRI-GRAMS ---- 

 ['Association Computational Linguistics', 'Computational Linguistics .'] 

 TOTAL TRIGRAMS --> 2 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['associ', 'comput', 'linguist', '.']

 TOTAL PORTER STEM WORDS ==> 4



 ---- SNOWBALL STEMMING ----

['associ', 'comput', 'linguist', '.']

 TOTAL SNOWBALL STEM WORDS ==> 4



 ---- LEMMATIZATION ----

['Association', 'Computational', 'Linguistics', '.']

 TOTAL LEMMATIZE WORDS ==> 4

************************************************************************************************************************

472 --> [31] McDonald, R., Crammer, K., & Pereira, F. (2005, October). 


 ---- TOKENS ----

 ['[', '31', ']', 'McDonald', ',', 'R.', ',', 'Crammer', ',', 'K.', ',', '&', 'Pereira', ',', 'F.', '(', '2005', ',', 'October', ')', '.'] 

 TOTAL TOKENS ==> 21

 ---- POST ----

 [('[', 'RB'), ('31', 'CD'), (']', 'JJ'), ('McDonald', 'NNP'), (',', ','), ('R.', 'NNP'), (',', ','), ('Crammer', 'NNP'), (',', ','), ('K.', 'NNP'), (',', ','), ('&', 'CC'), ('Pereira', 'NNP'), (',', ','), ('F.', 'NNP'), ('(', '('), ('2005', 'CD'), (',', ','), ('October', 'NNP'), (')', ')'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['[', '31', ']', 'McDonald', ',', 'R.', ',', 'Crammer', ',', 'K.', ',', '&', 'Pereira', ',', 'F.', '(', '2005', ',', 'October', ')', '.']

 TOTAL FILTERED TOKENS ==>  21

 ---- POST FOR FILTERED TOKENS ----

 [('[', 'RB'), ('31', 'CD'), (']', 'JJ'), ('McDonald', 'NNP'), (',', ','), ('R.', 'NNP'), (',', ','), ('Crammer', 'NNP'), (',', ','), ('K.', 'NNP'), (',', ','), ('&', 'CC'), ('Pereira', 'NNP'), (',', ','), ('F.', 'NNP'), ('(', '('), ('2005', 'CD'), (',', ','), ('October', 'NNP'), (')', ')'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['[ 31', '31 ]', '] McDonald', 'McDonald ,', ', R.', 'R. ,', ', Crammer', 'Crammer ,', ', K.', 'K. ,', ', &', '& Pereira', 'Pereira ,', ', F.', 'F. (', '( 2005', '2005 ,', ', October', 'October )', ') .'] 

 TOTAL BIGRAMS --> 20 



 ---- TRI-GRAMS ---- 

 ['[ 31 ]', '31 ] McDonald', '] McDonald ,', 'McDonald , R.', ', R. ,', 'R. , Crammer', ', Crammer ,', 'Crammer , K.', ', K. ,', 'K. , &', ', & Pereira', '& Pereira ,', 'Pereira , F.', ', F. (', 'F. ( 2005', '( 2005 ,', '2005 , October', ', October )', 'October ) .'] 

 TOTAL TRIGRAMS --> 19 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> ['McDonald']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> ['Pereira']
 TOTAL PERSON ENTITY --> 1 


 GPE ---> ['Crammer']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['[', '31', ']', 'mcdonald', ',', 'r.', ',', 'crammer', ',', 'k.', ',', '&', 'pereira', ',', 'f.', '(', '2005', ',', 'octob', ')', '.']

 TOTAL PORTER STEM WORDS ==> 21



 ---- SNOWBALL STEMMING ----

['[', '31', ']', 'mcdonald', ',', 'r.', ',', 'crammer', ',', 'k.', ',', '&', 'pereira', ',', 'f.', '(', '2005', ',', 'octob', ')', '.']

 TOTAL SNOWBALL STEM WORDS ==> 21



 ---- LEMMATIZATION ----

['[', '31', ']', 'McDonald', ',', 'R.', ',', 'Crammer', ',', 'K.', ',', '&', 'Pereira', ',', 'F.', '(', '2005', ',', 'October', ')', '.']

 TOTAL LEMMATIZE WORDS ==> 21

************************************************************************************************************************

473 --> Flexible text segmentation  with structured multilabel classification. 


 ---- TOKENS ----

 ['Flexible', 'text', 'segmentation', 'with', 'structured', 'multilabel', 'classification', '.'] 

 TOTAL TOKENS ==> 8

 ---- POST ----

 [('Flexible', 'JJ'), ('text', 'NN'), ('segmentation', 'NN'), ('with', 'IN'), ('structured', 'JJ'), ('multilabel', 'NN'), ('classification', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Flexible', 'text', 'segmentation', 'structured', 'multilabel', 'classification', '.']

 TOTAL FILTERED TOKENS ==>  7

 ---- POST FOR FILTERED TOKENS ----

 [('Flexible', 'JJ'), ('text', 'NN'), ('segmentation', 'NN'), ('structured', 'VBD'), ('multilabel', 'JJ'), ('classification', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Flexible text', 'text segmentation', 'segmentation structured', 'structured multilabel', 'multilabel classification', 'classification .'] 

 TOTAL BIGRAMS --> 6 



 ---- TRI-GRAMS ---- 

 ['Flexible text segmentation', 'text segmentation structured', 'segmentation structured multilabel', 'structured multilabel classification', 'multilabel classification .'] 

 TOTAL TRIGRAMS --> 5 



 ---- NOUN PHRASES ---- 

 ['Flexible text', 'segmentation', 'multilabel classification'] 

 TOTAL NOUN PHRASES --> 3 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> ['Flexible']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['flexibl', 'text', 'segment', 'structur', 'multilabel', 'classif', '.']

 TOTAL PORTER STEM WORDS ==> 7



 ---- SNOWBALL STEMMING ----

['flexibl', 'text', 'segment', 'structur', 'multilabel', 'classif', '.']

 TOTAL SNOWBALL STEM WORDS ==> 7



 ---- LEMMATIZATION ----

['Flexible', 'text', 'segmentation', 'structured', 'multilabel', 'classification', '.']

 TOTAL LEMMATIZE WORDS ==> 7

************************************************************************************************************************

474 --> In Proceedings of the conference on Human  Language Technology and Empirical Methods in Natural Language Processing (pp. 


 ---- TOKENS ----

 ['In', 'Proceedings', 'of', 'the', 'conference', 'on', 'Human', 'Language', 'Technology', 'and', 'Empirical', 'Methods', 'in', 'Natural', 'Language', 'Processing', '(', 'pp', '.'] 

 TOTAL TOKENS ==> 19

 ---- POST ----

 [('In', 'IN'), ('Proceedings', 'NNP'), ('of', 'IN'), ('the', 'DT'), ('conference', 'NN'), ('on', 'IN'), ('Human', 'NNP'), ('Language', 'NNP'), ('Technology', 'NNP'), ('and', 'CC'), ('Empirical', 'NNP'), ('Methods', 'NNP'), ('in', 'IN'), ('Natural', 'NNP'), ('Language', 'NNP'), ('Processing', 'NNP'), ('(', '('), ('pp', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Proceedings', 'conference', 'Human', 'Language', 'Technology', 'Empirical', 'Methods', 'Natural', 'Language', 'Processing', '(', 'pp', '.']

 TOTAL FILTERED TOKENS ==>  13

 ---- POST FOR FILTERED TOKENS ----

 [('Proceedings', 'NNS'), ('conference', 'NN'), ('Human', 'NNP'), ('Language', 'NNP'), ('Technology', 'NNP'), ('Empirical', 'NNP'), ('Methods', 'NNP'), ('Natural', 'NNP'), ('Language', 'NNP'), ('Processing', 'NNP'), ('(', '('), ('pp', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Proceedings conference', 'conference Human', 'Human Language', 'Language Technology', 'Technology Empirical', 'Empirical Methods', 'Methods Natural', 'Natural Language', 'Language Processing', 'Processing (', '( pp', 'pp .'] 

 TOTAL BIGRAMS --> 12 



 ---- TRI-GRAMS ---- 

 ['Proceedings conference Human', 'conference Human Language', 'Human Language Technology', 'Language Technology Empirical', 'Technology Empirical Methods', 'Empirical Methods Natural', 'Methods Natural Language', 'Natural Language Processing', 'Language Processing (', 'Processing ( pp', '( pp .'] 

 TOTAL TRIGRAMS --> 11 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 []

 ---- PORTER STEMMING ----

['proceed', 'confer', 'human', 'languag', 'technolog', 'empir', 'method', 'natur', 'languag', 'process', '(', 'pp', '.']

 TOTAL PORTER STEM WORDS ==> 13



 ---- SNOWBALL STEMMING ----

['proceed', 'confer', 'human', 'languag', 'technolog', 'empir', 'method', 'natur', 'languag', 'process', '(', 'pp', '.']

 TOTAL SNOWBALL STEM WORDS ==> 13



 ---- LEMMATIZATION ----

['Proceedings', 'conference', 'Human', 'Language', 'Technology', 'Empirical', 'Methods', 'Natural', 'Language', 'Processing', '(', 'pp', '.']

 TOTAL LEMMATIZE WORDS ==> 13

************************************************************************************************************************

475 --> 987- 994). 


 ---- TOKENS ----

 ['987-', '994', ')', '.'] 

 TOTAL TOKENS ==> 4

 ---- POST ----

 [('987-', 'CD'), ('994', 'CD'), (')', ')'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['987-', '994', ')', '.']

 TOTAL FILTERED TOKENS ==>  4

 ---- POST FOR FILTERED TOKENS ----

 [('987-', 'CD'), ('994', 'CD'), (')', ')'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['987- 994', '994 )', ') .'] 

 TOTAL BIGRAMS --> 3 



 ---- TRI-GRAMS ---- 

 ['987- 994 )', '994 ) .'] 

 TOTAL TRIGRAMS --> 2 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 []

 ---- PORTER STEMMING ----

['987-', '994', ')', '.']

 TOTAL PORTER STEM WORDS ==> 4



 ---- SNOWBALL STEMMING ----

['987-', '994', ')', '.']

 TOTAL SNOWBALL STEM WORDS ==> 4



 ---- LEMMATIZATION ----

['987-', '994', ')', '.']

 TOTAL LEMMATIZE WORDS ==> 4

************************************************************************************************************************

476 --> Association for Computational Linguistics. 


 ---- TOKENS ----

 ['Association', 'for', 'Computational', 'Linguistics', '.'] 

 TOTAL TOKENS ==> 5

 ---- POST ----

 [('Association', 'NNP'), ('for', 'IN'), ('Computational', 'NNP'), ('Linguistics', 'NNP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Association', 'Computational', 'Linguistics', '.']

 TOTAL FILTERED TOKENS ==>  4

 ---- POST FOR FILTERED TOKENS ----

 [('Association', 'NNP'), ('Computational', 'NNP'), ('Linguistics', 'NNP'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Association Computational', 'Computational Linguistics', 'Linguistics .'] 

 TOTAL BIGRAMS --> 3 



 ---- TRI-GRAMS ---- 

 ['Association Computational Linguistics', 'Computational Linguistics .'] 

 TOTAL TRIGRAMS --> 2 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['associ', 'comput', 'linguist', '.']

 TOTAL PORTER STEM WORDS ==> 4



 ---- SNOWBALL STEMMING ----

['associ', 'comput', 'linguist', '.']

 TOTAL SNOWBALL STEM WORDS ==> 4



 ---- LEMMATIZATION ----

['Association', 'Computational', 'Linguistics', '.']

 TOTAL LEMMATIZE WORDS ==> 4

************************************************************************************************************************

477 --> [32] Sun, X., Morency, L. P., Okanohara, D., & Tsujii, J. I. 


 ---- TOKENS ----

 ['[', '32', ']', 'Sun', ',', 'X.', ',', 'Morency', ',', 'L.', 'P.', ',', 'Okanohara', ',', 'D.', ',', '&', 'Tsujii', ',', 'J.', 'I', '.'] 

 TOTAL TOKENS ==> 22

 ---- POST ----

 [('[', 'RB'), ('32', 'CD'), (']', 'JJ'), ('Sun', 'NNP'), (',', ','), ('X.', 'NNP'), (',', ','), ('Morency', 'NNP'), (',', ','), ('L.', 'NNP'), ('P.', 'NNP'), (',', ','), ('Okanohara', 'NNP'), (',', ','), ('D.', 'NNP'), (',', ','), ('&', 'CC'), ('Tsujii', 'NNP'), (',', ','), ('J.', 'NNP'), ('I', 'PRP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['[', '32', ']', 'Sun', ',', 'X.', ',', 'Morency', ',', 'L.', 'P.', ',', 'Okanohara', ',', 'D.', ',', '&', 'Tsujii', ',', 'J.', '.']

 TOTAL FILTERED TOKENS ==>  21

 ---- POST FOR FILTERED TOKENS ----

 [('[', 'RB'), ('32', 'CD'), (']', 'JJ'), ('Sun', 'NNP'), (',', ','), ('X.', 'NNP'), (',', ','), ('Morency', 'NNP'), (',', ','), ('L.', 'NNP'), ('P.', 'NNP'), (',', ','), ('Okanohara', 'NNP'), (',', ','), ('D.', 'NNP'), (',', ','), ('&', 'CC'), ('Tsujii', 'NNP'), (',', ','), ('J.', 'NNP'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['[ 32', '32 ]', '] Sun', 'Sun ,', ', X.', 'X. ,', ', Morency', 'Morency ,', ', L.', 'L. P.', 'P. ,', ', Okanohara', 'Okanohara ,', ', D.', 'D. ,', ', &', '& Tsujii', 'Tsujii ,', ', J.', 'J. .'] 

 TOTAL BIGRAMS --> 20 



 ---- TRI-GRAMS ---- 

 ['[ 32 ]', '32 ] Sun', '] Sun ,', 'Sun , X.', ', X. ,', 'X. , Morency', ', Morency ,', 'Morency , L.', ', L. P.', 'L. P. ,', 'P. , Okanohara', ', Okanohara ,', 'Okanohara , D.', ', D. ,', 'D. , &', ', & Tsujii', '& Tsujii ,', 'Tsujii , J.', ', J. .'] 

 TOTAL TRIGRAMS --> 19 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> ['Morency']
 TOTAL PERSON ENTITY --> 1 


 GPE ---> ['Okanohara', 'Tsujii']
 TOTAL GPE ENTITY --> 2 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['[', '32', ']', 'sun', ',', 'x.', ',', 'morenc', ',', 'l.', 'p.', ',', 'okanohara', ',', 'd.', ',', '&', 'tsujii', ',', 'j.', '.']

 TOTAL PORTER STEM WORDS ==> 21



 ---- SNOWBALL STEMMING ----

['[', '32', ']', 'sun', ',', 'x.', ',', 'morenc', ',', 'l.', 'p.', ',', 'okanohara', ',', 'd.', ',', '&', 'tsujii', ',', 'j.', '.']

 TOTAL SNOWBALL STEM WORDS ==> 21



 ---- LEMMATIZATION ----

['[', '32', ']', 'Sun', ',', 'X.', ',', 'Morency', ',', 'L.', 'P.', ',', 'Okanohara', ',', 'D.', ',', '&', 'Tsujii', ',', 'J.', '.']

 TOTAL LEMMATIZE WORDS ==> 21

************************************************************************************************************************

478 --> (2008, August). 


 ---- TOKENS ----

 ['(', '2008', ',', 'August', ')', '.'] 

 TOTAL TOKENS ==> 6

 ---- POST ----

 [('(', '('), ('2008', 'CD'), (',', ','), ('August', 'NNP'), (')', ')'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['(', '2008', ',', 'August', ')', '.']

 TOTAL FILTERED TOKENS ==>  6

 ---- POST FOR FILTERED TOKENS ----

 [('(', '('), ('2008', 'CD'), (',', ','), ('August', 'NNP'), (')', ')'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['( 2008', '2008 ,', ', August', 'August )', ') .'] 

 TOTAL BIGRAMS --> 5 



 ---- TRI-GRAMS ---- 

 ['( 2008 ,', '2008 , August', ', August )', 'August ) .'] 

 TOTAL TRIGRAMS --> 4 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['(', '2008', ',', 'august', ')', '.']

 TOTAL PORTER STEM WORDS ==> 6



 ---- SNOWBALL STEMMING ----

['(', '2008', ',', 'august', ')', '.']

 TOTAL SNOWBALL STEM WORDS ==> 6



 ---- LEMMATIZATION ----

['(', '2008', ',', 'August', ')', '.']

 TOTAL LEMMATIZE WORDS ==> 6

************************************************************************************************************************

479 --> Modeling latent- dynamic in shallow parsing: a latent conditional model with improved inference. 


 ---- TOKENS ----

 ['Modeling', 'latent-', 'dynamic', 'in', 'shallow', 'parsing', ':', 'a', 'latent', 'conditional', 'model', 'with', 'improved', 'inference', '.'] 

 TOTAL TOKENS ==> 15

 ---- POST ----

 [('Modeling', 'VBG'), ('latent-', 'JJ'), ('dynamic', 'JJ'), ('in', 'IN'), ('shallow', 'JJ'), ('parsing', 'NN'), (':', ':'), ('a', 'DT'), ('latent', 'JJ'), ('conditional', 'JJ'), ('model', 'NN'), ('with', 'IN'), ('improved', 'JJ'), ('inference', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Modeling', 'latent-', 'dynamic', 'shallow', 'parsing', ':', 'latent', 'conditional', 'model', 'improved', 'inference', '.']

 TOTAL FILTERED TOKENS ==>  12

 ---- POST FOR FILTERED TOKENS ----

 [('Modeling', 'VBG'), ('latent-', 'JJ'), ('dynamic', 'JJ'), ('shallow', 'JJ'), ('parsing', 'NN'), (':', ':'), ('latent', 'JJ'), ('conditional', 'JJ'), ('model', 'NN'), ('improved', 'VBD'), ('inference', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Modeling latent-', 'latent- dynamic', 'dynamic shallow', 'shallow parsing', 'parsing :', ': latent', 'latent conditional', 'conditional model', 'model improved', 'improved inference', 'inference .'] 

 TOTAL BIGRAMS --> 11 



 ---- TRI-GRAMS ---- 

 ['Modeling latent- dynamic', 'latent- dynamic shallow', 'dynamic shallow parsing', 'shallow parsing :', 'parsing : latent', ': latent conditional', 'latent conditional model', 'conditional model improved', 'model improved inference', 'improved inference .'] 

 TOTAL TRIGRAMS --> 10 



 ---- NOUN PHRASES ---- 

 ['latent- dynamic shallow parsing', 'latent conditional model', 'inference'] 

 TOTAL NOUN PHRASES --> 3 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['model', 'latent-', 'dynam', 'shallow', 'pars', ':', 'latent', 'condit', 'model', 'improv', 'infer', '.']

 TOTAL PORTER STEM WORDS ==> 12



 ---- SNOWBALL STEMMING ----

['model', 'latent-', 'dynam', 'shallow', 'pars', ':', 'latent', 'condit', 'model', 'improv', 'infer', '.']

 TOTAL SNOWBALL STEM WORDS ==> 12



 ---- LEMMATIZATION ----

['Modeling', 'latent-', 'dynamic', 'shallow', 'parsing', ':', 'latent', 'conditional', 'model', 'improved', 'inference', '.']

 TOTAL LEMMATIZE WORDS ==> 12

************************************************************************************************************************

480 --> In Proceedings of the 22nd International Conference on Computational Linguistics-Volume  1 (pp. 


 ---- TOKENS ----

 ['In', 'Proceedings', 'of', 'the', '22nd', 'International', 'Conference', 'on', 'Computational', 'Linguistics-Volume', '1', '(', 'pp', '.'] 

 TOTAL TOKENS ==> 14

 ---- POST ----

 [('In', 'IN'), ('Proceedings', 'NNP'), ('of', 'IN'), ('the', 'DT'), ('22nd', 'CD'), ('International', 'NNP'), ('Conference', 'NNP'), ('on', 'IN'), ('Computational', 'NNP'), ('Linguistics-Volume', 'NNP'), ('1', 'CD'), ('(', '('), ('pp', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Proceedings', '22nd', 'International', 'Conference', 'Computational', 'Linguistics-Volume', '1', '(', 'pp', '.']

 TOTAL FILTERED TOKENS ==>  10

 ---- POST FOR FILTERED TOKENS ----

 [('Proceedings', 'NNS'), ('22nd', 'CD'), ('International', 'NNP'), ('Conference', 'NNP'), ('Computational', 'NNP'), ('Linguistics-Volume', 'NNP'), ('1', 'CD'), ('(', '('), ('pp', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Proceedings 22nd', '22nd International', 'International Conference', 'Conference Computational', 'Computational Linguistics-Volume', 'Linguistics-Volume 1', '1 (', '( pp', 'pp .'] 

 TOTAL BIGRAMS --> 9 



 ---- TRI-GRAMS ---- 

 ['Proceedings 22nd International', '22nd International Conference', 'International Conference Computational', 'Conference Computational Linguistics-Volume', 'Computational Linguistics-Volume 1', 'Linguistics-Volume 1 (', '1 ( pp', '( pp .'] 

 TOTAL TRIGRAMS --> 8 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 []

 ---- PORTER STEMMING ----

['proceed', '22nd', 'intern', 'confer', 'comput', 'linguistics-volum', '1', '(', 'pp', '.']

 TOTAL PORTER STEM WORDS ==> 10



 ---- SNOWBALL STEMMING ----

['proceed', '22nd', 'intern', 'confer', 'comput', 'linguistics-volum', '1', '(', 'pp', '.']

 TOTAL SNOWBALL STEM WORDS ==> 10



 ---- LEMMATIZATION ----

['Proceedings', '22nd', 'International', 'Conference', 'Computational', 'Linguistics-Volume', '1', '(', 'pp', '.']

 TOTAL LEMMATIZE WORDS ==> 10

************************************************************************************************************************

481 --> 841-848). 


 ---- TOKENS ----

 ['841-848', ')', '.'] 

 TOTAL TOKENS ==> 3

 ---- POST ----

 [('841-848', 'JJ'), (')', ')'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['841-848', ')', '.']

 TOTAL FILTERED TOKENS ==>  3

 ---- POST FOR FILTERED TOKENS ----

 [('841-848', 'JJ'), (')', ')'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['841-848 )', ') .'] 

 TOTAL BIGRAMS --> 2 



 ---- TRI-GRAMS ---- 

 ['841-848 ) .'] 

 TOTAL TRIGRAMS --> 1 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 []

 ---- PORTER STEMMING ----

['841-848', ')', '.']

 TOTAL PORTER STEM WORDS ==> 3



 ---- SNOWBALL STEMMING ----

['841-848', ')', '.']

 TOTAL SNOWBALL STEM WORDS ==> 3



 ---- LEMMATIZATION ----

['841-848', ')', '.']

 TOTAL LEMMATIZE WORDS ==> 3

************************************************************************************************************************

482 --> Association for Computational Linguistics. 


 ---- TOKENS ----

 ['Association', 'for', 'Computational', 'Linguistics', '.'] 

 TOTAL TOKENS ==> 5

 ---- POST ----

 [('Association', 'NNP'), ('for', 'IN'), ('Computational', 'NNP'), ('Linguistics', 'NNP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Association', 'Computational', 'Linguistics', '.']

 TOTAL FILTERED TOKENS ==>  4

 ---- POST FOR FILTERED TOKENS ----

 [('Association', 'NNP'), ('Computational', 'NNP'), ('Linguistics', 'NNP'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Association Computational', 'Computational Linguistics', 'Linguistics .'] 

 TOTAL BIGRAMS --> 3 



 ---- TRI-GRAMS ---- 

 ['Association Computational Linguistics', 'Computational Linguistics .'] 

 TOTAL TRIGRAMS --> 2 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['associ', 'comput', 'linguist', '.']

 TOTAL PORTER STEM WORDS ==> 4



 ---- SNOWBALL STEMMING ----

['associ', 'comput', 'linguist', '.']

 TOTAL SNOWBALL STEM WORDS ==> 4



 ---- LEMMATIZATION ----

['Association', 'Computational', 'Linguistics', '.']

 TOTAL LEMMATIZE WORDS ==> 4

************************************************************************************************************************

483 --> [33] Ritter, A., Clark, S., & Etzioni, O. 


 ---- TOKENS ----

 ['[', '33', ']', 'Ritter', ',', 'A.', ',', 'Clark', ',', 'S.', ',', '&', 'Etzioni', ',', 'O', '.'] 

 TOTAL TOKENS ==> 16

 ---- POST ----

 [('[', 'RB'), ('33', 'CD'), (']', 'JJ'), ('Ritter', 'NNP'), (',', ','), ('A.', 'NNP'), (',', ','), ('Clark', 'NNP'), (',', ','), ('S.', 'NNP'), (',', ','), ('&', 'CC'), ('Etzioni', 'NNP'), (',', ','), ('O', 'NNP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['[', '33', ']', 'Ritter', ',', 'A.', ',', 'Clark', ',', 'S.', ',', '&', 'Etzioni', ',', '.']

 TOTAL FILTERED TOKENS ==>  15

 ---- POST FOR FILTERED TOKENS ----

 [('[', 'RB'), ('33', 'CD'), (']', 'JJ'), ('Ritter', 'NNP'), (',', ','), ('A.', 'NNP'), (',', ','), ('Clark', 'NNP'), (',', ','), ('S.', 'NNP'), (',', ','), ('&', 'CC'), ('Etzioni', 'NNP'), (',', ','), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['[ 33', '33 ]', '] Ritter', 'Ritter ,', ', A.', 'A. ,', ', Clark', 'Clark ,', ', S.', 'S. ,', ', &', '& Etzioni', 'Etzioni ,', ', .'] 

 TOTAL BIGRAMS --> 14 



 ---- TRI-GRAMS ---- 

 ['[ 33 ]', '33 ] Ritter', '] Ritter ,', 'Ritter , A.', ', A. ,', 'A. , Clark', ', Clark ,', 'Clark , S.', ', S. ,', 'S. , &', ', & Etzioni', '& Etzioni ,', 'Etzioni , .'] 

 TOTAL TRIGRAMS --> 13 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> ['Ritter', 'Clark']
 TOTAL PERSON ENTITY --> 2 


 GPE ---> ['Etzioni']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['[', '33', ']', 'ritter', ',', 'a.', ',', 'clark', ',', 's.', ',', '&', 'etzioni', ',', '.']

 TOTAL PORTER STEM WORDS ==> 15



 ---- SNOWBALL STEMMING ----

['[', '33', ']', 'ritter', ',', 'a.', ',', 'clark', ',', 's.', ',', '&', 'etzioni', ',', '.']

 TOTAL SNOWBALL STEM WORDS ==> 15



 ---- LEMMATIZATION ----

['[', '33', ']', 'Ritter', ',', 'A.', ',', 'Clark', ',', 'S.', ',', '&', 'Etzioni', ',', '.']

 TOTAL LEMMATIZE WORDS ==> 15

************************************************************************************************************************

484 --> (2011, July). 


 ---- TOKENS ----

 ['(', '2011', ',', 'July', ')', '.'] 

 TOTAL TOKENS ==> 6

 ---- POST ----

 [('(', '('), ('2011', 'CD'), (',', ','), ('July', 'NNP'), (')', ')'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['(', '2011', ',', 'July', ')', '.']

 TOTAL FILTERED TOKENS ==>  6

 ---- POST FOR FILTERED TOKENS ----

 [('(', '('), ('2011', 'CD'), (',', ','), ('July', 'NNP'), (')', ')'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['( 2011', '2011 ,', ', July', 'July )', ') .'] 

 TOTAL BIGRAMS --> 5 



 ---- TRI-GRAMS ---- 

 ['( 2011 ,', '2011 , July', ', July )', 'July ) .'] 

 TOTAL TRIGRAMS --> 4 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['(', '2011', ',', 'juli', ')', '.']

 TOTAL PORTER STEM WORDS ==> 6



 ---- SNOWBALL STEMMING ----

['(', '2011', ',', 'juli', ')', '.']

 TOTAL SNOWBALL STEM WORDS ==> 6



 ---- LEMMATIZATION ----

['(', '2011', ',', 'July', ')', '.']

 TOTAL LEMMATIZE WORDS ==> 6

************************************************************************************************************************

485 --> Named entity recognition in tweets: an  experimental study. 


 ---- TOKENS ----

 ['Named', 'entity', 'recognition', 'in', 'tweets', ':', 'an', 'experimental', 'study', '.'] 

 TOTAL TOKENS ==> 10

 ---- POST ----

 [('Named', 'VBN'), ('entity', 'NN'), ('recognition', 'NN'), ('in', 'IN'), ('tweets', 'NNS'), (':', ':'), ('an', 'DT'), ('experimental', 'JJ'), ('study', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Named', 'entity', 'recognition', 'tweets', ':', 'experimental', 'study', '.']

 TOTAL FILTERED TOKENS ==>  8

 ---- POST FOR FILTERED TOKENS ----

 [('Named', 'VBN'), ('entity', 'NN'), ('recognition', 'NN'), ('tweets', 'NNS'), (':', ':'), ('experimental', 'JJ'), ('study', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Named entity', 'entity recognition', 'recognition tweets', 'tweets :', ': experimental', 'experimental study', 'study .'] 

 TOTAL BIGRAMS --> 7 



 ---- TRI-GRAMS ---- 

 ['Named entity recognition', 'entity recognition tweets', 'recognition tweets :', 'tweets : experimental', ': experimental study', 'experimental study .'] 

 TOTAL TRIGRAMS --> 6 



 ---- NOUN PHRASES ---- 

 ['entity', 'recognition', 'experimental study'] 

 TOTAL NOUN PHRASES --> 3 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['name', 'entiti', 'recognit', 'tweet', ':', 'experiment', 'studi', '.']

 TOTAL PORTER STEM WORDS ==> 8



 ---- SNOWBALL STEMMING ----

['name', 'entiti', 'recognit', 'tweet', ':', 'experiment', 'studi', '.']

 TOTAL SNOWBALL STEM WORDS ==> 8



 ---- LEMMATIZATION ----

['Named', 'entity', 'recognition', 'tweet', ':', 'experimental', 'study', '.']

 TOTAL LEMMATIZE WORDS ==> 8

************************************************************************************************************************

486 --> In Proceedings of the Conference on Empirical Methods in Natural  Language Processing (pp. 


 ---- TOKENS ----

 ['In', 'Proceedings', 'of', 'the', 'Conference', 'on', 'Empirical', 'Methods', 'in', 'Natural', 'Language', 'Processing', '(', 'pp', '.'] 

 TOTAL TOKENS ==> 15

 ---- POST ----

 [('In', 'IN'), ('Proceedings', 'NNP'), ('of', 'IN'), ('the', 'DT'), ('Conference', 'NNP'), ('on', 'IN'), ('Empirical', 'NNP'), ('Methods', 'NNP'), ('in', 'IN'), ('Natural', 'NNP'), ('Language', 'NNP'), ('Processing', 'NNP'), ('(', '('), ('pp', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Proceedings', 'Conference', 'Empirical', 'Methods', 'Natural', 'Language', 'Processing', '(', 'pp', '.']

 TOTAL FILTERED TOKENS ==>  10

 ---- POST FOR FILTERED TOKENS ----

 [('Proceedings', 'NNS'), ('Conference', 'NNP'), ('Empirical', 'NNP'), ('Methods', 'NNP'), ('Natural', 'NNP'), ('Language', 'NNP'), ('Processing', 'NNP'), ('(', '('), ('pp', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Proceedings Conference', 'Conference Empirical', 'Empirical Methods', 'Methods Natural', 'Natural Language', 'Language Processing', 'Processing (', '( pp', 'pp .'] 

 TOTAL BIGRAMS --> 9 



 ---- TRI-GRAMS ---- 

 ['Proceedings Conference Empirical', 'Conference Empirical Methods', 'Empirical Methods Natural', 'Methods Natural Language', 'Natural Language Processing', 'Language Processing (', 'Processing ( pp', '( pp .'] 

 TOTAL TRIGRAMS --> 8 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 []

 ---- PORTER STEMMING ----

['proceed', 'confer', 'empir', 'method', 'natur', 'languag', 'process', '(', 'pp', '.']

 TOTAL PORTER STEM WORDS ==> 10



 ---- SNOWBALL STEMMING ----

['proceed', 'confer', 'empir', 'method', 'natur', 'languag', 'process', '(', 'pp', '.']

 TOTAL SNOWBALL STEM WORDS ==> 10



 ---- LEMMATIZATION ----

['Proceedings', 'Conference', 'Empirical', 'Methods', 'Natural', 'Language', 'Processing', '(', 'pp', '.']

 TOTAL LEMMATIZE WORDS ==> 10

************************************************************************************************************************

487 --> 1524-1534). 


 ---- TOKENS ----

 ['1524-1534', ')', '.'] 

 TOTAL TOKENS ==> 3

 ---- POST ----

 [('1524-1534', 'JJ'), (')', ')'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['1524-1534', ')', '.']

 TOTAL FILTERED TOKENS ==>  3

 ---- POST FOR FILTERED TOKENS ----

 [('1524-1534', 'JJ'), (')', ')'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['1524-1534 )', ') .'] 

 TOTAL BIGRAMS --> 2 



 ---- TRI-GRAMS ---- 

 ['1524-1534 ) .'] 

 TOTAL TRIGRAMS --> 1 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 []

 ---- PORTER STEMMING ----

['1524-1534', ')', '.']

 TOTAL PORTER STEM WORDS ==> 3



 ---- SNOWBALL STEMMING ----

['1524-1534', ')', '.']

 TOTAL SNOWBALL STEM WORDS ==> 3



 ---- LEMMATIZATION ----

['1524-1534', ')', '.']

 TOTAL LEMMATIZE WORDS ==> 3

************************************************************************************************************************

488 --> Association for Computational Linguistics. 


 ---- TOKENS ----

 ['Association', 'for', 'Computational', 'Linguistics', '.'] 

 TOTAL TOKENS ==> 5

 ---- POST ----

 [('Association', 'NNP'), ('for', 'IN'), ('Computational', 'NNP'), ('Linguistics', 'NNP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Association', 'Computational', 'Linguistics', '.']

 TOTAL FILTERED TOKENS ==>  4

 ---- POST FOR FILTERED TOKENS ----

 [('Association', 'NNP'), ('Computational', 'NNP'), ('Linguistics', 'NNP'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Association Computational', 'Computational Linguistics', 'Linguistics .'] 

 TOTAL BIGRAMS --> 3 



 ---- TRI-GRAMS ---- 

 ['Association Computational Linguistics', 'Computational Linguistics .'] 

 TOTAL TRIGRAMS --> 2 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['associ', 'comput', 'linguist', '.']

 TOTAL PORTER STEM WORDS ==> 4



 ---- SNOWBALL STEMMING ----

['associ', 'comput', 'linguist', '.']

 TOTAL SNOWBALL STEM WORDS ==> 4



 ---- LEMMATIZATION ----

['Association', 'Computational', 'Linguistics', '.']

 TOTAL LEMMATIZE WORDS ==> 4

************************************************************************************************************************

489 --> [34] Sharma, S., Srinivas, PYKL, & Balabantaray, RC (2016). 


 ---- TOKENS ----

 ['[', '34', ']', 'Sharma', ',', 'S.', ',', 'Srinivas', ',', 'PYKL', ',', '&', 'Balabantaray', ',', 'RC', '(', '2016', ')', '.'] 

 TOTAL TOKENS ==> 19

 ---- POST ----

 [('[', 'RB'), ('34', 'CD'), (']', 'JJ'), ('Sharma', 'NNP'), (',', ','), ('S.', 'NNP'), (',', ','), ('Srinivas', 'NNP'), (',', ','), ('PYKL', 'NNP'), (',', ','), ('&', 'CC'), ('Balabantaray', 'NNP'), (',', ','), ('RC', 'NNP'), ('(', '('), ('2016', 'CD'), (')', ')'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['[', '34', ']', 'Sharma', ',', 'S.', ',', 'Srinivas', ',', 'PYKL', ',', '&', 'Balabantaray', ',', 'RC', '(', '2016', ')', '.']

 TOTAL FILTERED TOKENS ==>  19

 ---- POST FOR FILTERED TOKENS ----

 [('[', 'RB'), ('34', 'CD'), (']', 'JJ'), ('Sharma', 'NNP'), (',', ','), ('S.', 'NNP'), (',', ','), ('Srinivas', 'NNP'), (',', ','), ('PYKL', 'NNP'), (',', ','), ('&', 'CC'), ('Balabantaray', 'NNP'), (',', ','), ('RC', 'NNP'), ('(', '('), ('2016', 'CD'), (')', ')'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['[ 34', '34 ]', '] Sharma', 'Sharma ,', ', S.', 'S. ,', ', Srinivas', 'Srinivas ,', ', PYKL', 'PYKL ,', ', &', '& Balabantaray', 'Balabantaray ,', ', RC', 'RC (', '( 2016', '2016 )', ') .'] 

 TOTAL BIGRAMS --> 18 



 ---- TRI-GRAMS ---- 

 ['[ 34 ]', '34 ] Sharma', '] Sharma ,', 'Sharma , S.', ', S. ,', 'S. , Srinivas', ', Srinivas ,', 'Srinivas , PYKL', ', PYKL ,', 'PYKL , &', ', & Balabantaray', '& Balabantaray ,', 'Balabantaray , RC', ', RC (', 'RC ( 2016', '( 2016 )', '2016 ) .'] 

 TOTAL TRIGRAMS --> 17 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> ['PYKL']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> ['Sharma', 'Srinivas', 'Balabantaray', 'RC']
 TOTAL GPE ENTITY --> 4 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['[', '34', ']', 'sharma', ',', 's.', ',', 'sriniva', ',', 'pykl', ',', '&', 'balabantaray', ',', 'rc', '(', '2016', ')', '.']

 TOTAL PORTER STEM WORDS ==> 19



 ---- SNOWBALL STEMMING ----

['[', '34', ']', 'sharma', ',', 's.', ',', 'sriniva', ',', 'pykl', ',', '&', 'balabantaray', ',', 'rc', '(', '2016', ')', '.']

 TOTAL SNOWBALL STEM WORDS ==> 19



 ---- LEMMATIZATION ----

['[', '34', ']', 'Sharma', ',', 'S.', ',', 'Srinivas', ',', 'PYKL', ',', '&', 'Balabantaray', ',', 'RC', '(', '2016', ')', '.']

 TOTAL LEMMATIZE WORDS ==> 19

************************************************************************************************************************

490 --> Emotion Detection using  Online Machine Learning Method and TLBO on Mixed Script. 


 ---- TOKENS ----

 ['Emotion', 'Detection', 'using', 'Online', 'Machine', 'Learning', 'Method', 'and', 'TLBO', 'on', 'Mixed', 'Script', '.'] 

 TOTAL TOKENS ==> 13

 ---- POST ----

 [('Emotion', 'NN'), ('Detection', 'NNP'), ('using', 'VBG'), ('Online', 'NNP'), ('Machine', 'NNP'), ('Learning', 'NNP'), ('Method', 'NNP'), ('and', 'CC'), ('TLBO', 'NNP'), ('on', 'IN'), ('Mixed', 'NNP'), ('Script', 'NNP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Emotion', 'Detection', 'using', 'Online', 'Machine', 'Learning', 'Method', 'TLBO', 'Mixed', 'Script', '.']

 TOTAL FILTERED TOKENS ==>  11

 ---- POST FOR FILTERED TOKENS ----

 [('Emotion', 'NN'), ('Detection', 'NNP'), ('using', 'VBG'), ('Online', 'NNP'), ('Machine', 'NNP'), ('Learning', 'NNP'), ('Method', 'NNP'), ('TLBO', 'NNP'), ('Mixed', 'NNP'), ('Script', 'NNP'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Emotion Detection', 'Detection using', 'using Online', 'Online Machine', 'Machine Learning', 'Learning Method', 'Method TLBO', 'TLBO Mixed', 'Mixed Script', 'Script .'] 

 TOTAL BIGRAMS --> 10 



 ---- TRI-GRAMS ---- 

 ['Emotion Detection using', 'Detection using Online', 'using Online Machine', 'Online Machine Learning', 'Machine Learning Method', 'Learning Method TLBO', 'Method TLBO Mixed', 'TLBO Mixed Script', 'Mixed Script .'] 

 TOTAL TRIGRAMS --> 9 



 ---- NOUN PHRASES ---- 

 ['Emotion'] 

 TOTAL NOUN PHRASES --> 1 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> ['Emotion Detection', 'Online Machine Learning Method']
 TOTAL PERSON ENTITY --> 2 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['emot', 'detect', 'use', 'onlin', 'machin', 'learn', 'method', 'tlbo', 'mix', 'script', '.']

 TOTAL PORTER STEM WORDS ==> 11



 ---- SNOWBALL STEMMING ----

['emot', 'detect', 'use', 'onlin', 'machin', 'learn', 'method', 'tlbo', 'mix', 'script', '.']

 TOTAL SNOWBALL STEM WORDS ==> 11



 ---- LEMMATIZATION ----

['Emotion', 'Detection', 'using', 'Online', 'Machine', 'Learning', 'Method', 'TLBO', 'Mixed', 'Script', '.']

 TOTAL LEMMATIZE WORDS ==> 11

************************************************************************************************************************

491 --> In Proceedings of Language  Resources and Evaluation Conference 2016 (pp. 


 ---- TOKENS ----

 ['In', 'Proceedings', 'of', 'Language', 'Resources', 'and', 'Evaluation', 'Conference', '2016', '(', 'pp', '.'] 

 TOTAL TOKENS ==> 12

 ---- POST ----

 [('In', 'IN'), ('Proceedings', 'NNP'), ('of', 'IN'), ('Language', 'NNP'), ('Resources', 'NNPS'), ('and', 'CC'), ('Evaluation', 'NNP'), ('Conference', 'NNP'), ('2016', 'CD'), ('(', '('), ('pp', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Proceedings', 'Language', 'Resources', 'Evaluation', 'Conference', '2016', '(', 'pp', '.']

 TOTAL FILTERED TOKENS ==>  9

 ---- POST FOR FILTERED TOKENS ----

 [('Proceedings', 'NNS'), ('Language', 'NNP'), ('Resources', 'NNPS'), ('Evaluation', 'NNP'), ('Conference', 'NNP'), ('2016', 'CD'), ('(', '('), ('pp', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Proceedings Language', 'Language Resources', 'Resources Evaluation', 'Evaluation Conference', 'Conference 2016', '2016 (', '( pp', 'pp .'] 

 TOTAL BIGRAMS --> 8 



 ---- TRI-GRAMS ---- 

 ['Proceedings Language Resources', 'Language Resources Evaluation', 'Resources Evaluation Conference', 'Evaluation Conference 2016', 'Conference 2016 (', '2016 ( pp', '( pp .'] 

 TOTAL TRIGRAMS --> 7 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 []

 ---- PORTER STEMMING ----

['proceed', 'languag', 'resourc', 'evalu', 'confer', '2016', '(', 'pp', '.']

 TOTAL PORTER STEM WORDS ==> 9



 ---- SNOWBALL STEMMING ----

['proceed', 'languag', 'resourc', 'evalu', 'confer', '2016', '(', 'pp', '.']

 TOTAL SNOWBALL STEM WORDS ==> 9



 ---- LEMMATIZATION ----

['Proceedings', 'Language', 'Resources', 'Evaluation', 'Conference', '2016', '(', 'pp', '.']

 TOTAL LEMMATIZE WORDS ==> 9

************************************************************************************************************************

492 --> 47-51). 


 ---- TOKENS ----

 ['47-51', ')', '.'] 

 TOTAL TOKENS ==> 3

 ---- POST ----

 [('47-51', 'JJ'), (')', ')'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['47-51', ')', '.']

 TOTAL FILTERED TOKENS ==>  3

 ---- POST FOR FILTERED TOKENS ----

 [('47-51', 'JJ'), (')', ')'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['47-51 )', ') .'] 

 TOTAL BIGRAMS --> 2 



 ---- TRI-GRAMS ---- 

 ['47-51 ) .'] 

 TOTAL TRIGRAMS --> 1 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 []

 ---- PORTER STEMMING ----

['47-51', ')', '.']

 TOTAL PORTER STEM WORDS ==> 3



 ---- SNOWBALL STEMMING ----

['47-51', ')', '.']

 TOTAL SNOWBALL STEM WORDS ==> 3



 ---- LEMMATIZATION ----

['47-51', ')', '.']

 TOTAL LEMMATIZE WORDS ==> 3

************************************************************************************************************************

493 --> [35] Palmer, M., Gildea, D., & Kingsbury, P. (2005). 


 ---- TOKENS ----

 ['[', '35', ']', 'Palmer', ',', 'M.', ',', 'Gildea', ',', 'D.', ',', '&', 'Kingsbury', ',', 'P.', '(', '2005', ')', '.'] 

 TOTAL TOKENS ==> 19

 ---- POST ----

 [('[', 'RB'), ('35', 'CD'), (']', 'JJ'), ('Palmer', 'NNP'), (',', ','), ('M.', 'NNP'), (',', ','), ('Gildea', 'NNP'), (',', ','), ('D.', 'NNP'), (',', ','), ('&', 'CC'), ('Kingsbury', 'NNP'), (',', ','), ('P.', 'NNP'), ('(', '('), ('2005', 'CD'), (')', ')'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['[', '35', ']', 'Palmer', ',', 'M.', ',', 'Gildea', ',', 'D.', ',', '&', 'Kingsbury', ',', 'P.', '(', '2005', ')', '.']

 TOTAL FILTERED TOKENS ==>  19

 ---- POST FOR FILTERED TOKENS ----

 [('[', 'RB'), ('35', 'CD'), (']', 'JJ'), ('Palmer', 'NNP'), (',', ','), ('M.', 'NNP'), (',', ','), ('Gildea', 'NNP'), (',', ','), ('D.', 'NNP'), (',', ','), ('&', 'CC'), ('Kingsbury', 'NNP'), (',', ','), ('P.', 'NNP'), ('(', '('), ('2005', 'CD'), (')', ')'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['[ 35', '35 ]', '] Palmer', 'Palmer ,', ', M.', 'M. ,', ', Gildea', 'Gildea ,', ', D.', 'D. ,', ', &', '& Kingsbury', 'Kingsbury ,', ', P.', 'P. (', '( 2005', '2005 )', ') .'] 

 TOTAL BIGRAMS --> 18 



 ---- TRI-GRAMS ---- 

 ['[ 35 ]', '35 ] Palmer', '] Palmer ,', 'Palmer , M.', ', M. ,', 'M. , Gildea', ', Gildea ,', 'Gildea , D.', ', D. ,', 'D. , &', ', & Kingsbury', '& Kingsbury ,', 'Kingsbury , P.', ', P. (', 'P. ( 2005', '( 2005 )', '2005 ) .'] 

 TOTAL TRIGRAMS --> 17 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> ['Kingsbury']
 TOTAL PERSON ENTITY --> 1 


 GPE ---> ['Palmer', 'Gildea']
 TOTAL GPE ENTITY --> 2 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['[', '35', ']', 'palmer', ',', 'm.', ',', 'gildea', ',', 'd.', ',', '&', 'kingsburi', ',', 'p.', '(', '2005', ')', '.']

 TOTAL PORTER STEM WORDS ==> 19



 ---- SNOWBALL STEMMING ----

['[', '35', ']', 'palmer', ',', 'm.', ',', 'gildea', ',', 'd.', ',', '&', 'kingsburi', ',', 'p.', '(', '2005', ')', '.']

 TOTAL SNOWBALL STEM WORDS ==> 19



 ---- LEMMATIZATION ----

['[', '35', ']', 'Palmer', ',', 'M.', ',', 'Gildea', ',', 'D.', ',', '&', 'Kingsbury', ',', 'P.', '(', '2005', ')', '.']

 TOTAL LEMMATIZE WORDS ==> 19

************************************************************************************************************************

494 --> The proposition bank: An annotated  corpus of semantic roles. 


 ---- TOKENS ----

 ['The', 'proposition', 'bank', ':', 'An', 'annotated', 'corpus', 'of', 'semantic', 'roles', '.'] 

 TOTAL TOKENS ==> 11

 ---- POST ----

 [('The', 'DT'), ('proposition', 'NN'), ('bank', 'NN'), (':', ':'), ('An', 'DT'), ('annotated', 'JJ'), ('corpus', 'NN'), ('of', 'IN'), ('semantic', 'JJ'), ('roles', 'NNS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['proposition', 'bank', ':', 'annotated', 'corpus', 'semantic', 'roles', '.']

 TOTAL FILTERED TOKENS ==>  8

 ---- POST FOR FILTERED TOKENS ----

 [('proposition', 'NN'), ('bank', 'NN'), (':', ':'), ('annotated', 'VBN'), ('corpus', 'NN'), ('semantic', 'JJ'), ('roles', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['proposition bank', 'bank :', ': annotated', 'annotated corpus', 'corpus semantic', 'semantic roles', 'roles .'] 

 TOTAL BIGRAMS --> 7 



 ---- TRI-GRAMS ---- 

 ['proposition bank :', 'bank : annotated', ': annotated corpus', 'annotated corpus semantic', 'corpus semantic roles', 'semantic roles .'] 

 TOTAL TRIGRAMS --> 6 



 ---- NOUN PHRASES ---- 

 ['proposition', 'bank', 'corpus'] 

 TOTAL NOUN PHRASES --> 3 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['proposit', 'bank', ':', 'annot', 'corpu', 'semant', 'role', '.']

 TOTAL PORTER STEM WORDS ==> 8



 ---- SNOWBALL STEMMING ----

['proposit', 'bank', ':', 'annot', 'corpus', 'semant', 'role', '.']

 TOTAL SNOWBALL STEM WORDS ==> 8



 ---- LEMMATIZATION ----

['proposition', 'bank', ':', 'annotated', 'corpus', 'semantic', 'role', '.']

 TOTAL LEMMATIZE WORDS ==> 8

************************************************************************************************************************

495 --> Computational linguistics, 31(1), 71-106. 


 ---- TOKENS ----

 ['Computational', 'linguistics', ',', '31', '(', '1', ')', ',', '71-106', '.'] 

 TOTAL TOKENS ==> 10

 ---- POST ----

 [('Computational', 'JJ'), ('linguistics', 'NNS'), (',', ','), ('31', 'CD'), ('(', '('), ('1', 'CD'), (')', ')'), (',', ','), ('71-106', 'JJ'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Computational', 'linguistics', ',', '31', '(', '1', ')', ',', '71-106', '.']

 TOTAL FILTERED TOKENS ==>  10

 ---- POST FOR FILTERED TOKENS ----

 [('Computational', 'JJ'), ('linguistics', 'NNS'), (',', ','), ('31', 'CD'), ('(', '('), ('1', 'CD'), (')', ')'), (',', ','), ('71-106', 'JJ'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Computational linguistics', 'linguistics ,', ', 31', '31 (', '( 1', '1 )', ') ,', ', 71-106', '71-106 .'] 

 TOTAL BIGRAMS --> 9 



 ---- TRI-GRAMS ---- 

 ['Computational linguistics ,', 'linguistics , 31', ', 31 (', '31 ( 1', '( 1 )', '1 ) ,', ') , 71-106', ', 71-106 .'] 

 TOTAL TRIGRAMS --> 8 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['comput', 'linguist', ',', '31', '(', '1', ')', ',', '71-106', '.']

 TOTAL PORTER STEM WORDS ==> 10



 ---- SNOWBALL STEMMING ----

['comput', 'linguist', ',', '31', '(', '1', ')', ',', '71-106', '.']

 TOTAL SNOWBALL STEM WORDS ==> 10



 ---- LEMMATIZATION ----

['Computational', 'linguistics', ',', '31', '(', '1', ')', ',', '71-106', '.']

 TOTAL LEMMATIZE WORDS ==> 10

************************************************************************************************************************

496 --> [36] Benson, E., Haghighi, A., & Barzilay, R. (2011, June). 


 ---- TOKENS ----

 ['[', '36', ']', 'Benson', ',', 'E.', ',', 'Haghighi', ',', 'A.', ',', '&', 'Barzilay', ',', 'R.', '(', '2011', ',', 'June', ')', '.'] 

 TOTAL TOKENS ==> 21

 ---- POST ----

 [('[', 'RB'), ('36', 'CD'), (']', 'NNP'), ('Benson', 'NNP'), (',', ','), ('E.', 'NNP'), (',', ','), ('Haghighi', 'NNP'), (',', ','), ('A.', 'NNP'), (',', ','), ('&', 'CC'), ('Barzilay', 'NNP'), (',', ','), ('R.', 'NNP'), ('(', '('), ('2011', 'CD'), (',', ','), ('June', 'NNP'), (')', ')'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['[', '36', ']', 'Benson', ',', 'E.', ',', 'Haghighi', ',', 'A.', ',', '&', 'Barzilay', ',', 'R.', '(', '2011', ',', 'June', ')', '.']

 TOTAL FILTERED TOKENS ==>  21

 ---- POST FOR FILTERED TOKENS ----

 [('[', 'RB'), ('36', 'CD'), (']', 'NNP'), ('Benson', 'NNP'), (',', ','), ('E.', 'NNP'), (',', ','), ('Haghighi', 'NNP'), (',', ','), ('A.', 'NNP'), (',', ','), ('&', 'CC'), ('Barzilay', 'NNP'), (',', ','), ('R.', 'NNP'), ('(', '('), ('2011', 'CD'), (',', ','), ('June', 'NNP'), (')', ')'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['[ 36', '36 ]', '] Benson', 'Benson ,', ', E.', 'E. ,', ', Haghighi', 'Haghighi ,', ', A.', 'A. ,', ', &', '& Barzilay', 'Barzilay ,', ', R.', 'R. (', '( 2011', '2011 ,', ', June', 'June )', ') .'] 

 TOTAL BIGRAMS --> 20 



 ---- TRI-GRAMS ---- 

 ['[ 36 ]', '36 ] Benson', '] Benson ,', 'Benson , E.', ', E. ,', 'E. , Haghighi', ', Haghighi ,', 'Haghighi , A.', ', A. ,', 'A. , &', ', & Barzilay', '& Barzilay ,', 'Barzilay , R.', ', R. (', 'R. ( 2011', '( 2011 ,', '2011 , June', ', June )', 'June ) .'] 

 TOTAL TRIGRAMS --> 19 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> ['Benson', 'Barzilay']
 TOTAL PERSON ENTITY --> 2 


 GPE ---> ['Haghighi']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['[', '36', ']', 'benson', ',', 'e.', ',', 'haghighi', ',', 'a.', ',', '&', 'barzilay', ',', 'r.', '(', '2011', ',', 'june', ')', '.']

 TOTAL PORTER STEM WORDS ==> 21



 ---- SNOWBALL STEMMING ----

['[', '36', ']', 'benson', ',', 'e.', ',', 'haghighi', ',', 'a.', ',', '&', 'barzilay', ',', 'r.', '(', '2011', ',', 'june', ')', '.']

 TOTAL SNOWBALL STEM WORDS ==> 21



 ---- LEMMATIZATION ----

['[', '36', ']', 'Benson', ',', 'E.', ',', 'Haghighi', ',', 'A.', ',', '&', 'Barzilay', ',', 'R.', '(', '2011', ',', 'June', ')', '.']

 TOTAL LEMMATIZE WORDS ==> 21

************************************************************************************************************************

497 --> Event discovery in social media  feeds. 


 ---- TOKENS ----

 ['Event', 'discovery', 'in', 'social', 'media', 'feeds', '.'] 

 TOTAL TOKENS ==> 7

 ---- POST ----

 [('Event', 'JJ'), ('discovery', 'NN'), ('in', 'IN'), ('social', 'JJ'), ('media', 'NNS'), ('feeds', 'NNS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Event', 'discovery', 'social', 'media', 'feeds', '.']

 TOTAL FILTERED TOKENS ==>  6

 ---- POST FOR FILTERED TOKENS ----

 [('Event', 'JJ'), ('discovery', 'NN'), ('social', 'JJ'), ('media', 'NNS'), ('feeds', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Event discovery', 'discovery social', 'social media', 'media feeds', 'feeds .'] 

 TOTAL BIGRAMS --> 5 



 ---- TRI-GRAMS ---- 

 ['Event discovery social', 'discovery social media', 'social media feeds', 'media feeds .'] 

 TOTAL TRIGRAMS --> 4 



 ---- NOUN PHRASES ---- 

 ['Event discovery'] 

 TOTAL NOUN PHRASES --> 1 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['event', 'discoveri', 'social', 'media', 'feed', '.']

 TOTAL PORTER STEM WORDS ==> 6



 ---- SNOWBALL STEMMING ----

['event', 'discoveri', 'social', 'media', 'feed', '.']

 TOTAL SNOWBALL STEM WORDS ==> 6



 ---- LEMMATIZATION ----

['Event', 'discovery', 'social', 'medium', 'feed', '.']

 TOTAL LEMMATIZE WORDS ==> 6

************************************************************************************************************************

498 --> In Proceedings of the 49th Annual Meeting of the Association for Computational  Linguistics: Human Language Technologies-Volume 1 (pp. 


 ---- TOKENS ----

 ['In', 'Proceedings', 'of', 'the', '49th', 'Annual', 'Meeting', 'of', 'the', 'Association', 'for', 'Computational', 'Linguistics', ':', 'Human', 'Language', 'Technologies-Volume', '1', '(', 'pp', '.'] 

 TOTAL TOKENS ==> 21

 ---- POST ----

 [('In', 'IN'), ('Proceedings', 'NNP'), ('of', 'IN'), ('the', 'DT'), ('49th', 'CD'), ('Annual', 'JJ'), ('Meeting', 'NN'), ('of', 'IN'), ('the', 'DT'), ('Association', 'NNP'), ('for', 'IN'), ('Computational', 'NNP'), ('Linguistics', 'NNPS'), (':', ':'), ('Human', 'JJ'), ('Language', 'NNP'), ('Technologies-Volume', 'NNP'), ('1', 'CD'), ('(', '('), ('pp', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Proceedings', '49th', 'Annual', 'Meeting', 'Association', 'Computational', 'Linguistics', ':', 'Human', 'Language', 'Technologies-Volume', '1', '(', 'pp', '.']

 TOTAL FILTERED TOKENS ==>  15

 ---- POST FOR FILTERED TOKENS ----

 [('Proceedings', 'NNS'), ('49th', 'CD'), ('Annual', 'NNP'), ('Meeting', 'NNP'), ('Association', 'NNP'), ('Computational', 'NNP'), ('Linguistics', 'NNPS'), (':', ':'), ('Human', 'JJ'), ('Language', 'NNP'), ('Technologies-Volume', 'NNP'), ('1', 'CD'), ('(', '('), ('pp', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Proceedings 49th', '49th Annual', 'Annual Meeting', 'Meeting Association', 'Association Computational', 'Computational Linguistics', 'Linguistics :', ': Human', 'Human Language', 'Language Technologies-Volume', 'Technologies-Volume 1', '1 (', '( pp', 'pp .'] 

 TOTAL BIGRAMS --> 14 



 ---- TRI-GRAMS ---- 

 ['Proceedings 49th Annual', '49th Annual Meeting', 'Annual Meeting Association', 'Meeting Association Computational', 'Association Computational Linguistics', 'Computational Linguistics :', 'Linguistics : Human', ': Human Language', 'Human Language Technologies-Volume', 'Language Technologies-Volume 1', 'Technologies-Volume 1 (', '1 ( pp', '( pp .'] 

 TOTAL TRIGRAMS --> 13 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 []

 ---- PORTER STEMMING ----

['proceed', '49th', 'annual', 'meet', 'associ', 'comput', 'linguist', ':', 'human', 'languag', 'technologies-volum', '1', '(', 'pp', '.']

 TOTAL PORTER STEM WORDS ==> 15



 ---- SNOWBALL STEMMING ----

['proceed', '49th', 'annual', 'meet', 'associ', 'comput', 'linguist', ':', 'human', 'languag', 'technologies-volum', '1', '(', 'pp', '.']

 TOTAL SNOWBALL STEM WORDS ==> 15



 ---- LEMMATIZATION ----

['Proceedings', '49th', 'Annual', 'Meeting', 'Association', 'Computational', 'Linguistics', ':', 'Human', 'Language', 'Technologies-Volume', '1', '(', 'pp', '.']

 TOTAL LEMMATIZE WORDS ==> 15

************************************************************************************************************************

499 --> 389-398). 


 ---- TOKENS ----

 ['389-398', ')', '.'] 

 TOTAL TOKENS ==> 3

 ---- POST ----

 [('389-398', 'JJ'), (')', ')'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['389-398', ')', '.']

 TOTAL FILTERED TOKENS ==>  3

 ---- POST FOR FILTERED TOKENS ----

 [('389-398', 'JJ'), (')', ')'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['389-398 )', ') .'] 

 TOTAL BIGRAMS --> 2 



 ---- TRI-GRAMS ---- 

 ['389-398 ) .'] 

 TOTAL TRIGRAMS --> 1 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 []

 ---- PORTER STEMMING ----

['389-398', ')', '.']

 TOTAL PORTER STEM WORDS ==> 3



 ---- SNOWBALL STEMMING ----

['389-398', ')', '.']

 TOTAL SNOWBALL STEM WORDS ==> 3



 ---- LEMMATIZATION ----

['389-398', ')', '.']

 TOTAL LEMMATIZE WORDS ==> 3

************************************************************************************************************************

500 --> Association for  Computational Linguistics. 


 ---- TOKENS ----

 ['Association', 'for', 'Computational', 'Linguistics', '.'] 

 TOTAL TOKENS ==> 5

 ---- POST ----

 [('Association', 'NNP'), ('for', 'IN'), ('Computational', 'NNP'), ('Linguistics', 'NNP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Association', 'Computational', 'Linguistics', '.']

 TOTAL FILTERED TOKENS ==>  4

 ---- POST FOR FILTERED TOKENS ----

 [('Association', 'NNP'), ('Computational', 'NNP'), ('Linguistics', 'NNP'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Association Computational', 'Computational Linguistics', 'Linguistics .'] 

 TOTAL BIGRAMS --> 3 



 ---- TRI-GRAMS ---- 

 ['Association Computational Linguistics', 'Computational Linguistics .'] 

 TOTAL TRIGRAMS --> 2 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['associ', 'comput', 'linguist', '.']

 TOTAL PORTER STEM WORDS ==> 4



 ---- SNOWBALL STEMMING ----

['associ', 'comput', 'linguist', '.']

 TOTAL SNOWBALL STEM WORDS ==> 4



 ---- LEMMATIZATION ----

['Association', 'Computational', 'Linguistics', '.']

 TOTAL LEMMATIZE WORDS ==> 4

************************************************************************************************************************

501 --> [37] Tillmann, C., Vogel, S., Ney, H., Zubiaga, A., & Sawaf, H. (1997, September). 


 ---- TOKENS ----

 ['[', '37', ']', 'Tillmann', ',', 'C.', ',', 'Vogel', ',', 'S.', ',', 'Ney', ',', 'H.', ',', 'Zubiaga', ',', 'A.', ',', '&', 'Sawaf', ',', 'H.', '(', '1997', ',', 'September', ')', '.'] 

 TOTAL TOKENS ==> 29

 ---- POST ----

 [('[', 'RB'), ('37', 'CD'), (']', 'JJ'), ('Tillmann', 'NNP'), (',', ','), ('C.', 'NNP'), (',', ','), ('Vogel', 'NNP'), (',', ','), ('S.', 'NNP'), (',', ','), ('Ney', 'NNP'), (',', ','), ('H.', 'NNP'), (',', ','), ('Zubiaga', 'NNP'), (',', ','), ('A.', 'NNP'), (',', ','), ('&', 'CC'), ('Sawaf', 'NNP'), (',', ','), ('H.', 'NNP'), ('(', '('), ('1997', 'CD'), (',', ','), ('September', 'NNP'), (')', ')'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['[', '37', ']', 'Tillmann', ',', 'C.', ',', 'Vogel', ',', 'S.', ',', 'Ney', ',', 'H.', ',', 'Zubiaga', ',', 'A.', ',', '&', 'Sawaf', ',', 'H.', '(', '1997', ',', 'September', ')', '.']

 TOTAL FILTERED TOKENS ==>  29

 ---- POST FOR FILTERED TOKENS ----

 [('[', 'RB'), ('37', 'CD'), (']', 'JJ'), ('Tillmann', 'NNP'), (',', ','), ('C.', 'NNP'), (',', ','), ('Vogel', 'NNP'), (',', ','), ('S.', 'NNP'), (',', ','), ('Ney', 'NNP'), (',', ','), ('H.', 'NNP'), (',', ','), ('Zubiaga', 'NNP'), (',', ','), ('A.', 'NNP'), (',', ','), ('&', 'CC'), ('Sawaf', 'NNP'), (',', ','), ('H.', 'NNP'), ('(', '('), ('1997', 'CD'), (',', ','), ('September', 'NNP'), (')', ')'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['[ 37', '37 ]', '] Tillmann', 'Tillmann ,', ', C.', 'C. ,', ', Vogel', 'Vogel ,', ', S.', 'S. ,', ', Ney', 'Ney ,', ', H.', 'H. ,', ', Zubiaga', 'Zubiaga ,', ', A.', 'A. ,', ', &', '& Sawaf', 'Sawaf ,', ', H.', 'H. (', '( 1997', '1997 ,', ', September', 'September )', ') .'] 

 TOTAL BIGRAMS --> 28 



 ---- TRI-GRAMS ---- 

 ['[ 37 ]', '37 ] Tillmann', '] Tillmann ,', 'Tillmann , C.', ', C. ,', 'C. , Vogel', ', Vogel ,', 'Vogel , S.', ', S. ,', 'S. , Ney', ', Ney ,', 'Ney , H.', ', H. ,', 'H. , Zubiaga', ', Zubiaga ,', 'Zubiaga , A.', ', A. ,', 'A. , &', ', & Sawaf', '& Sawaf ,', 'Sawaf , H.', ', H. (', 'H. ( 1997', '( 1997 ,', '1997 , September', ', September )', 'September ) .'] 

 TOTAL TRIGRAMS --> 27 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> ['Vogel', 'Ney', 'Zubiaga', 'Sawaf']
 TOTAL PERSON ENTITY --> 4 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['[', '37', ']', 'tillmann', ',', 'c.', ',', 'vogel', ',', 's.', ',', 'ney', ',', 'h.', ',', 'zubiaga', ',', 'a.', ',', '&', 'sawaf', ',', 'h.', '(', '1997', ',', 'septemb', ')', '.']

 TOTAL PORTER STEM WORDS ==> 29



 ---- SNOWBALL STEMMING ----

['[', '37', ']', 'tillmann', ',', 'c.', ',', 'vogel', ',', 's.', ',', 'ney', ',', 'h.', ',', 'zubiaga', ',', 'a.', ',', '&', 'sawaf', ',', 'h.', '(', '1997', ',', 'septemb', ')', '.']

 TOTAL SNOWBALL STEM WORDS ==> 29



 ---- LEMMATIZATION ----

['[', '37', ']', 'Tillmann', ',', 'C.', ',', 'Vogel', ',', 'S.', ',', 'Ney', ',', 'H.', ',', 'Zubiaga', ',', 'A.', ',', '&', 'Sawaf', ',', 'H.', '(', '1997', ',', 'September', ')', '.']

 TOTAL LEMMATIZE WORDS ==> 29

************************************************************************************************************************

502 --> Accelerated DP based search for statistical translation. 


 ---- TOKENS ----

 ['Accelerated', 'DP', 'based', 'search', 'for', 'statistical', 'translation', '.'] 

 TOTAL TOKENS ==> 8

 ---- POST ----

 [('Accelerated', 'NNP'), ('DP', 'NNP'), ('based', 'VBN'), ('search', 'NN'), ('for', 'IN'), ('statistical', 'JJ'), ('translation', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Accelerated', 'DP', 'based', 'search', 'statistical', 'translation', '.']

 TOTAL FILTERED TOKENS ==>  7

 ---- POST FOR FILTERED TOKENS ----

 [('Accelerated', 'NNP'), ('DP', 'NNP'), ('based', 'VBN'), ('search', 'NN'), ('statistical', 'JJ'), ('translation', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Accelerated DP', 'DP based', 'based search', 'search statistical', 'statistical translation', 'translation .'] 

 TOTAL BIGRAMS --> 6 



 ---- TRI-GRAMS ---- 

 ['Accelerated DP based', 'DP based search', 'based search statistical', 'search statistical translation', 'statistical translation .'] 

 TOTAL TRIGRAMS --> 5 



 ---- NOUN PHRASES ---- 

 ['search', 'statistical translation'] 

 TOTAL NOUN PHRASES --> 2 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> ['Accelerated']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['acceler', 'dp', 'base', 'search', 'statist', 'translat', '.']

 TOTAL PORTER STEM WORDS ==> 7



 ---- SNOWBALL STEMMING ----

['acceler', 'dp', 'base', 'search', 'statist', 'translat', '.']

 TOTAL SNOWBALL STEM WORDS ==> 7



 ---- LEMMATIZATION ----

['Accelerated', 'DP', 'based', 'search', 'statistical', 'translation', '.']

 TOTAL LEMMATIZE WORDS ==> 7

************************************************************************************************************************

503 --> In Eurospeech. 


 ---- TOKENS ----

 ['In', 'Eurospeech', '.'] 

 TOTAL TOKENS ==> 3

 ---- POST ----

 [('In', 'IN'), ('Eurospeech', 'NNP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Eurospeech', '.']

 TOTAL FILTERED TOKENS ==>  2

 ---- POST FOR FILTERED TOKENS ----

 [('Eurospeech', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Eurospeech .'] 

 TOTAL BIGRAMS --> 1 



 ---- TRI-GRAMS ---- 

 [] 

 TOTAL TRIGRAMS --> 0 



 ---- NOUN PHRASES ---- 

 ['Eurospeech'] 

 TOTAL NOUN PHRASES --> 1 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> ['Eurospeech']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['eurospeech', '.']

 TOTAL PORTER STEM WORDS ==> 2



 ---- SNOWBALL STEMMING ----

['eurospeech', '.']

 TOTAL SNOWBALL STEM WORDS ==> 2



 ---- LEMMATIZATION ----

['Eurospeech', '.']

 TOTAL LEMMATIZE WORDS ==> 2

************************************************************************************************************************

504 --> [38] Bangalore, S., Rambow, O., & Whittaker, S. (2000, June). 


 ---- TOKENS ----

 ['[', '38', ']', 'Bangalore', ',', 'S.', ',', 'Rambow', ',', 'O.', ',', '&', 'Whittaker', ',', 'S.', '(', '2000', ',', 'June', ')', '.'] 

 TOTAL TOKENS ==> 21

 ---- POST ----

 [('[', 'RB'), ('38', 'CD'), (']', 'NNS'), ('Bangalore', 'NNP'), (',', ','), ('S.', 'NNP'), (',', ','), ('Rambow', 'NNP'), (',', ','), ('O.', 'NNP'), (',', ','), ('&', 'CC'), ('Whittaker', 'NNP'), (',', ','), ('S.', 'NNP'), ('(', '('), ('2000', 'CD'), (',', ','), ('June', 'NNP'), (')', ')'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['[', '38', ']', 'Bangalore', ',', 'S.', ',', 'Rambow', ',', 'O.', ',', '&', 'Whittaker', ',', 'S.', '(', '2000', ',', 'June', ')', '.']

 TOTAL FILTERED TOKENS ==>  21

 ---- POST FOR FILTERED TOKENS ----

 [('[', 'RB'), ('38', 'CD'), (']', 'NNS'), ('Bangalore', 'NNP'), (',', ','), ('S.', 'NNP'), (',', ','), ('Rambow', 'NNP'), (',', ','), ('O.', 'NNP'), (',', ','), ('&', 'CC'), ('Whittaker', 'NNP'), (',', ','), ('S.', 'NNP'), ('(', '('), ('2000', 'CD'), (',', ','), ('June', 'NNP'), (')', ')'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['[ 38', '38 ]', '] Bangalore', 'Bangalore ,', ', S.', 'S. ,', ', Rambow', 'Rambow ,', ', O.', 'O. ,', ', &', '& Whittaker', 'Whittaker ,', ', S.', 'S. (', '( 2000', '2000 ,', ', June', 'June )', ') .'] 

 TOTAL BIGRAMS --> 20 



 ---- TRI-GRAMS ---- 

 ['[ 38 ]', '38 ] Bangalore', '] Bangalore ,', 'Bangalore , S.', ', S. ,', 'S. , Rambow', ', Rambow ,', 'Rambow , O.', ', O. ,', 'O. , &', ', & Whittaker', '& Whittaker ,', 'Whittaker , S.', ', S. (', 'S. ( 2000', '( 2000 ,', '2000 , June', ', June )', 'June ) .'] 

 TOTAL TRIGRAMS --> 19 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> ['Whittaker']
 TOTAL PERSON ENTITY --> 1 


 GPE ---> ['Bangalore', 'Rambow']
 TOTAL GPE ENTITY --> 2 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['[', '38', ']', 'bangalor', ',', 's.', ',', 'rambow', ',', 'o.', ',', '&', 'whittak', ',', 's.', '(', '2000', ',', 'june', ')', '.']

 TOTAL PORTER STEM WORDS ==> 21



 ---- SNOWBALL STEMMING ----

['[', '38', ']', 'bangalor', ',', 's.', ',', 'rambow', ',', 'o.', ',', '&', 'whittak', ',', 's.', '(', '2000', ',', 'june', ')', '.']

 TOTAL SNOWBALL STEM WORDS ==> 21



 ---- LEMMATIZATION ----

['[', '38', ']', 'Bangalore', ',', 'S.', ',', 'Rambow', ',', 'O.', ',', '&', 'Whittaker', ',', 'S.', '(', '2000', ',', 'June', ')', '.']

 TOTAL LEMMATIZE WORDS ==> 21

************************************************************************************************************************

505 --> Evaluation metrics for  generation. 


 ---- TOKENS ----

 ['Evaluation', 'metrics', 'for', 'generation', '.'] 

 TOTAL TOKENS ==> 5

 ---- POST ----

 [('Evaluation', 'NN'), ('metrics', 'NNS'), ('for', 'IN'), ('generation', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Evaluation', 'metrics', 'generation', '.']

 TOTAL FILTERED TOKENS ==>  4

 ---- POST FOR FILTERED TOKENS ----

 [('Evaluation', 'NN'), ('metrics', 'NNS'), ('generation', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Evaluation metrics', 'metrics generation', 'generation .'] 

 TOTAL BIGRAMS --> 3 



 ---- TRI-GRAMS ---- 

 ['Evaluation metrics generation', 'metrics generation .'] 

 TOTAL TRIGRAMS --> 2 



 ---- NOUN PHRASES ---- 

 ['Evaluation', 'generation'] 

 TOTAL NOUN PHRASES --> 2 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> ['Evaluation']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['evalu', 'metric', 'gener', '.']

 TOTAL PORTER STEM WORDS ==> 4



 ---- SNOWBALL STEMMING ----

['evalu', 'metric', 'generat', '.']

 TOTAL SNOWBALL STEM WORDS ==> 4



 ---- LEMMATIZATION ----

['Evaluation', 'metric', 'generation', '.']

 TOTAL LEMMATIZE WORDS ==> 4

************************************************************************************************************************

506 --> In Proceedings of the first international conference on Natural language  generation-Volume 14 (pp. 


 ---- TOKENS ----

 ['In', 'Proceedings', 'of', 'the', 'first', 'international', 'conference', 'on', 'Natural', 'language', 'generation-Volume', '14', '(', 'pp', '.'] 

 TOTAL TOKENS ==> 15

 ---- POST ----

 [('In', 'IN'), ('Proceedings', 'NNP'), ('of', 'IN'), ('the', 'DT'), ('first', 'JJ'), ('international', 'JJ'), ('conference', 'NN'), ('on', 'IN'), ('Natural', 'JJ'), ('language', 'NN'), ('generation-Volume', 'JJ'), ('14', 'CD'), ('(', '('), ('pp', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Proceedings', 'first', 'international', 'conference', 'Natural', 'language', 'generation-Volume', '14', '(', 'pp', '.']

 TOTAL FILTERED TOKENS ==>  11

 ---- POST FOR FILTERED TOKENS ----

 [('Proceedings', 'NNS'), ('first', 'JJ'), ('international', 'JJ'), ('conference', 'NN'), ('Natural', 'NNP'), ('language', 'NN'), ('generation-Volume', 'JJ'), ('14', 'CD'), ('(', '('), ('pp', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Proceedings first', 'first international', 'international conference', 'conference Natural', 'Natural language', 'language generation-Volume', 'generation-Volume 14', '14 (', '( pp', 'pp .'] 

 TOTAL BIGRAMS --> 10 



 ---- TRI-GRAMS ---- 

 ['Proceedings first international', 'first international conference', 'international conference Natural', 'conference Natural language', 'Natural language generation-Volume', 'language generation-Volume 14', 'generation-Volume 14 (', '14 ( pp', '( pp .'] 

 TOTAL TRIGRAMS --> 9 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 []

 ---- PORTER STEMMING ----

['proceed', 'first', 'intern', 'confer', 'natur', 'languag', 'generation-volum', '14', '(', 'pp', '.']

 TOTAL PORTER STEM WORDS ==> 11



 ---- SNOWBALL STEMMING ----

['proceed', 'first', 'intern', 'confer', 'natur', 'languag', 'generation-volum', '14', '(', 'pp', '.']

 TOTAL SNOWBALL STEM WORDS ==> 11



 ---- LEMMATIZATION ----

['Proceedings', 'first', 'international', 'conference', 'Natural', 'language', 'generation-Volume', '14', '(', 'pp', '.']

 TOTAL LEMMATIZE WORDS ==> 11

************************************************************************************************************************

507 --> 1-8). 


 ---- TOKENS ----

 ['1-8', ')', '.'] 

 TOTAL TOKENS ==> 3

 ---- POST ----

 [('1-8', 'JJ'), (')', ')'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['1-8', ')', '.']

 TOTAL FILTERED TOKENS ==>  3

 ---- POST FOR FILTERED TOKENS ----

 [('1-8', 'JJ'), (')', ')'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['1-8 )', ') .'] 

 TOTAL BIGRAMS --> 2 



 ---- TRI-GRAMS ---- 

 ['1-8 ) .'] 

 TOTAL TRIGRAMS --> 1 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 []

 ---- PORTER STEMMING ----

['1-8', ')', '.']

 TOTAL PORTER STEM WORDS ==> 3



 ---- SNOWBALL STEMMING ----

['1-8', ')', '.']

 TOTAL SNOWBALL STEM WORDS ==> 3



 ---- LEMMATIZATION ----

['1-8', ')', '.']

 TOTAL LEMMATIZE WORDS ==> 3

************************************************************************************************************************

508 --> Association for Computational Linguistics  [39] Nießen, S., Och, F. J., Leusch, G., & Ney, H. (2000, May). 


 ---- TOKENS ----

 ['Association', 'for', 'Computational', 'Linguistics', '[', '39', ']', 'Nießen', ',', 'S.', ',', 'Och', ',', 'F.', 'J.', ',', 'Leusch', ',', 'G.', ',', '&', 'Ney', ',', 'H.', '(', '2000', ',', 'May', ')', '.'] 

 TOTAL TOKENS ==> 30

 ---- POST ----

 [('Association', 'NNP'), ('for', 'IN'), ('Computational', 'NNP'), ('Linguistics', 'NNP'), ('[', 'VBD'), ('39', 'CD'), (']', 'NNP'), ('Nießen', 'NNP'), (',', ','), ('S.', 'NNP'), (',', ','), ('Och', 'NNP'), (',', ','), ('F.', 'NNP'), ('J.', 'NNP'), (',', ','), ('Leusch', 'NNP'), (',', ','), ('G.', 'NNP'), (',', ','), ('&', 'CC'), ('Ney', 'NNP'), (',', ','), ('H.', 'NNP'), ('(', '('), ('2000', 'CD'), (',', ','), ('May', 'NNP'), (')', ')'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Association', 'Computational', 'Linguistics', '[', '39', ']', 'Nießen', ',', 'S.', ',', 'Och', ',', 'F.', 'J.', ',', 'Leusch', ',', 'G.', ',', '&', 'Ney', ',', 'H.', '(', '2000', ',', 'May', ')', '.']

 TOTAL FILTERED TOKENS ==>  29

 ---- POST FOR FILTERED TOKENS ----

 [('Association', 'NNP'), ('Computational', 'NNP'), ('Linguistics', 'NNP'), ('[', 'VBD'), ('39', 'CD'), (']', 'NNP'), ('Nießen', 'NNP'), (',', ','), ('S.', 'NNP'), (',', ','), ('Och', 'NNP'), (',', ','), ('F.', 'NNP'), ('J.', 'NNP'), (',', ','), ('Leusch', 'NNP'), (',', ','), ('G.', 'NNP'), (',', ','), ('&', 'CC'), ('Ney', 'NNP'), (',', ','), ('H.', 'NNP'), ('(', '('), ('2000', 'CD'), (',', ','), ('May', 'NNP'), (')', ')'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Association Computational', 'Computational Linguistics', 'Linguistics [', '[ 39', '39 ]', '] Nießen', 'Nießen ,', ', S.', 'S. ,', ', Och', 'Och ,', ', F.', 'F. J.', 'J. ,', ', Leusch', 'Leusch ,', ', G.', 'G. ,', ', &', '& Ney', 'Ney ,', ', H.', 'H. (', '( 2000', '2000 ,', ', May', 'May )', ') .'] 

 TOTAL BIGRAMS --> 28 



 ---- TRI-GRAMS ---- 

 ['Association Computational Linguistics', 'Computational Linguistics [', 'Linguistics [ 39', '[ 39 ]', '39 ] Nießen', '] Nießen ,', 'Nießen , S.', ', S. ,', 'S. , Och', ', Och ,', 'Och , F.', ', F. J.', 'F. J. ,', 'J. , Leusch', ', Leusch ,', 'Leusch , G.', ', G. ,', 'G. , &', ', & Ney', '& Ney ,', 'Ney , H.', ', H. (', 'H. ( 2000', '( 2000 ,', '2000 , May', ', May )', 'May ) .'] 

 TOTAL TRIGRAMS --> 27 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> ['Och', 'Ney']
 TOTAL PERSON ENTITY --> 2 


 GPE ---> ['Leusch']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['associ', 'comput', 'linguist', '[', '39', ']', 'nießen', ',', 's.', ',', 'och', ',', 'f.', 'j.', ',', 'leusch', ',', 'g.', ',', '&', 'ney', ',', 'h.', '(', '2000', ',', 'may', ')', '.']

 TOTAL PORTER STEM WORDS ==> 29



 ---- SNOWBALL STEMMING ----

['associ', 'comput', 'linguist', '[', '39', ']', 'nießen', ',', 's.', ',', 'och', ',', 'f.', 'j.', ',', 'leusch', ',', 'g.', ',', '&', 'ney', ',', 'h.', '(', '2000', ',', 'may', ')', '.']

 TOTAL SNOWBALL STEM WORDS ==> 29



 ---- LEMMATIZATION ----

['Association', 'Computational', 'Linguistics', '[', '39', ']', 'Nießen', ',', 'S.', ',', 'Och', ',', 'F.', 'J.', ',', 'Leusch', ',', 'G.', ',', '&', 'Ney', ',', 'H.', '(', '2000', ',', 'May', ')', '.']

 TOTAL LEMMATIZE WORDS ==> 29

************************************************************************************************************************

509 --> An Evaluation Tool for  Machine Translation: Fast Evaluation for MT Research. 


 ---- TOKENS ----

 ['An', 'Evaluation', 'Tool', 'for', 'Machine', 'Translation', ':', 'Fast', 'Evaluation', 'for', 'MT', 'Research', '.'] 

 TOTAL TOKENS ==> 13

 ---- POST ----

 [('An', 'DT'), ('Evaluation', 'NN'), ('Tool', 'NN'), ('for', 'IN'), ('Machine', 'NNP'), ('Translation', 'NNP'), (':', ':'), ('Fast', 'NNP'), ('Evaluation', 'NNP'), ('for', 'IN'), ('MT', 'NNP'), ('Research', 'NNP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Evaluation', 'Tool', 'Machine', 'Translation', ':', 'Fast', 'Evaluation', 'MT', 'Research', '.']

 TOTAL FILTERED TOKENS ==>  10

 ---- POST FOR FILTERED TOKENS ----

 [('Evaluation', 'NN'), ('Tool', 'NNP'), ('Machine', 'NNP'), ('Translation', 'NNP'), (':', ':'), ('Fast', 'NNP'), ('Evaluation', 'NNP'), ('MT', 'NNP'), ('Research', 'NNP'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Evaluation Tool', 'Tool Machine', 'Machine Translation', 'Translation :', ': Fast', 'Fast Evaluation', 'Evaluation MT', 'MT Research', 'Research .'] 

 TOTAL BIGRAMS --> 9 



 ---- TRI-GRAMS ---- 

 ['Evaluation Tool Machine', 'Tool Machine Translation', 'Machine Translation :', 'Translation : Fast', ': Fast Evaluation', 'Fast Evaluation MT', 'Evaluation MT Research', 'MT Research .'] 

 TOTAL TRIGRAMS --> 8 



 ---- NOUN PHRASES ---- 

 ['Evaluation'] 

 TOTAL NOUN PHRASES --> 1 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> ['Evaluation', 'Tool Machine', 'Fast Evaluation MT Research']
 TOTAL PERSON ENTITY --> 3 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['evalu', 'tool', 'machin', 'translat', ':', 'fast', 'evalu', 'mt', 'research', '.']

 TOTAL PORTER STEM WORDS ==> 10



 ---- SNOWBALL STEMMING ----

['evalu', 'tool', 'machin', 'translat', ':', 'fast', 'evalu', 'mt', 'research', '.']

 TOTAL SNOWBALL STEM WORDS ==> 10



 ---- LEMMATIZATION ----

['Evaluation', 'Tool', 'Machine', 'Translation', ':', 'Fast', 'Evaluation', 'MT', 'Research', '.']

 TOTAL LEMMATIZE WORDS ==> 10

************************************************************************************************************************

510 --> In LREC  [40] Papineni, K., Roukos, S., Ward, T., & Zhu, W. J. 


 ---- TOKENS ----

 ['In', 'LREC', '[', '40', ']', 'Papineni', ',', 'K.', ',', 'Roukos', ',', 'S.', ',', 'Ward', ',', 'T.', ',', '&', 'Zhu', ',', 'W.', 'J', '.'] 

 TOTAL TOKENS ==> 23

 ---- POST ----

 [('In', 'IN'), ('LREC', 'NNP'), ('[', 'NNP'), ('40', 'CD'), (']', 'NNP'), ('Papineni', 'NNP'), (',', ','), ('K.', 'NNP'), (',', ','), ('Roukos', 'NNP'), (',', ','), ('S.', 'NNP'), (',', ','), ('Ward', 'NNP'), (',', ','), ('T.', 'NNP'), (',', ','), ('&', 'CC'), ('Zhu', 'NNP'), (',', ','), ('W.', 'NNP'), ('J', 'NNP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['LREC', '[', '40', ']', 'Papineni', ',', 'K.', ',', 'Roukos', ',', 'S.', ',', 'Ward', ',', 'T.', ',', '&', 'Zhu', ',', 'W.', 'J', '.']

 TOTAL FILTERED TOKENS ==>  22

 ---- POST FOR FILTERED TOKENS ----

 [('LREC', 'NNP'), ('[', 'VBD'), ('40', 'CD'), (']', 'NNP'), ('Papineni', 'NNP'), (',', ','), ('K.', 'NNP'), (',', ','), ('Roukos', 'NNP'), (',', ','), ('S.', 'NNP'), (',', ','), ('Ward', 'NNP'), (',', ','), ('T.', 'NNP'), (',', ','), ('&', 'CC'), ('Zhu', 'NNP'), (',', ','), ('W.', 'NNP'), ('J', 'NNP'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['LREC [', '[ 40', '40 ]', '] Papineni', 'Papineni ,', ', K.', 'K. ,', ', Roukos', 'Roukos ,', ', S.', 'S. ,', ', Ward', 'Ward ,', ', T.', 'T. ,', ', &', '& Zhu', 'Zhu ,', ', W.', 'W. J', 'J .'] 

 TOTAL BIGRAMS --> 21 



 ---- TRI-GRAMS ---- 

 ['LREC [ 40', '[ 40 ]', '40 ] Papineni', '] Papineni ,', 'Papineni , K.', ', K. ,', 'K. , Roukos', ', Roukos ,', 'Roukos , S.', ', S. ,', 'S. , Ward', ', Ward ,', 'Ward , T.', ', T. ,', 'T. , &', ', & Zhu', '& Zhu ,', 'Zhu , W.', ', W. J', 'W. J .'] 

 TOTAL TRIGRAMS --> 20 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> ['LREC']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> ['Ward', 'Zhu']
 TOTAL PERSON ENTITY --> 2 


 GPE ---> ['Roukos']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['lrec', '[', '40', ']', 'papineni', ',', 'k.', ',', 'rouko', ',', 's.', ',', 'ward', ',', 't.', ',', '&', 'zhu', ',', 'w.', 'j', '.']

 TOTAL PORTER STEM WORDS ==> 22



 ---- SNOWBALL STEMMING ----

['lrec', '[', '40', ']', 'papineni', ',', 'k.', ',', 'rouko', ',', 's.', ',', 'ward', ',', 't.', ',', '&', 'zhu', ',', 'w.', 'j', '.']

 TOTAL SNOWBALL STEM WORDS ==> 22



 ---- LEMMATIZATION ----

['LREC', '[', '40', ']', 'Papineni', ',', 'K.', ',', 'Roukos', ',', 'S.', ',', 'Ward', ',', 'T.', ',', '&', 'Zhu', ',', 'W.', 'J', '.']

 TOTAL LEMMATIZE WORDS ==> 22

************************************************************************************************************************

511 --> (2002, July). 


 ---- TOKENS ----

 ['(', '2002', ',', 'July', ')', '.'] 

 TOTAL TOKENS ==> 6

 ---- POST ----

 [('(', '('), ('2002', 'CD'), (',', ','), ('July', 'NNP'), (')', ')'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['(', '2002', ',', 'July', ')', '.']

 TOTAL FILTERED TOKENS ==>  6

 ---- POST FOR FILTERED TOKENS ----

 [('(', '('), ('2002', 'CD'), (',', ','), ('July', 'NNP'), (')', ')'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['( 2002', '2002 ,', ', July', 'July )', ') .'] 

 TOTAL BIGRAMS --> 5 



 ---- TRI-GRAMS ---- 

 ['( 2002 ,', '2002 , July', ', July )', 'July ) .'] 

 TOTAL TRIGRAMS --> 4 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['(', '2002', ',', 'juli', ')', '.']

 TOTAL PORTER STEM WORDS ==> 6



 ---- SNOWBALL STEMMING ----

['(', '2002', ',', 'juli', ')', '.']

 TOTAL SNOWBALL STEM WORDS ==> 6



 ---- LEMMATIZATION ----

['(', '2002', ',', 'July', ')', '.']

 TOTAL LEMMATIZE WORDS ==> 6

************************************************************************************************************************

512 --> BLEU: a method for  automatic evaluation of machine translation. 


 ---- TOKENS ----

 ['BLEU', ':', 'a', 'method', 'for', 'automatic', 'evaluation', 'of', 'machine', 'translation', '.'] 

 TOTAL TOKENS ==> 11

 ---- POST ----

 [('BLEU', 'NN'), (':', ':'), ('a', 'DT'), ('method', 'NN'), ('for', 'IN'), ('automatic', 'JJ'), ('evaluation', 'NN'), ('of', 'IN'), ('machine', 'NN'), ('translation', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['BLEU', ':', 'method', 'automatic', 'evaluation', 'machine', 'translation', '.']

 TOTAL FILTERED TOKENS ==>  8

 ---- POST FOR FILTERED TOKENS ----

 [('BLEU', 'NN'), (':', ':'), ('method', 'NN'), ('automatic', 'JJ'), ('evaluation', 'NN'), ('machine', 'NN'), ('translation', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['BLEU :', ': method', 'method automatic', 'automatic evaluation', 'evaluation machine', 'machine translation', 'translation .'] 

 TOTAL BIGRAMS --> 7 



 ---- TRI-GRAMS ---- 

 ['BLEU : method', ': method automatic', 'method automatic evaluation', 'automatic evaluation machine', 'evaluation machine translation', 'machine translation .'] 

 TOTAL TRIGRAMS --> 6 



 ---- NOUN PHRASES ---- 

 ['BLEU', 'method', 'automatic evaluation', 'machine', 'translation'] 

 TOTAL NOUN PHRASES --> 5 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['bleu', ':', 'method', 'automat', 'evalu', 'machin', 'translat', '.']

 TOTAL PORTER STEM WORDS ==> 8



 ---- SNOWBALL STEMMING ----

['bleu', ':', 'method', 'automat', 'evalu', 'machin', 'translat', '.']

 TOTAL SNOWBALL STEM WORDS ==> 8



 ---- LEMMATIZATION ----

['BLEU', ':', 'method', 'automatic', 'evaluation', 'machine', 'translation', '.']

 TOTAL LEMMATIZE WORDS ==> 8

************************************************************************************************************************

513 --> In Proceedings of the 40th annual meeting on  association for computational linguistics (pp. 


 ---- TOKENS ----

 ['In', 'Proceedings', 'of', 'the', '40th', 'annual', 'meeting', 'on', 'association', 'for', 'computational', 'linguistics', '(', 'pp', '.'] 

 TOTAL TOKENS ==> 15

 ---- POST ----

 [('In', 'IN'), ('Proceedings', 'NNP'), ('of', 'IN'), ('the', 'DT'), ('40th', 'JJ'), ('annual', 'JJ'), ('meeting', 'NN'), ('on', 'IN'), ('association', 'NN'), ('for', 'IN'), ('computational', 'JJ'), ('linguistics', 'NNS'), ('(', '('), ('pp', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Proceedings', '40th', 'annual', 'meeting', 'association', 'computational', 'linguistics', '(', 'pp', '.']

 TOTAL FILTERED TOKENS ==>  10

 ---- POST FOR FILTERED TOKENS ----

 [('Proceedings', 'NNS'), ('40th', 'CD'), ('annual', 'JJ'), ('meeting', 'NN'), ('association', 'NN'), ('computational', 'NN'), ('linguistics', 'NNS'), ('(', '('), ('pp', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Proceedings 40th', '40th annual', 'annual meeting', 'meeting association', 'association computational', 'computational linguistics', 'linguistics (', '( pp', 'pp .'] 

 TOTAL BIGRAMS --> 9 



 ---- TRI-GRAMS ---- 

 ['Proceedings 40th annual', '40th annual meeting', 'annual meeting association', 'meeting association computational', 'association computational linguistics', 'computational linguistics (', 'linguistics ( pp', '( pp .'] 

 TOTAL TRIGRAMS --> 8 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 []

 ---- PORTER STEMMING ----

['proceed', '40th', 'annual', 'meet', 'associ', 'comput', 'linguist', '(', 'pp', '.']

 TOTAL PORTER STEM WORDS ==> 10



 ---- SNOWBALL STEMMING ----

['proceed', '40th', 'annual', 'meet', 'associ', 'comput', 'linguist', '(', 'pp', '.']

 TOTAL SNOWBALL STEM WORDS ==> 10



 ---- LEMMATIZATION ----

['Proceedings', '40th', 'annual', 'meeting', 'association', 'computational', 'linguistics', '(', 'pp', '.']

 TOTAL LEMMATIZE WORDS ==> 10

************************************************************************************************************************

514 --> 311-318). 


 ---- TOKENS ----

 ['311-318', ')', '.'] 

 TOTAL TOKENS ==> 3

 ---- POST ----

 [('311-318', 'JJ'), (')', ')'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['311-318', ')', '.']

 TOTAL FILTERED TOKENS ==>  3

 ---- POST FOR FILTERED TOKENS ----

 [('311-318', 'JJ'), (')', ')'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['311-318 )', ') .'] 

 TOTAL BIGRAMS --> 2 



 ---- TRI-GRAMS ---- 

 ['311-318 ) .'] 

 TOTAL TRIGRAMS --> 1 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 []

 ---- PORTER STEMMING ----

['311-318', ')', '.']

 TOTAL PORTER STEM WORDS ==> 3



 ---- SNOWBALL STEMMING ----

['311-318', ')', '.']

 TOTAL SNOWBALL STEM WORDS ==> 3



 ---- LEMMATIZATION ----

['311-318', ')', '.']

 TOTAL LEMMATIZE WORDS ==> 3

************************************************************************************************************************

515 --> Association for Computational  Linguistics  [41] Doddington, G. (2002, March). 


 ---- TOKENS ----

 ['Association', 'for', 'Computational', 'Linguistics', '[', '41', ']', 'Doddington', ',', 'G.', '(', '2002', ',', 'March', ')', '.'] 

 TOTAL TOKENS ==> 16

 ---- POST ----

 [('Association', 'NNP'), ('for', 'IN'), ('Computational', 'NNP'), ('Linguistics', 'NNP'), ('[', 'VBD'), ('41', 'CD'), (']', 'NNP'), ('Doddington', 'NNP'), (',', ','), ('G.', 'NNP'), ('(', '('), ('2002', 'CD'), (',', ','), ('March', 'NNP'), (')', ')'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Association', 'Computational', 'Linguistics', '[', '41', ']', 'Doddington', ',', 'G.', '(', '2002', ',', 'March', ')', '.']

 TOTAL FILTERED TOKENS ==>  15

 ---- POST FOR FILTERED TOKENS ----

 [('Association', 'NNP'), ('Computational', 'NNP'), ('Linguistics', 'NNP'), ('[', 'VBD'), ('41', 'CD'), (']', 'NNP'), ('Doddington', 'NNP'), (',', ','), ('G.', 'NNP'), ('(', '('), ('2002', 'CD'), (',', ','), ('March', 'NNP'), (')', ')'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Association Computational', 'Computational Linguistics', 'Linguistics [', '[ 41', '41 ]', '] Doddington', 'Doddington ,', ', G.', 'G. (', '( 2002', '2002 ,', ', March', 'March )', ') .'] 

 TOTAL BIGRAMS --> 14 



 ---- TRI-GRAMS ---- 

 ['Association Computational Linguistics', 'Computational Linguistics [', 'Linguistics [ 41', '[ 41 ]', '41 ] Doddington', '] Doddington ,', 'Doddington , G.', ', G. (', 'G. ( 2002', '( 2002 ,', '2002 , March', ', March )', 'March ) .'] 

 TOTAL TRIGRAMS --> 13 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> ['Doddington']
 TOTAL PERSON ENTITY --> 1 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['associ', 'comput', 'linguist', '[', '41', ']', 'doddington', ',', 'g.', '(', '2002', ',', 'march', ')', '.']

 TOTAL PORTER STEM WORDS ==> 15



 ---- SNOWBALL STEMMING ----

['associ', 'comput', 'linguist', '[', '41', ']', 'doddington', ',', 'g.', '(', '2002', ',', 'march', ')', '.']

 TOTAL SNOWBALL STEM WORDS ==> 15



 ---- LEMMATIZATION ----

['Association', 'Computational', 'Linguistics', '[', '41', ']', 'Doddington', ',', 'G.', '(', '2002', ',', 'March', ')', '.']

 TOTAL LEMMATIZE WORDS ==> 15

************************************************************************************************************************

516 --> Automatic evaluation of machine translation quality  using n-gram co-occurrence statistics. 


 ---- TOKENS ----

 ['Automatic', 'evaluation', 'of', 'machine', 'translation', 'quality', 'using', 'n-gram', 'co-occurrence', 'statistics', '.'] 

 TOTAL TOKENS ==> 11

 ---- POST ----

 [('Automatic', 'JJ'), ('evaluation', 'NN'), ('of', 'IN'), ('machine', 'NN'), ('translation', 'NN'), ('quality', 'NN'), ('using', 'VBG'), ('n-gram', 'JJ'), ('co-occurrence', 'NN'), ('statistics', 'NNS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Automatic', 'evaluation', 'machine', 'translation', 'quality', 'using', 'n-gram', 'co-occurrence', 'statistics', '.']

 TOTAL FILTERED TOKENS ==>  10

 ---- POST FOR FILTERED TOKENS ----

 [('Automatic', 'JJ'), ('evaluation', 'NN'), ('machine', 'NN'), ('translation', 'NN'), ('quality', 'NN'), ('using', 'VBG'), ('n-gram', 'JJ'), ('co-occurrence', 'NN'), ('statistics', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Automatic evaluation', 'evaluation machine', 'machine translation', 'translation quality', 'quality using', 'using n-gram', 'n-gram co-occurrence', 'co-occurrence statistics', 'statistics .'] 

 TOTAL BIGRAMS --> 9 



 ---- TRI-GRAMS ---- 

 ['Automatic evaluation machine', 'evaluation machine translation', 'machine translation quality', 'translation quality using', 'quality using n-gram', 'using n-gram co-occurrence', 'n-gram co-occurrence statistics', 'co-occurrence statistics .'] 

 TOTAL TRIGRAMS --> 8 



 ---- NOUN PHRASES ---- 

 ['Automatic evaluation', 'machine', 'translation', 'quality', 'n-gram co-occurrence'] 

 TOTAL NOUN PHRASES --> 5 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> ['Automatic']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['automat', 'evalu', 'machin', 'translat', 'qualiti', 'use', 'n-gram', 'co-occurr', 'statist', '.']

 TOTAL PORTER STEM WORDS ==> 10



 ---- SNOWBALL STEMMING ----

['automat', 'evalu', 'machin', 'translat', 'qualiti', 'use', 'n-gram', 'co-occurr', 'statist', '.']

 TOTAL SNOWBALL STEM WORDS ==> 10



 ---- LEMMATIZATION ----

['Automatic', 'evaluation', 'machine', 'translation', 'quality', 'using', 'n-gram', 'co-occurrence', 'statistic', '.']

 TOTAL LEMMATIZE WORDS ==> 10

************************************************************************************************************************

517 --> In Proceedings of the second international conference  on Human Language Technology Research (pp. 


 ---- TOKENS ----

 ['In', 'Proceedings', 'of', 'the', 'second', 'international', 'conference', 'on', 'Human', 'Language', 'Technology', 'Research', '(', 'pp', '.'] 

 TOTAL TOKENS ==> 15

 ---- POST ----

 [('In', 'IN'), ('Proceedings', 'NNP'), ('of', 'IN'), ('the', 'DT'), ('second', 'JJ'), ('international', 'JJ'), ('conference', 'NN'), ('on', 'IN'), ('Human', 'NNP'), ('Language', 'NNP'), ('Technology', 'NNP'), ('Research', 'NNP'), ('(', '('), ('pp', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Proceedings', 'second', 'international', 'conference', 'Human', 'Language', 'Technology', 'Research', '(', 'pp', '.']

 TOTAL FILTERED TOKENS ==>  11

 ---- POST FOR FILTERED TOKENS ----

 [('Proceedings', 'NNS'), ('second', 'JJ'), ('international', 'JJ'), ('conference', 'NN'), ('Human', 'NNP'), ('Language', 'NNP'), ('Technology', 'NNP'), ('Research', 'NNP'), ('(', '('), ('pp', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Proceedings second', 'second international', 'international conference', 'conference Human', 'Human Language', 'Language Technology', 'Technology Research', 'Research (', '( pp', 'pp .'] 

 TOTAL BIGRAMS --> 10 



 ---- TRI-GRAMS ---- 

 ['Proceedings second international', 'second international conference', 'international conference Human', 'conference Human Language', 'Human Language Technology', 'Language Technology Research', 'Technology Research (', 'Research ( pp', '( pp .'] 

 TOTAL TRIGRAMS --> 9 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 []

 ---- PORTER STEMMING ----

['proceed', 'second', 'intern', 'confer', 'human', 'languag', 'technolog', 'research', '(', 'pp', '.']

 TOTAL PORTER STEM WORDS ==> 11



 ---- SNOWBALL STEMMING ----

['proceed', 'second', 'intern', 'confer', 'human', 'languag', 'technolog', 'research', '(', 'pp', '.']

 TOTAL SNOWBALL STEM WORDS ==> 11



 ---- LEMMATIZATION ----

['Proceedings', 'second', 'international', 'conference', 'Human', 'Language', 'Technology', 'Research', '(', 'pp', '.']

 TOTAL LEMMATIZE WORDS ==> 11

************************************************************************************************************************

518 --> 138-145). 


 ---- TOKENS ----

 ['138-145', ')', '.'] 

 TOTAL TOKENS ==> 3

 ---- POST ----

 [('138-145', 'JJ'), (')', ')'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['138-145', ')', '.']

 TOTAL FILTERED TOKENS ==>  3

 ---- POST FOR FILTERED TOKENS ----

 [('138-145', 'JJ'), (')', ')'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['138-145 )', ') .'] 

 TOTAL BIGRAMS --> 2 



 ---- TRI-GRAMS ---- 

 ['138-145 ) .'] 

 TOTAL TRIGRAMS --> 1 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 []

 ---- PORTER STEMMING ----

['138-145', ')', '.']

 TOTAL PORTER STEM WORDS ==> 3



 ---- SNOWBALL STEMMING ----

['138-145', ')', '.']

 TOTAL SNOWBALL STEM WORDS ==> 3



 ---- LEMMATIZATION ----

['138-145', ')', '.']

 TOTAL LEMMATIZE WORDS ==> 3

************************************************************************************************************************

519 --> Morgan Kaufmann Publishers Inc   [42] Papineni, K., Roukos, S., Ward, T., & Zhu, W. J. 


 ---- TOKENS ----

 ['Morgan', 'Kaufmann', 'Publishers', 'Inc', '[', '42', ']', 'Papineni', ',', 'K.', ',', 'Roukos', ',', 'S.', ',', 'Ward', ',', 'T.', ',', '&', 'Zhu', ',', 'W.', 'J', '.'] 

 TOTAL TOKENS ==> 25

 ---- POST ----

 [('Morgan', 'NNP'), ('Kaufmann', 'NNP'), ('Publishers', 'NNP'), ('Inc', 'NNP'), ('[', 'VBD'), ('42', 'CD'), (']', 'NNP'), ('Papineni', 'NNP'), (',', ','), ('K.', 'NNP'), (',', ','), ('Roukos', 'NNP'), (',', ','), ('S.', 'NNP'), (',', ','), ('Ward', 'NNP'), (',', ','), ('T.', 'NNP'), (',', ','), ('&', 'CC'), ('Zhu', 'NNP'), (',', ','), ('W.', 'NNP'), ('J', 'NNP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Morgan', 'Kaufmann', 'Publishers', 'Inc', '[', '42', ']', 'Papineni', ',', 'K.', ',', 'Roukos', ',', 'S.', ',', 'Ward', ',', 'T.', ',', '&', 'Zhu', ',', 'W.', 'J', '.']

 TOTAL FILTERED TOKENS ==>  25

 ---- POST FOR FILTERED TOKENS ----

 [('Morgan', 'NNP'), ('Kaufmann', 'NNP'), ('Publishers', 'NNP'), ('Inc', 'NNP'), ('[', 'VBD'), ('42', 'CD'), (']', 'NNP'), ('Papineni', 'NNP'), (',', ','), ('K.', 'NNP'), (',', ','), ('Roukos', 'NNP'), (',', ','), ('S.', 'NNP'), (',', ','), ('Ward', 'NNP'), (',', ','), ('T.', 'NNP'), (',', ','), ('&', 'CC'), ('Zhu', 'NNP'), (',', ','), ('W.', 'NNP'), ('J', 'NNP'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Morgan Kaufmann', 'Kaufmann Publishers', 'Publishers Inc', 'Inc [', '[ 42', '42 ]', '] Papineni', 'Papineni ,', ', K.', 'K. ,', ', Roukos', 'Roukos ,', ', S.', 'S. ,', ', Ward', 'Ward ,', ', T.', 'T. ,', ', &', '& Zhu', 'Zhu ,', ', W.', 'W. J', 'J .'] 

 TOTAL BIGRAMS --> 24 



 ---- TRI-GRAMS ---- 

 ['Morgan Kaufmann Publishers', 'Kaufmann Publishers Inc', 'Publishers Inc [', 'Inc [ 42', '[ 42 ]', '42 ] Papineni', '] Papineni ,', 'Papineni , K.', ', K. ,', 'K. , Roukos', ', Roukos ,', 'Roukos , S.', ', S. ,', 'S. , Ward', ', Ward ,', 'Ward , T.', ', T. ,', 'T. , &', ', & Zhu', '& Zhu ,', 'Zhu , W.', ', W. J', 'W. J .'] 

 TOTAL TRIGRAMS --> 23 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> ['Morgan', 'Kaufmann Publishers Inc', 'Ward', 'Zhu']
 TOTAL PERSON ENTITY --> 4 


 GPE ---> ['Roukos']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['morgan', 'kaufmann', 'publish', 'inc', '[', '42', ']', 'papineni', ',', 'k.', ',', 'rouko', ',', 's.', ',', 'ward', ',', 't.', ',', '&', 'zhu', ',', 'w.', 'j', '.']

 TOTAL PORTER STEM WORDS ==> 25



 ---- SNOWBALL STEMMING ----

['morgan', 'kaufmann', 'publish', 'inc', '[', '42', ']', 'papineni', ',', 'k.', ',', 'rouko', ',', 's.', ',', 'ward', ',', 't.', ',', '&', 'zhu', ',', 'w.', 'j', '.']

 TOTAL SNOWBALL STEM WORDS ==> 25



 ---- LEMMATIZATION ----

['Morgan', 'Kaufmann', 'Publishers', 'Inc', '[', '42', ']', 'Papineni', ',', 'K.', ',', 'Roukos', ',', 'S.', ',', 'Ward', ',', 'T.', ',', '&', 'Zhu', ',', 'W.', 'J', '.']

 TOTAL LEMMATIZE WORDS ==> 25

************************************************************************************************************************

520 --> (2002, July). 


 ---- TOKENS ----

 ['(', '2002', ',', 'July', ')', '.'] 

 TOTAL TOKENS ==> 6

 ---- POST ----

 [('(', '('), ('2002', 'CD'), (',', ','), ('July', 'NNP'), (')', ')'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['(', '2002', ',', 'July', ')', '.']

 TOTAL FILTERED TOKENS ==>  6

 ---- POST FOR FILTERED TOKENS ----

 [('(', '('), ('2002', 'CD'), (',', ','), ('July', 'NNP'), (')', ')'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['( 2002', '2002 ,', ', July', 'July )', ') .'] 

 TOTAL BIGRAMS --> 5 



 ---- TRI-GRAMS ---- 

 ['( 2002 ,', '2002 , July', ', July )', 'July ) .'] 

 TOTAL TRIGRAMS --> 4 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['(', '2002', ',', 'juli', ')', '.']

 TOTAL PORTER STEM WORDS ==> 6



 ---- SNOWBALL STEMMING ----

['(', '2002', ',', 'juli', ')', '.']

 TOTAL SNOWBALL STEM WORDS ==> 6



 ---- LEMMATIZATION ----

['(', '2002', ',', 'July', ')', '.']

 TOTAL LEMMATIZE WORDS ==> 6

************************************************************************************************************************

521 --> BLEU: a method for  automatic evaluation of machine translation. 


 ---- TOKENS ----

 ['BLEU', ':', 'a', 'method', 'for', 'automatic', 'evaluation', 'of', 'machine', 'translation', '.'] 

 TOTAL TOKENS ==> 11

 ---- POST ----

 [('BLEU', 'NN'), (':', ':'), ('a', 'DT'), ('method', 'NN'), ('for', 'IN'), ('automatic', 'JJ'), ('evaluation', 'NN'), ('of', 'IN'), ('machine', 'NN'), ('translation', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['BLEU', ':', 'method', 'automatic', 'evaluation', 'machine', 'translation', '.']

 TOTAL FILTERED TOKENS ==>  8

 ---- POST FOR FILTERED TOKENS ----

 [('BLEU', 'NN'), (':', ':'), ('method', 'NN'), ('automatic', 'JJ'), ('evaluation', 'NN'), ('machine', 'NN'), ('translation', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['BLEU :', ': method', 'method automatic', 'automatic evaluation', 'evaluation machine', 'machine translation', 'translation .'] 

 TOTAL BIGRAMS --> 7 



 ---- TRI-GRAMS ---- 

 ['BLEU : method', ': method automatic', 'method automatic evaluation', 'automatic evaluation machine', 'evaluation machine translation', 'machine translation .'] 

 TOTAL TRIGRAMS --> 6 



 ---- NOUN PHRASES ---- 

 ['BLEU', 'method', 'automatic evaluation', 'machine', 'translation'] 

 TOTAL NOUN PHRASES --> 5 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['bleu', ':', 'method', 'automat', 'evalu', 'machin', 'translat', '.']

 TOTAL PORTER STEM WORDS ==> 8



 ---- SNOWBALL STEMMING ----

['bleu', ':', 'method', 'automat', 'evalu', 'machin', 'translat', '.']

 TOTAL SNOWBALL STEM WORDS ==> 8



 ---- LEMMATIZATION ----

['BLEU', ':', 'method', 'automatic', 'evaluation', 'machine', 'translation', '.']

 TOTAL LEMMATIZE WORDS ==> 8

************************************************************************************************************************

522 --> In Proceedings of the 40th annual meeting on  association for computational linguistics (pp. 


 ---- TOKENS ----

 ['In', 'Proceedings', 'of', 'the', '40th', 'annual', 'meeting', 'on', 'association', 'for', 'computational', 'linguistics', '(', 'pp', '.'] 

 TOTAL TOKENS ==> 15

 ---- POST ----

 [('In', 'IN'), ('Proceedings', 'NNP'), ('of', 'IN'), ('the', 'DT'), ('40th', 'JJ'), ('annual', 'JJ'), ('meeting', 'NN'), ('on', 'IN'), ('association', 'NN'), ('for', 'IN'), ('computational', 'JJ'), ('linguistics', 'NNS'), ('(', '('), ('pp', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Proceedings', '40th', 'annual', 'meeting', 'association', 'computational', 'linguistics', '(', 'pp', '.']

 TOTAL FILTERED TOKENS ==>  10

 ---- POST FOR FILTERED TOKENS ----

 [('Proceedings', 'NNS'), ('40th', 'CD'), ('annual', 'JJ'), ('meeting', 'NN'), ('association', 'NN'), ('computational', 'NN'), ('linguistics', 'NNS'), ('(', '('), ('pp', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Proceedings 40th', '40th annual', 'annual meeting', 'meeting association', 'association computational', 'computational linguistics', 'linguistics (', '( pp', 'pp .'] 

 TOTAL BIGRAMS --> 9 



 ---- TRI-GRAMS ---- 

 ['Proceedings 40th annual', '40th annual meeting', 'annual meeting association', 'meeting association computational', 'association computational linguistics', 'computational linguistics (', 'linguistics ( pp', '( pp .'] 

 TOTAL TRIGRAMS --> 8 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 []

 ---- PORTER STEMMING ----

['proceed', '40th', 'annual', 'meet', 'associ', 'comput', 'linguist', '(', 'pp', '.']

 TOTAL PORTER STEM WORDS ==> 10



 ---- SNOWBALL STEMMING ----

['proceed', '40th', 'annual', 'meet', 'associ', 'comput', 'linguist', '(', 'pp', '.']

 TOTAL SNOWBALL STEM WORDS ==> 10



 ---- LEMMATIZATION ----

['Proceedings', '40th', 'annual', 'meeting', 'association', 'computational', 'linguistics', '(', 'pp', '.']

 TOTAL LEMMATIZE WORDS ==> 10

************************************************************************************************************************

523 --> 311-318). 


 ---- TOKENS ----

 ['311-318', ')', '.'] 

 TOTAL TOKENS ==> 3

 ---- POST ----

 [('311-318', 'JJ'), (')', ')'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['311-318', ')', '.']

 TOTAL FILTERED TOKENS ==>  3

 ---- POST FOR FILTERED TOKENS ----

 [('311-318', 'JJ'), (')', ')'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['311-318 )', ') .'] 

 TOTAL BIGRAMS --> 2 



 ---- TRI-GRAMS ---- 

 ['311-318 ) .'] 

 TOTAL TRIGRAMS --> 1 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 []

 ---- PORTER STEMMING ----

['311-318', ')', '.']

 TOTAL PORTER STEM WORDS ==> 3



 ---- SNOWBALL STEMMING ----

['311-318', ')', '.']

 TOTAL SNOWBALL STEM WORDS ==> 3



 ---- LEMMATIZATION ----

['311-318', ')', '.']

 TOTAL LEMMATIZE WORDS ==> 3

************************************************************************************************************************

524 --> Association for Computational  Linguistics  [43] Doddington, G. (2002, March). 


 ---- TOKENS ----

 ['Association', 'for', 'Computational', 'Linguistics', '[', '43', ']', 'Doddington', ',', 'G.', '(', '2002', ',', 'March', ')', '.'] 

 TOTAL TOKENS ==> 16

 ---- POST ----

 [('Association', 'NNP'), ('for', 'IN'), ('Computational', 'NNP'), ('Linguistics', 'NNP'), ('[', 'VBD'), ('43', 'CD'), (']', 'NNP'), ('Doddington', 'NNP'), (',', ','), ('G.', 'NNP'), ('(', '('), ('2002', 'CD'), (',', ','), ('March', 'NNP'), (')', ')'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Association', 'Computational', 'Linguistics', '[', '43', ']', 'Doddington', ',', 'G.', '(', '2002', ',', 'March', ')', '.']

 TOTAL FILTERED TOKENS ==>  15

 ---- POST FOR FILTERED TOKENS ----

 [('Association', 'NNP'), ('Computational', 'NNP'), ('Linguistics', 'NNP'), ('[', 'VBD'), ('43', 'CD'), (']', 'NNP'), ('Doddington', 'NNP'), (',', ','), ('G.', 'NNP'), ('(', '('), ('2002', 'CD'), (',', ','), ('March', 'NNP'), (')', ')'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Association Computational', 'Computational Linguistics', 'Linguistics [', '[ 43', '43 ]', '] Doddington', 'Doddington ,', ', G.', 'G. (', '( 2002', '2002 ,', ', March', 'March )', ') .'] 

 TOTAL BIGRAMS --> 14 



 ---- TRI-GRAMS ---- 

 ['Association Computational Linguistics', 'Computational Linguistics [', 'Linguistics [ 43', '[ 43 ]', '43 ] Doddington', '] Doddington ,', 'Doddington , G.', ', G. (', 'G. ( 2002', '( 2002 ,', '2002 , March', ', March )', 'March ) .'] 

 TOTAL TRIGRAMS --> 13 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> ['Doddington']
 TOTAL PERSON ENTITY --> 1 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['associ', 'comput', 'linguist', '[', '43', ']', 'doddington', ',', 'g.', '(', '2002', ',', 'march', ')', '.']

 TOTAL PORTER STEM WORDS ==> 15



 ---- SNOWBALL STEMMING ----

['associ', 'comput', 'linguist', '[', '43', ']', 'doddington', ',', 'g.', '(', '2002', ',', 'march', ')', '.']

 TOTAL SNOWBALL STEM WORDS ==> 15



 ---- LEMMATIZATION ----

['Association', 'Computational', 'Linguistics', '[', '43', ']', 'Doddington', ',', 'G.', '(', '2002', ',', 'March', ')', '.']

 TOTAL LEMMATIZE WORDS ==> 15

************************************************************************************************************************

525 --> Automatic evaluation of machine translation quality  using n-gram co-occurrence statistics. 


 ---- TOKENS ----

 ['Automatic', 'evaluation', 'of', 'machine', 'translation', 'quality', 'using', 'n-gram', 'co-occurrence', 'statistics', '.'] 

 TOTAL TOKENS ==> 11

 ---- POST ----

 [('Automatic', 'JJ'), ('evaluation', 'NN'), ('of', 'IN'), ('machine', 'NN'), ('translation', 'NN'), ('quality', 'NN'), ('using', 'VBG'), ('n-gram', 'JJ'), ('co-occurrence', 'NN'), ('statistics', 'NNS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Automatic', 'evaluation', 'machine', 'translation', 'quality', 'using', 'n-gram', 'co-occurrence', 'statistics', '.']

 TOTAL FILTERED TOKENS ==>  10

 ---- POST FOR FILTERED TOKENS ----

 [('Automatic', 'JJ'), ('evaluation', 'NN'), ('machine', 'NN'), ('translation', 'NN'), ('quality', 'NN'), ('using', 'VBG'), ('n-gram', 'JJ'), ('co-occurrence', 'NN'), ('statistics', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Automatic evaluation', 'evaluation machine', 'machine translation', 'translation quality', 'quality using', 'using n-gram', 'n-gram co-occurrence', 'co-occurrence statistics', 'statistics .'] 

 TOTAL BIGRAMS --> 9 



 ---- TRI-GRAMS ---- 

 ['Automatic evaluation machine', 'evaluation machine translation', 'machine translation quality', 'translation quality using', 'quality using n-gram', 'using n-gram co-occurrence', 'n-gram co-occurrence statistics', 'co-occurrence statistics .'] 

 TOTAL TRIGRAMS --> 8 



 ---- NOUN PHRASES ---- 

 ['Automatic evaluation', 'machine', 'translation', 'quality', 'n-gram co-occurrence'] 

 TOTAL NOUN PHRASES --> 5 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> ['Automatic']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['automat', 'evalu', 'machin', 'translat', 'qualiti', 'use', 'n-gram', 'co-occurr', 'statist', '.']

 TOTAL PORTER STEM WORDS ==> 10



 ---- SNOWBALL STEMMING ----

['automat', 'evalu', 'machin', 'translat', 'qualiti', 'use', 'n-gram', 'co-occurr', 'statist', '.']

 TOTAL SNOWBALL STEM WORDS ==> 10



 ---- LEMMATIZATION ----

['Automatic', 'evaluation', 'machine', 'translation', 'quality', 'using', 'n-gram', 'co-occurrence', 'statistic', '.']

 TOTAL LEMMATIZE WORDS ==> 10

************************************************************************************************************************

526 --> In Proceedings of the second international conference  on Human Language Technology Research (pp. 


 ---- TOKENS ----

 ['In', 'Proceedings', 'of', 'the', 'second', 'international', 'conference', 'on', 'Human', 'Language', 'Technology', 'Research', '(', 'pp', '.'] 

 TOTAL TOKENS ==> 15

 ---- POST ----

 [('In', 'IN'), ('Proceedings', 'NNP'), ('of', 'IN'), ('the', 'DT'), ('second', 'JJ'), ('international', 'JJ'), ('conference', 'NN'), ('on', 'IN'), ('Human', 'NNP'), ('Language', 'NNP'), ('Technology', 'NNP'), ('Research', 'NNP'), ('(', '('), ('pp', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Proceedings', 'second', 'international', 'conference', 'Human', 'Language', 'Technology', 'Research', '(', 'pp', '.']

 TOTAL FILTERED TOKENS ==>  11

 ---- POST FOR FILTERED TOKENS ----

 [('Proceedings', 'NNS'), ('second', 'JJ'), ('international', 'JJ'), ('conference', 'NN'), ('Human', 'NNP'), ('Language', 'NNP'), ('Technology', 'NNP'), ('Research', 'NNP'), ('(', '('), ('pp', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Proceedings second', 'second international', 'international conference', 'conference Human', 'Human Language', 'Language Technology', 'Technology Research', 'Research (', '( pp', 'pp .'] 

 TOTAL BIGRAMS --> 10 



 ---- TRI-GRAMS ---- 

 ['Proceedings second international', 'second international conference', 'international conference Human', 'conference Human Language', 'Human Language Technology', 'Language Technology Research', 'Technology Research (', 'Research ( pp', '( pp .'] 

 TOTAL TRIGRAMS --> 9 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 []

 ---- PORTER STEMMING ----

['proceed', 'second', 'intern', 'confer', 'human', 'languag', 'technolog', 'research', '(', 'pp', '.']

 TOTAL PORTER STEM WORDS ==> 11



 ---- SNOWBALL STEMMING ----

['proceed', 'second', 'intern', 'confer', 'human', 'languag', 'technolog', 'research', '(', 'pp', '.']

 TOTAL SNOWBALL STEM WORDS ==> 11



 ---- LEMMATIZATION ----

['Proceedings', 'second', 'international', 'conference', 'Human', 'Language', 'Technology', 'Research', '(', 'pp', '.']

 TOTAL LEMMATIZE WORDS ==> 11

************************************************************************************************************************

527 --> 138-145). 


 ---- TOKENS ----

 ['138-145', ')', '.'] 

 TOTAL TOKENS ==> 3

 ---- POST ----

 [('138-145', 'JJ'), (')', ')'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['138-145', ')', '.']

 TOTAL FILTERED TOKENS ==>  3

 ---- POST FOR FILTERED TOKENS ----

 [('138-145', 'JJ'), (')', ')'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['138-145 )', ') .'] 

 TOTAL BIGRAMS --> 2 



 ---- TRI-GRAMS ---- 

 ['138-145 ) .'] 

 TOTAL TRIGRAMS --> 1 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 []

 ---- PORTER STEMMING ----

['138-145', ')', '.']

 TOTAL PORTER STEM WORDS ==> 3



 ---- SNOWBALL STEMMING ----

['138-145', ')', '.']

 TOTAL SNOWBALL STEM WORDS ==> 3



 ---- LEMMATIZATION ----

['138-145', ')', '.']

 TOTAL LEMMATIZE WORDS ==> 3

************************************************************************************************************************

528 --> Morgan Kaufmann Publishers Inc   [44] Hayes, P. J. 


 ---- TOKENS ----

 ['Morgan', 'Kaufmann', 'Publishers', 'Inc', '[', '44', ']', 'Hayes', ',', 'P.', 'J', '.'] 

 TOTAL TOKENS ==> 12

 ---- POST ----

 [('Morgan', 'NNP'), ('Kaufmann', 'NNP'), ('Publishers', 'NNP'), ('Inc', 'NNP'), ('[', 'VBD'), ('44', 'CD'), (']', 'JJ'), ('Hayes', 'NNP'), (',', ','), ('P.', 'NNP'), ('J', 'NNP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Morgan', 'Kaufmann', 'Publishers', 'Inc', '[', '44', ']', 'Hayes', ',', 'P.', 'J', '.']

 TOTAL FILTERED TOKENS ==>  12

 ---- POST FOR FILTERED TOKENS ----

 [('Morgan', 'NNP'), ('Kaufmann', 'NNP'), ('Publishers', 'NNP'), ('Inc', 'NNP'), ('[', 'VBD'), ('44', 'CD'), (']', 'JJ'), ('Hayes', 'NNP'), (',', ','), ('P.', 'NNP'), ('J', 'NNP'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Morgan Kaufmann', 'Kaufmann Publishers', 'Publishers Inc', 'Inc [', '[ 44', '44 ]', '] Hayes', 'Hayes ,', ', P.', 'P. J', 'J .'] 

 TOTAL BIGRAMS --> 11 



 ---- TRI-GRAMS ---- 

 ['Morgan Kaufmann Publishers', 'Kaufmann Publishers Inc', 'Publishers Inc [', 'Inc [ 44', '[ 44 ]', '44 ] Hayes', '] Hayes ,', 'Hayes , P.', ', P. J', 'P. J .'] 

 TOTAL TRIGRAMS --> 10 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> ['Morgan', 'Kaufmann Publishers Inc']
 TOTAL PERSON ENTITY --> 2 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['morgan', 'kaufmann', 'publish', 'inc', '[', '44', ']', 'hay', ',', 'p.', 'j', '.']

 TOTAL PORTER STEM WORDS ==> 12



 ---- SNOWBALL STEMMING ----

['morgan', 'kaufmann', 'publish', 'inc', '[', '44', ']', 'hay', ',', 'p.', 'j', '.']

 TOTAL SNOWBALL STEM WORDS ==> 12



 ---- LEMMATIZATION ----

['Morgan', 'Kaufmann', 'Publishers', 'Inc', '[', '44', ']', 'Hayes', ',', 'P.', 'J', '.']

 TOTAL LEMMATIZE WORDS ==> 12

************************************************************************************************************************

529 --> (1992). 


 ---- TOKENS ----

 ['(', '1992', ')', '.'] 

 TOTAL TOKENS ==> 4

 ---- POST ----

 [('(', '('), ('1992', 'CD'), (')', ')'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['(', '1992', ')', '.']

 TOTAL FILTERED TOKENS ==>  4

 ---- POST FOR FILTERED TOKENS ----

 [('(', '('), ('1992', 'CD'), (')', ')'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['( 1992', '1992 )', ') .'] 

 TOTAL BIGRAMS --> 3 



 ---- TRI-GRAMS ---- 

 ['( 1992 )', '1992 ) .'] 

 TOTAL TRIGRAMS --> 2 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['(', '1992', ')', '.']

 TOTAL PORTER STEM WORDS ==> 4



 ---- SNOWBALL STEMMING ----

['(', '1992', ')', '.']

 TOTAL SNOWBALL STEM WORDS ==> 4



 ---- LEMMATIZATION ----

['(', '1992', ')', '.']

 TOTAL LEMMATIZE WORDS ==> 4

************************************************************************************************************************

530 --> Intelligent high-volume text processing using shallow, domain- specific techniques. 


 ---- TOKENS ----

 ['Intelligent', 'high-volume', 'text', 'processing', 'using', 'shallow', ',', 'domain-', 'specific', 'techniques', '.'] 

 TOTAL TOKENS ==> 11

 ---- POST ----

 [('Intelligent', 'JJ'), ('high-volume', 'JJ'), ('text', 'NN'), ('processing', 'NN'), ('using', 'VBG'), ('shallow', 'JJ'), (',', ','), ('domain-', 'JJ'), ('specific', 'JJ'), ('techniques', 'NNS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Intelligent', 'high-volume', 'text', 'processing', 'using', 'shallow', ',', 'domain-', 'specific', 'techniques', '.']

 TOTAL FILTERED TOKENS ==>  11

 ---- POST FOR FILTERED TOKENS ----

 [('Intelligent', 'JJ'), ('high-volume', 'JJ'), ('text', 'NN'), ('processing', 'NN'), ('using', 'VBG'), ('shallow', 'JJ'), (',', ','), ('domain-', 'JJ'), ('specific', 'JJ'), ('techniques', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Intelligent high-volume', 'high-volume text', 'text processing', 'processing using', 'using shallow', 'shallow ,', ', domain-', 'domain- specific', 'specific techniques', 'techniques .'] 

 TOTAL BIGRAMS --> 10 



 ---- TRI-GRAMS ---- 

 ['Intelligent high-volume text', 'high-volume text processing', 'text processing using', 'processing using shallow', 'using shallow ,', 'shallow , domain-', ', domain- specific', 'domain- specific techniques', 'specific techniques .'] 

 TOTAL TRIGRAMS --> 9 



 ---- NOUN PHRASES ---- 

 ['Intelligent high-volume text', 'processing'] 

 TOTAL NOUN PHRASES --> 2 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['intellig', 'high-volum', 'text', 'process', 'use', 'shallow', ',', 'domain-', 'specif', 'techniqu', '.']

 TOTAL PORTER STEM WORDS ==> 11



 ---- SNOWBALL STEMMING ----

['intellig', 'high-volum', 'text', 'process', 'use', 'shallow', ',', 'domain-', 'specif', 'techniqu', '.']

 TOTAL SNOWBALL STEM WORDS ==> 11



 ---- LEMMATIZATION ----

['Intelligent', 'high-volume', 'text', 'processing', 'using', 'shallow', ',', 'domain-', 'specific', 'technique', '.']

 TOTAL LEMMATIZE WORDS ==> 11

************************************************************************************************************************

531 --> Text-based intelligent systems: Current research and practice in  information extraction and retrieval, 227-242. 


 ---- TOKENS ----

 ['Text-based', 'intelligent', 'systems', ':', 'Current', 'research', 'and', 'practice', 'in', 'information', 'extraction', 'and', 'retrieval', ',', '227-242', '.'] 

 TOTAL TOKENS ==> 16

 ---- POST ----

 [('Text-based', 'JJ'), ('intelligent', 'JJ'), ('systems', 'NNS'), (':', ':'), ('Current', 'JJ'), ('research', 'NN'), ('and', 'CC'), ('practice', 'NN'), ('in', 'IN'), ('information', 'NN'), ('extraction', 'NN'), ('and', 'CC'), ('retrieval', 'NN'), (',', ','), ('227-242', 'JJ'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Text-based', 'intelligent', 'systems', ':', 'Current', 'research', 'practice', 'information', 'extraction', 'retrieval', ',', '227-242', '.']

 TOTAL FILTERED TOKENS ==>  13

 ---- POST FOR FILTERED TOKENS ----

 [('Text-based', 'JJ'), ('intelligent', 'JJ'), ('systems', 'NNS'), (':', ':'), ('Current', 'NNP'), ('research', 'NN'), ('practice', 'NN'), ('information', 'NN'), ('extraction', 'NN'), ('retrieval', 'NN'), (',', ','), ('227-242', 'JJ'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Text-based intelligent', 'intelligent systems', 'systems :', ': Current', 'Current research', 'research practice', 'practice information', 'information extraction', 'extraction retrieval', 'retrieval ,', ', 227-242', '227-242 .'] 

 TOTAL BIGRAMS --> 12 



 ---- TRI-GRAMS ---- 

 ['Text-based intelligent systems', 'intelligent systems :', 'systems : Current', ': Current research', 'Current research practice', 'research practice information', 'practice information extraction', 'information extraction retrieval', 'extraction retrieval ,', 'retrieval , 227-242', ', 227-242 .'] 

 TOTAL TRIGRAMS --> 11 



 ---- NOUN PHRASES ---- 

 ['research', 'practice', 'information', 'extraction', 'retrieval'] 

 TOTAL NOUN PHRASES --> 5 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['text-bas', 'intellig', 'system', ':', 'current', 'research', 'practic', 'inform', 'extract', 'retriev', ',', '227-242', '.']

 TOTAL PORTER STEM WORDS ==> 13



 ---- SNOWBALL STEMMING ----

['text-bas', 'intellig', 'system', ':', 'current', 'research', 'practic', 'inform', 'extract', 'retriev', ',', '227-242', '.']

 TOTAL SNOWBALL STEM WORDS ==> 13



 ---- LEMMATIZATION ----

['Text-based', 'intelligent', 'system', ':', 'Current', 'research', 'practice', 'information', 'extraction', 'retrieval', ',', '227-242', '.']

 TOTAL LEMMATIZE WORDS ==> 13

************************************************************************************************************************

532 --> [45] Cohen, W. W. (1996, March). 


 ---- TOKENS ----

 ['[', '45', ']', 'Cohen', ',', 'W.', 'W.', '(', '1996', ',', 'March', ')', '.'] 

 TOTAL TOKENS ==> 13

 ---- POST ----

 [('[', 'RB'), ('45', 'CD'), (']', 'NN'), ('Cohen', 'NNP'), (',', ','), ('W.', 'NNP'), ('W.', 'NNP'), ('(', '('), ('1996', 'CD'), (',', ','), ('March', 'NNP'), (')', ')'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['[', '45', ']', 'Cohen', ',', 'W.', 'W.', '(', '1996', ',', 'March', ')', '.']

 TOTAL FILTERED TOKENS ==>  13

 ---- POST FOR FILTERED TOKENS ----

 [('[', 'RB'), ('45', 'CD'), (']', 'NN'), ('Cohen', 'NNP'), (',', ','), ('W.', 'NNP'), ('W.', 'NNP'), ('(', '('), ('1996', 'CD'), (',', ','), ('March', 'NNP'), (')', ')'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['[ 45', '45 ]', '] Cohen', 'Cohen ,', ', W.', 'W. W.', 'W. (', '( 1996', '1996 ,', ', March', 'March )', ') .'] 

 TOTAL BIGRAMS --> 12 



 ---- TRI-GRAMS ---- 

 ['[ 45 ]', '45 ] Cohen', '] Cohen ,', 'Cohen , W.', ', W. W.', 'W. W. (', 'W. ( 1996', '( 1996 ,', '1996 , March', ', March )', 'March ) .'] 

 TOTAL TRIGRAMS --> 11 



 ---- NOUN PHRASES ---- 

 [']'] 

 TOTAL NOUN PHRASES --> 1 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> ['Cohen']
 TOTAL PERSON ENTITY --> 1 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['[', '45', ']', 'cohen', ',', 'w.', 'w.', '(', '1996', ',', 'march', ')', '.']

 TOTAL PORTER STEM WORDS ==> 13



 ---- SNOWBALL STEMMING ----

['[', '45', ']', 'cohen', ',', 'w.', 'w.', '(', '1996', ',', 'march', ')', '.']

 TOTAL SNOWBALL STEM WORDS ==> 13



 ---- LEMMATIZATION ----

['[', '45', ']', 'Cohen', ',', 'W.', 'W.', '(', '1996', ',', 'March', ')', '.']

 TOTAL LEMMATIZE WORDS ==> 13

************************************************************************************************************************

533 --> Learning rules that classify e-mail. 


 ---- TOKENS ----

 ['Learning', 'rules', 'that', 'classify', 'e-mail', '.'] 

 TOTAL TOKENS ==> 6

 ---- POST ----

 [('Learning', 'VBG'), ('rules', 'NNS'), ('that', 'WDT'), ('classify', 'VBP'), ('e-mail', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Learning', 'rules', 'classify', 'e-mail', '.']

 TOTAL FILTERED TOKENS ==>  5

 ---- POST FOR FILTERED TOKENS ----

 [('Learning', 'VBG'), ('rules', 'NNS'), ('classify', 'VB'), ('e-mail', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Learning rules', 'rules classify', 'classify e-mail', 'e-mail .'] 

 TOTAL BIGRAMS --> 4 



 ---- TRI-GRAMS ---- 

 ['Learning rules classify', 'rules classify e-mail', 'classify e-mail .'] 

 TOTAL TRIGRAMS --> 3 



 ---- NOUN PHRASES ---- 

 ['e-mail'] 

 TOTAL NOUN PHRASES --> 1 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['learn', 'rule', 'classifi', 'e-mail', '.']

 TOTAL PORTER STEM WORDS ==> 5



 ---- SNOWBALL STEMMING ----

['learn', 'rule', 'classifi', 'e-mail', '.']

 TOTAL SNOWBALL STEM WORDS ==> 5



 ---- LEMMATIZATION ----

['Learning', 'rule', 'classify', 'e-mail', '.']

 TOTAL LEMMATIZE WORDS ==> 5

************************************************************************************************************************

534 --> In AAAI spring  symposium on machine learning in information access (Vol. 


 ---- TOKENS ----

 ['In', 'AAAI', 'spring', 'symposium', 'on', 'machine', 'learning', 'in', 'information', 'access', '(', 'Vol', '.'] 

 TOTAL TOKENS ==> 13

 ---- POST ----

 [('In', 'IN'), ('AAAI', 'NNP'), ('spring', 'NN'), ('symposium', 'NN'), ('on', 'IN'), ('machine', 'NN'), ('learning', 'NN'), ('in', 'IN'), ('information', 'NN'), ('access', 'NN'), ('(', '('), ('Vol', 'NNP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['AAAI', 'spring', 'symposium', 'machine', 'learning', 'information', 'access', '(', 'Vol', '.']

 TOTAL FILTERED TOKENS ==>  10

 ---- POST FOR FILTERED TOKENS ----

 [('AAAI', 'NNP'), ('spring', 'NN'), ('symposium', 'NN'), ('machine', 'NN'), ('learning', 'VBG'), ('information', 'NN'), ('access', 'NN'), ('(', '('), ('Vol', 'NNP'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['AAAI spring', 'spring symposium', 'symposium machine', 'machine learning', 'learning information', 'information access', 'access (', '( Vol', 'Vol .'] 

 TOTAL BIGRAMS --> 9 



 ---- TRI-GRAMS ---- 

 ['AAAI spring symposium', 'spring symposium machine', 'symposium machine learning', 'machine learning information', 'learning information access', 'information access (', 'access ( Vol', '( Vol .'] 

 TOTAL TRIGRAMS --> 8 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 []

 ---- PORTER STEMMING ----

['aaai', 'spring', 'symposium', 'machin', 'learn', 'inform', 'access', '(', 'vol', '.']

 TOTAL PORTER STEM WORDS ==> 10



 ---- SNOWBALL STEMMING ----

['aaai', 'spring', 'symposium', 'machin', 'learn', 'inform', 'access', '(', 'vol', '.']

 TOTAL SNOWBALL STEM WORDS ==> 10



 ---- LEMMATIZATION ----

['AAAI', 'spring', 'symposium', 'machine', 'learning', 'information', 'access', '(', 'Vol', '.']

 TOTAL LEMMATIZE WORDS ==> 10

************************************************************************************************************************

535 --> 18, p. 25). 


 ---- TOKENS ----

 ['18', ',', 'p.', '25', ')', '.'] 

 TOTAL TOKENS ==> 6

 ---- POST ----

 [('18', 'CD'), (',', ','), ('p.', 'RB'), ('25', 'CD'), (')', ')'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['18', ',', 'p.', '25', ')', '.']

 TOTAL FILTERED TOKENS ==>  6

 ---- POST FOR FILTERED TOKENS ----

 [('18', 'CD'), (',', ','), ('p.', 'RB'), ('25', 'CD'), (')', ')'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['18 ,', ', p.', 'p. 25', '25 )', ') .'] 

 TOTAL BIGRAMS --> 5 



 ---- TRI-GRAMS ---- 

 ['18 , p.', ', p. 25', 'p. 25 )', '25 ) .'] 

 TOTAL TRIGRAMS --> 4 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 []

 ---- PORTER STEMMING ----

['18', ',', 'p.', '25', ')', '.']

 TOTAL PORTER STEM WORDS ==> 6



 ---- SNOWBALL STEMMING ----

['18', ',', 'p.', '25', ')', '.']

 TOTAL SNOWBALL STEM WORDS ==> 6



 ---- LEMMATIZATION ----

['18', ',', 'p.', '25', ')', '.']

 TOTAL LEMMATIZE WORDS ==> 6

************************************************************************************************************************

536 --> [46] Sahami, M., Dumais, S., Heckerman, D., & Horvitz, E. (1998, July). 


 ---- TOKENS ----

 ['[', '46', ']', 'Sahami', ',', 'M.', ',', 'Dumais', ',', 'S.', ',', 'Heckerman', ',', 'D.', ',', '&', 'Horvitz', ',', 'E.', '(', '1998', ',', 'July', ')', '.'] 

 TOTAL TOKENS ==> 25

 ---- POST ----

 [('[', 'RB'), ('46', 'CD'), (']', 'NNP'), ('Sahami', 'NNP'), (',', ','), ('M.', 'NNP'), (',', ','), ('Dumais', 'NNP'), (',', ','), ('S.', 'NNP'), (',', ','), ('Heckerman', 'NNP'), (',', ','), ('D.', 'NNP'), (',', ','), ('&', 'CC'), ('Horvitz', 'NNP'), (',', ','), ('E.', 'NNP'), ('(', '('), ('1998', 'CD'), (',', ','), ('July', 'NNP'), (')', ')'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['[', '46', ']', 'Sahami', ',', 'M.', ',', 'Dumais', ',', 'S.', ',', 'Heckerman', ',', 'D.', ',', '&', 'Horvitz', ',', 'E.', '(', '1998', ',', 'July', ')', '.']

 TOTAL FILTERED TOKENS ==>  25

 ---- POST FOR FILTERED TOKENS ----

 [('[', 'RB'), ('46', 'CD'), (']', 'NNP'), ('Sahami', 'NNP'), (',', ','), ('M.', 'NNP'), (',', ','), ('Dumais', 'NNP'), (',', ','), ('S.', 'NNP'), (',', ','), ('Heckerman', 'NNP'), (',', ','), ('D.', 'NNP'), (',', ','), ('&', 'CC'), ('Horvitz', 'NNP'), (',', ','), ('E.', 'NNP'), ('(', '('), ('1998', 'CD'), (',', ','), ('July', 'NNP'), (')', ')'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['[ 46', '46 ]', '] Sahami', 'Sahami ,', ', M.', 'M. ,', ', Dumais', 'Dumais ,', ', S.', 'S. ,', ', Heckerman', 'Heckerman ,', ', D.', 'D. ,', ', &', '& Horvitz', 'Horvitz ,', ', E.', 'E. (', '( 1998', '1998 ,', ', July', 'July )', ') .'] 

 TOTAL BIGRAMS --> 24 



 ---- TRI-GRAMS ---- 

 ['[ 46 ]', '46 ] Sahami', '] Sahami ,', 'Sahami , M.', ', M. ,', 'M. , Dumais', ', Dumais ,', 'Dumais , S.', ', S. ,', 'S. , Heckerman', ', Heckerman ,', 'Heckerman , D.', ', D. ,', 'D. , &', ', & Horvitz', '& Horvitz ,', 'Horvitz , E.', ', E. (', 'E. ( 1998', '( 1998 ,', '1998 , July', ', July )', 'July ) .'] 

 TOTAL TRIGRAMS --> 23 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> ['Heckerman', 'Horvitz']
 TOTAL PERSON ENTITY --> 2 


 GPE ---> ['Dumais']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['[', '46', ']', 'sahami', ',', 'm.', ',', 'dumai', ',', 's.', ',', 'heckerman', ',', 'd.', ',', '&', 'horvitz', ',', 'e.', '(', '1998', ',', 'juli', ')', '.']

 TOTAL PORTER STEM WORDS ==> 25



 ---- SNOWBALL STEMMING ----

['[', '46', ']', 'sahami', ',', 'm.', ',', 'dumai', ',', 's.', ',', 'heckerman', ',', 'd.', ',', '&', 'horvitz', ',', 'e.', '(', '1998', ',', 'juli', ')', '.']

 TOTAL SNOWBALL STEM WORDS ==> 25



 ---- LEMMATIZATION ----

['[', '46', ']', 'Sahami', ',', 'M.', ',', 'Dumais', ',', 'S.', ',', 'Heckerman', ',', 'D.', ',', '&', 'Horvitz', ',', 'E.', '(', '1998', ',', 'July', ')', '.']

 TOTAL LEMMATIZE WORDS ==> 25

************************************************************************************************************************

537 --> A Bayesian  approach to filtering junk e-mail. 


 ---- TOKENS ----

 ['A', 'Bayesian', 'approach', 'to', 'filtering', 'junk', 'e-mail', '.'] 

 TOTAL TOKENS ==> 8

 ---- POST ----

 [('A', 'DT'), ('Bayesian', 'JJ'), ('approach', 'NN'), ('to', 'TO'), ('filtering', 'VBG'), ('junk', 'NN'), ('e-mail', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Bayesian', 'approach', 'filtering', 'junk', 'e-mail', '.']

 TOTAL FILTERED TOKENS ==>  6

 ---- POST FOR FILTERED TOKENS ----

 [('Bayesian', 'JJ'), ('approach', 'NN'), ('filtering', 'VBG'), ('junk', 'NN'), ('e-mail', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Bayesian approach', 'approach filtering', 'filtering junk', 'junk e-mail', 'e-mail .'] 

 TOTAL BIGRAMS --> 5 



 ---- TRI-GRAMS ---- 

 ['Bayesian approach filtering', 'approach filtering junk', 'filtering junk e-mail', 'junk e-mail .'] 

 TOTAL TRIGRAMS --> 4 



 ---- NOUN PHRASES ---- 

 ['Bayesian approach', 'junk', 'e-mail'] 

 TOTAL NOUN PHRASES --> 3 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> ['Bayesian']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['bayesian', 'approach', 'filter', 'junk', 'e-mail', '.']

 TOTAL PORTER STEM WORDS ==> 6



 ---- SNOWBALL STEMMING ----

['bayesian', 'approach', 'filter', 'junk', 'e-mail', '.']

 TOTAL SNOWBALL STEM WORDS ==> 6



 ---- LEMMATIZATION ----

['Bayesian', 'approach', 'filtering', 'junk', 'e-mail', '.']

 TOTAL LEMMATIZE WORDS ==> 6

************************************************************************************************************************

538 --> In Learning for Text Categorization: Papers from the 1998  workshop (Vol. 


 ---- TOKENS ----

 ['In', 'Learning', 'for', 'Text', 'Categorization', ':', 'Papers', 'from', 'the', '1998', 'workshop', '(', 'Vol', '.'] 

 TOTAL TOKENS ==> 14

 ---- POST ----

 [('In', 'IN'), ('Learning', 'NNP'), ('for', 'IN'), ('Text', 'NNP'), ('Categorization', 'NNP'), (':', ':'), ('Papers', 'NNS'), ('from', 'IN'), ('the', 'DT'), ('1998', 'CD'), ('workshop', 'NN'), ('(', '('), ('Vol', 'NNP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Learning', 'Text', 'Categorization', ':', 'Papers', '1998', 'workshop', '(', 'Vol', '.']

 TOTAL FILTERED TOKENS ==>  10

 ---- POST FOR FILTERED TOKENS ----

 [('Learning', 'VBG'), ('Text', 'NNP'), ('Categorization', 'NN'), (':', ':'), ('Papers', 'NNS'), ('1998', 'CD'), ('workshop', 'NN'), ('(', '('), ('Vol', 'NNP'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Learning Text', 'Text Categorization', 'Categorization :', ': Papers', 'Papers 1998', '1998 workshop', 'workshop (', '( Vol', 'Vol .'] 

 TOTAL BIGRAMS --> 9 



 ---- TRI-GRAMS ---- 

 ['Learning Text Categorization', 'Text Categorization :', 'Categorization : Papers', ': Papers 1998', 'Papers 1998 workshop', '1998 workshop (', 'workshop ( Vol', '( Vol .'] 

 TOTAL TRIGRAMS --> 8 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 []

 ---- PORTER STEMMING ----

['learn', 'text', 'categor', ':', 'paper', '1998', 'workshop', '(', 'vol', '.']

 TOTAL PORTER STEM WORDS ==> 10



 ---- SNOWBALL STEMMING ----

['learn', 'text', 'categor', ':', 'paper', '1998', 'workshop', '(', 'vol', '.']

 TOTAL SNOWBALL STEM WORDS ==> 10



 ---- LEMMATIZATION ----

['Learning', 'Text', 'Categorization', ':', 'Papers', '1998', 'workshop', '(', 'Vol', '.']

 TOTAL LEMMATIZE WORDS ==> 10

************************************************************************************************************************

539 --> 62, pp. 


 ---- TOKENS ----

 ['62', ',', 'pp', '.'] 

 TOTAL TOKENS ==> 4

 ---- POST ----

 [('62', 'CD'), (',', ','), ('pp', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['62', ',', 'pp', '.']

 TOTAL FILTERED TOKENS ==>  4

 ---- POST FOR FILTERED TOKENS ----

 [('62', 'CD'), (',', ','), ('pp', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['62 ,', ', pp', 'pp .'] 

 TOTAL BIGRAMS --> 3 



 ---- TRI-GRAMS ---- 

 ['62 , pp', ', pp .'] 

 TOTAL TRIGRAMS --> 2 



 ---- NOUN PHRASES ---- 

 ['pp'] 

 TOTAL NOUN PHRASES --> 1 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['62', ',', 'pp', '.']

 TOTAL PORTER STEM WORDS ==> 4



 ---- SNOWBALL STEMMING ----

['62', ',', 'pp', '.']

 TOTAL SNOWBALL STEM WORDS ==> 4



 ---- LEMMATIZATION ----

['62', ',', 'pp', '.']

 TOTAL LEMMATIZE WORDS ==> 4

************************************************************************************************************************

540 --> 98-105). 


 ---- TOKENS ----

 ['98-105', ')', '.'] 

 TOTAL TOKENS ==> 3

 ---- POST ----

 [('98-105', 'CD'), (')', ')'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['98-105', ')', '.']

 TOTAL FILTERED TOKENS ==>  3

 ---- POST FOR FILTERED TOKENS ----

 [('98-105', 'CD'), (')', ')'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['98-105 )', ') .'] 

 TOTAL BIGRAMS --> 2 



 ---- TRI-GRAMS ---- 

 ['98-105 ) .'] 

 TOTAL TRIGRAMS --> 1 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 []

 ---- PORTER STEMMING ----

['98-105', ')', '.']

 TOTAL PORTER STEM WORDS ==> 3



 ---- SNOWBALL STEMMING ----

['98-105', ')', '.']

 TOTAL SNOWBALL STEM WORDS ==> 3



 ---- LEMMATIZATION ----

['98-105', ')', '.']

 TOTAL LEMMATIZE WORDS ==> 3

************************************************************************************************************************

541 --> [47] Androutsopoulos, I., Paliouras, G., Karkaletsis, V., Sakkis, G., Spyropoulos, C. D., &  Stamatopoulos, P. (2000). 


 ---- TOKENS ----

 ['[', '47', ']', 'Androutsopoulos', ',', 'I.', ',', 'Paliouras', ',', 'G.', ',', 'Karkaletsis', ',', 'V.', ',', 'Sakkis', ',', 'G.', ',', 'Spyropoulos', ',', 'C.', 'D.', ',', '&', 'Stamatopoulos', ',', 'P.', '(', '2000', ')', '.'] 

 TOTAL TOKENS ==> 32

 ---- POST ----

 [('[', 'RB'), ('47', 'CD'), (']', 'JJ'), ('Androutsopoulos', 'NNP'), (',', ','), ('I.', 'NNP'), (',', ','), ('Paliouras', 'NNP'), (',', ','), ('G.', 'NNP'), (',', ','), ('Karkaletsis', 'NNP'), (',', ','), ('V.', 'NNP'), (',', ','), ('Sakkis', 'NNP'), (',', ','), ('G.', 'NNP'), (',', ','), ('Spyropoulos', 'NNP'), (',', ','), ('C.', 'NNP'), ('D.', 'NNP'), (',', ','), ('&', 'CC'), ('Stamatopoulos', 'NNP'), (',', ','), ('P.', 'NNP'), ('(', '('), ('2000', 'CD'), (')', ')'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['[', '47', ']', 'Androutsopoulos', ',', 'I.', ',', 'Paliouras', ',', 'G.', ',', 'Karkaletsis', ',', 'V.', ',', 'Sakkis', ',', 'G.', ',', 'Spyropoulos', ',', 'C.', 'D.', ',', '&', 'Stamatopoulos', ',', 'P.', '(', '2000', ')', '.']

 TOTAL FILTERED TOKENS ==>  32

 ---- POST FOR FILTERED TOKENS ----

 [('[', 'RB'), ('47', 'CD'), (']', 'JJ'), ('Androutsopoulos', 'NNP'), (',', ','), ('I.', 'NNP'), (',', ','), ('Paliouras', 'NNP'), (',', ','), ('G.', 'NNP'), (',', ','), ('Karkaletsis', 'NNP'), (',', ','), ('V.', 'NNP'), (',', ','), ('Sakkis', 'NNP'), (',', ','), ('G.', 'NNP'), (',', ','), ('Spyropoulos', 'NNP'), (',', ','), ('C.', 'NNP'), ('D.', 'NNP'), (',', ','), ('&', 'CC'), ('Stamatopoulos', 'NNP'), (',', ','), ('P.', 'NNP'), ('(', '('), ('2000', 'CD'), (')', ')'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['[ 47', '47 ]', '] Androutsopoulos', 'Androutsopoulos ,', ', I.', 'I. ,', ', Paliouras', 'Paliouras ,', ', G.', 'G. ,', ', Karkaletsis', 'Karkaletsis ,', ', V.', 'V. ,', ', Sakkis', 'Sakkis ,', ', G.', 'G. ,', ', Spyropoulos', 'Spyropoulos ,', ', C.', 'C. D.', 'D. ,', ', &', '& Stamatopoulos', 'Stamatopoulos ,', ', P.', 'P. (', '( 2000', '2000 )', ') .'] 

 TOTAL BIGRAMS --> 31 



 ---- TRI-GRAMS ---- 

 ['[ 47 ]', '47 ] Androutsopoulos', '] Androutsopoulos ,', 'Androutsopoulos , I.', ', I. ,', 'I. , Paliouras', ', Paliouras ,', 'Paliouras , G.', ', G. ,', 'G. , Karkaletsis', ', Karkaletsis ,', 'Karkaletsis , V.', ', V. ,', 'V. , Sakkis', ', Sakkis ,', 'Sakkis , G.', ', G. ,', 'G. , Spyropoulos', ', Spyropoulos ,', 'Spyropoulos , C.', ', C. D.', 'C. D. ,', 'D. , &', ', & Stamatopoulos', '& Stamatopoulos ,', 'Stamatopoulos , P.', ', P. (', 'P. ( 2000', '( 2000 )', '2000 ) .'] 

 TOTAL TRIGRAMS --> 30 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> ['Androutsopoulos', 'Paliouras', 'Karkaletsis', 'Sakkis', 'Spyropoulos', 'Stamatopoulos']
 TOTAL GPE ENTITY --> 6 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['[', '47', ']', 'androutsopoulo', ',', 'i.', ',', 'palioura', ',', 'g.', ',', 'karkaletsi', ',', 'v.', ',', 'sakki', ',', 'g.', ',', 'spyropoulo', ',', 'c.', 'd.', ',', '&', 'stamatopoulo', ',', 'p.', '(', '2000', ')', '.']

 TOTAL PORTER STEM WORDS ==> 32



 ---- SNOWBALL STEMMING ----

['[', '47', ']', 'androutsopoulo', ',', 'i.', ',', 'palioura', ',', 'g.', ',', 'karkaletsi', ',', 'v.', ',', 'sakki', ',', 'g.', ',', 'spyropoulo', ',', 'c.', 'd.', ',', '&', 'stamatopoulo', ',', 'p.', '(', '2000', ')', '.']

 TOTAL SNOWBALL STEM WORDS ==> 32



 ---- LEMMATIZATION ----

['[', '47', ']', 'Androutsopoulos', ',', 'I.', ',', 'Paliouras', ',', 'G.', ',', 'Karkaletsis', ',', 'V.', ',', 'Sakkis', ',', 'G.', ',', 'Spyropoulos', ',', 'C.', 'D.', ',', '&', 'Stamatopoulos', ',', 'P.', '(', '2000', ')', '.']

 TOTAL LEMMATIZE WORDS ==> 32

************************************************************************************************************************

542 --> Learning to filter spam e-mail: A comparison of a naive bayesian  and a memory-based approach. 


 ---- TOKENS ----

 ['Learning', 'to', 'filter', 'spam', 'e-mail', ':', 'A', 'comparison', 'of', 'a', 'naive', 'bayesian', 'and', 'a', 'memory-based', 'approach', '.'] 

 TOTAL TOKENS ==> 17

 ---- POST ----

 [('Learning', 'VBG'), ('to', 'TO'), ('filter', 'VB'), ('spam', 'JJ'), ('e-mail', 'NN'), (':', ':'), ('A', 'DT'), ('comparison', 'NN'), ('of', 'IN'), ('a', 'DT'), ('naive', 'JJ'), ('bayesian', 'NN'), ('and', 'CC'), ('a', 'DT'), ('memory-based', 'JJ'), ('approach', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Learning', 'filter', 'spam', 'e-mail', ':', 'comparison', 'naive', 'bayesian', 'memory-based', 'approach', '.']

 TOTAL FILTERED TOKENS ==>  11

 ---- POST FOR FILTERED TOKENS ----

 [('Learning', 'VBG'), ('filter', 'JJ'), ('spam', 'JJ'), ('e-mail', 'NN'), (':', ':'), ('comparison', 'NN'), ('naive', 'JJ'), ('bayesian', 'JJ'), ('memory-based', 'JJ'), ('approach', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Learning filter', 'filter spam', 'spam e-mail', 'e-mail :', ': comparison', 'comparison naive', 'naive bayesian', 'bayesian memory-based', 'memory-based approach', 'approach .'] 

 TOTAL BIGRAMS --> 10 



 ---- TRI-GRAMS ---- 

 ['Learning filter spam', 'filter spam e-mail', 'spam e-mail :', 'e-mail : comparison', ': comparison naive', 'comparison naive bayesian', 'naive bayesian memory-based', 'bayesian memory-based approach', 'memory-based approach .'] 

 TOTAL TRIGRAMS --> 9 



 ---- NOUN PHRASES ---- 

 ['filter spam e-mail', 'comparison', 'naive bayesian memory-based approach'] 

 TOTAL NOUN PHRASES --> 3 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['learn', 'filter', 'spam', 'e-mail', ':', 'comparison', 'naiv', 'bayesian', 'memory-bas', 'approach', '.']

 TOTAL PORTER STEM WORDS ==> 11



 ---- SNOWBALL STEMMING ----

['learn', 'filter', 'spam', 'e-mail', ':', 'comparison', 'naiv', 'bayesian', 'memory-bas', 'approach', '.']

 TOTAL SNOWBALL STEM WORDS ==> 11



 ---- LEMMATIZATION ----

['Learning', 'filter', 'spam', 'e-mail', ':', 'comparison', 'naive', 'bayesian', 'memory-based', 'approach', '.']

 TOTAL LEMMATIZE WORDS ==> 11

************************************************************************************************************************

543 --> arXiv preprint cs/0009009. 


 ---- TOKENS ----

 ['arXiv', 'preprint', 'cs/0009009', '.'] 

 TOTAL TOKENS ==> 4

 ---- POST ----

 [('arXiv', 'JJ'), ('preprint', 'NN'), ('cs/0009009', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['arXiv', 'preprint', 'cs/0009009', '.']

 TOTAL FILTERED TOKENS ==>  4

 ---- POST FOR FILTERED TOKENS ----

 [('arXiv', 'JJ'), ('preprint', 'NN'), ('cs/0009009', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['arXiv preprint', 'preprint cs/0009009', 'cs/0009009 .'] 

 TOTAL BIGRAMS --> 3 



 ---- TRI-GRAMS ---- 

 ['arXiv preprint cs/0009009', 'preprint cs/0009009 .'] 

 TOTAL TRIGRAMS --> 2 



 ---- NOUN PHRASES ---- 

 ['arXiv preprint', 'cs'] 

 TOTAL NOUN PHRASES --> 2 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['arxiv', 'preprint', 'cs/0009009', '.']

 TOTAL PORTER STEM WORDS ==> 4



 ---- SNOWBALL STEMMING ----

['arxiv', 'preprint', 'cs/0009009', '.']

 TOTAL SNOWBALL STEM WORDS ==> 4



 ---- LEMMATIZATION ----

['arXiv', 'preprint', 'cs/0009009', '.']

 TOTAL LEMMATIZE WORDS ==> 4

************************************************************************************************************************

544 --> [48] Rennie, J. 


 ---- TOKENS ----

 ['[', '48', ']', 'Rennie', ',', 'J', '.'] 

 TOTAL TOKENS ==> 7

 ---- POST ----

 [('[', 'RB'), ('48', 'CD'), (']', 'JJ'), ('Rennie', 'NNP'), (',', ','), ('J', 'NNP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['[', '48', ']', 'Rennie', ',', 'J', '.']

 TOTAL FILTERED TOKENS ==>  7

 ---- POST FOR FILTERED TOKENS ----

 [('[', 'RB'), ('48', 'CD'), (']', 'JJ'), ('Rennie', 'NNP'), (',', ','), ('J', 'NNP'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['[ 48', '48 ]', '] Rennie', 'Rennie ,', ', J', 'J .'] 

 TOTAL BIGRAMS --> 6 



 ---- TRI-GRAMS ---- 

 ['[ 48 ]', '48 ] Rennie', '] Rennie ,', 'Rennie , J', ', J .'] 

 TOTAL TRIGRAMS --> 5 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> ['Rennie']
 TOTAL PERSON ENTITY --> 1 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['[', '48', ']', 'renni', ',', 'j', '.']

 TOTAL PORTER STEM WORDS ==> 7



 ---- SNOWBALL STEMMING ----

['[', '48', ']', 'renni', ',', 'j', '.']

 TOTAL SNOWBALL STEM WORDS ==> 7



 ---- LEMMATIZATION ----

['[', '48', ']', 'Rennie', ',', 'J', '.']

 TOTAL LEMMATIZE WORDS ==> 7

************************************************************************************************************************

545 --> (2000, August). 


 ---- TOKENS ----

 ['(', '2000', ',', 'August', ')', '.'] 

 TOTAL TOKENS ==> 6

 ---- POST ----

 [('(', '('), ('2000', 'CD'), (',', ','), ('August', 'NNP'), (')', ')'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['(', '2000', ',', 'August', ')', '.']

 TOTAL FILTERED TOKENS ==>  6

 ---- POST FOR FILTERED TOKENS ----

 [('(', '('), ('2000', 'CD'), (',', ','), ('August', 'NNP'), (')', ')'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['( 2000', '2000 ,', ', August', 'August )', ') .'] 

 TOTAL BIGRAMS --> 5 



 ---- TRI-GRAMS ---- 

 ['( 2000 ,', '2000 , August', ', August )', 'August ) .'] 

 TOTAL TRIGRAMS --> 4 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['(', '2000', ',', 'august', ')', '.']

 TOTAL PORTER STEM WORDS ==> 6



 ---- SNOWBALL STEMMING ----

['(', '2000', ',', 'august', ')', '.']

 TOTAL SNOWBALL STEM WORDS ==> 6



 ---- LEMMATIZATION ----

['(', '2000', ',', 'August', ')', '.']

 TOTAL LEMMATIZE WORDS ==> 6

************************************************************************************************************************

546 --> ifile: An application of machine learning to e-mail filtering. 


 ---- TOKENS ----

 ['ifile', ':', 'An', 'application', 'of', 'machine', 'learning', 'to', 'e-mail', 'filtering', '.'] 

 TOTAL TOKENS ==> 11

 ---- POST ----

 [('ifile', 'NN'), (':', ':'), ('An', 'DT'), ('application', 'NN'), ('of', 'IN'), ('machine', 'NN'), ('learning', 'VBG'), ('to', 'TO'), ('e-mail', 'JJ'), ('filtering', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['ifile', ':', 'application', 'machine', 'learning', 'e-mail', 'filtering', '.']

 TOTAL FILTERED TOKENS ==>  8

 ---- POST FOR FILTERED TOKENS ----

 [('ifile', 'NN'), (':', ':'), ('application', 'NN'), ('machine', 'NN'), ('learning', 'VBG'), ('e-mail', 'JJ'), ('filtering', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['ifile :', ': application', 'application machine', 'machine learning', 'learning e-mail', 'e-mail filtering', 'filtering .'] 

 TOTAL BIGRAMS --> 7 



 ---- TRI-GRAMS ---- 

 ['ifile : application', ': application machine', 'application machine learning', 'machine learning e-mail', 'learning e-mail filtering', 'e-mail filtering .'] 

 TOTAL TRIGRAMS --> 6 



 ---- NOUN PHRASES ---- 

 ['ifile', 'application', 'machine', 'e-mail filtering'] 

 TOTAL NOUN PHRASES --> 4 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['ifil', ':', 'applic', 'machin', 'learn', 'e-mail', 'filter', '.']

 TOTAL PORTER STEM WORDS ==> 8



 ---- SNOWBALL STEMMING ----

['ifil', ':', 'applic', 'machin', 'learn', 'e-mail', 'filter', '.']

 TOTAL SNOWBALL STEM WORDS ==> 8



 ---- LEMMATIZATION ----

['ifile', ':', 'application', 'machine', 'learning', 'e-mail', 'filtering', '.']

 TOTAL LEMMATIZE WORDS ==> 8

************************************************************************************************************************

547 --> In Proc. 


 ---- TOKENS ----

 ['In', 'Proc', '.'] 

 TOTAL TOKENS ==> 3

 ---- POST ----

 [('In', 'IN'), ('Proc', 'NNP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Proc', '.']

 TOTAL FILTERED TOKENS ==>  2

 ---- POST FOR FILTERED TOKENS ----

 [('Proc', 'NNP'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Proc .'] 

 TOTAL BIGRAMS --> 1 



 ---- TRI-GRAMS ---- 

 [] 

 TOTAL TRIGRAMS --> 0 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> ['Proc']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['proc', '.']

 TOTAL PORTER STEM WORDS ==> 2



 ---- SNOWBALL STEMMING ----

['proc', '.']

 TOTAL SNOWBALL STEM WORDS ==> 2



 ---- LEMMATIZATION ----

['Proc', '.']

 TOTAL LEMMATIZE WORDS ==> 2

************************************************************************************************************************

548 --> KDD 2000 Workshop on Text Mining, Boston, MA  [49] Drucker, H., Wu, D., & Vapnik, V. N. (1999). 


 ---- TOKENS ----

 ['KDD', '2000', 'Workshop', 'on', 'Text', 'Mining', ',', 'Boston', ',', 'MA', '[', '49', ']', 'Drucker', ',', 'H.', ',', 'Wu', ',', 'D.', ',', '&', 'Vapnik', ',', 'V.', 'N.', '(', '1999', ')', '.'] 

 TOTAL TOKENS ==> 30

 ---- POST ----

 [('KDD', 'NNP'), ('2000', 'CD'), ('Workshop', 'NNP'), ('on', 'IN'), ('Text', 'NNP'), ('Mining', 'NNP'), (',', ','), ('Boston', 'NNP'), (',', ','), ('MA', 'NNP'), ('[', 'VBZ'), ('49', 'CD'), (']', 'NN'), ('Drucker', 'NNP'), (',', ','), ('H.', 'NNP'), (',', ','), ('Wu', 'NNP'), (',', ','), ('D.', 'NNP'), (',', ','), ('&', 'CC'), ('Vapnik', 'NNP'), (',', ','), ('V.', 'NNP'), ('N.', 'NNP'), ('(', '('), ('1999', 'CD'), (')', ')'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['KDD', '2000', 'Workshop', 'Text', 'Mining', ',', 'Boston', ',', '[', '49', ']', 'Drucker', ',', 'H.', ',', 'Wu', ',', 'D.', ',', '&', 'Vapnik', ',', 'V.', 'N.', '(', '1999', ')', '.']

 TOTAL FILTERED TOKENS ==>  28

 ---- POST FOR FILTERED TOKENS ----

 [('KDD', 'NNP'), ('2000', 'CD'), ('Workshop', 'NNP'), ('Text', 'NNP'), ('Mining', 'NNP'), (',', ','), ('Boston', 'NNP'), (',', ','), ('[', 'VBZ'), ('49', 'CD'), (']', 'NN'), ('Drucker', 'NNP'), (',', ','), ('H.', 'NNP'), (',', ','), ('Wu', 'NNP'), (',', ','), ('D.', 'NNP'), (',', ','), ('&', 'CC'), ('Vapnik', 'NNP'), (',', ','), ('V.', 'NNP'), ('N.', 'NNP'), ('(', '('), ('1999', 'CD'), (')', ')'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['KDD 2000', '2000 Workshop', 'Workshop Text', 'Text Mining', 'Mining ,', ', Boston', 'Boston ,', ', [', '[ 49', '49 ]', '] Drucker', 'Drucker ,', ', H.', 'H. ,', ', Wu', 'Wu ,', ', D.', 'D. ,', ', &', '& Vapnik', 'Vapnik ,', ', V.', 'V. N.', 'N. (', '( 1999', '1999 )', ') .'] 

 TOTAL BIGRAMS --> 27 



 ---- TRI-GRAMS ---- 

 ['KDD 2000 Workshop', '2000 Workshop Text', 'Workshop Text Mining', 'Text Mining ,', 'Mining , Boston', ', Boston ,', 'Boston , [', ', [ 49', '[ 49 ]', '49 ] Drucker', '] Drucker ,', 'Drucker , H.', ', H. ,', 'H. , Wu', ', Wu ,', 'Wu , D.', ', D. ,', 'D. , &', ', & Vapnik', '& Vapnik ,', 'Vapnik , V.', ', V. N.', 'V. N. (', 'N. ( 1999', '( 1999 )', '1999 ) .'] 

 TOTAL TRIGRAMS --> 26 



 ---- NOUN PHRASES ---- 

 [']'] 

 TOTAL NOUN PHRASES --> 1 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> ['Drucker', 'Vapnik']
 TOTAL PERSON ENTITY --> 2 


 GPE ---> ['Boston', 'Wu']
 TOTAL GPE ENTITY --> 2 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['kdd', '2000', 'workshop', 'text', 'mine', ',', 'boston', ',', '[', '49', ']', 'drucker', ',', 'h.', ',', 'wu', ',', 'd.', ',', '&', 'vapnik', ',', 'v.', 'n.', '(', '1999', ')', '.']

 TOTAL PORTER STEM WORDS ==> 28



 ---- SNOWBALL STEMMING ----

['kdd', '2000', 'workshop', 'text', 'mine', ',', 'boston', ',', '[', '49', ']', 'drucker', ',', 'h.', ',', 'wu', ',', 'd.', ',', '&', 'vapnik', ',', 'v.', 'n.', '(', '1999', ')', '.']

 TOTAL SNOWBALL STEM WORDS ==> 28



 ---- LEMMATIZATION ----

['KDD', '2000', 'Workshop', 'Text', 'Mining', ',', 'Boston', ',', '[', '49', ']', 'Drucker', ',', 'H.', ',', 'Wu', ',', 'D.', ',', '&', 'Vapnik', ',', 'V.', 'N.', '(', '1999', ')', '.']

 TOTAL LEMMATIZE WORDS ==> 28

************************************************************************************************************************

549 --> Support vector machines for spam  categorization. 


 ---- TOKENS ----

 ['Support', 'vector', 'machines', 'for', 'spam', 'categorization', '.'] 

 TOTAL TOKENS ==> 7

 ---- POST ----

 [('Support', 'NNP'), ('vector', 'NN'), ('machines', 'NNS'), ('for', 'IN'), ('spam', 'JJ'), ('categorization', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Support', 'vector', 'machines', 'spam', 'categorization', '.']

 TOTAL FILTERED TOKENS ==>  6

 ---- POST FOR FILTERED TOKENS ----

 [('Support', 'NNP'), ('vector', 'NN'), ('machines', 'NNS'), ('spam', 'JJ'), ('categorization', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Support vector', 'vector machines', 'machines spam', 'spam categorization', 'categorization .'] 

 TOTAL BIGRAMS --> 5 



 ---- TRI-GRAMS ---- 

 ['Support vector machines', 'vector machines spam', 'machines spam categorization', 'spam categorization .'] 

 TOTAL TRIGRAMS --> 4 



 ---- NOUN PHRASES ---- 

 ['vector', 'spam categorization'] 

 TOTAL NOUN PHRASES --> 2 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> ['Support']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['support', 'vector', 'machin', 'spam', 'categor', '.']

 TOTAL PORTER STEM WORDS ==> 6



 ---- SNOWBALL STEMMING ----

['support', 'vector', 'machin', 'spam', 'categor', '.']

 TOTAL SNOWBALL STEM WORDS ==> 6



 ---- LEMMATIZATION ----

['Support', 'vector', 'machine', 'spam', 'categorization', '.']

 TOTAL LEMMATIZE WORDS ==> 6

************************************************************************************************************************

550 --> IEEE Transactions on Neural networks, 10(5), 1048-1054  [50] Carreras, X., & Marquez, L. (2001). 


 ---- TOKENS ----

 ['IEEE', 'Transactions', 'on', 'Neural', 'networks', ',', '10', '(', '5', ')', ',', '1048-1054', '[', '50', ']', 'Carreras', ',', 'X.', ',', '&', 'Marquez', ',', 'L.', '(', '2001', ')', '.'] 

 TOTAL TOKENS ==> 27

 ---- POST ----

 [('IEEE', 'NNP'), ('Transactions', 'NNP'), ('on', 'IN'), ('Neural', 'NNP'), ('networks', 'NNS'), (',', ','), ('10', 'CD'), ('(', '('), ('5', 'CD'), (')', ')'), (',', ','), ('1048-1054', 'JJ'), ('[', 'NN'), ('50', 'CD'), (']', 'NN'), ('Carreras', 'NNP'), (',', ','), ('X.', 'NNP'), (',', ','), ('&', 'CC'), ('Marquez', 'NNP'), (',', ','), ('L.', 'NNP'), ('(', '('), ('2001', 'CD'), (')', ')'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['IEEE', 'Transactions', 'Neural', 'networks', ',', '10', '(', '5', ')', ',', '1048-1054', '[', '50', ']', 'Carreras', ',', 'X.', ',', '&', 'Marquez', ',', 'L.', '(', '2001', ')', '.']

 TOTAL FILTERED TOKENS ==>  26

 ---- POST FOR FILTERED TOKENS ----

 [('IEEE', 'NNP'), ('Transactions', 'NNP'), ('Neural', 'NNP'), ('networks', 'NNS'), (',', ','), ('10', 'CD'), ('(', '('), ('5', 'CD'), (')', ')'), (',', ','), ('1048-1054', 'JJ'), ('[', 'NN'), ('50', 'CD'), (']', 'NN'), ('Carreras', 'NNP'), (',', ','), ('X.', 'NNP'), (',', ','), ('&', 'CC'), ('Marquez', 'NNP'), (',', ','), ('L.', 'NNP'), ('(', '('), ('2001', 'CD'), (')', ')'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['IEEE Transactions', 'Transactions Neural', 'Neural networks', 'networks ,', ', 10', '10 (', '( 5', '5 )', ') ,', ', 1048-1054', '1048-1054 [', '[ 50', '50 ]', '] Carreras', 'Carreras ,', ', X.', 'X. ,', ', &', '& Marquez', 'Marquez ,', ', L.', 'L. (', '( 2001', '2001 )', ') .'] 

 TOTAL BIGRAMS --> 25 



 ---- TRI-GRAMS ---- 

 ['IEEE Transactions Neural', 'Transactions Neural networks', 'Neural networks ,', 'networks , 10', ', 10 (', '10 ( 5', '( 5 )', '5 ) ,', ') , 1048-1054', ', 1048-1054 [', '1048-1054 [ 50', '[ 50 ]', '50 ] Carreras', '] Carreras ,', 'Carreras , X.', ', X. ,', 'X. , &', ', & Marquez', '& Marquez ,', 'Marquez , L.', ', L. (', 'L. ( 2001', '( 2001 )', '2001 ) .'] 

 TOTAL TRIGRAMS --> 24 



 ---- NOUN PHRASES ---- 

 ['1048-1054 [', ']'] 

 TOTAL NOUN PHRASES --> 2 



 ---- NER ----

 
 ORGANIZATION ---> ['IEEE Transactions']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> ['Carreras', 'Marquez']
 TOTAL PERSON ENTITY --> 2 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['ieee', 'transact', 'neural', 'network', ',', '10', '(', '5', ')', ',', '1048-1054', '[', '50', ']', 'carrera', ',', 'x.', ',', '&', 'marquez', ',', 'l.', '(', '2001', ')', '.']

 TOTAL PORTER STEM WORDS ==> 26



 ---- SNOWBALL STEMMING ----

['ieee', 'transact', 'neural', 'network', ',', '10', '(', '5', ')', ',', '1048-1054', '[', '50', ']', 'carrera', ',', 'x.', ',', '&', 'marquez', ',', 'l.', '(', '2001', ')', '.']

 TOTAL SNOWBALL STEM WORDS ==> 26



 ---- LEMMATIZATION ----

['IEEE', 'Transactions', 'Neural', 'network', ',', '10', '(', '5', ')', ',', '1048-1054', '[', '50', ']', 'Carreras', ',', 'X.', ',', '&', 'Marquez', ',', 'L.', '(', '2001', ')', '.']

 TOTAL LEMMATIZE WORDS ==> 26

************************************************************************************************************************

551 --> Boosting trees for anti-spam email filtering. 


 ---- TOKENS ----

 ['Boosting', 'trees', 'for', 'anti-spam', 'email', 'filtering', '.'] 

 TOTAL TOKENS ==> 7

 ---- POST ----

 [('Boosting', 'VBG'), ('trees', 'NNS'), ('for', 'IN'), ('anti-spam', 'JJ'), ('email', 'NN'), ('filtering', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Boosting', 'trees', 'anti-spam', 'email', 'filtering', '.']

 TOTAL FILTERED TOKENS ==>  6

 ---- POST FOR FILTERED TOKENS ----

 [('Boosting', 'VBG'), ('trees', 'NNS'), ('anti-spam', 'JJ'), ('email', 'NN'), ('filtering', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Boosting trees', 'trees anti-spam', 'anti-spam email', 'email filtering', 'filtering .'] 

 TOTAL BIGRAMS --> 5 



 ---- TRI-GRAMS ---- 

 ['Boosting trees anti-spam', 'trees anti-spam email', 'anti-spam email filtering', 'email filtering .'] 

 TOTAL TRIGRAMS --> 4 



 ---- NOUN PHRASES ---- 

 ['anti-spam email', 'filtering'] 

 TOTAL NOUN PHRASES --> 2 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['boost', 'tree', 'anti-spam', 'email', 'filter', '.']

 TOTAL PORTER STEM WORDS ==> 6



 ---- SNOWBALL STEMMING ----

['boost', 'tree', 'anti-spam', 'email', 'filter', '.']

 TOTAL SNOWBALL STEM WORDS ==> 6



 ---- LEMMATIZATION ----

['Boosting', 'tree', 'anti-spam', 'email', 'filtering', '.']

 TOTAL LEMMATIZE WORDS ==> 6

************************************************************************************************************************

552 --> arXiv  preprint cs/0109015  [51] BERGER, A. L., DELLA PIETRA, S. A., AND DELLA PIETRA, V. J. 


 ---- TOKENS ----

 ['arXiv', 'preprint', 'cs/0109015', '[', '51', ']', 'BERGER', ',', 'A.', 'L.', ',', 'DELLA', 'PIETRA', ',', 'S.', 'A.', ',', 'AND', 'DELLA', 'PIETRA', ',', 'V.', 'J', '.'] 

 TOTAL TOKENS ==> 24

 ---- POST ----

 [('arXiv', 'JJ'), ('preprint', 'NN'), ('cs/0109015', 'NN'), ('[', 'VBD'), ('51', 'CD'), (']', 'NNP'), ('BERGER', 'NNP'), (',', ','), ('A.', 'NNP'), ('L.', 'NNP'), (',', ','), ('DELLA', 'NNP'), ('PIETRA', 'NNP'), (',', ','), ('S.', 'NNP'), ('A.', 'NNP'), (',', ','), ('AND', 'NNP'), ('DELLA', 'NNP'), ('PIETRA', 'NNP'), (',', ','), ('V.', 'NNP'), ('J', 'NNP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['arXiv', 'preprint', 'cs/0109015', '[', '51', ']', 'BERGER', ',', 'A.', 'L.', ',', 'DELLA', 'PIETRA', ',', 'S.', 'A.', ',', 'DELLA', 'PIETRA', ',', 'V.', 'J', '.']

 TOTAL FILTERED TOKENS ==>  23

 ---- POST FOR FILTERED TOKENS ----

 [('arXiv', 'JJ'), ('preprint', 'NN'), ('cs/0109015', 'NN'), ('[', 'VBD'), ('51', 'CD'), (']', 'NNP'), ('BERGER', 'NNP'), (',', ','), ('A.', 'NNP'), ('L.', 'NNP'), (',', ','), ('DELLA', 'NNP'), ('PIETRA', 'NNP'), (',', ','), ('S.', 'NNP'), ('A.', 'NNP'), (',', ','), ('DELLA', 'NNP'), ('PIETRA', 'NNP'), (',', ','), ('V.', 'NNP'), ('J', 'NNP'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['arXiv preprint', 'preprint cs/0109015', 'cs/0109015 [', '[ 51', '51 ]', '] BERGER', 'BERGER ,', ', A.', 'A. L.', 'L. ,', ', DELLA', 'DELLA PIETRA', 'PIETRA ,', ', S.', 'S. A.', 'A. ,', ', DELLA', 'DELLA PIETRA', 'PIETRA ,', ', V.', 'V. J', 'J .'] 

 TOTAL BIGRAMS --> 22 



 ---- TRI-GRAMS ---- 

 ['arXiv preprint cs/0109015', 'preprint cs/0109015 [', 'cs/0109015 [ 51', '[ 51 ]', '51 ] BERGER', '] BERGER ,', 'BERGER , A.', ', A. L.', 'A. L. ,', 'L. , DELLA', ', DELLA PIETRA', 'DELLA PIETRA ,', 'PIETRA , S.', ', S. A.', 'S. A. ,', 'A. , DELLA', ', DELLA PIETRA', 'DELLA PIETRA ,', 'PIETRA , V.', ', V. J', 'V. J .'] 

 TOTAL TRIGRAMS --> 21 



 ---- NOUN PHRASES ---- 

 ['arXiv preprint', 'cs'] 

 TOTAL NOUN PHRASES --> 2 



 ---- NER ----

 
 ORGANIZATION ---> ['DELLA', 'DELLA']
 TOTAL ORGANIZATION ENTITY --> 2 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['arxiv', 'preprint', 'cs/0109015', '[', '51', ']', 'berger', ',', 'a.', 'l.', ',', 'della', 'pietra', ',', 's.', 'a.', ',', 'della', 'pietra', ',', 'v.', 'j', '.']

 TOTAL PORTER STEM WORDS ==> 23



 ---- SNOWBALL STEMMING ----

['arxiv', 'preprint', 'cs/0109015', '[', '51', ']', 'berger', ',', 'a.', 'l.', ',', 'della', 'pietra', ',', 's.', 'a.', ',', 'della', 'pietra', ',', 'v.', 'j', '.']

 TOTAL SNOWBALL STEM WORDS ==> 23



 ---- LEMMATIZATION ----

['arXiv', 'preprint', 'cs/0109015', '[', '51', ']', 'BERGER', ',', 'A.', 'L.', ',', 'DELLA', 'PIETRA', ',', 'S.', 'A.', ',', 'DELLA', 'PIETRA', ',', 'V.', 'J', '.']

 TOTAL LEMMATIZE WORDS ==> 23

************************************************************************************************************************

553 --> 1996. 


 ---- TOKENS ----

 ['1996', '.'] 

 TOTAL TOKENS ==> 2

 ---- POST ----

 [('1996', 'CD'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['1996', '.']

 TOTAL FILTERED TOKENS ==>  2

 ---- POST FOR FILTERED TOKENS ----

 [('1996', 'CD'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['1996 .'] 

 TOTAL BIGRAMS --> 1 



 ---- TRI-GRAMS ---- 

 [] 

 TOTAL TRIGRAMS --> 0 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['1996', '.']

 TOTAL PORTER STEM WORDS ==> 2



 ---- SNOWBALL STEMMING ----

['1996', '.']

 TOTAL SNOWBALL STEM WORDS ==> 2



 ---- LEMMATIZATION ----

['1996', '.']

 TOTAL LEMMATIZE WORDS ==> 2

************************************************************************************************************************

554 --> A  maximum entropy approach to natural language processing. 


 ---- TOKENS ----

 ['A', 'maximum', 'entropy', 'approach', 'to', 'natural', 'language', 'processing', '.'] 

 TOTAL TOKENS ==> 9

 ---- POST ----

 [('A', 'DT'), ('maximum', 'JJ'), ('entropy', 'NN'), ('approach', 'NN'), ('to', 'TO'), ('natural', 'JJ'), ('language', 'NN'), ('processing', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['maximum', 'entropy', 'approach', 'natural', 'language', 'processing', '.']

 TOTAL FILTERED TOKENS ==>  7

 ---- POST FOR FILTERED TOKENS ----

 [('maximum', 'JJ'), ('entropy', 'NN'), ('approach', 'NN'), ('natural', 'JJ'), ('language', 'NN'), ('processing', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['maximum entropy', 'entropy approach', 'approach natural', 'natural language', 'language processing', 'processing .'] 

 TOTAL BIGRAMS --> 6 



 ---- TRI-GRAMS ---- 

 ['maximum entropy approach', 'entropy approach natural', 'approach natural language', 'natural language processing', 'language processing .'] 

 TOTAL TRIGRAMS --> 5 



 ---- NOUN PHRASES ---- 

 ['maximum entropy', 'approach', 'natural language', 'processing'] 

 TOTAL NOUN PHRASES --> 4 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['maximum', 'entropi', 'approach', 'natur', 'languag', 'process', '.']

 TOTAL PORTER STEM WORDS ==> 7



 ---- SNOWBALL STEMMING ----

['maximum', 'entropi', 'approach', 'natur', 'languag', 'process', '.']

 TOTAL SNOWBALL STEM WORDS ==> 7



 ---- LEMMATIZATION ----

['maximum', 'entropy', 'approach', 'natural', 'language', 'processing', '.']

 TOTAL LEMMATIZE WORDS ==> 7

************************************************************************************************************************

555 --> Computational Linguistics 22, 1,  39–71  [52] Sakkis, G., Androutsopoulos, I., Paliouras, G., Karkaletsis, V., Spyropoulos, C. D., &  Stamatopoulos, P. (2001). 


 ---- TOKENS ----

 ['Computational', 'Linguistics', '22', ',', '1', ',', '39–71', '[', '52', ']', 'Sakkis', ',', 'G.', ',', 'Androutsopoulos', ',', 'I.', ',', 'Paliouras', ',', 'G.', ',', 'Karkaletsis', ',', 'V.', ',', 'Spyropoulos', ',', 'C.', 'D.', ',', '&', 'Stamatopoulos', ',', 'P.', '(', '2001', ')', '.'] 

 TOTAL TOKENS ==> 39

 ---- POST ----

 [('Computational', 'JJ'), ('Linguistics', 'NNS'), ('22', 'CD'), (',', ','), ('1', 'CD'), (',', ','), ('39–71', 'CD'), ('[', 'NN'), ('52', 'CD'), (']', 'NN'), ('Sakkis', 'NNP'), (',', ','), ('G.', 'NNP'), (',', ','), ('Androutsopoulos', 'NNP'), (',', ','), ('I.', 'NNP'), (',', ','), ('Paliouras', 'NNP'), (',', ','), ('G.', 'NNP'), (',', ','), ('Karkaletsis', 'NNP'), (',', ','), ('V.', 'NNP'), (',', ','), ('Spyropoulos', 'NNP'), (',', ','), ('C.', 'NNP'), ('D.', 'NNP'), (',', ','), ('&', 'CC'), ('Stamatopoulos', 'NNP'), (',', ','), ('P.', 'NNP'), ('(', '('), ('2001', 'CD'), (')', ')'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Computational', 'Linguistics', '22', ',', '1', ',', '39–71', '[', '52', ']', 'Sakkis', ',', 'G.', ',', 'Androutsopoulos', ',', 'I.', ',', 'Paliouras', ',', 'G.', ',', 'Karkaletsis', ',', 'V.', ',', 'Spyropoulos', ',', 'C.', 'D.', ',', '&', 'Stamatopoulos', ',', 'P.', '(', '2001', ')', '.']

 TOTAL FILTERED TOKENS ==>  39

 ---- POST FOR FILTERED TOKENS ----

 [('Computational', 'JJ'), ('Linguistics', 'NNS'), ('22', 'CD'), (',', ','), ('1', 'CD'), (',', ','), ('39–71', 'CD'), ('[', 'NN'), ('52', 'CD'), (']', 'NN'), ('Sakkis', 'NNP'), (',', ','), ('G.', 'NNP'), (',', ','), ('Androutsopoulos', 'NNP'), (',', ','), ('I.', 'NNP'), (',', ','), ('Paliouras', 'NNP'), (',', ','), ('G.', 'NNP'), (',', ','), ('Karkaletsis', 'NNP'), (',', ','), ('V.', 'NNP'), (',', ','), ('Spyropoulos', 'NNP'), (',', ','), ('C.', 'NNP'), ('D.', 'NNP'), (',', ','), ('&', 'CC'), ('Stamatopoulos', 'NNP'), (',', ','), ('P.', 'NNP'), ('(', '('), ('2001', 'CD'), (')', ')'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Computational Linguistics', 'Linguistics 22', '22 ,', ', 1', '1 ,', ', 39–71', '39–71 [', '[ 52', '52 ]', '] Sakkis', 'Sakkis ,', ', G.', 'G. ,', ', Androutsopoulos', 'Androutsopoulos ,', ', I.', 'I. ,', ', Paliouras', 'Paliouras ,', ', G.', 'G. ,', ', Karkaletsis', 'Karkaletsis ,', ', V.', 'V. ,', ', Spyropoulos', 'Spyropoulos ,', ', C.', 'C. D.', 'D. ,', ', &', '& Stamatopoulos', 'Stamatopoulos ,', ', P.', 'P. (', '( 2001', '2001 )', ') .'] 

 TOTAL BIGRAMS --> 38 



 ---- TRI-GRAMS ---- 

 ['Computational Linguistics 22', 'Linguistics 22 ,', '22 , 1', ', 1 ,', '1 , 39–71', ', 39–71 [', '39–71 [ 52', '[ 52 ]', '52 ] Sakkis', '] Sakkis ,', 'Sakkis , G.', ', G. ,', 'G. , Androutsopoulos', ', Androutsopoulos ,', 'Androutsopoulos , I.', ', I. ,', 'I. , Paliouras', ', Paliouras ,', 'Paliouras , G.', ', G. ,', 'G. , Karkaletsis', ', Karkaletsis ,', 'Karkaletsis , V.', ', V. ,', 'V. , Spyropoulos', ', Spyropoulos ,', 'Spyropoulos , C.', ', C. D.', 'C. D. ,', 'D. , &', ', & Stamatopoulos', '& Stamatopoulos ,', 'Stamatopoulos , P.', ', P. (', 'P. ( 2001', '( 2001 )', '2001 ) .'] 

 TOTAL TRIGRAMS --> 37 



 ---- NOUN PHRASES ---- 

 ['[', ']'] 

 TOTAL NOUN PHRASES --> 2 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> ['Sakkis']
 TOTAL PERSON ENTITY --> 1 


 GPE ---> ['Androutsopoulos', 'Paliouras', 'Karkaletsis', 'Spyropoulos', 'Stamatopoulos']
 TOTAL GPE ENTITY --> 5 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['comput', 'linguist', '22', ',', '1', ',', '39–71', '[', '52', ']', 'sakki', ',', 'g.', ',', 'androutsopoulo', ',', 'i.', ',', 'palioura', ',', 'g.', ',', 'karkaletsi', ',', 'v.', ',', 'spyropoulo', ',', 'c.', 'd.', ',', '&', 'stamatopoulo', ',', 'p.', '(', '2001', ')', '.']

 TOTAL PORTER STEM WORDS ==> 39



 ---- SNOWBALL STEMMING ----

['comput', 'linguist', '22', ',', '1', ',', '39–71', '[', '52', ']', 'sakki', ',', 'g.', ',', 'androutsopoulo', ',', 'i.', ',', 'palioura', ',', 'g.', ',', 'karkaletsi', ',', 'v.', ',', 'spyropoulo', ',', 'c.', 'd.', ',', '&', 'stamatopoulo', ',', 'p.', '(', '2001', ')', '.']

 TOTAL SNOWBALL STEM WORDS ==> 39



 ---- LEMMATIZATION ----

['Computational', 'Linguistics', '22', ',', '1', ',', '39–71', '[', '52', ']', 'Sakkis', ',', 'G.', ',', 'Androutsopoulos', ',', 'I.', ',', 'Paliouras', ',', 'G.', ',', 'Karkaletsis', ',', 'V.', ',', 'Spyropoulos', ',', 'C.', 'D.', ',', '&', 'Stamatopoulos', ',', 'P.', '(', '2001', ')', '.']

 TOTAL LEMMATIZE WORDS ==> 39

************************************************************************************************************************

556 --> Stacking classifiers for anti-spam filtering of e-mail. 


 ---- TOKENS ----

 ['Stacking', 'classifiers', 'for', 'anti-spam', 'filtering', 'of', 'e-mail', '.'] 

 TOTAL TOKENS ==> 8

 ---- POST ----

 [('Stacking', 'VBG'), ('classifiers', 'NNS'), ('for', 'IN'), ('anti-spam', 'JJ'), ('filtering', 'NN'), ('of', 'IN'), ('e-mail', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Stacking', 'classifiers', 'anti-spam', 'filtering', 'e-mail', '.']

 TOTAL FILTERED TOKENS ==>  6

 ---- POST FOR FILTERED TOKENS ----

 [('Stacking', 'VBG'), ('classifiers', 'NNS'), ('anti-spam', 'JJ'), ('filtering', 'JJ'), ('e-mail', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Stacking classifiers', 'classifiers anti-spam', 'anti-spam filtering', 'filtering e-mail', 'e-mail .'] 

 TOTAL BIGRAMS --> 5 



 ---- TRI-GRAMS ---- 

 ['Stacking classifiers anti-spam', 'classifiers anti-spam filtering', 'anti-spam filtering e-mail', 'filtering e-mail .'] 

 TOTAL TRIGRAMS --> 4 



 ---- NOUN PHRASES ---- 

 ['anti-spam filtering e-mail'] 

 TOTAL NOUN PHRASES --> 1 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['stack', 'classifi', 'anti-spam', 'filter', 'e-mail', '.']

 TOTAL PORTER STEM WORDS ==> 6



 ---- SNOWBALL STEMMING ----

['stack', 'classifi', 'anti-spam', 'filter', 'e-mail', '.']

 TOTAL SNOWBALL STEM WORDS ==> 6



 ---- LEMMATIZATION ----

['Stacking', 'classifier', 'anti-spam', 'filtering', 'e-mail', '.']

 TOTAL LEMMATIZE WORDS ==> 6

************************************************************************************************************************

557 --> arXiv preprint  cs/0106040..  [53] Lewis, D. D. (1998, April). 


 ---- TOKENS ----

 ['arXiv', 'preprint', 'cs/0106040', '..', '[', '53', ']', 'Lewis', ',', 'D.', 'D.', '(', '1998', ',', 'April', ')', '.'] 

 TOTAL TOKENS ==> 17

 ---- POST ----

 [('arXiv', 'JJ'), ('preprint', 'NN'), ('cs/0106040', 'NN'), ('..', 'NNP'), ('[', 'VBZ'), ('53', 'CD'), (']', 'NN'), ('Lewis', 'NNP'), (',', ','), ('D.', 'NNP'), ('D.', 'NNP'), ('(', '('), ('1998', 'CD'), (',', ','), ('April', 'NNP'), (')', ')'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['arXiv', 'preprint', 'cs/0106040', '..', '[', '53', ']', 'Lewis', ',', 'D.', 'D.', '(', '1998', ',', 'April', ')', '.']

 TOTAL FILTERED TOKENS ==>  17

 ---- POST FOR FILTERED TOKENS ----

 [('arXiv', 'JJ'), ('preprint', 'NN'), ('cs/0106040', 'NN'), ('..', 'NNP'), ('[', 'VBZ'), ('53', 'CD'), (']', 'NN'), ('Lewis', 'NNP'), (',', ','), ('D.', 'NNP'), ('D.', 'NNP'), ('(', '('), ('1998', 'CD'), (',', ','), ('April', 'NNP'), (')', ')'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['arXiv preprint', 'preprint cs/0106040', 'cs/0106040 ..', '.. [', '[ 53', '53 ]', '] Lewis', 'Lewis ,', ', D.', 'D. D.', 'D. (', '( 1998', '1998 ,', ', April', 'April )', ') .'] 

 TOTAL BIGRAMS --> 16 



 ---- TRI-GRAMS ---- 

 ['arXiv preprint cs/0106040', 'preprint cs/0106040 ..', 'cs/0106040 .. [', '.. [ 53', '[ 53 ]', '53 ] Lewis', '] Lewis ,', 'Lewis , D.', ', D. D.', 'D. D. (', 'D. ( 1998', '( 1998 ,', '1998 , April', ', April )', 'April ) .'] 

 TOTAL TRIGRAMS --> 15 



 ---- NOUN PHRASES ---- 

 ['arXiv preprint', 'cs', ']'] 

 TOTAL NOUN PHRASES --> 3 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> ['Lewis']
 TOTAL PERSON ENTITY --> 1 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['arxiv', 'preprint', 'cs/0106040', '..', '[', '53', ']', 'lewi', ',', 'd.', 'd.', '(', '1998', ',', 'april', ')', '.']

 TOTAL PORTER STEM WORDS ==> 17



 ---- SNOWBALL STEMMING ----

['arxiv', 'preprint', 'cs/0106040', '..', '[', '53', ']', 'lewi', ',', 'd.', 'd.', '(', '1998', ',', 'april', ')', '.']

 TOTAL SNOWBALL STEM WORDS ==> 17



 ---- LEMMATIZATION ----

['arXiv', 'preprint', 'cs/0106040', '..', '[', '53', ']', 'Lewis', ',', 'D.', 'D.', '(', '1998', ',', 'April', ')', '.']

 TOTAL LEMMATIZE WORDS ==> 17

************************************************************************************************************************

558 --> Naive (Bayes) at forty: The independence assumption in  information retrieval. 


 ---- TOKENS ----

 ['Naive', '(', 'Bayes', ')', 'at', 'forty', ':', 'The', 'independence', 'assumption', 'in', 'information', 'retrieval', '.'] 

 TOTAL TOKENS ==> 14

 ---- POST ----

 [('Naive', 'NNP'), ('(', '('), ('Bayes', 'NNP'), (')', ')'), ('at', 'IN'), ('forty', 'NN'), (':', ':'), ('The', 'DT'), ('independence', 'NN'), ('assumption', 'NN'), ('in', 'IN'), ('information', 'NN'), ('retrieval', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Naive', '(', 'Bayes', ')', 'forty', ':', 'independence', 'assumption', 'information', 'retrieval', '.']

 TOTAL FILTERED TOKENS ==>  11

 ---- POST FOR FILTERED TOKENS ----

 [('Naive', 'NNP'), ('(', '('), ('Bayes', 'NNP'), (')', ')'), ('forty', 'NN'), (':', ':'), ('independence', 'NN'), ('assumption', 'NN'), ('information', 'NN'), ('retrieval', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Naive (', '( Bayes', 'Bayes )', ') forty', 'forty :', ': independence', 'independence assumption', 'assumption information', 'information retrieval', 'retrieval .'] 

 TOTAL BIGRAMS --> 10 



 ---- TRI-GRAMS ---- 

 ['Naive ( Bayes', '( Bayes )', 'Bayes ) forty', ') forty :', 'forty : independence', ': independence assumption', 'independence assumption information', 'assumption information retrieval', 'information retrieval .'] 

 TOTAL TRIGRAMS --> 9 



 ---- NOUN PHRASES ---- 

 ['forty', 'independence', 'assumption', 'information', 'retrieval'] 

 TOTAL NOUN PHRASES --> 5 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> ['Naive']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['naiv', '(', 'bay', ')', 'forti', ':', 'independ', 'assumpt', 'inform', 'retriev', '.']

 TOTAL PORTER STEM WORDS ==> 11



 ---- SNOWBALL STEMMING ----

['naiv', '(', 'bay', ')', 'forti', ':', 'independ', 'assumpt', 'inform', 'retriev', '.']

 TOTAL SNOWBALL STEM WORDS ==> 11



 ---- LEMMATIZATION ----

['Naive', '(', 'Bayes', ')', 'forty', ':', 'independence', 'assumption', 'information', 'retrieval', '.']

 TOTAL LEMMATIZE WORDS ==> 11

************************************************************************************************************************

559 --> In European conference on machine learning (pp. 


 ---- TOKENS ----

 ['In', 'European', 'conference', 'on', 'machine', 'learning', '(', 'pp', '.'] 

 TOTAL TOKENS ==> 9

 ---- POST ----

 [('In', 'IN'), ('European', 'JJ'), ('conference', 'NN'), ('on', 'IN'), ('machine', 'NN'), ('learning', 'NN'), ('(', '('), ('pp', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['European', 'conference', 'machine', 'learning', '(', 'pp', '.']

 TOTAL FILTERED TOKENS ==>  7

 ---- POST FOR FILTERED TOKENS ----

 [('European', 'JJ'), ('conference', 'NN'), ('machine', 'NN'), ('learning', 'NN'), ('(', '('), ('pp', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['European conference', 'conference machine', 'machine learning', 'learning (', '( pp', 'pp .'] 

 TOTAL BIGRAMS --> 6 



 ---- TRI-GRAMS ---- 

 ['European conference machine', 'conference machine learning', 'machine learning (', 'learning ( pp', '( pp .'] 

 TOTAL TRIGRAMS --> 5 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 []

 ---- PORTER STEMMING ----

['european', 'confer', 'machin', 'learn', '(', 'pp', '.']

 TOTAL PORTER STEM WORDS ==> 7



 ---- SNOWBALL STEMMING ----

['european', 'confer', 'machin', 'learn', '(', 'pp', '.']

 TOTAL SNOWBALL STEM WORDS ==> 7



 ---- LEMMATIZATION ----

['European', 'conference', 'machine', 'learning', '(', 'pp', '.']

 TOTAL LEMMATIZE WORDS ==> 7

************************************************************************************************************************

560 --> 4-15). 


 ---- TOKENS ----

 ['4-15', ')', '.'] 

 TOTAL TOKENS ==> 3

 ---- POST ----

 [('4-15', 'JJ'), (')', ')'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['4-15', ')', '.']

 TOTAL FILTERED TOKENS ==>  3

 ---- POST FOR FILTERED TOKENS ----

 [('4-15', 'JJ'), (')', ')'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['4-15 )', ') .'] 

 TOTAL BIGRAMS --> 2 



 ---- TRI-GRAMS ---- 

 ['4-15 ) .'] 

 TOTAL TRIGRAMS --> 1 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 []

 ---- PORTER STEMMING ----

['4-15', ')', '.']

 TOTAL PORTER STEM WORDS ==> 3



 ---- SNOWBALL STEMMING ----

['4-15', ')', '.']

 TOTAL SNOWBALL STEM WORDS ==> 3



 ---- LEMMATIZATION ----

['4-15', ')', '.']

 TOTAL LEMMATIZE WORDS ==> 3

************************************************************************************************************************

561 --> Springer  Berlin Heidelberg  [54] McCallum, A., & Nigam, K. (1998, July). 


 ---- TOKENS ----

 ['Springer', 'Berlin', 'Heidelberg', '[', '54', ']', 'McCallum', ',', 'A.', ',', '&', 'Nigam', ',', 'K.', '(', '1998', ',', 'July', ')', '.'] 

 TOTAL TOKENS ==> 20

 ---- POST ----

 [('Springer', 'NNP'), ('Berlin', 'NNP'), ('Heidelberg', 'NNP'), ('[', 'VBD'), ('54', 'CD'), (']', 'NNP'), ('McCallum', 'NNP'), (',', ','), ('A.', 'NNP'), (',', ','), ('&', 'CC'), ('Nigam', 'NNP'), (',', ','), ('K.', 'NNP'), ('(', '('), ('1998', 'CD'), (',', ','), ('July', 'NNP'), (')', ')'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Springer', 'Berlin', 'Heidelberg', '[', '54', ']', 'McCallum', ',', 'A.', ',', '&', 'Nigam', ',', 'K.', '(', '1998', ',', 'July', ')', '.']

 TOTAL FILTERED TOKENS ==>  20

 ---- POST FOR FILTERED TOKENS ----

 [('Springer', 'NNP'), ('Berlin', 'NNP'), ('Heidelberg', 'NNP'), ('[', 'VBD'), ('54', 'CD'), (']', 'NNP'), ('McCallum', 'NNP'), (',', ','), ('A.', 'NNP'), (',', ','), ('&', 'CC'), ('Nigam', 'NNP'), (',', ','), ('K.', 'NNP'), ('(', '('), ('1998', 'CD'), (',', ','), ('July', 'NNP'), (')', ')'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Springer Berlin', 'Berlin Heidelberg', 'Heidelberg [', '[ 54', '54 ]', '] McCallum', 'McCallum ,', ', A.', 'A. ,', ', &', '& Nigam', 'Nigam ,', ', K.', 'K. (', '( 1998', '1998 ,', ', July', 'July )', ') .'] 

 TOTAL BIGRAMS --> 19 



 ---- TRI-GRAMS ---- 

 ['Springer Berlin Heidelberg', 'Berlin Heidelberg [', 'Heidelberg [ 54', '[ 54 ]', '54 ] McCallum', '] McCallum ,', 'McCallum , A.', ', A. ,', 'A. , &', ', & Nigam', '& Nigam ,', 'Nigam , K.', ', K. (', 'K. ( 1998', '( 1998 ,', '1998 , July', ', July )', 'July ) .'] 

 TOTAL TRIGRAMS --> 18 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> ['Springer', 'Berlin Heidelberg']
 TOTAL PERSON ENTITY --> 2 


 GPE ---> ['Nigam']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['springer', 'berlin', 'heidelberg', '[', '54', ']', 'mccallum', ',', 'a.', ',', '&', 'nigam', ',', 'k.', '(', '1998', ',', 'juli', ')', '.']

 TOTAL PORTER STEM WORDS ==> 20



 ---- SNOWBALL STEMMING ----

['springer', 'berlin', 'heidelberg', '[', '54', ']', 'mccallum', ',', 'a.', ',', '&', 'nigam', ',', 'k.', '(', '1998', ',', 'juli', ')', '.']

 TOTAL SNOWBALL STEM WORDS ==> 20



 ---- LEMMATIZATION ----

['Springer', 'Berlin', 'Heidelberg', '[', '54', ']', 'McCallum', ',', 'A.', ',', '&', 'Nigam', ',', 'K.', '(', '1998', ',', 'July', ')', '.']

 TOTAL LEMMATIZE WORDS ==> 20

************************************************************************************************************************

562 --> A comparison of event models for naive bayes  text classification. 


 ---- TOKENS ----

 ['A', 'comparison', 'of', 'event', 'models', 'for', 'naive', 'bayes', 'text', 'classification', '.'] 

 TOTAL TOKENS ==> 11

 ---- POST ----

 [('A', 'DT'), ('comparison', 'NN'), ('of', 'IN'), ('event', 'NN'), ('models', 'NNS'), ('for', 'IN'), ('naive', 'JJ'), ('bayes', 'NNS'), ('text', 'JJ'), ('classification', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['comparison', 'event', 'models', 'naive', 'bayes', 'text', 'classification', '.']

 TOTAL FILTERED TOKENS ==>  8

 ---- POST FOR FILTERED TOKENS ----

 [('comparison', 'NN'), ('event', 'NN'), ('models', 'NNS'), ('naive', 'JJ'), ('bayes', 'NNS'), ('text', 'JJ'), ('classification', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['comparison event', 'event models', 'models naive', 'naive bayes', 'bayes text', 'text classification', 'classification .'] 

 TOTAL BIGRAMS --> 7 



 ---- TRI-GRAMS ---- 

 ['comparison event models', 'event models naive', 'models naive bayes', 'naive bayes text', 'bayes text classification', 'text classification .'] 

 TOTAL TRIGRAMS --> 6 



 ---- NOUN PHRASES ---- 

 ['comparison', 'event', 'text classification'] 

 TOTAL NOUN PHRASES --> 3 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['comparison', 'event', 'model', 'naiv', 'bay', 'text', 'classif', '.']

 TOTAL PORTER STEM WORDS ==> 8



 ---- SNOWBALL STEMMING ----

['comparison', 'event', 'model', 'naiv', 'bay', 'text', 'classif', '.']

 TOTAL SNOWBALL STEM WORDS ==> 8



 ---- LEMMATIZATION ----

['comparison', 'event', 'model', 'naive', 'bayes', 'text', 'classification', '.']

 TOTAL LEMMATIZE WORDS ==> 8

************************************************************************************************************************

563 --> In AAAI-98 workshop on learning for text categorization (Vol. 


 ---- TOKENS ----

 ['In', 'AAAI-98', 'workshop', 'on', 'learning', 'for', 'text', 'categorization', '(', 'Vol', '.'] 

 TOTAL TOKENS ==> 11

 ---- POST ----

 [('In', 'IN'), ('AAAI-98', 'NNP'), ('workshop', 'NN'), ('on', 'IN'), ('learning', 'VBG'), ('for', 'IN'), ('text', 'JJ'), ('categorization', 'NN'), ('(', '('), ('Vol', 'NNP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['AAAI-98', 'workshop', 'learning', 'text', 'categorization', '(', 'Vol', '.']

 TOTAL FILTERED TOKENS ==>  8

 ---- POST FOR FILTERED TOKENS ----

 [('AAAI-98', 'NNP'), ('workshop', 'NN'), ('learning', 'VBG'), ('text', 'JJ'), ('categorization', 'NN'), ('(', '('), ('Vol', 'NNP'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['AAAI-98 workshop', 'workshop learning', 'learning text', 'text categorization', 'categorization (', '( Vol', 'Vol .'] 

 TOTAL BIGRAMS --> 7 



 ---- TRI-GRAMS ---- 

 ['AAAI-98 workshop learning', 'workshop learning text', 'learning text categorization', 'text categorization (', 'categorization ( Vol', '( Vol .'] 

 TOTAL TRIGRAMS --> 6 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 []

 ---- PORTER STEMMING ----

['aaai-98', 'workshop', 'learn', 'text', 'categor', '(', 'vol', '.']

 TOTAL PORTER STEM WORDS ==> 8



 ---- SNOWBALL STEMMING ----

['aaai-98', 'workshop', 'learn', 'text', 'categor', '(', 'vol', '.']

 TOTAL SNOWBALL STEM WORDS ==> 8



 ---- LEMMATIZATION ----

['AAAI-98', 'workshop', 'learning', 'text', 'categorization', '(', 'Vol', '.']

 TOTAL LEMMATIZE WORDS ==> 8

************************************************************************************************************************

564 --> 752, pp. 


 ---- TOKENS ----

 ['752', ',', 'pp', '.'] 

 TOTAL TOKENS ==> 4

 ---- POST ----

 [('752', 'CD'), (',', ','), ('pp', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['752', ',', 'pp', '.']

 TOTAL FILTERED TOKENS ==>  4

 ---- POST FOR FILTERED TOKENS ----

 [('752', 'CD'), (',', ','), ('pp', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['752 ,', ', pp', 'pp .'] 

 TOTAL BIGRAMS --> 3 



 ---- TRI-GRAMS ---- 

 ['752 , pp', ', pp .'] 

 TOTAL TRIGRAMS --> 2 



 ---- NOUN PHRASES ---- 

 ['pp'] 

 TOTAL NOUN PHRASES --> 1 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['752', ',', 'pp', '.']

 TOTAL PORTER STEM WORDS ==> 4



 ---- SNOWBALL STEMMING ----

['752', ',', 'pp', '.']

 TOTAL SNOWBALL STEM WORDS ==> 4



 ---- LEMMATIZATION ----

['752', ',', 'pp', '.']

 TOTAL LEMMATIZE WORDS ==> 4

************************************************************************************************************************

565 --> 41-48). 


 ---- TOKENS ----

 ['41-48', ')', '.'] 

 TOTAL TOKENS ==> 3

 ---- POST ----

 [('41-48', 'JJ'), (')', ')'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['41-48', ')', '.']

 TOTAL FILTERED TOKENS ==>  3

 ---- POST FOR FILTERED TOKENS ----

 [('41-48', 'JJ'), (')', ')'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['41-48 )', ') .'] 

 TOTAL BIGRAMS --> 2 



 ---- TRI-GRAMS ---- 

 ['41-48 ) .'] 

 TOTAL TRIGRAMS --> 1 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 []

 ---- PORTER STEMMING ----

['41-48', ')', '.']

 TOTAL PORTER STEM WORDS ==> 3



 ---- SNOWBALL STEMMING ----

['41-48', ')', '.']

 TOTAL SNOWBALL STEM WORDS ==> 3



 ---- LEMMATIZATION ----

['41-48', ')', '.']

 TOTAL LEMMATIZE WORDS ==> 3

************************************************************************************************************************

566 --> [55] McCallum, A., & Nigam, K. (1998, July). 


 ---- TOKENS ----

 ['[', '55', ']', 'McCallum', ',', 'A.', ',', '&', 'Nigam', ',', 'K.', '(', '1998', ',', 'July', ')', '.'] 

 TOTAL TOKENS ==> 17

 ---- POST ----

 [('[', 'RB'), ('55', 'CD'), (']', 'JJ'), ('McCallum', 'NNP'), (',', ','), ('A.', 'NNP'), (',', ','), ('&', 'CC'), ('Nigam', 'NNP'), (',', ','), ('K.', 'NNP'), ('(', '('), ('1998', 'CD'), (',', ','), ('July', 'NNP'), (')', ')'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['[', '55', ']', 'McCallum', ',', 'A.', ',', '&', 'Nigam', ',', 'K.', '(', '1998', ',', 'July', ')', '.']

 TOTAL FILTERED TOKENS ==>  17

 ---- POST FOR FILTERED TOKENS ----

 [('[', 'RB'), ('55', 'CD'), (']', 'JJ'), ('McCallum', 'NNP'), (',', ','), ('A.', 'NNP'), (',', ','), ('&', 'CC'), ('Nigam', 'NNP'), (',', ','), ('K.', 'NNP'), ('(', '('), ('1998', 'CD'), (',', ','), ('July', 'NNP'), (')', ')'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['[ 55', '55 ]', '] McCallum', 'McCallum ,', ', A.', 'A. ,', ', &', '& Nigam', 'Nigam ,', ', K.', 'K. (', '( 1998', '1998 ,', ', July', 'July )', ') .'] 

 TOTAL BIGRAMS --> 16 



 ---- TRI-GRAMS ---- 

 ['[ 55 ]', '55 ] McCallum', '] McCallum ,', 'McCallum , A.', ', A. ,', 'A. , &', ', & Nigam', '& Nigam ,', 'Nigam , K.', ', K. (', 'K. ( 1998', '( 1998 ,', '1998 , July', ', July )', 'July ) .'] 

 TOTAL TRIGRAMS --> 15 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> ['McCallum']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> ['Nigam']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['[', '55', ']', 'mccallum', ',', 'a.', ',', '&', 'nigam', ',', 'k.', '(', '1998', ',', 'juli', ')', '.']

 TOTAL PORTER STEM WORDS ==> 17



 ---- SNOWBALL STEMMING ----

['[', '55', ']', 'mccallum', ',', 'a.', ',', '&', 'nigam', ',', 'k.', '(', '1998', ',', 'juli', ')', '.']

 TOTAL SNOWBALL STEM WORDS ==> 17



 ---- LEMMATIZATION ----

['[', '55', ']', 'McCallum', ',', 'A.', ',', '&', 'Nigam', ',', 'K.', '(', '1998', ',', 'July', ')', '.']

 TOTAL LEMMATIZE WORDS ==> 17

************************************************************************************************************************

567 --> A comparison of event models for naive bayes  text classification. 


 ---- TOKENS ----

 ['A', 'comparison', 'of', 'event', 'models', 'for', 'naive', 'bayes', 'text', 'classification', '.'] 

 TOTAL TOKENS ==> 11

 ---- POST ----

 [('A', 'DT'), ('comparison', 'NN'), ('of', 'IN'), ('event', 'NN'), ('models', 'NNS'), ('for', 'IN'), ('naive', 'JJ'), ('bayes', 'NNS'), ('text', 'JJ'), ('classification', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['comparison', 'event', 'models', 'naive', 'bayes', 'text', 'classification', '.']

 TOTAL FILTERED TOKENS ==>  8

 ---- POST FOR FILTERED TOKENS ----

 [('comparison', 'NN'), ('event', 'NN'), ('models', 'NNS'), ('naive', 'JJ'), ('bayes', 'NNS'), ('text', 'JJ'), ('classification', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['comparison event', 'event models', 'models naive', 'naive bayes', 'bayes text', 'text classification', 'classification .'] 

 TOTAL BIGRAMS --> 7 



 ---- TRI-GRAMS ---- 

 ['comparison event models', 'event models naive', 'models naive bayes', 'naive bayes text', 'bayes text classification', 'text classification .'] 

 TOTAL TRIGRAMS --> 6 



 ---- NOUN PHRASES ---- 

 ['comparison', 'event', 'text classification'] 

 TOTAL NOUN PHRASES --> 3 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['comparison', 'event', 'model', 'naiv', 'bay', 'text', 'classif', '.']

 TOTAL PORTER STEM WORDS ==> 8



 ---- SNOWBALL STEMMING ----

['comparison', 'event', 'model', 'naiv', 'bay', 'text', 'classif', '.']

 TOTAL SNOWBALL STEM WORDS ==> 8



 ---- LEMMATIZATION ----

['comparison', 'event', 'model', 'naive', 'bayes', 'text', 'classification', '.']

 TOTAL LEMMATIZE WORDS ==> 8

************************************************************************************************************************

568 --> In AAAI-98 workshop on learning for text categorization (Vol. 


 ---- TOKENS ----

 ['In', 'AAAI-98', 'workshop', 'on', 'learning', 'for', 'text', 'categorization', '(', 'Vol', '.'] 

 TOTAL TOKENS ==> 11

 ---- POST ----

 [('In', 'IN'), ('AAAI-98', 'NNP'), ('workshop', 'NN'), ('on', 'IN'), ('learning', 'VBG'), ('for', 'IN'), ('text', 'JJ'), ('categorization', 'NN'), ('(', '('), ('Vol', 'NNP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['AAAI-98', 'workshop', 'learning', 'text', 'categorization', '(', 'Vol', '.']

 TOTAL FILTERED TOKENS ==>  8

 ---- POST FOR FILTERED TOKENS ----

 [('AAAI-98', 'NNP'), ('workshop', 'NN'), ('learning', 'VBG'), ('text', 'JJ'), ('categorization', 'NN'), ('(', '('), ('Vol', 'NNP'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['AAAI-98 workshop', 'workshop learning', 'learning text', 'text categorization', 'categorization (', '( Vol', 'Vol .'] 

 TOTAL BIGRAMS --> 7 



 ---- TRI-GRAMS ---- 

 ['AAAI-98 workshop learning', 'workshop learning text', 'learning text categorization', 'text categorization (', 'categorization ( Vol', '( Vol .'] 

 TOTAL TRIGRAMS --> 6 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 []

 ---- PORTER STEMMING ----

['aaai-98', 'workshop', 'learn', 'text', 'categor', '(', 'vol', '.']

 TOTAL PORTER STEM WORDS ==> 8



 ---- SNOWBALL STEMMING ----

['aaai-98', 'workshop', 'learn', 'text', 'categor', '(', 'vol', '.']

 TOTAL SNOWBALL STEM WORDS ==> 8



 ---- LEMMATIZATION ----

['AAAI-98', 'workshop', 'learning', 'text', 'categorization', '(', 'Vol', '.']

 TOTAL LEMMATIZE WORDS ==> 8

************************************************************************************************************************

569 --> 752, pp. 


 ---- TOKENS ----

 ['752', ',', 'pp', '.'] 

 TOTAL TOKENS ==> 4

 ---- POST ----

 [('752', 'CD'), (',', ','), ('pp', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['752', ',', 'pp', '.']

 TOTAL FILTERED TOKENS ==>  4

 ---- POST FOR FILTERED TOKENS ----

 [('752', 'CD'), (',', ','), ('pp', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['752 ,', ', pp', 'pp .'] 

 TOTAL BIGRAMS --> 3 



 ---- TRI-GRAMS ---- 

 ['752 , pp', ', pp .'] 

 TOTAL TRIGRAMS --> 2 



 ---- NOUN PHRASES ---- 

 ['pp'] 

 TOTAL NOUN PHRASES --> 1 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['752', ',', 'pp', '.']

 TOTAL PORTER STEM WORDS ==> 4



 ---- SNOWBALL STEMMING ----

['752', ',', 'pp', '.']

 TOTAL SNOWBALL STEM WORDS ==> 4



 ---- LEMMATIZATION ----

['752', ',', 'pp', '.']

 TOTAL LEMMATIZE WORDS ==> 4

************************************************************************************************************************

570 --> 41-48). 


 ---- TOKENS ----

 ['41-48', ')', '.'] 

 TOTAL TOKENS ==> 3

 ---- POST ----

 [('41-48', 'JJ'), (')', ')'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['41-48', ')', '.']

 TOTAL FILTERED TOKENS ==>  3

 ---- POST FOR FILTERED TOKENS ----

 [('41-48', 'JJ'), (')', ')'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['41-48 )', ') .'] 

 TOTAL BIGRAMS --> 2 



 ---- TRI-GRAMS ---- 

 ['41-48 ) .'] 

 TOTAL TRIGRAMS --> 1 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 []

 ---- PORTER STEMMING ----

['41-48', ')', '.']

 TOTAL PORTER STEM WORDS ==> 3



 ---- SNOWBALL STEMMING ----

['41-48', ')', '.']

 TOTAL SNOWBALL STEM WORDS ==> 3



 ---- LEMMATIZATION ----

['41-48', ')', '.']

 TOTAL LEMMATIZE WORDS ==> 3

************************************************************************************************************************

571 --> [56] Porter, M. F. (1980). 


 ---- TOKENS ----

 ['[', '56', ']', 'Porter', ',', 'M.', 'F.', '(', '1980', ')', '.'] 

 TOTAL TOKENS ==> 11

 ---- POST ----

 [('[', 'RB'), ('56', 'CD'), (']', 'JJ'), ('Porter', 'NNP'), (',', ','), ('M.', 'NNP'), ('F.', 'NNP'), ('(', '('), ('1980', 'CD'), (')', ')'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['[', '56', ']', 'Porter', ',', 'M.', 'F.', '(', '1980', ')', '.']

 TOTAL FILTERED TOKENS ==>  11

 ---- POST FOR FILTERED TOKENS ----

 [('[', 'RB'), ('56', 'CD'), (']', 'JJ'), ('Porter', 'NNP'), (',', ','), ('M.', 'NNP'), ('F.', 'NNP'), ('(', '('), ('1980', 'CD'), (')', ')'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['[ 56', '56 ]', '] Porter', 'Porter ,', ', M.', 'M. F.', 'F. (', '( 1980', '1980 )', ') .'] 

 TOTAL BIGRAMS --> 10 



 ---- TRI-GRAMS ---- 

 ['[ 56 ]', '56 ] Porter', '] Porter ,', 'Porter , M.', ', M. F.', 'M. F. (', 'F. ( 1980', '( 1980 )', '1980 ) .'] 

 TOTAL TRIGRAMS --> 9 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['[', '56', ']', 'porter', ',', 'm.', 'f.', '(', '1980', ')', '.']

 TOTAL PORTER STEM WORDS ==> 11



 ---- SNOWBALL STEMMING ----

['[', '56', ']', 'porter', ',', 'm.', 'f.', '(', '1980', ')', '.']

 TOTAL SNOWBALL STEM WORDS ==> 11



 ---- LEMMATIZATION ----

['[', '56', ']', 'Porter', ',', 'M.', 'F.', '(', '1980', ')', '.']

 TOTAL LEMMATIZE WORDS ==> 11

************************************************************************************************************************

572 --> An algorithm for suffix stripping. 


 ---- TOKENS ----

 ['An', 'algorithm', 'for', 'suffix', 'stripping', '.'] 

 TOTAL TOKENS ==> 6

 ---- POST ----

 [('An', 'DT'), ('algorithm', 'NN'), ('for', 'IN'), ('suffix', 'NN'), ('stripping', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['algorithm', 'suffix', 'stripping', '.']

 TOTAL FILTERED TOKENS ==>  4

 ---- POST FOR FILTERED TOKENS ----

 [('algorithm', 'JJ'), ('suffix', 'NN'), ('stripping', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['algorithm suffix', 'suffix stripping', 'stripping .'] 

 TOTAL BIGRAMS --> 3 



 ---- TRI-GRAMS ---- 

 ['algorithm suffix stripping', 'suffix stripping .'] 

 TOTAL TRIGRAMS --> 2 



 ---- NOUN PHRASES ---- 

 ['algorithm suffix', 'stripping'] 

 TOTAL NOUN PHRASES --> 2 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['algorithm', 'suffix', 'strip', '.']

 TOTAL PORTER STEM WORDS ==> 4



 ---- SNOWBALL STEMMING ----

['algorithm', 'suffix', 'strip', '.']

 TOTAL SNOWBALL STEM WORDS ==> 4



 ---- LEMMATIZATION ----

['algorithm', 'suffix', 'stripping', '.']

 TOTAL LEMMATIZE WORDS ==> 4

************************************************************************************************************************

573 --> Program, 14(3), 130-137   [57] Hayes, P. J. 


 ---- TOKENS ----

 ['Program', ',', '14', '(', '3', ')', ',', '130-137', '[', '57', ']', 'Hayes', ',', 'P.', 'J', '.'] 

 TOTAL TOKENS ==> 16

 ---- POST ----

 [('Program', 'NNP'), (',', ','), ('14', 'CD'), ('(', '('), ('3', 'CD'), (')', ')'), (',', ','), ('130-137', 'JJ'), ('[', 'NN'), ('57', 'CD'), (']', 'JJ'), ('Hayes', 'NNP'), (',', ','), ('P.', 'NNP'), ('J', 'NNP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Program', ',', '14', '(', '3', ')', ',', '130-137', '[', '57', ']', 'Hayes', ',', 'P.', 'J', '.']

 TOTAL FILTERED TOKENS ==>  16

 ---- POST FOR FILTERED TOKENS ----

 [('Program', 'NNP'), (',', ','), ('14', 'CD'), ('(', '('), ('3', 'CD'), (')', ')'), (',', ','), ('130-137', 'JJ'), ('[', 'NN'), ('57', 'CD'), (']', 'JJ'), ('Hayes', 'NNP'), (',', ','), ('P.', 'NNP'), ('J', 'NNP'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Program ,', ', 14', '14 (', '( 3', '3 )', ') ,', ', 130-137', '130-137 [', '[ 57', '57 ]', '] Hayes', 'Hayes ,', ', P.', 'P. J', 'J .'] 

 TOTAL BIGRAMS --> 15 



 ---- TRI-GRAMS ---- 

 ['Program , 14', ', 14 (', '14 ( 3', '( 3 )', '3 ) ,', ') , 130-137', ', 130-137 [', '130-137 [ 57', '[ 57 ]', '57 ] Hayes', '] Hayes ,', 'Hayes , P.', ', P. J', 'P. J .'] 

 TOTAL TRIGRAMS --> 14 



 ---- NOUN PHRASES ---- 

 ['130-137 ['] 

 TOTAL NOUN PHRASES --> 1 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> ['Program']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['program', ',', '14', '(', '3', ')', ',', '130-137', '[', '57', ']', 'hay', ',', 'p.', 'j', '.']

 TOTAL PORTER STEM WORDS ==> 16



 ---- SNOWBALL STEMMING ----

['program', ',', '14', '(', '3', ')', ',', '130-137', '[', '57', ']', 'hay', ',', 'p.', 'j', '.']

 TOTAL SNOWBALL STEM WORDS ==> 16



 ---- LEMMATIZATION ----

['Program', ',', '14', '(', '3', ')', ',', '130-137', '[', '57', ']', 'Hayes', ',', 'P.', 'J', '.']

 TOTAL LEMMATIZE WORDS ==> 16

************************************************************************************************************************

574 --> (1992). 


 ---- TOKENS ----

 ['(', '1992', ')', '.'] 

 TOTAL TOKENS ==> 4

 ---- POST ----

 [('(', '('), ('1992', 'CD'), (')', ')'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['(', '1992', ')', '.']

 TOTAL FILTERED TOKENS ==>  4

 ---- POST FOR FILTERED TOKENS ----

 [('(', '('), ('1992', 'CD'), (')', ')'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['( 1992', '1992 )', ') .'] 

 TOTAL BIGRAMS --> 3 



 ---- TRI-GRAMS ---- 

 ['( 1992 )', '1992 ) .'] 

 TOTAL TRIGRAMS --> 2 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['(', '1992', ')', '.']

 TOTAL PORTER STEM WORDS ==> 4



 ---- SNOWBALL STEMMING ----

['(', '1992', ')', '.']

 TOTAL SNOWBALL STEM WORDS ==> 4



 ---- LEMMATIZATION ----

['(', '1992', ')', '.']

 TOTAL LEMMATIZE WORDS ==> 4

************************************************************************************************************************

575 --> Intelligent high-volume text processing using shallow, domain- specific techniques. 


 ---- TOKENS ----

 ['Intelligent', 'high-volume', 'text', 'processing', 'using', 'shallow', ',', 'domain-', 'specific', 'techniques', '.'] 

 TOTAL TOKENS ==> 11

 ---- POST ----

 [('Intelligent', 'JJ'), ('high-volume', 'JJ'), ('text', 'NN'), ('processing', 'NN'), ('using', 'VBG'), ('shallow', 'JJ'), (',', ','), ('domain-', 'JJ'), ('specific', 'JJ'), ('techniques', 'NNS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Intelligent', 'high-volume', 'text', 'processing', 'using', 'shallow', ',', 'domain-', 'specific', 'techniques', '.']

 TOTAL FILTERED TOKENS ==>  11

 ---- POST FOR FILTERED TOKENS ----

 [('Intelligent', 'JJ'), ('high-volume', 'JJ'), ('text', 'NN'), ('processing', 'NN'), ('using', 'VBG'), ('shallow', 'JJ'), (',', ','), ('domain-', 'JJ'), ('specific', 'JJ'), ('techniques', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Intelligent high-volume', 'high-volume text', 'text processing', 'processing using', 'using shallow', 'shallow ,', ', domain-', 'domain- specific', 'specific techniques', 'techniques .'] 

 TOTAL BIGRAMS --> 10 



 ---- TRI-GRAMS ---- 

 ['Intelligent high-volume text', 'high-volume text processing', 'text processing using', 'processing using shallow', 'using shallow ,', 'shallow , domain-', ', domain- specific', 'domain- specific techniques', 'specific techniques .'] 

 TOTAL TRIGRAMS --> 9 



 ---- NOUN PHRASES ---- 

 ['Intelligent high-volume text', 'processing'] 

 TOTAL NOUN PHRASES --> 2 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['intellig', 'high-volum', 'text', 'process', 'use', 'shallow', ',', 'domain-', 'specif', 'techniqu', '.']

 TOTAL PORTER STEM WORDS ==> 11



 ---- SNOWBALL STEMMING ----

['intellig', 'high-volum', 'text', 'process', 'use', 'shallow', ',', 'domain-', 'specif', 'techniqu', '.']

 TOTAL SNOWBALL STEM WORDS ==> 11



 ---- LEMMATIZATION ----

['Intelligent', 'high-volume', 'text', 'processing', 'using', 'shallow', ',', 'domain-', 'specific', 'technique', '.']

 TOTAL LEMMATIZE WORDS ==> 11

************************************************************************************************************************

576 --> Text-based intelligent systems: Current research and practice in  information extraction and retrieval, 227-242  [58] Morin, E. (1999, August). 


 ---- TOKENS ----

 ['Text-based', 'intelligent', 'systems', ':', 'Current', 'research', 'and', 'practice', 'in', 'information', 'extraction', 'and', 'retrieval', ',', '227-242', '[', '58', ']', 'Morin', ',', 'E.', '(', '1999', ',', 'August', ')', '.'] 

 TOTAL TOKENS ==> 27

 ---- POST ----

 [('Text-based', 'JJ'), ('intelligent', 'JJ'), ('systems', 'NNS'), (':', ':'), ('Current', 'JJ'), ('research', 'NN'), ('and', 'CC'), ('practice', 'NN'), ('in', 'IN'), ('information', 'NN'), ('extraction', 'NN'), ('and', 'CC'), ('retrieval', 'NN'), (',', ','), ('227-242', 'JJ'), ('[', 'NN'), ('58', 'CD'), (']', 'NN'), ('Morin', 'NNP'), (',', ','), ('E.', 'NNP'), ('(', '('), ('1999', 'CD'), (',', ','), ('August', 'NNP'), (')', ')'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Text-based', 'intelligent', 'systems', ':', 'Current', 'research', 'practice', 'information', 'extraction', 'retrieval', ',', '227-242', '[', '58', ']', 'Morin', ',', 'E.', '(', '1999', ',', 'August', ')', '.']

 TOTAL FILTERED TOKENS ==>  24

 ---- POST FOR FILTERED TOKENS ----

 [('Text-based', 'JJ'), ('intelligent', 'JJ'), ('systems', 'NNS'), (':', ':'), ('Current', 'NNP'), ('research', 'NN'), ('practice', 'NN'), ('information', 'NN'), ('extraction', 'NN'), ('retrieval', 'NN'), (',', ','), ('227-242', 'JJ'), ('[', 'NN'), ('58', 'CD'), (']', 'NN'), ('Morin', 'NNP'), (',', ','), ('E.', 'NNP'), ('(', '('), ('1999', 'CD'), (',', ','), ('August', 'NNP'), (')', ')'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Text-based intelligent', 'intelligent systems', 'systems :', ': Current', 'Current research', 'research practice', 'practice information', 'information extraction', 'extraction retrieval', 'retrieval ,', ', 227-242', '227-242 [', '[ 58', '58 ]', '] Morin', 'Morin ,', ', E.', 'E. (', '( 1999', '1999 ,', ', August', 'August )', ') .'] 

 TOTAL BIGRAMS --> 23 



 ---- TRI-GRAMS ---- 

 ['Text-based intelligent systems', 'intelligent systems :', 'systems : Current', ': Current research', 'Current research practice', 'research practice information', 'practice information extraction', 'information extraction retrieval', 'extraction retrieval ,', 'retrieval , 227-242', ', 227-242 [', '227-242 [ 58', '[ 58 ]', '58 ] Morin', '] Morin ,', 'Morin , E.', ', E. (', 'E. ( 1999', '( 1999 ,', '1999 , August', ', August )', 'August ) .'] 

 TOTAL TRIGRAMS --> 22 



 ---- NOUN PHRASES ---- 

 ['research', 'practice', 'information', 'extraction', 'retrieval', '227-242 [', ']'] 

 TOTAL NOUN PHRASES --> 7 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> ['Morin']
 TOTAL PERSON ENTITY --> 1 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['text-bas', 'intellig', 'system', ':', 'current', 'research', 'practic', 'inform', 'extract', 'retriev', ',', '227-242', '[', '58', ']', 'morin', ',', 'e.', '(', '1999', ',', 'august', ')', '.']

 TOTAL PORTER STEM WORDS ==> 24



 ---- SNOWBALL STEMMING ----

['text-bas', 'intellig', 'system', ':', 'current', 'research', 'practic', 'inform', 'extract', 'retriev', ',', '227-242', '[', '58', ']', 'morin', ',', 'e.', '(', '1999', ',', 'august', ')', '.']

 TOTAL SNOWBALL STEM WORDS ==> 24



 ---- LEMMATIZATION ----

['Text-based', 'intelligent', 'system', ':', 'Current', 'research', 'practice', 'information', 'extraction', 'retrieval', ',', '227-242', '[', '58', ']', 'Morin', ',', 'E.', '(', '1999', ',', 'August', ')', '.']

 TOTAL LEMMATIZE WORDS ==> 24

************************************************************************************************************************

577 --> Automatic acquisition of semantic relations between terms  from technical corpora. 


 ---- TOKENS ----

 ['Automatic', 'acquisition', 'of', 'semantic', 'relations', 'between', 'terms', 'from', 'technical', 'corpora', '.'] 

 TOTAL TOKENS ==> 11

 ---- POST ----

 [('Automatic', 'JJ'), ('acquisition', 'NN'), ('of', 'IN'), ('semantic', 'JJ'), ('relations', 'NNS'), ('between', 'IN'), ('terms', 'NNS'), ('from', 'IN'), ('technical', 'JJ'), ('corpora', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Automatic', 'acquisition', 'semantic', 'relations', 'terms', 'technical', 'corpora', '.']

 TOTAL FILTERED TOKENS ==>  8

 ---- POST FOR FILTERED TOKENS ----

 [('Automatic', 'JJ'), ('acquisition', 'NN'), ('semantic', 'JJ'), ('relations', 'NNS'), ('terms', 'NNS'), ('technical', 'JJ'), ('corpora', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Automatic acquisition', 'acquisition semantic', 'semantic relations', 'relations terms', 'terms technical', 'technical corpora', 'corpora .'] 

 TOTAL BIGRAMS --> 7 



 ---- TRI-GRAMS ---- 

 ['Automatic acquisition semantic', 'acquisition semantic relations', 'semantic relations terms', 'relations terms technical', 'terms technical corpora', 'technical corpora .'] 

 TOTAL TRIGRAMS --> 6 



 ---- NOUN PHRASES ---- 

 ['Automatic acquisition', 'technical corpora'] 

 TOTAL NOUN PHRASES --> 2 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> ['Automatic']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['automat', 'acquisit', 'semant', 'relat', 'term', 'technic', 'corpora', '.']

 TOTAL PORTER STEM WORDS ==> 8



 ---- SNOWBALL STEMMING ----

['automat', 'acquisit', 'semant', 'relat', 'term', 'technic', 'corpora', '.']

 TOTAL SNOWBALL STEM WORDS ==> 8



 ---- LEMMATIZATION ----

['Automatic', 'acquisition', 'semantic', 'relation', 'term', 'technical', 'corpus', '.']

 TOTAL LEMMATIZE WORDS ==> 8

************************************************************************************************************************

578 --> In Proc. 


 ---- TOKENS ----

 ['In', 'Proc', '.'] 

 TOTAL TOKENS ==> 3

 ---- POST ----

 [('In', 'IN'), ('Proc', 'NNP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Proc', '.']

 TOTAL FILTERED TOKENS ==>  2

 ---- POST FOR FILTERED TOKENS ----

 [('Proc', 'NNP'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Proc .'] 

 TOTAL BIGRAMS --> 1 



 ---- TRI-GRAMS ---- 

 [] 

 TOTAL TRIGRAMS --> 0 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> ['Proc']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['proc', '.']

 TOTAL PORTER STEM WORDS ==> 2



 ---- SNOWBALL STEMMING ----

['proc', '.']

 TOTAL SNOWBALL STEM WORDS ==> 2



 ---- LEMMATIZATION ----

['Proc', '.']

 TOTAL LEMMATIZE WORDS ==> 2

************************************************************************************************************************

579 --> of the Fifth International Congress on Terminology and  Knowledge Engineering-TKE’99. 


 ---- TOKENS ----

 ['of', 'the', 'Fifth', 'International', 'Congress', 'on', 'Terminology', 'and', 'Knowledge', 'Engineering-TKE', '’', '99', '.'] 

 TOTAL TOKENS ==> 13

 ---- POST ----

 [('of', 'IN'), ('the', 'DT'), ('Fifth', 'NNP'), ('International', 'NNP'), ('Congress', 'NNP'), ('on', 'IN'), ('Terminology', 'NNP'), ('and', 'CC'), ('Knowledge', 'NNP'), ('Engineering-TKE', 'NNP'), ('’', 'NNP'), ('99', 'CD'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Fifth', 'International', 'Congress', 'Terminology', 'Knowledge', 'Engineering-TKE', '’', '99', '.']

 TOTAL FILTERED TOKENS ==>  9

 ---- POST FOR FILTERED TOKENS ----

 [('Fifth', 'NNP'), ('International', 'NNP'), ('Congress', 'NNP'), ('Terminology', 'NNP'), ('Knowledge', 'NNP'), ('Engineering-TKE', 'NNP'), ('’', 'NNP'), ('99', 'CD'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Fifth International', 'International Congress', 'Congress Terminology', 'Terminology Knowledge', 'Knowledge Engineering-TKE', 'Engineering-TKE ’', '’ 99', '99 .'] 

 TOTAL BIGRAMS --> 8 



 ---- TRI-GRAMS ---- 

 ['Fifth International Congress', 'International Congress Terminology', 'Congress Terminology Knowledge', 'Terminology Knowledge Engineering-TKE', 'Knowledge Engineering-TKE ’', 'Engineering-TKE ’ 99', '’ 99 .'] 

 TOTAL TRIGRAMS --> 7 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> ['International Congress Terminology']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> ['Fifth']
 TOTAL PERSON ENTITY --> 1 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['fifth', 'intern', 'congress', 'terminolog', 'knowledg', 'engineering-tk', '’', '99', '.']

 TOTAL PORTER STEM WORDS ==> 9



 ---- SNOWBALL STEMMING ----

['fifth', 'intern', 'congress', 'terminolog', 'knowledg', 'engineering-tk', '’', '99', '.']

 TOTAL SNOWBALL STEM WORDS ==> 9



 ---- LEMMATIZATION ----

['Fifth', 'International', 'Congress', 'Terminology', 'Knowledge', 'Engineering-TKE', '’', '99', '.']

 TOTAL LEMMATIZE WORDS ==> 9

************************************************************************************************************************

580 --> [59] Bondale, N., Maloor, P., Vaidyanathan, A., Sengupta, S., & Rao, P. V. (1999). 


 ---- TOKENS ----

 ['[', '59', ']', 'Bondale', ',', 'N.', ',', 'Maloor', ',', 'P.', ',', 'Vaidyanathan', ',', 'A.', ',', 'Sengupta', ',', 'S.', ',', '&', 'Rao', ',', 'P.', 'V.', '(', '1999', ')', '.'] 

 TOTAL TOKENS ==> 28

 ---- POST ----

 [('[', 'RB'), ('59', 'CD'), (']', 'JJ'), ('Bondale', 'NNP'), (',', ','), ('N.', 'NNP'), (',', ','), ('Maloor', 'NNP'), (',', ','), ('P.', 'NNP'), (',', ','), ('Vaidyanathan', 'NNP'), (',', ','), ('A.', 'NNP'), (',', ','), ('Sengupta', 'NNP'), (',', ','), ('S.', 'NNP'), (',', ','), ('&', 'CC'), ('Rao', 'NNP'), (',', ','), ('P.', 'NNP'), ('V.', 'NNP'), ('(', '('), ('1999', 'CD'), (')', ')'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['[', '59', ']', 'Bondale', ',', 'N.', ',', 'Maloor', ',', 'P.', ',', 'Vaidyanathan', ',', 'A.', ',', 'Sengupta', ',', 'S.', ',', '&', 'Rao', ',', 'P.', 'V.', '(', '1999', ')', '.']

 TOTAL FILTERED TOKENS ==>  28

 ---- POST FOR FILTERED TOKENS ----

 [('[', 'RB'), ('59', 'CD'), (']', 'JJ'), ('Bondale', 'NNP'), (',', ','), ('N.', 'NNP'), (',', ','), ('Maloor', 'NNP'), (',', ','), ('P.', 'NNP'), (',', ','), ('Vaidyanathan', 'NNP'), (',', ','), ('A.', 'NNP'), (',', ','), ('Sengupta', 'NNP'), (',', ','), ('S.', 'NNP'), (',', ','), ('&', 'CC'), ('Rao', 'NNP'), (',', ','), ('P.', 'NNP'), ('V.', 'NNP'), ('(', '('), ('1999', 'CD'), (')', ')'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['[ 59', '59 ]', '] Bondale', 'Bondale ,', ', N.', 'N. ,', ', Maloor', 'Maloor ,', ', P.', 'P. ,', ', Vaidyanathan', 'Vaidyanathan ,', ', A.', 'A. ,', ', Sengupta', 'Sengupta ,', ', S.', 'S. ,', ', &', '& Rao', 'Rao ,', ', P.', 'P. V.', 'V. (', '( 1999', '1999 )', ') .'] 

 TOTAL BIGRAMS --> 27 



 ---- TRI-GRAMS ---- 

 ['[ 59 ]', '59 ] Bondale', '] Bondale ,', 'Bondale , N.', ', N. ,', 'N. , Maloor', ', Maloor ,', 'Maloor , P.', ', P. ,', 'P. , Vaidyanathan', ', Vaidyanathan ,', 'Vaidyanathan , A.', ', A. ,', 'A. , Sengupta', ', Sengupta ,', 'Sengupta , S.', ', S. ,', 'S. , &', ', & Rao', '& Rao ,', 'Rao , P.', ', P. V.', 'P. V. (', 'V. ( 1999', '( 1999 )', '1999 ) .'] 

 TOTAL TRIGRAMS --> 26 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> ['Rao']
 TOTAL PERSON ENTITY --> 1 


 GPE ---> ['Bondale', 'Maloor', 'Vaidyanathan', 'Sengupta']
 TOTAL GPE ENTITY --> 4 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['[', '59', ']', 'bondal', ',', 'n.', ',', 'maloor', ',', 'p.', ',', 'vaidyanathan', ',', 'a.', ',', 'sengupta', ',', 's.', ',', '&', 'rao', ',', 'p.', 'v.', '(', '1999', ')', '.']

 TOTAL PORTER STEM WORDS ==> 28



 ---- SNOWBALL STEMMING ----

['[', '59', ']', 'bondal', ',', 'n.', ',', 'maloor', ',', 'p.', ',', 'vaidyanathan', ',', 'a.', ',', 'sengupta', ',', 's.', ',', '&', 'rao', ',', 'p.', 'v.', '(', '1999', ')', '.']

 TOTAL SNOWBALL STEM WORDS ==> 28



 ---- LEMMATIZATION ----

['[', '59', ']', 'Bondale', ',', 'N.', ',', 'Maloor', ',', 'P.', ',', 'Vaidyanathan', ',', 'A.', ',', 'Sengupta', ',', 'S.', ',', '&', 'Rao', ',', 'P.', 'V.', '(', '1999', ')', '.']

 TOTAL LEMMATIZE WORDS ==> 28

************************************************************************************************************************

581 --> Extraction of information from open-ended questionnaires using natural language processing  techniques. 


 ---- TOKENS ----

 ['Extraction', 'of', 'information', 'from', 'open-ended', 'questionnaires', 'using', 'natural', 'language', 'processing', 'techniques', '.'] 

 TOTAL TOKENS ==> 12

 ---- POST ----

 [('Extraction', 'NN'), ('of', 'IN'), ('information', 'NN'), ('from', 'IN'), ('open-ended', 'JJ'), ('questionnaires', 'NNS'), ('using', 'VBG'), ('natural', 'JJ'), ('language', 'NN'), ('processing', 'NN'), ('techniques', 'NNS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Extraction', 'information', 'open-ended', 'questionnaires', 'using', 'natural', 'language', 'processing', 'techniques', '.']

 TOTAL FILTERED TOKENS ==>  10

 ---- POST FOR FILTERED TOKENS ----

 [('Extraction', 'NN'), ('information', 'NN'), ('open-ended', 'JJ'), ('questionnaires', 'NNS'), ('using', 'VBG'), ('natural', 'JJ'), ('language', 'NN'), ('processing', 'NN'), ('techniques', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Extraction information', 'information open-ended', 'open-ended questionnaires', 'questionnaires using', 'using natural', 'natural language', 'language processing', 'processing techniques', 'techniques .'] 

 TOTAL BIGRAMS --> 9 



 ---- TRI-GRAMS ---- 

 ['Extraction information open-ended', 'information open-ended questionnaires', 'open-ended questionnaires using', 'questionnaires using natural', 'using natural language', 'natural language processing', 'language processing techniques', 'processing techniques .'] 

 TOTAL TRIGRAMS --> 8 



 ---- NOUN PHRASES ---- 

 ['Extraction', 'information', 'natural language', 'processing'] 

 TOTAL NOUN PHRASES --> 4 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> ['Extraction']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['extract', 'inform', 'open-end', 'questionnair', 'use', 'natur', 'languag', 'process', 'techniqu', '.']

 TOTAL PORTER STEM WORDS ==> 10



 ---- SNOWBALL STEMMING ----

['extract', 'inform', 'open-end', 'questionnair', 'use', 'natur', 'languag', 'process', 'techniqu', '.']

 TOTAL SNOWBALL STEM WORDS ==> 10



 ---- LEMMATIZATION ----

['Extraction', 'information', 'open-ended', 'questionnaire', 'using', 'natural', 'language', 'processing', 'technique', '.']

 TOTAL LEMMATIZE WORDS ==> 10

************************************************************************************************************************

582 --> Computer Science and Informatics, 29(2), 15-22  [60] Glasgow, B., Mandell, A., Binney, D., Ghemri, L., & Fisher, D. (1998). 


 ---- TOKENS ----

 ['Computer', 'Science', 'and', 'Informatics', ',', '29', '(', '2', ')', ',', '15-22', '[', '60', ']', 'Glasgow', ',', 'B.', ',', 'Mandell', ',', 'A.', ',', 'Binney', ',', 'D.', ',', 'Ghemri', ',', 'L.', ',', '&', 'Fisher', ',', 'D.', '(', '1998', ')', '.'] 

 TOTAL TOKENS ==> 38

 ---- POST ----

 [('Computer', 'NNP'), ('Science', 'NNP'), ('and', 'CC'), ('Informatics', 'NNP'), (',', ','), ('29', 'CD'), ('(', '('), ('2', 'CD'), (')', ')'), (',', ','), ('15-22', 'JJ'), ('[', 'NN'), ('60', 'CD'), (']', 'NN'), ('Glasgow', 'NNP'), (',', ','), ('B.', 'NNP'), (',', ','), ('Mandell', 'NNP'), (',', ','), ('A.', 'NNP'), (',', ','), ('Binney', 'NNP'), (',', ','), ('D.', 'NNP'), (',', ','), ('Ghemri', 'NNP'), (',', ','), ('L.', 'NNP'), (',', ','), ('&', 'CC'), ('Fisher', 'NNP'), (',', ','), ('D.', 'NNP'), ('(', '('), ('1998', 'CD'), (')', ')'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Computer', 'Science', 'Informatics', ',', '29', '(', '2', ')', ',', '15-22', '[', '60', ']', 'Glasgow', ',', 'B.', ',', 'Mandell', ',', 'A.', ',', 'Binney', ',', 'D.', ',', 'Ghemri', ',', 'L.', ',', '&', 'Fisher', ',', 'D.', '(', '1998', ')', '.']

 TOTAL FILTERED TOKENS ==>  37

 ---- POST FOR FILTERED TOKENS ----

 [('Computer', 'NNP'), ('Science', 'NNP'), ('Informatics', 'NNP'), (',', ','), ('29', 'CD'), ('(', '('), ('2', 'CD'), (')', ')'), (',', ','), ('15-22', 'JJ'), ('[', 'NN'), ('60', 'CD'), (']', 'NN'), ('Glasgow', 'NNP'), (',', ','), ('B.', 'NNP'), (',', ','), ('Mandell', 'NNP'), (',', ','), ('A.', 'NNP'), (',', ','), ('Binney', 'NNP'), (',', ','), ('D.', 'NNP'), (',', ','), ('Ghemri', 'NNP'), (',', ','), ('L.', 'NNP'), (',', ','), ('&', 'CC'), ('Fisher', 'NNP'), (',', ','), ('D.', 'NNP'), ('(', '('), ('1998', 'CD'), (')', ')'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Computer Science', 'Science Informatics', 'Informatics ,', ', 29', '29 (', '( 2', '2 )', ') ,', ', 15-22', '15-22 [', '[ 60', '60 ]', '] Glasgow', 'Glasgow ,', ', B.', 'B. ,', ', Mandell', 'Mandell ,', ', A.', 'A. ,', ', Binney', 'Binney ,', ', D.', 'D. ,', ', Ghemri', 'Ghemri ,', ', L.', 'L. ,', ', &', '& Fisher', 'Fisher ,', ', D.', 'D. (', '( 1998', '1998 )', ') .'] 

 TOTAL BIGRAMS --> 36 



 ---- TRI-GRAMS ---- 

 ['Computer Science Informatics', 'Science Informatics ,', 'Informatics , 29', ', 29 (', '29 ( 2', '( 2 )', '2 ) ,', ') , 15-22', ', 15-22 [', '15-22 [ 60', '[ 60 ]', '60 ] Glasgow', '] Glasgow ,', 'Glasgow , B.', ', B. ,', 'B. , Mandell', ', Mandell ,', 'Mandell , A.', ', A. ,', 'A. , Binney', ', Binney ,', 'Binney , D.', ', D. ,', 'D. , Ghemri', ', Ghemri ,', 'Ghemri , L.', ', L. ,', 'L. , &', ', & Fisher', '& Fisher ,', 'Fisher , D.', ', D. (', 'D. ( 1998', '( 1998 )', '1998 ) .'] 

 TOTAL TRIGRAMS --> 35 



 ---- NOUN PHRASES ---- 

 ['15-22 [', ']'] 

 TOTAL NOUN PHRASES --> 2 



 ---- NER ----

 
 ORGANIZATION ---> ['Computer Science Informatics']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> ['Glasgow', 'Mandell', 'Fisher']
 TOTAL PERSON ENTITY --> 3 


 GPE ---> ['Binney', 'Ghemri']
 TOTAL GPE ENTITY --> 2 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['comput', 'scienc', 'informat', ',', '29', '(', '2', ')', ',', '15-22', '[', '60', ']', 'glasgow', ',', 'b.', ',', 'mandel', ',', 'a.', ',', 'binney', ',', 'd.', ',', 'ghemri', ',', 'l.', ',', '&', 'fisher', ',', 'd.', '(', '1998', ')', '.']

 TOTAL PORTER STEM WORDS ==> 37



 ---- SNOWBALL STEMMING ----

['comput', 'scienc', 'informat', ',', '29', '(', '2', ')', ',', '15-22', '[', '60', ']', 'glasgow', ',', 'b.', ',', 'mandel', ',', 'a.', ',', 'binney', ',', 'd.', ',', 'ghemri', ',', 'l.', ',', '&', 'fisher', ',', 'd.', '(', '1998', ')', '.']

 TOTAL SNOWBALL STEM WORDS ==> 37



 ---- LEMMATIZATION ----

['Computer', 'Science', 'Informatics', ',', '29', '(', '2', ')', ',', '15-22', '[', '60', ']', 'Glasgow', ',', 'B.', ',', 'Mandell', ',', 'A.', ',', 'Binney', ',', 'D.', ',', 'Ghemri', ',', 'L.', ',', '&', 'Fisher', ',', 'D.', '(', '1998', ')', '.']

 TOTAL LEMMATIZE WORDS ==> 37

************************************************************************************************************************

583 --> MITA: An  information-extraction approach to the analysis of free-form text in life insurance  applications. 


 ---- TOKENS ----

 ['MITA', ':', 'An', 'information-extraction', 'approach', 'to', 'the', 'analysis', 'of', 'free-form', 'text', 'in', 'life', 'insurance', 'applications', '.'] 

 TOTAL TOKENS ==> 16

 ---- POST ----

 [('MITA', 'NN'), (':', ':'), ('An', 'DT'), ('information-extraction', 'NN'), ('approach', 'NN'), ('to', 'TO'), ('the', 'DT'), ('analysis', 'NN'), ('of', 'IN'), ('free-form', 'JJ'), ('text', 'NN'), ('in', 'IN'), ('life', 'NN'), ('insurance', 'NN'), ('applications', 'NNS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['MITA', ':', 'information-extraction', 'approach', 'analysis', 'free-form', 'text', 'life', 'insurance', 'applications', '.']

 TOTAL FILTERED TOKENS ==>  11

 ---- POST FOR FILTERED TOKENS ----

 [('MITA', 'NN'), (':', ':'), ('information-extraction', 'JJ'), ('approach', 'NN'), ('analysis', 'NN'), ('free-form', 'JJ'), ('text', 'JJ'), ('life', 'NN'), ('insurance', 'NN'), ('applications', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['MITA :', ': information-extraction', 'information-extraction approach', 'approach analysis', 'analysis free-form', 'free-form text', 'text life', 'life insurance', 'insurance applications', 'applications .'] 

 TOTAL BIGRAMS --> 10 



 ---- TRI-GRAMS ---- 

 ['MITA : information-extraction', ': information-extraction approach', 'information-extraction approach analysis', 'approach analysis free-form', 'analysis free-form text', 'free-form text life', 'text life insurance', 'life insurance applications', 'insurance applications .'] 

 TOTAL TRIGRAMS --> 9 



 ---- NOUN PHRASES ---- 

 ['MITA', 'information-extraction approach', 'analysis', 'free-form text life', 'insurance'] 

 TOTAL NOUN PHRASES --> 5 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> ['MITA']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['mita', ':', 'information-extract', 'approach', 'analysi', 'free-form', 'text', 'life', 'insur', 'applic', '.']

 TOTAL PORTER STEM WORDS ==> 11



 ---- SNOWBALL STEMMING ----

['mita', ':', 'information-extract', 'approach', 'analysi', 'free-form', 'text', 'life', 'insur', 'applic', '.']

 TOTAL SNOWBALL STEM WORDS ==> 11



 ---- LEMMATIZATION ----

['MITA', ':', 'information-extraction', 'approach', 'analysis', 'free-form', 'text', 'life', 'insurance', 'application', '.']

 TOTAL LEMMATIZE WORDS ==> 11

************************************************************************************************************************

584 --> AI magazine, 19(1), 59. 


 ---- TOKENS ----

 ['AI', 'magazine', ',', '19', '(', '1', ')', ',', '59', '.'] 

 TOTAL TOKENS ==> 10

 ---- POST ----

 [('AI', 'NNP'), ('magazine', 'NN'), (',', ','), ('19', 'CD'), ('(', '('), ('1', 'CD'), (')', ')'), (',', ','), ('59', 'CD'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['AI', 'magazine', ',', '19', '(', '1', ')', ',', '59', '.']

 TOTAL FILTERED TOKENS ==>  10

 ---- POST FOR FILTERED TOKENS ----

 [('AI', 'NNP'), ('magazine', 'NN'), (',', ','), ('19', 'CD'), ('(', '('), ('1', 'CD'), (')', ')'), (',', ','), ('59', 'CD'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['AI magazine', 'magazine ,', ', 19', '19 (', '( 1', '1 )', ') ,', ', 59', '59 .'] 

 TOTAL BIGRAMS --> 9 



 ---- TRI-GRAMS ---- 

 ['AI magazine ,', 'magazine , 19', ', 19 (', '19 ( 1', '( 1 )', '1 ) ,', ') , 59', ', 59 .'] 

 TOTAL TRIGRAMS --> 8 



 ---- NOUN PHRASES ---- 

 ['magazine'] 

 TOTAL NOUN PHRASES --> 1 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['ai', 'magazin', ',', '19', '(', '1', ')', ',', '59', '.']

 TOTAL PORTER STEM WORDS ==> 10



 ---- SNOWBALL STEMMING ----

['ai', 'magazin', ',', '19', '(', '1', ')', ',', '59', '.']

 TOTAL SNOWBALL STEM WORDS ==> 10



 ---- LEMMATIZATION ----

['AI', 'magazine', ',', '19', '(', '1', ')', ',', '59', '.']

 TOTAL LEMMATIZE WORDS ==> 10

************************************************************************************************************************

585 --> [61] Ahonen, H., Heinonen, O., Klemettinen, M., & Verkamo, A. I. 


 ---- TOKENS ----

 ['[', '61', ']', 'Ahonen', ',', 'H.', ',', 'Heinonen', ',', 'O.', ',', 'Klemettinen', ',', 'M.', ',', '&', 'Verkamo', ',', 'A.', 'I', '.'] 

 TOTAL TOKENS ==> 21

 ---- POST ----

 [('[', 'RB'), ('61', 'CD'), (']', 'JJ'), ('Ahonen', 'NNP'), (',', ','), ('H.', 'NNP'), (',', ','), ('Heinonen', 'NNP'), (',', ','), ('O.', 'NNP'), (',', ','), ('Klemettinen', 'NNP'), (',', ','), ('M.', 'NNP'), (',', ','), ('&', 'CC'), ('Verkamo', 'NNP'), (',', ','), ('A.', 'NNP'), ('I', 'PRP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['[', '61', ']', 'Ahonen', ',', 'H.', ',', 'Heinonen', ',', 'O.', ',', 'Klemettinen', ',', 'M.', ',', '&', 'Verkamo', ',', 'A.', '.']

 TOTAL FILTERED TOKENS ==>  20

 ---- POST FOR FILTERED TOKENS ----

 [('[', 'RB'), ('61', 'CD'), (']', 'JJ'), ('Ahonen', 'NNP'), (',', ','), ('H.', 'NNP'), (',', ','), ('Heinonen', 'NNP'), (',', ','), ('O.', 'NNP'), (',', ','), ('Klemettinen', 'NNP'), (',', ','), ('M.', 'NNP'), (',', ','), ('&', 'CC'), ('Verkamo', 'NNP'), (',', ','), ('A.', 'NNP'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['[ 61', '61 ]', '] Ahonen', 'Ahonen ,', ', H.', 'H. ,', ', Heinonen', 'Heinonen ,', ', O.', 'O. ,', ', Klemettinen', 'Klemettinen ,', ', M.', 'M. ,', ', &', '& Verkamo', 'Verkamo ,', ', A.', 'A. .'] 

 TOTAL BIGRAMS --> 19 



 ---- TRI-GRAMS ---- 

 ['[ 61 ]', '61 ] Ahonen', '] Ahonen ,', 'Ahonen , H.', ', H. ,', 'H. , Heinonen', ', Heinonen ,', 'Heinonen , O.', ', O. ,', 'O. , Klemettinen', ', Klemettinen ,', 'Klemettinen , M.', ', M. ,', 'M. , &', ', & Verkamo', '& Verkamo ,', 'Verkamo , A.', ', A. .'] 

 TOTAL TRIGRAMS --> 18 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> ['Verkamo']
 TOTAL PERSON ENTITY --> 1 


 GPE ---> ['Ahonen', 'Heinonen', 'Klemettinen']
 TOTAL GPE ENTITY --> 3 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['[', '61', ']', 'ahonen', ',', 'h.', ',', 'heinonen', ',', 'o.', ',', 'klemettinen', ',', 'm.', ',', '&', 'verkamo', ',', 'a.', '.']

 TOTAL PORTER STEM WORDS ==> 20



 ---- SNOWBALL STEMMING ----

['[', '61', ']', 'ahonen', ',', 'h.', ',', 'heinonen', ',', 'o.', ',', 'klemettinen', ',', 'm.', ',', '&', 'verkamo', ',', 'a.', '.']

 TOTAL SNOWBALL STEM WORDS ==> 20



 ---- LEMMATIZATION ----

['[', '61', ']', 'Ahonen', ',', 'H.', ',', 'Heinonen', ',', 'O.', ',', 'Klemettinen', ',', 'M.', ',', '&', 'Verkamo', ',', 'A.', '.']

 TOTAL LEMMATIZE WORDS ==> 20

************************************************************************************************************************

586 --> (1998, April). 


 ---- TOKENS ----

 ['(', '1998', ',', 'April', ')', '.'] 

 TOTAL TOKENS ==> 6

 ---- POST ----

 [('(', '('), ('1998', 'CD'), (',', ','), ('April', 'NNP'), (')', ')'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['(', '1998', ',', 'April', ')', '.']

 TOTAL FILTERED TOKENS ==>  6

 ---- POST FOR FILTERED TOKENS ----

 [('(', '('), ('1998', 'CD'), (',', ','), ('April', 'NNP'), (')', ')'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['( 1998', '1998 ,', ', April', 'April )', ') .'] 

 TOTAL BIGRAMS --> 5 



 ---- TRI-GRAMS ---- 

 ['( 1998 ,', '1998 , April', ', April )', 'April ) .'] 

 TOTAL TRIGRAMS --> 4 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['(', '1998', ',', 'april', ')', '.']

 TOTAL PORTER STEM WORDS ==> 6



 ---- SNOWBALL STEMMING ----

['(', '1998', ',', 'april', ')', '.']

 TOTAL SNOWBALL STEM WORDS ==> 6



 ---- LEMMATIZATION ----

['(', '1998', ',', 'April', ')', '.']

 TOTAL LEMMATIZE WORDS ==> 6

************************************************************************************************************************

587 --> Applying  data mining techniques for descriptive phrase extraction in digital document collections. 


 ---- TOKENS ----

 ['Applying', 'data', 'mining', 'techniques', 'for', 'descriptive', 'phrase', 'extraction', 'in', 'digital', 'document', 'collections', '.'] 

 TOTAL TOKENS ==> 13

 ---- POST ----

 [('Applying', 'VBG'), ('data', 'NNS'), ('mining', 'NN'), ('techniques', 'NNS'), ('for', 'IN'), ('descriptive', 'JJ'), ('phrase', 'NN'), ('extraction', 'NN'), ('in', 'IN'), ('digital', 'JJ'), ('document', 'NN'), ('collections', 'NNS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Applying', 'data', 'mining', 'techniques', 'descriptive', 'phrase', 'extraction', 'digital', 'document', 'collections', '.']

 TOTAL FILTERED TOKENS ==>  11

 ---- POST FOR FILTERED TOKENS ----

 [('Applying', 'VBG'), ('data', 'NNS'), ('mining', 'NN'), ('techniques', 'NNS'), ('descriptive', 'JJ'), ('phrase', 'NN'), ('extraction', 'NN'), ('digital', 'JJ'), ('document', 'NN'), ('collections', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Applying data', 'data mining', 'mining techniques', 'techniques descriptive', 'descriptive phrase', 'phrase extraction', 'extraction digital', 'digital document', 'document collections', 'collections .'] 

 TOTAL BIGRAMS --> 10 



 ---- TRI-GRAMS ---- 

 ['Applying data mining', 'data mining techniques', 'mining techniques descriptive', 'techniques descriptive phrase', 'descriptive phrase extraction', 'phrase extraction digital', 'extraction digital document', 'digital document collections', 'document collections .'] 

 TOTAL TRIGRAMS --> 9 



 ---- NOUN PHRASES ---- 

 ['mining', 'descriptive phrase', 'extraction', 'digital document'] 

 TOTAL NOUN PHRASES --> 4 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['appli', 'data', 'mine', 'techniqu', 'descript', 'phrase', 'extract', 'digit', 'document', 'collect', '.']

 TOTAL PORTER STEM WORDS ==> 11



 ---- SNOWBALL STEMMING ----

['appli', 'data', 'mine', 'techniqu', 'descript', 'phrase', 'extract', 'digit', 'document', 'collect', '.']

 TOTAL SNOWBALL STEM WORDS ==> 11



 ---- LEMMATIZATION ----

['Applying', 'data', 'mining', 'technique', 'descriptive', 'phrase', 'extraction', 'digital', 'document', 'collection', '.']

 TOTAL LEMMATIZE WORDS ==> 11

************************************************************************************************************************

588 --> In Research and Technology Advances in Digital Libraries, 1998. 


 ---- TOKENS ----

 ['In', 'Research', 'and', 'Technology', 'Advances', 'in', 'Digital', 'Libraries', ',', '1998', '.'] 

 TOTAL TOKENS ==> 11

 ---- POST ----

 [('In', 'IN'), ('Research', 'NNP'), ('and', 'CC'), ('Technology', 'NNP'), ('Advances', 'NNP'), ('in', 'IN'), ('Digital', 'NNP'), ('Libraries', 'NNP'), (',', ','), ('1998', 'CD'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Research', 'Technology', 'Advances', 'Digital', 'Libraries', ',', '1998', '.']

 TOTAL FILTERED TOKENS ==>  8

 ---- POST FOR FILTERED TOKENS ----

 [('Research', 'NNP'), ('Technology', 'NNP'), ('Advances', 'NNP'), ('Digital', 'NNP'), ('Libraries', 'NNP'), (',', ','), ('1998', 'CD'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Research Technology', 'Technology Advances', 'Advances Digital', 'Digital Libraries', 'Libraries ,', ', 1998', '1998 .'] 

 TOTAL BIGRAMS --> 7 



 ---- TRI-GRAMS ---- 

 ['Research Technology Advances', 'Technology Advances Digital', 'Advances Digital Libraries', 'Digital Libraries ,', 'Libraries , 1998', ', 1998 .'] 

 TOTAL TRIGRAMS --> 6 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> ['Digital Libraries']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['research', 'technolog', 'advanc', 'digit', 'librari', ',', '1998', '.']

 TOTAL PORTER STEM WORDS ==> 8



 ---- SNOWBALL STEMMING ----

['research', 'technolog', 'advanc', 'digit', 'librari', ',', '1998', '.']

 TOTAL SNOWBALL STEM WORDS ==> 8



 ---- LEMMATIZATION ----

['Research', 'Technology', 'Advances', 'Digital', 'Libraries', ',', '1998', '.']

 TOTAL LEMMATIZE WORDS ==> 8

************************************************************************************************************************

589 --> ADL 98. 


 ---- TOKENS ----

 ['ADL', '98', '.'] 

 TOTAL TOKENS ==> 3

 ---- POST ----

 [('ADL', '$'), ('98', 'CD'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['ADL', '98', '.']

 TOTAL FILTERED TOKENS ==>  3

 ---- POST FOR FILTERED TOKENS ----

 [('ADL', '$'), ('98', 'CD'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['ADL 98', '98 .'] 

 TOTAL BIGRAMS --> 2 



 ---- TRI-GRAMS ---- 

 ['ADL 98 .'] 

 TOTAL TRIGRAMS --> 1 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['adl', '98', '.']

 TOTAL PORTER STEM WORDS ==> 3



 ---- SNOWBALL STEMMING ----

['adl', '98', '.']

 TOTAL SNOWBALL STEM WORDS ==> 3



 ---- LEMMATIZATION ----

['ADL', '98', '.']

 TOTAL LEMMATIZE WORDS ==> 3

************************************************************************************************************************

590 --> Proceedings. 


 ---- TOKENS ----

 ['Proceedings', '.'] 

 TOTAL TOKENS ==> 2

 ---- POST ----

 [('Proceedings', 'NNS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Proceedings', '.']

 TOTAL FILTERED TOKENS ==>  2

 ---- POST FOR FILTERED TOKENS ----

 [('Proceedings', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Proceedings .'] 

 TOTAL BIGRAMS --> 1 



 ---- TRI-GRAMS ---- 

 [] 

 TOTAL TRIGRAMS --> 0 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['proceed', '.']

 TOTAL PORTER STEM WORDS ==> 2



 ---- SNOWBALL STEMMING ----

['proceed', '.']

 TOTAL SNOWBALL STEM WORDS ==> 2



 ---- LEMMATIZATION ----

['Proceedings', '.']

 TOTAL LEMMATIZE WORDS ==> 2

************************************************************************************************************************

591 --> IEEE International Forum on (pp. 


 ---- TOKENS ----

 ['IEEE', 'International', 'Forum', 'on', '(', 'pp', '.'] 

 TOTAL TOKENS ==> 7

 ---- POST ----

 [('IEEE', 'NNP'), ('International', 'NNP'), ('Forum', 'NNP'), ('on', 'IN'), ('(', '('), ('pp', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['IEEE', 'International', 'Forum', '(', 'pp', '.']

 TOTAL FILTERED TOKENS ==>  6

 ---- POST FOR FILTERED TOKENS ----

 [('IEEE', 'NNP'), ('International', 'NNP'), ('Forum', 'NNP'), ('(', '('), ('pp', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['IEEE International', 'International Forum', 'Forum (', '( pp', 'pp .'] 

 TOTAL BIGRAMS --> 5 



 ---- TRI-GRAMS ---- 

 ['IEEE International Forum', 'International Forum (', 'Forum ( pp', '( pp .'] 

 TOTAL TRIGRAMS --> 4 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 []

 ---- PORTER STEMMING ----

['ieee', 'intern', 'forum', '(', 'pp', '.']

 TOTAL PORTER STEM WORDS ==> 6



 ---- SNOWBALL STEMMING ----

['ieee', 'intern', 'forum', '(', 'pp', '.']

 TOTAL SNOWBALL STEM WORDS ==> 6



 ---- LEMMATIZATION ----

['IEEE', 'International', 'Forum', '(', 'pp', '.']

 TOTAL LEMMATIZE WORDS ==> 6

************************************************************************************************************************

592 --> 2-11). 


 ---- TOKENS ----

 ['2-11', ')', '.'] 

 TOTAL TOKENS ==> 3

 ---- POST ----

 [('2-11', 'JJ'), (')', ')'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['2-11', ')', '.']

 TOTAL FILTERED TOKENS ==>  3

 ---- POST FOR FILTERED TOKENS ----

 [('2-11', 'JJ'), (')', ')'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['2-11 )', ') .'] 

 TOTAL BIGRAMS --> 2 



 ---- TRI-GRAMS ---- 

 ['2-11 ) .'] 

 TOTAL TRIGRAMS --> 1 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 []

 ---- PORTER STEMMING ----

['2-11', ')', '.']

 TOTAL PORTER STEM WORDS ==> 3



 ---- SNOWBALL STEMMING ----

['2-11', ')', '.']

 TOTAL SNOWBALL STEM WORDS ==> 3



 ---- LEMMATIZATION ----

['2-11', ')', '.']

 TOTAL LEMMATIZE WORDS ==> 3

************************************************************************************************************************

593 --> IEEE. 


 ---- TOKENS ----

 ['IEEE', '.'] 

 TOTAL TOKENS ==> 2

 ---- POST ----

 [('IEEE', 'NNP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['IEEE', '.']

 TOTAL FILTERED TOKENS ==>  2

 ---- POST FOR FILTERED TOKENS ----

 [('IEEE', 'NNP'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['IEEE .'] 

 TOTAL BIGRAMS --> 1 



 ---- TRI-GRAMS ---- 

 [] 

 TOTAL TRIGRAMS --> 0 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['ieee', '.']

 TOTAL PORTER STEM WORDS ==> 2



 ---- SNOWBALL STEMMING ----

['ieee', '.']

 TOTAL SNOWBALL STEM WORDS ==> 2



 ---- LEMMATIZATION ----

['IEEE', '.']

 TOTAL LEMMATIZE WORDS ==> 2

************************************************************************************************************************

594 --> [62] Zajic, D. M., Dorr, B. J., & Lin, J. 


 ---- TOKENS ----

 ['[', '62', ']', 'Zajic', ',', 'D.', 'M.', ',', 'Dorr', ',', 'B.', 'J.', ',', '&', 'Lin', ',', 'J', '.'] 

 TOTAL TOKENS ==> 18

 ---- POST ----

 [('[', 'RB'), ('62', 'CD'), (']', 'JJ'), ('Zajic', 'NNP'), (',', ','), ('D.', 'NNP'), ('M.', 'NNP'), (',', ','), ('Dorr', 'NNP'), (',', ','), ('B.', 'NNP'), ('J.', 'NNP'), (',', ','), ('&', 'CC'), ('Lin', 'NNP'), (',', ','), ('J', 'NNP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['[', '62', ']', 'Zajic', ',', 'D.', 'M.', ',', 'Dorr', ',', 'B.', 'J.', ',', '&', 'Lin', ',', 'J', '.']

 TOTAL FILTERED TOKENS ==>  18

 ---- POST FOR FILTERED TOKENS ----

 [('[', 'RB'), ('62', 'CD'), (']', 'JJ'), ('Zajic', 'NNP'), (',', ','), ('D.', 'NNP'), ('M.', 'NNP'), (',', ','), ('Dorr', 'NNP'), (',', ','), ('B.', 'NNP'), ('J.', 'NNP'), (',', ','), ('&', 'CC'), ('Lin', 'NNP'), (',', ','), ('J', 'NNP'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['[ 62', '62 ]', '] Zajic', 'Zajic ,', ', D.', 'D. M.', 'M. ,', ', Dorr', 'Dorr ,', ', B.', 'B. J.', 'J. ,', ', &', '& Lin', 'Lin ,', ', J', 'J .'] 

 TOTAL BIGRAMS --> 17 



 ---- TRI-GRAMS ---- 

 ['[ 62 ]', '62 ] Zajic', '] Zajic ,', 'Zajic , D.', ', D. M.', 'D. M. ,', 'M. , Dorr', ', Dorr ,', 'Dorr , B.', ', B. J.', 'B. J. ,', 'J. , &', ', & Lin', '& Lin ,', 'Lin , J', ', J .'] 

 TOTAL TRIGRAMS --> 16 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> ['Dorr', 'Lin']
 TOTAL PERSON ENTITY --> 2 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['[', '62', ']', 'zajic', ',', 'd.', 'm.', ',', 'dorr', ',', 'b.', 'j.', ',', '&', 'lin', ',', 'j', '.']

 TOTAL PORTER STEM WORDS ==> 18



 ---- SNOWBALL STEMMING ----

['[', '62', ']', 'zajic', ',', 'd.', 'm.', ',', 'dorr', ',', 'b.', 'j.', ',', '&', 'lin', ',', 'j', '.']

 TOTAL SNOWBALL STEM WORDS ==> 18



 ---- LEMMATIZATION ----

['[', '62', ']', 'Zajic', ',', 'D.', 'M.', ',', 'Dorr', ',', 'B.', 'J.', ',', '&', 'Lin', ',', 'J', '.']

 TOTAL LEMMATIZE WORDS ==> 18

************************************************************************************************************************

595 --> (2008). 


 ---- TOKENS ----

 ['(', '2008', ')', '.'] 

 TOTAL TOKENS ==> 4

 ---- POST ----

 [('(', '('), ('2008', 'CD'), (')', ')'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['(', '2008', ')', '.']

 TOTAL FILTERED TOKENS ==>  4

 ---- POST FOR FILTERED TOKENS ----

 [('(', '('), ('2008', 'CD'), (')', ')'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['( 2008', '2008 )', ') .'] 

 TOTAL BIGRAMS --> 3 



 ---- TRI-GRAMS ---- 

 ['( 2008 )', '2008 ) .'] 

 TOTAL TRIGRAMS --> 2 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['(', '2008', ')', '.']

 TOTAL PORTER STEM WORDS ==> 4



 ---- SNOWBALL STEMMING ----

['(', '2008', ')', '.']

 TOTAL SNOWBALL STEM WORDS ==> 4



 ---- LEMMATIZATION ----

['(', '2008', ')', '.']

 TOTAL LEMMATIZE WORDS ==> 4

************************************************************************************************************************

596 --> Single-document and multi-document  summarization techniques for email threads using sentence compression. 


 ---- TOKENS ----

 ['Single-document', 'and', 'multi-document', 'summarization', 'techniques', 'for', 'email', 'threads', 'using', 'sentence', 'compression', '.'] 

 TOTAL TOKENS ==> 12

 ---- POST ----

 [('Single-document', 'JJ'), ('and', 'CC'), ('multi-document', 'JJ'), ('summarization', 'NN'), ('techniques', 'NNS'), ('for', 'IN'), ('email', 'NN'), ('threads', 'NNS'), ('using', 'VBG'), ('sentence', 'NN'), ('compression', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Single-document', 'multi-document', 'summarization', 'techniques', 'email', 'threads', 'using', 'sentence', 'compression', '.']

 TOTAL FILTERED TOKENS ==>  10

 ---- POST FOR FILTERED TOKENS ----

 [('Single-document', 'JJ'), ('multi-document', 'JJ'), ('summarization', 'NN'), ('techniques', 'NNS'), ('email', 'VBP'), ('threads', 'NNS'), ('using', 'VBG'), ('sentence', 'NN'), ('compression', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Single-document multi-document', 'multi-document summarization', 'summarization techniques', 'techniques email', 'email threads', 'threads using', 'using sentence', 'sentence compression', 'compression .'] 

 TOTAL BIGRAMS --> 9 



 ---- TRI-GRAMS ---- 

 ['Single-document multi-document summarization', 'multi-document summarization techniques', 'summarization techniques email', 'techniques email threads', 'email threads using', 'threads using sentence', 'using sentence compression', 'sentence compression .'] 

 TOTAL TRIGRAMS --> 8 



 ---- NOUN PHRASES ---- 

 ['Single-document multi-document summarization', 'sentence', 'compression'] 

 TOTAL NOUN PHRASES --> 3 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['single-docu', 'multi-docu', 'summar', 'techniqu', 'email', 'thread', 'use', 'sentenc', 'compress', '.']

 TOTAL PORTER STEM WORDS ==> 10



 ---- SNOWBALL STEMMING ----

['single-docu', 'multi-docu', 'summar', 'techniqu', 'email', 'thread', 'use', 'sentenc', 'compress', '.']

 TOTAL SNOWBALL STEM WORDS ==> 10



 ---- LEMMATIZATION ----

['Single-document', 'multi-document', 'summarization', 'technique', 'email', 'thread', 'using', 'sentence', 'compression', '.']

 TOTAL LEMMATIZE WORDS ==> 10

************************************************************************************************************************

597 --> Information  Processing & Management, 44(4), 1600-1610. 


 ---- TOKENS ----

 ['Information', 'Processing', '&', 'Management', ',', '44', '(', '4', ')', ',', '1600-1610', '.'] 

 TOTAL TOKENS ==> 12

 ---- POST ----

 [('Information', 'NNP'), ('Processing', 'NNP'), ('&', 'CC'), ('Management', 'NNP'), (',', ','), ('44', 'CD'), ('(', '('), ('4', 'CD'), (')', ')'), (',', ','), ('1600-1610', 'JJ'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Information', 'Processing', '&', 'Management', ',', '44', '(', '4', ')', ',', '1600-1610', '.']

 TOTAL FILTERED TOKENS ==>  12

 ---- POST FOR FILTERED TOKENS ----

 [('Information', 'NNP'), ('Processing', 'NNP'), ('&', 'CC'), ('Management', 'NNP'), (',', ','), ('44', 'CD'), ('(', '('), ('4', 'CD'), (')', ')'), (',', ','), ('1600-1610', 'JJ'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Information Processing', 'Processing &', '& Management', 'Management ,', ', 44', '44 (', '( 4', '4 )', ') ,', ', 1600-1610', '1600-1610 .'] 

 TOTAL BIGRAMS --> 11 



 ---- TRI-GRAMS ---- 

 ['Information Processing &', 'Processing & Management', '& Management ,', 'Management , 44', ', 44 (', '44 ( 4', '( 4 )', '4 ) ,', ') , 1600-1610', ', 1600-1610 .'] 

 TOTAL TRIGRAMS --> 10 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['inform', 'process', '&', 'manag', ',', '44', '(', '4', ')', ',', '1600-1610', '.']

 TOTAL PORTER STEM WORDS ==> 12



 ---- SNOWBALL STEMMING ----

['inform', 'process', '&', 'manag', ',', '44', '(', '4', ')', ',', '1600-1610', '.']

 TOTAL SNOWBALL STEM WORDS ==> 12



 ---- LEMMATIZATION ----

['Information', 'Processing', '&', 'Management', ',', '44', '(', '4', ')', ',', '1600-1610', '.']

 TOTAL LEMMATIZE WORDS ==> 12

************************************************************************************************************************

598 --> [63] Fattah, M. A., & Ren, F. (2009). 


 ---- TOKENS ----

 ['[', '63', ']', 'Fattah', ',', 'M.', 'A.', ',', '&', 'Ren', ',', 'F.', '(', '2009', ')', '.'] 

 TOTAL TOKENS ==> 16

 ---- POST ----

 [('[', 'RB'), ('63', 'CD'), (']', 'JJ'), ('Fattah', 'NNP'), (',', ','), ('M.', 'NNP'), ('A.', 'NNP'), (',', ','), ('&', 'CC'), ('Ren', 'NNP'), (',', ','), ('F.', 'NNP'), ('(', '('), ('2009', 'CD'), (')', ')'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['[', '63', ']', 'Fattah', ',', 'M.', 'A.', ',', '&', 'Ren', ',', 'F.', '(', '2009', ')', '.']

 TOTAL FILTERED TOKENS ==>  16

 ---- POST FOR FILTERED TOKENS ----

 [('[', 'RB'), ('63', 'CD'), (']', 'JJ'), ('Fattah', 'NNP'), (',', ','), ('M.', 'NNP'), ('A.', 'NNP'), (',', ','), ('&', 'CC'), ('Ren', 'NNP'), (',', ','), ('F.', 'NNP'), ('(', '('), ('2009', 'CD'), (')', ')'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['[ 63', '63 ]', '] Fattah', 'Fattah ,', ', M.', 'M. A.', 'A. ,', ', &', '& Ren', 'Ren ,', ', F.', 'F. (', '( 2009', '2009 )', ') .'] 

 TOTAL BIGRAMS --> 15 



 ---- TRI-GRAMS ---- 

 ['[ 63 ]', '63 ] Fattah', '] Fattah ,', 'Fattah , M.', ', M. A.', 'M. A. ,', 'A. , &', ', & Ren', '& Ren ,', 'Ren , F.', ', F. (', 'F. ( 2009', '( 2009 )', '2009 ) .'] 

 TOTAL TRIGRAMS --> 14 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> ['Ren']
 TOTAL PERSON ENTITY --> 1 


 GPE ---> ['Fattah']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['[', '63', ']', 'fattah', ',', 'm.', 'a.', ',', '&', 'ren', ',', 'f.', '(', '2009', ')', '.']

 TOTAL PORTER STEM WORDS ==> 16



 ---- SNOWBALL STEMMING ----

['[', '63', ']', 'fattah', ',', 'm.', 'a.', ',', '&', 'ren', ',', 'f.', '(', '2009', ')', '.']

 TOTAL SNOWBALL STEM WORDS ==> 16



 ---- LEMMATIZATION ----

['[', '63', ']', 'Fattah', ',', 'M.', 'A.', ',', '&', 'Ren', ',', 'F.', '(', '2009', ')', '.']

 TOTAL LEMMATIZE WORDS ==> 16

************************************************************************************************************************

599 --> GA, MR, FFNN, PNN and GMM based models for  automatic text summarization. 


 ---- TOKENS ----

 ['GA', ',', 'MR', ',', 'FFNN', ',', 'PNN', 'and', 'GMM', 'based', 'models', 'for', 'automatic', 'text', 'summarization', '.'] 

 TOTAL TOKENS ==> 16

 ---- POST ----

 [('GA', 'NNP'), (',', ','), ('MR', 'NNP'), (',', ','), ('FFNN', 'NNP'), (',', ','), ('PNN', 'NNP'), ('and', 'CC'), ('GMM', 'NNP'), ('based', 'VBN'), ('models', 'NNS'), ('for', 'IN'), ('automatic', 'JJ'), ('text', 'NN'), ('summarization', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['GA', ',', 'MR', ',', 'FFNN', ',', 'PNN', 'GMM', 'based', 'models', 'automatic', 'text', 'summarization', '.']

 TOTAL FILTERED TOKENS ==>  14

 ---- POST FOR FILTERED TOKENS ----

 [('GA', 'NNP'), (',', ','), ('MR', 'NNP'), (',', ','), ('FFNN', 'NNP'), (',', ','), ('PNN', 'NNP'), ('GMM', 'NNP'), ('based', 'VBN'), ('models', 'NNS'), ('automatic', 'JJ'), ('text', 'JJ'), ('summarization', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['GA ,', ', MR', 'MR ,', ', FFNN', 'FFNN ,', ', PNN', 'PNN GMM', 'GMM based', 'based models', 'models automatic', 'automatic text', 'text summarization', 'summarization .'] 

 TOTAL BIGRAMS --> 13 



 ---- TRI-GRAMS ---- 

 ['GA , MR', ', MR ,', 'MR , FFNN', ', FFNN ,', 'FFNN , PNN', ', PNN GMM', 'PNN GMM based', 'GMM based models', 'based models automatic', 'models automatic text', 'automatic text summarization', 'text summarization .'] 

 TOTAL TRIGRAMS --> 12 



 ---- NOUN PHRASES ---- 

 ['automatic text summarization'] 

 TOTAL NOUN PHRASES --> 1 



 ---- NER ----

 
 ORGANIZATION ---> ['FFNN', 'PNN']
 TOTAL ORGANIZATION ENTITY --> 2 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['ga', ',', 'mr', ',', 'ffnn', ',', 'pnn', 'gmm', 'base', 'model', 'automat', 'text', 'summar', '.']

 TOTAL PORTER STEM WORDS ==> 14



 ---- SNOWBALL STEMMING ----

['ga', ',', 'mr', ',', 'ffnn', ',', 'pnn', 'gmm', 'base', 'model', 'automat', 'text', 'summar', '.']

 TOTAL SNOWBALL STEM WORDS ==> 14



 ---- LEMMATIZATION ----

['GA', ',', 'MR', ',', 'FFNN', ',', 'PNN', 'GMM', 'based', 'model', 'automatic', 'text', 'summarization', '.']

 TOTAL LEMMATIZE WORDS ==> 14

************************************************************************************************************************

600 --> Computer Speech & Language, 23(1), 126-144. 


 ---- TOKENS ----

 ['Computer', 'Speech', '&', 'Language', ',', '23', '(', '1', ')', ',', '126-144', '.'] 

 TOTAL TOKENS ==> 12

 ---- POST ----

 [('Computer', 'NNP'), ('Speech', 'NNP'), ('&', 'CC'), ('Language', 'NNP'), (',', ','), ('23', 'CD'), ('(', '('), ('1', 'CD'), (')', ')'), (',', ','), ('126-144', 'JJ'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Computer', 'Speech', '&', 'Language', ',', '23', '(', '1', ')', ',', '126-144', '.']

 TOTAL FILTERED TOKENS ==>  12

 ---- POST FOR FILTERED TOKENS ----

 [('Computer', 'NNP'), ('Speech', 'NNP'), ('&', 'CC'), ('Language', 'NNP'), (',', ','), ('23', 'CD'), ('(', '('), ('1', 'CD'), (')', ')'), (',', ','), ('126-144', 'JJ'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Computer Speech', 'Speech &', '& Language', 'Language ,', ', 23', '23 (', '( 1', '1 )', ') ,', ', 126-144', '126-144 .'] 

 TOTAL BIGRAMS --> 11 



 ---- TRI-GRAMS ---- 

 ['Computer Speech &', 'Speech & Language', '& Language ,', 'Language , 23', ', 23 (', '23 ( 1', '( 1 )', '1 ) ,', ') , 126-144', ', 126-144 .'] 

 TOTAL TRIGRAMS --> 10 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> ['Computer Speech']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> ['Language']
 TOTAL PERSON ENTITY --> 1 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['comput', 'speech', '&', 'languag', ',', '23', '(', '1', ')', ',', '126-144', '.']

 TOTAL PORTER STEM WORDS ==> 12



 ---- SNOWBALL STEMMING ----

['comput', 'speech', '&', 'languag', ',', '23', '(', '1', ')', ',', '126-144', '.']

 TOTAL SNOWBALL STEM WORDS ==> 12



 ---- LEMMATIZATION ----

['Computer', 'Speech', '&', 'Language', ',', '23', '(', '1', ')', ',', '126-144', '.']

 TOTAL LEMMATIZE WORDS ==> 12

************************************************************************************************************************

601 --> [64] Gong, Y., & Liu, X. 


 ---- TOKENS ----

 ['[', '64', ']', 'Gong', ',', 'Y.', ',', '&', 'Liu', ',', 'X', '.'] 

 TOTAL TOKENS ==> 12

 ---- POST ----

 [('[', 'RB'), ('64', 'CD'), (']', 'NN'), ('Gong', 'NNP'), (',', ','), ('Y.', 'NNP'), (',', ','), ('&', 'CC'), ('Liu', 'NNP'), (',', ','), ('X', 'NNP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['[', '64', ']', 'Gong', ',', 'Y.', ',', '&', 'Liu', ',', 'X', '.']

 TOTAL FILTERED TOKENS ==>  12

 ---- POST FOR FILTERED TOKENS ----

 [('[', 'RB'), ('64', 'CD'), (']', 'NN'), ('Gong', 'NNP'), (',', ','), ('Y.', 'NNP'), (',', ','), ('&', 'CC'), ('Liu', 'NNP'), (',', ','), ('X', 'NNP'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['[ 64', '64 ]', '] Gong', 'Gong ,', ', Y.', 'Y. ,', ', &', '& Liu', 'Liu ,', ', X', 'X .'] 

 TOTAL BIGRAMS --> 11 



 ---- TRI-GRAMS ---- 

 ['[ 64 ]', '64 ] Gong', '] Gong ,', 'Gong , Y.', ', Y. ,', 'Y. , &', ', & Liu', '& Liu ,', 'Liu , X', ', X .'] 

 TOTAL TRIGRAMS --> 10 



 ---- NOUN PHRASES ---- 

 [']'] 

 TOTAL NOUN PHRASES --> 1 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> ['Liu']
 TOTAL PERSON ENTITY --> 1 


 GPE ---> ['Gong']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['[', '64', ']', 'gong', ',', 'y.', ',', '&', 'liu', ',', 'x', '.']

 TOTAL PORTER STEM WORDS ==> 12



 ---- SNOWBALL STEMMING ----

['[', '64', ']', 'gong', ',', 'y.', ',', '&', 'liu', ',', 'x', '.']

 TOTAL SNOWBALL STEM WORDS ==> 12



 ---- LEMMATIZATION ----

['[', '64', ']', 'Gong', ',', 'Y.', ',', '&', 'Liu', ',', 'X', '.']

 TOTAL LEMMATIZE WORDS ==> 12

************************************************************************************************************************

602 --> (2001, September). 


 ---- TOKENS ----

 ['(', '2001', ',', 'September', ')', '.'] 

 TOTAL TOKENS ==> 6

 ---- POST ----

 [('(', '('), ('2001', 'CD'), (',', ','), ('September', 'NNP'), (')', ')'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['(', '2001', ',', 'September', ')', '.']

 TOTAL FILTERED TOKENS ==>  6

 ---- POST FOR FILTERED TOKENS ----

 [('(', '('), ('2001', 'CD'), (',', ','), ('September', 'NNP'), (')', ')'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['( 2001', '2001 ,', ', September', 'September )', ') .'] 

 TOTAL BIGRAMS --> 5 



 ---- TRI-GRAMS ---- 

 ['( 2001 ,', '2001 , September', ', September )', 'September ) .'] 

 TOTAL TRIGRAMS --> 4 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['(', '2001', ',', 'septemb', ')', '.']

 TOTAL PORTER STEM WORDS ==> 6



 ---- SNOWBALL STEMMING ----

['(', '2001', ',', 'septemb', ')', '.']

 TOTAL SNOWBALL STEM WORDS ==> 6



 ---- LEMMATIZATION ----

['(', '2001', ',', 'September', ')', '.']

 TOTAL LEMMATIZE WORDS ==> 6

************************************************************************************************************************

603 --> Generic text summarization using relevance  measure and latent semantic analysis. 


 ---- TOKENS ----

 ['Generic', 'text', 'summarization', 'using', 'relevance', 'measure', 'and', 'latent', 'semantic', 'analysis', '.'] 

 TOTAL TOKENS ==> 11

 ---- POST ----

 [('Generic', 'NNP'), ('text', 'NN'), ('summarization', 'NN'), ('using', 'VBG'), ('relevance', 'NN'), ('measure', 'NN'), ('and', 'CC'), ('latent', 'JJ'), ('semantic', 'JJ'), ('analysis', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Generic', 'text', 'summarization', 'using', 'relevance', 'measure', 'latent', 'semantic', 'analysis', '.']

 TOTAL FILTERED TOKENS ==>  10

 ---- POST FOR FILTERED TOKENS ----

 [('Generic', 'NNP'), ('text', 'NN'), ('summarization', 'NN'), ('using', 'VBG'), ('relevance', 'NN'), ('measure', 'NN'), ('latent', 'JJ'), ('semantic', 'JJ'), ('analysis', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Generic text', 'text summarization', 'summarization using', 'using relevance', 'relevance measure', 'measure latent', 'latent semantic', 'semantic analysis', 'analysis .'] 

 TOTAL BIGRAMS --> 9 



 ---- TRI-GRAMS ---- 

 ['Generic text summarization', 'text summarization using', 'summarization using relevance', 'using relevance measure', 'relevance measure latent', 'measure latent semantic', 'latent semantic analysis', 'semantic analysis .'] 

 TOTAL TRIGRAMS --> 8 



 ---- NOUN PHRASES ---- 

 ['text', 'summarization', 'relevance', 'measure', 'latent semantic analysis'] 

 TOTAL NOUN PHRASES --> 5 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> ['Generic']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['gener', 'text', 'summar', 'use', 'relev', 'measur', 'latent', 'semant', 'analysi', '.']

 TOTAL PORTER STEM WORDS ==> 10



 ---- SNOWBALL STEMMING ----

['generic', 'text', 'summar', 'use', 'relev', 'measur', 'latent', 'semant', 'analysi', '.']

 TOTAL SNOWBALL STEM WORDS ==> 10



 ---- LEMMATIZATION ----

['Generic', 'text', 'summarization', 'using', 'relevance', 'measure', 'latent', 'semantic', 'analysis', '.']

 TOTAL LEMMATIZE WORDS ==> 10

************************************************************************************************************************

604 --> In Proceedings of the 24th annual international ACM  SIGIR conference on Research and development in information retrieval (pp. 


 ---- TOKENS ----

 ['In', 'Proceedings', 'of', 'the', '24th', 'annual', 'international', 'ACM', 'SIGIR', 'conference', 'on', 'Research', 'and', 'development', 'in', 'information', 'retrieval', '(', 'pp', '.'] 

 TOTAL TOKENS ==> 20

 ---- POST ----

 [('In', 'IN'), ('Proceedings', 'NNP'), ('of', 'IN'), ('the', 'DT'), ('24th', 'CD'), ('annual', 'JJ'), ('international', 'JJ'), ('ACM', 'NNP'), ('SIGIR', 'NNP'), ('conference', 'NN'), ('on', 'IN'), ('Research', 'NNP'), ('and', 'CC'), ('development', 'NN'), ('in', 'IN'), ('information', 'NN'), ('retrieval', 'NN'), ('(', '('), ('pp', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Proceedings', '24th', 'annual', 'international', 'ACM', 'SIGIR', 'conference', 'Research', 'development', 'information', 'retrieval', '(', 'pp', '.']

 TOTAL FILTERED TOKENS ==>  14

 ---- POST FOR FILTERED TOKENS ----

 [('Proceedings', 'NNS'), ('24th', 'CD'), ('annual', 'JJ'), ('international', 'JJ'), ('ACM', 'NNP'), ('SIGIR', 'NNP'), ('conference', 'NN'), ('Research', 'NNP'), ('development', 'NN'), ('information', 'NN'), ('retrieval', 'NN'), ('(', '('), ('pp', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Proceedings 24th', '24th annual', 'annual international', 'international ACM', 'ACM SIGIR', 'SIGIR conference', 'conference Research', 'Research development', 'development information', 'information retrieval', 'retrieval (', '( pp', 'pp .'] 

 TOTAL BIGRAMS --> 13 



 ---- TRI-GRAMS ---- 

 ['Proceedings 24th annual', '24th annual international', 'annual international ACM', 'international ACM SIGIR', 'ACM SIGIR conference', 'SIGIR conference Research', 'conference Research development', 'Research development information', 'development information retrieval', 'information retrieval (', 'retrieval ( pp', '( pp .'] 

 TOTAL TRIGRAMS --> 12 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 []

 ---- PORTER STEMMING ----

['proceed', '24th', 'annual', 'intern', 'acm', 'sigir', 'confer', 'research', 'develop', 'inform', 'retriev', '(', 'pp', '.']

 TOTAL PORTER STEM WORDS ==> 14



 ---- SNOWBALL STEMMING ----

['proceed', '24th', 'annual', 'intern', 'acm', 'sigir', 'confer', 'research', 'develop', 'inform', 'retriev', '(', 'pp', '.']

 TOTAL SNOWBALL STEM WORDS ==> 14



 ---- LEMMATIZATION ----

['Proceedings', '24th', 'annual', 'international', 'ACM', 'SIGIR', 'conference', 'Research', 'development', 'information', 'retrieval', '(', 'pp', '.']

 TOTAL LEMMATIZE WORDS ==> 14

************************************************************************************************************************

605 --> 19-25). 


 ---- TOKENS ----

 ['19-25', ')', '.'] 

 TOTAL TOKENS ==> 3

 ---- POST ----

 [('19-25', 'JJ'), (')', ')'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['19-25', ')', '.']

 TOTAL FILTERED TOKENS ==>  3

 ---- POST FOR FILTERED TOKENS ----

 [('19-25', 'JJ'), (')', ')'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['19-25 )', ') .'] 

 TOTAL BIGRAMS --> 2 



 ---- TRI-GRAMS ---- 

 ['19-25 ) .'] 

 TOTAL TRIGRAMS --> 1 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 []

 ---- PORTER STEMMING ----

['19-25', ')', '.']

 TOTAL PORTER STEM WORDS ==> 3



 ---- SNOWBALL STEMMING ----

['19-25', ')', '.']

 TOTAL SNOWBALL STEM WORDS ==> 3



 ---- LEMMATIZATION ----

['19-25', ')', '.']

 TOTAL LEMMATIZE WORDS ==> 3

************************************************************************************************************************

606 --> ACM. 


 ---- TOKENS ----

 ['ACM', '.'] 

 TOTAL TOKENS ==> 2

 ---- POST ----

 [('ACM', 'NNP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['ACM', '.']

 TOTAL FILTERED TOKENS ==>  2

 ---- POST FOR FILTERED TOKENS ----

 [('ACM', 'NNP'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['ACM .'] 

 TOTAL BIGRAMS --> 1 



 ---- TRI-GRAMS ---- 

 [] 

 TOTAL TRIGRAMS --> 0 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['acm', '.']

 TOTAL PORTER STEM WORDS ==> 2



 ---- SNOWBALL STEMMING ----

['acm', '.']

 TOTAL SNOWBALL STEM WORDS ==> 2



 ---- LEMMATIZATION ----

['ACM', '.']

 TOTAL LEMMATIZE WORDS ==> 2

************************************************************************************************************************

607 --> [65] Dunlavy, D. M., O’Leary, D. P., Conroy, J. M., & Schlesinger, J. D. (2007). 


 ---- TOKENS ----

 ['[', '65', ']', 'Dunlavy', ',', 'D.', 'M.', ',', 'O', '’', 'Leary', ',', 'D.', 'P.', ',', 'Conroy', ',', 'J.', 'M.', ',', '&', 'Schlesinger', ',', 'J.', 'D.', '(', '2007', ')', '.'] 

 TOTAL TOKENS ==> 29

 ---- POST ----

 [('[', 'RB'), ('65', 'CD'), (']', 'JJ'), ('Dunlavy', 'NNP'), (',', ','), ('D.', 'NNP'), ('M.', 'NNP'), (',', ','), ('O', 'NNP'), ('’', 'NNP'), ('Leary', 'NNP'), (',', ','), ('D.', 'NNP'), ('P.', 'NNP'), (',', ','), ('Conroy', 'NNP'), (',', ','), ('J.', 'NNP'), ('M.', 'NNP'), (',', ','), ('&', 'CC'), ('Schlesinger', 'NNP'), (',', ','), ('J.', 'NNP'), ('D.', 'NNP'), ('(', '('), ('2007', 'CD'), (')', ')'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['[', '65', ']', 'Dunlavy', ',', 'D.', 'M.', ',', '’', 'Leary', ',', 'D.', 'P.', ',', 'Conroy', ',', 'J.', 'M.', ',', '&', 'Schlesinger', ',', 'J.', 'D.', '(', '2007', ')', '.']

 TOTAL FILTERED TOKENS ==>  28

 ---- POST FOR FILTERED TOKENS ----

 [('[', 'RB'), ('65', 'CD'), (']', 'JJ'), ('Dunlavy', 'NNP'), (',', ','), ('D.', 'NNP'), ('M.', 'NNP'), (',', ','), ('’', 'NNP'), ('Leary', 'NNP'), (',', ','), ('D.', 'NNP'), ('P.', 'NNP'), (',', ','), ('Conroy', 'NNP'), (',', ','), ('J.', 'NNP'), ('M.', 'NNP'), (',', ','), ('&', 'CC'), ('Schlesinger', 'NNP'), (',', ','), ('J.', 'NNP'), ('D.', 'NNP'), ('(', '('), ('2007', 'CD'), (')', ')'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['[ 65', '65 ]', '] Dunlavy', 'Dunlavy ,', ', D.', 'D. M.', 'M. ,', ', ’', '’ Leary', 'Leary ,', ', D.', 'D. P.', 'P. ,', ', Conroy', 'Conroy ,', ', J.', 'J. M.', 'M. ,', ', &', '& Schlesinger', 'Schlesinger ,', ', J.', 'J. D.', 'D. (', '( 2007', '2007 )', ') .'] 

 TOTAL BIGRAMS --> 27 



 ---- TRI-GRAMS ---- 

 ['[ 65 ]', '65 ] Dunlavy', '] Dunlavy ,', 'Dunlavy , D.', ', D. M.', 'D. M. ,', 'M. , ’', ', ’ Leary', '’ Leary ,', 'Leary , D.', ', D. P.', 'D. P. ,', 'P. , Conroy', ', Conroy ,', 'Conroy , J.', ', J. M.', 'J. M. ,', 'M. , &', ', & Schlesinger', '& Schlesinger ,', 'Schlesinger , J.', ', J. D.', 'J. D. (', 'D. ( 2007', '( 2007 )', '2007 ) .'] 

 TOTAL TRIGRAMS --> 26 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> ['J. M.', 'Schlesinger', 'J. D.']
 TOTAL PERSON ENTITY --> 3 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['[', '65', ']', 'dunlavi', ',', 'd.', 'm.', ',', '’', 'leari', ',', 'd.', 'p.', ',', 'conroy', ',', 'j.', 'm.', ',', '&', 'schlesing', ',', 'j.', 'd.', '(', '2007', ')', '.']

 TOTAL PORTER STEM WORDS ==> 28



 ---- SNOWBALL STEMMING ----

['[', '65', ']', 'dunlavi', ',', 'd.', 'm.', ',', '’', 'leari', ',', 'd.', 'p.', ',', 'conroy', ',', 'j.', 'm.', ',', '&', 'schlesing', ',', 'j.', 'd.', '(', '2007', ')', '.']

 TOTAL SNOWBALL STEM WORDS ==> 28



 ---- LEMMATIZATION ----

['[', '65', ']', 'Dunlavy', ',', 'D.', 'M.', ',', '’', 'Leary', ',', 'D.', 'P.', ',', 'Conroy', ',', 'J.', 'M.', ',', '&', 'Schlesinger', ',', 'J.', 'D.', '(', '2007', ')', '.']

 TOTAL LEMMATIZE WORDS ==> 28

************************************************************************************************************************

608 --> QCS: A  system for querying, clustering and summarizing documents. 


 ---- TOKENS ----

 ['QCS', ':', 'A', 'system', 'for', 'querying', ',', 'clustering', 'and', 'summarizing', 'documents', '.'] 

 TOTAL TOKENS ==> 12

 ---- POST ----

 [('QCS', 'NN'), (':', ':'), ('A', 'DT'), ('system', 'NN'), ('for', 'IN'), ('querying', 'VBG'), (',', ','), ('clustering', 'VBG'), ('and', 'CC'), ('summarizing', 'VBG'), ('documents', 'NNS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['QCS', ':', 'system', 'querying', ',', 'clustering', 'summarizing', 'documents', '.']

 TOTAL FILTERED TOKENS ==>  9

 ---- POST FOR FILTERED TOKENS ----

 [('QCS', 'NN'), (':', ':'), ('system', 'NN'), ('querying', 'VBG'), (',', ','), ('clustering', 'VBG'), ('summarizing', 'VBG'), ('documents', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['QCS :', ': system', 'system querying', 'querying ,', ', clustering', 'clustering summarizing', 'summarizing documents', 'documents .'] 

 TOTAL BIGRAMS --> 8 



 ---- TRI-GRAMS ---- 

 ['QCS : system', ': system querying', 'system querying ,', 'querying , clustering', ', clustering summarizing', 'clustering summarizing documents', 'summarizing documents .'] 

 TOTAL TRIGRAMS --> 7 



 ---- NOUN PHRASES ---- 

 ['QCS', 'system'] 

 TOTAL NOUN PHRASES --> 2 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['qc', ':', 'system', 'queri', ',', 'cluster', 'summar', 'document', '.']

 TOTAL PORTER STEM WORDS ==> 9



 ---- SNOWBALL STEMMING ----

['qcs', ':', 'system', 'queri', ',', 'cluster', 'summar', 'document', '.']

 TOTAL SNOWBALL STEM WORDS ==> 9



 ---- LEMMATIZATION ----

['QCS', ':', 'system', 'querying', ',', 'clustering', 'summarizing', 'document', '.']

 TOTAL LEMMATIZE WORDS ==> 9

************************************************************************************************************************

609 --> Information processing &  management, 43(6), 1588-1605. 


 ---- TOKENS ----

 ['Information', 'processing', '&', 'management', ',', '43', '(', '6', ')', ',', '1588-1605', '.'] 

 TOTAL TOKENS ==> 12

 ---- POST ----

 [('Information', 'NNP'), ('processing', 'NN'), ('&', 'CC'), ('management', 'NN'), (',', ','), ('43', 'CD'), ('(', '('), ('6', 'CD'), (')', ')'), (',', ','), ('1588-1605', 'JJ'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Information', 'processing', '&', 'management', ',', '43', '(', '6', ')', ',', '1588-1605', '.']

 TOTAL FILTERED TOKENS ==>  12

 ---- POST FOR FILTERED TOKENS ----

 [('Information', 'NNP'), ('processing', 'NN'), ('&', 'CC'), ('management', 'NN'), (',', ','), ('43', 'CD'), ('(', '('), ('6', 'CD'), (')', ')'), (',', ','), ('1588-1605', 'JJ'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Information processing', 'processing &', '& management', 'management ,', ', 43', '43 (', '( 6', '6 )', ') ,', ', 1588-1605', '1588-1605 .'] 

 TOTAL BIGRAMS --> 11 



 ---- TRI-GRAMS ---- 

 ['Information processing &', 'processing & management', '& management ,', 'management , 43', ', 43 (', '43 ( 6', '( 6 )', '6 ) ,', ') , 1588-1605', ', 1588-1605 .'] 

 TOTAL TRIGRAMS --> 10 



 ---- NOUN PHRASES ---- 

 ['processing', 'management'] 

 TOTAL NOUN PHRASES --> 2 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> ['Information']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['inform', 'process', '&', 'manag', ',', '43', '(', '6', ')', ',', '1588-1605', '.']

 TOTAL PORTER STEM WORDS ==> 12



 ---- SNOWBALL STEMMING ----

['inform', 'process', '&', 'manag', ',', '43', '(', '6', ')', ',', '1588-1605', '.']

 TOTAL SNOWBALL STEM WORDS ==> 12



 ---- LEMMATIZATION ----

['Information', 'processing', '&', 'management', ',', '43', '(', '6', ')', ',', '1588-1605', '.']

 TOTAL LEMMATIZE WORDS ==> 12

************************************************************************************************************************

610 --> [66] Wan, X. 


 ---- TOKENS ----

 ['[', '66', ']', 'Wan', ',', 'X', '.'] 

 TOTAL TOKENS ==> 7

 ---- POST ----

 [('[', 'RB'), ('66', 'CD'), (']', 'JJ'), ('Wan', 'NNP'), (',', ','), ('X', 'NNP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['[', '66', ']', 'Wan', ',', 'X', '.']

 TOTAL FILTERED TOKENS ==>  7

 ---- POST FOR FILTERED TOKENS ----

 [('[', 'RB'), ('66', 'CD'), (']', 'JJ'), ('Wan', 'NNP'), (',', ','), ('X', 'NNP'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['[ 66', '66 ]', '] Wan', 'Wan ,', ', X', 'X .'] 

 TOTAL BIGRAMS --> 6 



 ---- TRI-GRAMS ---- 

 ['[ 66 ]', '66 ] Wan', '] Wan ,', 'Wan , X', ', X .'] 

 TOTAL TRIGRAMS --> 5 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['[', '66', ']', 'wan', ',', 'x', '.']

 TOTAL PORTER STEM WORDS ==> 7



 ---- SNOWBALL STEMMING ----

['[', '66', ']', 'wan', ',', 'x', '.']

 TOTAL SNOWBALL STEM WORDS ==> 7



 ---- LEMMATIZATION ----

['[', '66', ']', 'Wan', ',', 'X', '.']

 TOTAL LEMMATIZE WORDS ==> 7

************************************************************************************************************************

611 --> (2008). 


 ---- TOKENS ----

 ['(', '2008', ')', '.'] 

 TOTAL TOKENS ==> 4

 ---- POST ----

 [('(', '('), ('2008', 'CD'), (')', ')'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['(', '2008', ')', '.']

 TOTAL FILTERED TOKENS ==>  4

 ---- POST FOR FILTERED TOKENS ----

 [('(', '('), ('2008', 'CD'), (')', ')'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['( 2008', '2008 )', ') .'] 

 TOTAL BIGRAMS --> 3 



 ---- TRI-GRAMS ---- 

 ['( 2008 )', '2008 ) .'] 

 TOTAL TRIGRAMS --> 2 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['(', '2008', ')', '.']

 TOTAL PORTER STEM WORDS ==> 4



 ---- SNOWBALL STEMMING ----

['(', '2008', ')', '.']

 TOTAL SNOWBALL STEM WORDS ==> 4



 ---- LEMMATIZATION ----

['(', '2008', ')', '.']

 TOTAL LEMMATIZE WORDS ==> 4

************************************************************************************************************************

612 --> Using only cross-document relationships for both generic and topic- focused multi-document summarizations. 


 ---- TOKENS ----

 ['Using', 'only', 'cross-document', 'relationships', 'for', 'both', 'generic', 'and', 'topic-', 'focused', 'multi-document', 'summarizations', '.'] 

 TOTAL TOKENS ==> 13

 ---- POST ----

 [('Using', 'VBG'), ('only', 'RB'), ('cross-document', 'JJ'), ('relationships', 'NNS'), ('for', 'IN'), ('both', 'DT'), ('generic', 'JJ'), ('and', 'CC'), ('topic-', 'JJ'), ('focused', 'JJ'), ('multi-document', 'JJ'), ('summarizations', 'NNS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Using', 'cross-document', 'relationships', 'generic', 'topic-', 'focused', 'multi-document', 'summarizations', '.']

 TOTAL FILTERED TOKENS ==>  9

 ---- POST FOR FILTERED TOKENS ----

 [('Using', 'VBG'), ('cross-document', 'JJ'), ('relationships', 'NNS'), ('generic', 'JJ'), ('topic-', 'JJ'), ('focused', 'VBD'), ('multi-document', 'JJ'), ('summarizations', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Using cross-document', 'cross-document relationships', 'relationships generic', 'generic topic-', 'topic- focused', 'focused multi-document', 'multi-document summarizations', 'summarizations .'] 

 TOTAL BIGRAMS --> 8 



 ---- TRI-GRAMS ---- 

 ['Using cross-document relationships', 'cross-document relationships generic', 'relationships generic topic-', 'generic topic- focused', 'topic- focused multi-document', 'focused multi-document summarizations', 'multi-document summarizations .'] 

 TOTAL TRIGRAMS --> 7 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['use', 'cross-docu', 'relationship', 'gener', 'topic-', 'focus', 'multi-docu', 'summar', '.']

 TOTAL PORTER STEM WORDS ==> 9



 ---- SNOWBALL STEMMING ----

['use', 'cross-docu', 'relationship', 'generic', 'topic-', 'focus', 'multi-docu', 'summar', '.']

 TOTAL SNOWBALL STEM WORDS ==> 9



 ---- LEMMATIZATION ----

['Using', 'cross-document', 'relationship', 'generic', 'topic-', 'focused', 'multi-document', 'summarization', '.']

 TOTAL LEMMATIZE WORDS ==> 9

************************************************************************************************************************

613 --> Information Retrieval, 11(1), 25-49. 


 ---- TOKENS ----

 ['Information', 'Retrieval', ',', '11', '(', '1', ')', ',', '25-49', '.'] 

 TOTAL TOKENS ==> 10

 ---- POST ----

 [('Information', 'NN'), ('Retrieval', 'NNP'), (',', ','), ('11', 'CD'), ('(', '('), ('1', 'CD'), (')', ')'), (',', ','), ('25-49', 'JJ'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Information', 'Retrieval', ',', '11', '(', '1', ')', ',', '25-49', '.']

 TOTAL FILTERED TOKENS ==>  10

 ---- POST FOR FILTERED TOKENS ----

 [('Information', 'NN'), ('Retrieval', 'NNP'), (',', ','), ('11', 'CD'), ('(', '('), ('1', 'CD'), (')', ')'), (',', ','), ('25-49', 'JJ'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Information Retrieval', 'Retrieval ,', ', 11', '11 (', '( 1', '1 )', ') ,', ', 25-49', '25-49 .'] 

 TOTAL BIGRAMS --> 9 



 ---- TRI-GRAMS ---- 

 ['Information Retrieval ,', 'Retrieval , 11', ', 11 (', '11 ( 1', '( 1 )', '1 ) ,', ') , 25-49', ', 25-49 .'] 

 TOTAL TRIGRAMS --> 8 



 ---- NOUN PHRASES ---- 

 ['Information'] 

 TOTAL NOUN PHRASES --> 1 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> ['Retrieval']
 TOTAL PERSON ENTITY --> 1 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['inform', 'retriev', ',', '11', '(', '1', ')', ',', '25-49', '.']

 TOTAL PORTER STEM WORDS ==> 10



 ---- SNOWBALL STEMMING ----

['inform', 'retriev', ',', '11', '(', '1', ')', ',', '25-49', '.']

 TOTAL SNOWBALL STEM WORDS ==> 10



 ---- LEMMATIZATION ----

['Information', 'Retrieval', ',', '11', '(', '1', ')', ',', '25-49', '.']

 TOTAL LEMMATIZE WORDS ==> 10

************************************************************************************************************************

614 --> [67] Ouyang, Y., Li, W., Li, S., & Lu, Q. 


 ---- TOKENS ----

 ['[', '67', ']', 'Ouyang', ',', 'Y.', ',', 'Li', ',', 'W.', ',', 'Li', ',', 'S.', ',', '&', 'Lu', ',', 'Q', '.'] 

 TOTAL TOKENS ==> 20

 ---- POST ----

 [('[', 'RB'), ('67', 'CD'), (']', 'NNP'), ('Ouyang', 'NNP'), (',', ','), ('Y.', 'NNP'), (',', ','), ('Li', 'NNP'), (',', ','), ('W.', 'NNP'), (',', ','), ('Li', 'NNP'), (',', ','), ('S.', 'NNP'), (',', ','), ('&', 'CC'), ('Lu', 'NNP'), (',', ','), ('Q', 'NNP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['[', '67', ']', 'Ouyang', ',', 'Y.', ',', 'Li', ',', 'W.', ',', 'Li', ',', 'S.', ',', '&', 'Lu', ',', 'Q', '.']

 TOTAL FILTERED TOKENS ==>  20

 ---- POST FOR FILTERED TOKENS ----

 [('[', 'RB'), ('67', 'CD'), (']', 'NNP'), ('Ouyang', 'NNP'), (',', ','), ('Y.', 'NNP'), (',', ','), ('Li', 'NNP'), (',', ','), ('W.', 'NNP'), (',', ','), ('Li', 'NNP'), (',', ','), ('S.', 'NNP'), (',', ','), ('&', 'CC'), ('Lu', 'NNP'), (',', ','), ('Q', 'NNP'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['[ 67', '67 ]', '] Ouyang', 'Ouyang ,', ', Y.', 'Y. ,', ', Li', 'Li ,', ', W.', 'W. ,', ', Li', 'Li ,', ', S.', 'S. ,', ', &', '& Lu', 'Lu ,', ', Q', 'Q .'] 

 TOTAL BIGRAMS --> 19 



 ---- TRI-GRAMS ---- 

 ['[ 67 ]', '67 ] Ouyang', '] Ouyang ,', 'Ouyang , Y.', ', Y. ,', 'Y. , Li', ', Li ,', 'Li , W.', ', W. ,', 'W. , Li', ', Li ,', 'Li , S.', ', S. ,', 'S. , &', ', & Lu', '& Lu ,', 'Lu , Q', ', Q .'] 

 TOTAL TRIGRAMS --> 18 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> ['Ouyang', 'Li', 'Li']
 TOTAL PERSON ENTITY --> 3 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['[', '67', ']', 'ouyang', ',', 'y.', ',', 'li', ',', 'w.', ',', 'li', ',', 's.', ',', '&', 'lu', ',', 'q', '.']

 TOTAL PORTER STEM WORDS ==> 20



 ---- SNOWBALL STEMMING ----

['[', '67', ']', 'ouyang', ',', 'y.', ',', 'li', ',', 'w.', ',', 'li', ',', 's.', ',', '&', 'lu', ',', 'q', '.']

 TOTAL SNOWBALL STEM WORDS ==> 20



 ---- LEMMATIZATION ----

['[', '67', ']', 'Ouyang', ',', 'Y.', ',', 'Li', ',', 'W.', ',', 'Li', ',', 'S.', ',', '&', 'Lu', ',', 'Q', '.']

 TOTAL LEMMATIZE WORDS ==> 20

************************************************************************************************************************

615 --> (2011). 


 ---- TOKENS ----

 ['(', '2011', ')', '.'] 

 TOTAL TOKENS ==> 4

 ---- POST ----

 [('(', '('), ('2011', 'CD'), (')', ')'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['(', '2011', ')', '.']

 TOTAL FILTERED TOKENS ==>  4

 ---- POST FOR FILTERED TOKENS ----

 [('(', '('), ('2011', 'CD'), (')', ')'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['( 2011', '2011 )', ') .'] 

 TOTAL BIGRAMS --> 3 



 ---- TRI-GRAMS ---- 

 ['( 2011 )', '2011 ) .'] 

 TOTAL TRIGRAMS --> 2 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['(', '2011', ')', '.']

 TOTAL PORTER STEM WORDS ==> 4



 ---- SNOWBALL STEMMING ----

['(', '2011', ')', '.']

 TOTAL SNOWBALL STEM WORDS ==> 4



 ---- LEMMATIZATION ----

['(', '2011', ')', '.']

 TOTAL LEMMATIZE WORDS ==> 4

************************************************************************************************************************

616 --> Applying regression models to query- focused multi-document summarization. 


 ---- TOKENS ----

 ['Applying', 'regression', 'models', 'to', 'query-', 'focused', 'multi-document', 'summarization', '.'] 

 TOTAL TOKENS ==> 9

 ---- POST ----

 [('Applying', 'VBG'), ('regression', 'NN'), ('models', 'NNS'), ('to', 'TO'), ('query-', 'JJ'), ('focused', 'JJ'), ('multi-document', 'JJ'), ('summarization', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Applying', 'regression', 'models', 'query-', 'focused', 'multi-document', 'summarization', '.']

 TOTAL FILTERED TOKENS ==>  8

 ---- POST FOR FILTERED TOKENS ----

 [('Applying', 'VBG'), ('regression', 'NN'), ('models', 'NNS'), ('query-', 'RB'), ('focused', 'VBD'), ('multi-document', 'JJ'), ('summarization', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Applying regression', 'regression models', 'models query-', 'query- focused', 'focused multi-document', 'multi-document summarization', 'summarization .'] 

 TOTAL BIGRAMS --> 7 



 ---- TRI-GRAMS ---- 

 ['Applying regression models', 'regression models query-', 'models query- focused', 'query- focused multi-document', 'focused multi-document summarization', 'multi-document summarization .'] 

 TOTAL TRIGRAMS --> 6 



 ---- NOUN PHRASES ---- 

 ['regression', 'multi-document summarization'] 

 TOTAL NOUN PHRASES --> 2 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['appli', 'regress', 'model', 'query-', 'focus', 'multi-docu', 'summar', '.']

 TOTAL PORTER STEM WORDS ==> 8



 ---- SNOWBALL STEMMING ----

['appli', 'regress', 'model', 'query-', 'focus', 'multi-docu', 'summar', '.']

 TOTAL SNOWBALL STEM WORDS ==> 8



 ---- LEMMATIZATION ----

['Applying', 'regression', 'model', 'query-', 'focused', 'multi-document', 'summarization', '.']

 TOTAL LEMMATIZE WORDS ==> 8

************************************************************************************************************************

617 --> Information Processing & Management, 47(2), 227- 237. 


 ---- TOKENS ----

 ['Information', 'Processing', '&', 'Management', ',', '47', '(', '2', ')', ',', '227-', '237', '.'] 

 TOTAL TOKENS ==> 13

 ---- POST ----

 [('Information', 'NNP'), ('Processing', 'NNP'), ('&', 'CC'), ('Management', 'NNP'), (',', ','), ('47', 'CD'), ('(', '('), ('2', 'CD'), (')', ')'), (',', ','), ('227-', 'JJ'), ('237', 'CD'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Information', 'Processing', '&', 'Management', ',', '47', '(', '2', ')', ',', '227-', '237', '.']

 TOTAL FILTERED TOKENS ==>  13

 ---- POST FOR FILTERED TOKENS ----

 [('Information', 'NNP'), ('Processing', 'NNP'), ('&', 'CC'), ('Management', 'NNP'), (',', ','), ('47', 'CD'), ('(', '('), ('2', 'CD'), (')', ')'), (',', ','), ('227-', 'JJ'), ('237', 'CD'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Information Processing', 'Processing &', '& Management', 'Management ,', ', 47', '47 (', '( 2', '2 )', ') ,', ', 227-', '227- 237', '237 .'] 

 TOTAL BIGRAMS --> 12 



 ---- TRI-GRAMS ---- 

 ['Information Processing &', 'Processing & Management', '& Management ,', 'Management , 47', ', 47 (', '47 ( 2', '( 2 )', '2 ) ,', ') , 227-', ', 227- 237', '227- 237 .'] 

 TOTAL TRIGRAMS --> 11 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['inform', 'process', '&', 'manag', ',', '47', '(', '2', ')', ',', '227-', '237', '.']

 TOTAL PORTER STEM WORDS ==> 13



 ---- SNOWBALL STEMMING ----

['inform', 'process', '&', 'manag', ',', '47', '(', '2', ')', ',', '227-', '237', '.']

 TOTAL SNOWBALL STEM WORDS ==> 13



 ---- LEMMATIZATION ----

['Information', 'Processing', '&', 'Management', ',', '47', '(', '2', ')', ',', '227-', '237', '.']

 TOTAL LEMMATIZE WORDS ==> 13

************************************************************************************************************************

618 --> [68] Mani, I., & Maybury, M. T. 


 ---- TOKENS ----

 ['[', '68', ']', 'Mani', ',', 'I.', ',', '&', 'Maybury', ',', 'M.', 'T', '.'] 

 TOTAL TOKENS ==> 13

 ---- POST ----

 [('[', 'RB'), ('68', 'CD'), (']', 'JJ'), ('Mani', 'NNP'), (',', ','), ('I.', 'NNP'), (',', ','), ('&', 'CC'), ('Maybury', 'NNP'), (',', ','), ('M.', 'NNP'), ('T', 'NNP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['[', '68', ']', 'Mani', ',', 'I.', ',', '&', 'Maybury', ',', 'M.', '.']

 TOTAL FILTERED TOKENS ==>  12

 ---- POST FOR FILTERED TOKENS ----

 [('[', 'RB'), ('68', 'CD'), (']', 'JJ'), ('Mani', 'NNP'), (',', ','), ('I.', 'NNP'), (',', ','), ('&', 'CC'), ('Maybury', 'NNP'), (',', ','), ('M.', 'NNP'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['[ 68', '68 ]', '] Mani', 'Mani ,', ', I.', 'I. ,', ', &', '& Maybury', 'Maybury ,', ', M.', 'M. .'] 

 TOTAL BIGRAMS --> 11 



 ---- TRI-GRAMS ---- 

 ['[ 68 ]', '68 ] Mani', '] Mani ,', 'Mani , I.', ', I. ,', 'I. , &', ', & Maybury', '& Maybury ,', 'Maybury , M.', ', M. .'] 

 TOTAL TRIGRAMS --> 10 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> ['Maybury']
 TOTAL PERSON ENTITY --> 1 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['[', '68', ']', 'mani', ',', 'i.', ',', '&', 'mayburi', ',', 'm.', '.']

 TOTAL PORTER STEM WORDS ==> 12



 ---- SNOWBALL STEMMING ----

['[', '68', ']', 'mani', ',', 'i.', ',', '&', 'mayburi', ',', 'm.', '.']

 TOTAL SNOWBALL STEM WORDS ==> 12



 ---- LEMMATIZATION ----

['[', '68', ']', 'Mani', ',', 'I.', ',', '&', 'Maybury', ',', 'M.', '.']

 TOTAL LEMMATIZE WORDS ==> 12

************************************************************************************************************************

619 --> (Eds.). 


 ---- TOKENS ----

 ['(', 'Eds', '.', ')', '.'] 

 TOTAL TOKENS ==> 5

 ---- POST ----

 [('(', '('), ('Eds', 'NNP'), ('.', '.'), (')', ')'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['(', 'Eds', '.', ')', '.']

 TOTAL FILTERED TOKENS ==>  5

 ---- POST FOR FILTERED TOKENS ----

 [('(', '('), ('Eds', 'NNP'), ('.', '.'), (')', ')'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['( Eds', 'Eds .', '. )', ') .'] 

 TOTAL BIGRAMS --> 4 



 ---- TRI-GRAMS ---- 

 ['( Eds .', 'Eds . )', '. ) .'] 

 TOTAL TRIGRAMS --> 3 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['(', 'ed', '.', ')', '.']

 TOTAL PORTER STEM WORDS ==> 5



 ---- SNOWBALL STEMMING ----

['(', 'ed', '.', ')', '.']

 TOTAL SNOWBALL STEM WORDS ==> 5



 ---- LEMMATIZATION ----

['(', 'Eds', '.', ')', '.']

 TOTAL LEMMATIZE WORDS ==> 5

************************************************************************************************************************

620 --> (1999). 


 ---- TOKENS ----

 ['(', '1999', ')', '.'] 

 TOTAL TOKENS ==> 4

 ---- POST ----

 [('(', '('), ('1999', 'CD'), (')', ')'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['(', '1999', ')', '.']

 TOTAL FILTERED TOKENS ==>  4

 ---- POST FOR FILTERED TOKENS ----

 [('(', '('), ('1999', 'CD'), (')', ')'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['( 1999', '1999 )', ') .'] 

 TOTAL BIGRAMS --> 3 



 ---- TRI-GRAMS ---- 

 ['( 1999 )', '1999 ) .'] 

 TOTAL TRIGRAMS --> 2 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['(', '1999', ')', '.']

 TOTAL PORTER STEM WORDS ==> 4



 ---- SNOWBALL STEMMING ----

['(', '1999', ')', '.']

 TOTAL SNOWBALL STEM WORDS ==> 4



 ---- LEMMATIZATION ----

['(', '1999', ')', '.']

 TOTAL LEMMATIZE WORDS ==> 4

************************************************************************************************************************

621 --> Advances in automatic text  summarization (Vol. 


 ---- TOKENS ----

 ['Advances', 'in', 'automatic', 'text', 'summarization', '(', 'Vol', '.'] 

 TOTAL TOKENS ==> 8

 ---- POST ----

 [('Advances', 'NNS'), ('in', 'IN'), ('automatic', 'JJ'), ('text', 'NN'), ('summarization', 'NN'), ('(', '('), ('Vol', 'NNP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Advances', 'automatic', 'text', 'summarization', '(', 'Vol', '.']

 TOTAL FILTERED TOKENS ==>  7

 ---- POST FOR FILTERED TOKENS ----

 [('Advances', 'NNS'), ('automatic', 'JJ'), ('text', 'JJ'), ('summarization', 'NN'), ('(', '('), ('Vol', 'NNP'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Advances automatic', 'automatic text', 'text summarization', 'summarization (', '( Vol', 'Vol .'] 

 TOTAL BIGRAMS --> 6 



 ---- TRI-GRAMS ---- 

 ['Advances automatic text', 'automatic text summarization', 'text summarization (', 'summarization ( Vol', '( Vol .'] 

 TOTAL TRIGRAMS --> 5 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 []

 ---- PORTER STEMMING ----

['advanc', 'automat', 'text', 'summar', '(', 'vol', '.']

 TOTAL PORTER STEM WORDS ==> 7



 ---- SNOWBALL STEMMING ----

['advanc', 'automat', 'text', 'summar', '(', 'vol', '.']

 TOTAL SNOWBALL STEM WORDS ==> 7



 ---- LEMMATIZATION ----

['Advances', 'automatic', 'text', 'summarization', '(', 'Vol', '.']

 TOTAL LEMMATIZE WORDS ==> 7

************************************************************************************************************************

622 --> 293). 


 ---- TOKENS ----

 ['293', ')', '.'] 

 TOTAL TOKENS ==> 3

 ---- POST ----

 [('293', 'CD'), (')', ')'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['293', ')', '.']

 TOTAL FILTERED TOKENS ==>  3

 ---- POST FOR FILTERED TOKENS ----

 [('293', 'CD'), (')', ')'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['293 )', ') .'] 

 TOTAL BIGRAMS --> 2 



 ---- TRI-GRAMS ---- 

 ['293 ) .'] 

 TOTAL TRIGRAMS --> 1 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 []

 ---- PORTER STEMMING ----

['293', ')', '.']

 TOTAL PORTER STEM WORDS ==> 3



 ---- SNOWBALL STEMMING ----

['293', ')', '.']

 TOTAL SNOWBALL STEM WORDS ==> 3



 ---- LEMMATIZATION ----

['293', ')', '.']

 TOTAL LEMMATIZE WORDS ==> 3

************************************************************************************************************************

623 --> Cambridge, MA: MIT press. 


 ---- TOKENS ----

 ['Cambridge', ',', 'MA', ':', 'MIT', 'press', '.'] 

 TOTAL TOKENS ==> 7

 ---- POST ----

 [('Cambridge', 'NNP'), (',', ','), ('MA', 'NNP'), (':', ':'), ('MIT', 'NNP'), ('press', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Cambridge', ',', ':', 'MIT', 'press', '.']

 TOTAL FILTERED TOKENS ==>  6

 ---- POST FOR FILTERED TOKENS ----

 [('Cambridge', 'NNP'), (',', ','), (':', ':'), ('MIT', 'NNP'), ('press', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Cambridge ,', ', :', ': MIT', 'MIT press', 'press .'] 

 TOTAL BIGRAMS --> 5 



 ---- TRI-GRAMS ---- 

 ['Cambridge , :', ', : MIT', ': MIT press', 'MIT press .'] 

 TOTAL TRIGRAMS --> 4 



 ---- NOUN PHRASES ---- 

 ['press'] 

 TOTAL NOUN PHRASES --> 1 



 ---- NER ----

 
 ORGANIZATION ---> ['MIT']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> ['Cambridge']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['cambridg', ',', ':', 'mit', 'press', '.']

 TOTAL PORTER STEM WORDS ==> 6



 ---- SNOWBALL STEMMING ----

['cambridg', ',', ':', 'mit', 'press', '.']

 TOTAL SNOWBALL STEM WORDS ==> 6



 ---- LEMMATIZATION ----

['Cambridge', ',', ':', 'MIT', 'press', '.']

 TOTAL LEMMATIZE WORDS ==> 6

************************************************************************************************************************

624 --> [69] Riedhammer, K., Favre, B., & Hakkani-Tür, D. (2010). 


 ---- TOKENS ----

 ['[', '69', ']', 'Riedhammer', ',', 'K.', ',', 'Favre', ',', 'B.', ',', '&', 'Hakkani-Tür', ',', 'D.', '(', '2010', ')', '.'] 

 TOTAL TOKENS ==> 19

 ---- POST ----

 [('[', 'RB'), ('69', 'CD'), (']', 'JJ'), ('Riedhammer', 'NNP'), (',', ','), ('K.', 'NNP'), (',', ','), ('Favre', 'NNP'), (',', ','), ('B.', 'NNP'), (',', ','), ('&', 'CC'), ('Hakkani-Tür', 'NNP'), (',', ','), ('D.', 'NNP'), ('(', '('), ('2010', 'CD'), (')', ')'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['[', '69', ']', 'Riedhammer', ',', 'K.', ',', 'Favre', ',', 'B.', ',', '&', 'Hakkani-Tür', ',', 'D.', '(', '2010', ')', '.']

 TOTAL FILTERED TOKENS ==>  19

 ---- POST FOR FILTERED TOKENS ----

 [('[', 'RB'), ('69', 'CD'), (']', 'JJ'), ('Riedhammer', 'NNP'), (',', ','), ('K.', 'NNP'), (',', ','), ('Favre', 'NNP'), (',', ','), ('B.', 'NNP'), (',', ','), ('&', 'CC'), ('Hakkani-Tür', 'NNP'), (',', ','), ('D.', 'NNP'), ('(', '('), ('2010', 'CD'), (')', ')'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['[ 69', '69 ]', '] Riedhammer', 'Riedhammer ,', ', K.', 'K. ,', ', Favre', 'Favre ,', ', B.', 'B. ,', ', &', '& Hakkani-Tür', 'Hakkani-Tür ,', ', D.', 'D. (', '( 2010', '2010 )', ') .'] 

 TOTAL BIGRAMS --> 18 



 ---- TRI-GRAMS ---- 

 ['[ 69 ]', '69 ] Riedhammer', '] Riedhammer ,', 'Riedhammer , K.', ', K. ,', 'K. , Favre', ', Favre ,', 'Favre , B.', ', B. ,', 'B. , &', ', & Hakkani-Tür', '& Hakkani-Tür ,', 'Hakkani-Tür , D.', ', D. (', 'D. ( 2010', '( 2010 )', '2010 ) .'] 

 TOTAL TRIGRAMS --> 17 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> ['Favre']
 TOTAL PERSON ENTITY --> 1 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['[', '69', ']', 'riedhamm', ',', 'k.', ',', 'favr', ',', 'b.', ',', '&', 'hakkani-tür', ',', 'd.', '(', '2010', ')', '.']

 TOTAL PORTER STEM WORDS ==> 19



 ---- SNOWBALL STEMMING ----

['[', '69', ']', 'riedhamm', ',', 'k.', ',', 'favr', ',', 'b.', ',', '&', 'hakkani-tür', ',', 'd.', '(', '2010', ')', '.']

 TOTAL SNOWBALL STEM WORDS ==> 19



 ---- LEMMATIZATION ----

['[', '69', ']', 'Riedhammer', ',', 'K.', ',', 'Favre', ',', 'B.', ',', '&', 'Hakkani-Tür', ',', 'D.', '(', '2010', ')', '.']

 TOTAL LEMMATIZE WORDS ==> 19

************************************************************************************************************************

625 --> Long story short–global  unsupervised models for keyphrase based meeting summarization. 


 ---- TOKENS ----

 ['Long', 'story', 'short–global', 'unsupervised', 'models', 'for', 'keyphrase', 'based', 'meeting', 'summarization', '.'] 

 TOTAL TOKENS ==> 11

 ---- POST ----

 [('Long', 'NNP'), ('story', 'NN'), ('short–global', 'NN'), ('unsupervised', 'JJ'), ('models', 'NNS'), ('for', 'IN'), ('keyphrase', 'NN'), ('based', 'VBN'), ('meeting', 'NN'), ('summarization', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Long', 'story', 'short–global', 'unsupervised', 'models', 'keyphrase', 'based', 'meeting', 'summarization', '.']

 TOTAL FILTERED TOKENS ==>  10

 ---- POST FOR FILTERED TOKENS ----

 [('Long', 'NNP'), ('story', 'NN'), ('short–global', 'NN'), ('unsupervised', 'JJ'), ('models', 'NNS'), ('keyphrase', 'VBP'), ('based', 'VBN'), ('meeting', 'NN'), ('summarization', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Long story', 'story short–global', 'short–global unsupervised', 'unsupervised models', 'models keyphrase', 'keyphrase based', 'based meeting', 'meeting summarization', 'summarization .'] 

 TOTAL BIGRAMS --> 9 



 ---- TRI-GRAMS ---- 

 ['Long story short–global', 'story short–global unsupervised', 'short–global unsupervised models', 'unsupervised models keyphrase', 'models keyphrase based', 'keyphrase based meeting', 'based meeting summarization', 'meeting summarization .'] 

 TOTAL TRIGRAMS --> 8 



 ---- NOUN PHRASES ---- 

 ['story', 'short–global', 'meeting', 'summarization'] 

 TOTAL NOUN PHRASES --> 4 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> ['Long']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['long', 'stori', 'short–glob', 'unsupervis', 'model', 'keyphras', 'base', 'meet', 'summar', '.']

 TOTAL PORTER STEM WORDS ==> 10



 ---- SNOWBALL STEMMING ----

['long', 'stori', 'short–glob', 'unsupervis', 'model', 'keyphras', 'base', 'meet', 'summar', '.']

 TOTAL SNOWBALL STEM WORDS ==> 10



 ---- LEMMATIZATION ----

['Long', 'story', 'short–global', 'unsupervised', 'model', 'keyphrase', 'based', 'meeting', 'summarization', '.']

 TOTAL LEMMATIZE WORDS ==> 10

************************************************************************************************************************

626 --> Speech  Communication, 52(10), 801-815. 


 ---- TOKENS ----

 ['Speech', 'Communication', ',', '52', '(', '10', ')', ',', '801-815', '.'] 

 TOTAL TOKENS ==> 10

 ---- POST ----

 [('Speech', 'NNP'), ('Communication', 'NNP'), (',', ','), ('52', 'CD'), ('(', '('), ('10', 'CD'), (')', ')'), (',', ','), ('801-815', 'JJ'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Speech', 'Communication', ',', '52', '(', '10', ')', ',', '801-815', '.']

 TOTAL FILTERED TOKENS ==>  10

 ---- POST FOR FILTERED TOKENS ----

 [('Speech', 'NNP'), ('Communication', 'NNP'), (',', ','), ('52', 'CD'), ('(', '('), ('10', 'CD'), (')', ')'), (',', ','), ('801-815', 'JJ'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Speech Communication', 'Communication ,', ', 52', '52 (', '( 10', '10 )', ') ,', ', 801-815', '801-815 .'] 

 TOTAL BIGRAMS --> 9 



 ---- TRI-GRAMS ---- 

 ['Speech Communication ,', 'Communication , 52', ', 52 (', '52 ( 10', '( 10 )', '10 ) ,', ') , 801-815', ', 801-815 .'] 

 TOTAL TRIGRAMS --> 8 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> ['Communication']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> ['Speech']
 TOTAL PERSON ENTITY --> 1 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['speech', 'commun', ',', '52', '(', '10', ')', ',', '801-815', '.']

 TOTAL PORTER STEM WORDS ==> 10



 ---- SNOWBALL STEMMING ----

['speech', 'communic', ',', '52', '(', '10', ')', ',', '801-815', '.']

 TOTAL SNOWBALL STEM WORDS ==> 10



 ---- LEMMATIZATION ----

['Speech', 'Communication', ',', '52', '(', '10', ')', ',', '801-815', '.']

 TOTAL LEMMATIZE WORDS ==> 10

************************************************************************************************************************

627 --> [70] Wang, D., Zhu, S., Li, T., & Gong, Y. 


 ---- TOKENS ----

 ['[', '70', ']', 'Wang', ',', 'D.', ',', 'Zhu', ',', 'S.', ',', 'Li', ',', 'T.', ',', '&', 'Gong', ',', 'Y', '.'] 

 TOTAL TOKENS ==> 20

 ---- POST ----

 [('[', 'RB'), ('70', 'CD'), (']', 'NNP'), ('Wang', 'NNP'), (',', ','), ('D.', 'NNP'), (',', ','), ('Zhu', 'NNP'), (',', ','), ('S.', 'NNP'), (',', ','), ('Li', 'NNP'), (',', ','), ('T.', 'NNP'), (',', ','), ('&', 'CC'), ('Gong', 'NNP'), (',', ','), ('Y', 'NNP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['[', '70', ']', 'Wang', ',', 'D.', ',', 'Zhu', ',', 'S.', ',', 'Li', ',', 'T.', ',', '&', 'Gong', ',', '.']

 TOTAL FILTERED TOKENS ==>  19

 ---- POST FOR FILTERED TOKENS ----

 [('[', 'RB'), ('70', 'CD'), (']', 'NNP'), ('Wang', 'NNP'), (',', ','), ('D.', 'NNP'), (',', ','), ('Zhu', 'NNP'), (',', ','), ('S.', 'NNP'), (',', ','), ('Li', 'NNP'), (',', ','), ('T.', 'NNP'), (',', ','), ('&', 'CC'), ('Gong', 'NNP'), (',', ','), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['[ 70', '70 ]', '] Wang', 'Wang ,', ', D.', 'D. ,', ', Zhu', 'Zhu ,', ', S.', 'S. ,', ', Li', 'Li ,', ', T.', 'T. ,', ', &', '& Gong', 'Gong ,', ', .'] 

 TOTAL BIGRAMS --> 18 



 ---- TRI-GRAMS ---- 

 ['[ 70 ]', '70 ] Wang', '] Wang ,', 'Wang , D.', ', D. ,', 'D. , Zhu', ', Zhu ,', 'Zhu , S.', ', S. ,', 'S. , Li', ', Li ,', 'Li , T.', ', T. ,', 'T. , &', ', & Gong', '& Gong ,', 'Gong , .'] 

 TOTAL TRIGRAMS --> 17 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> ['Wang', 'Zhu', 'Li']
 TOTAL PERSON ENTITY --> 3 


 GPE ---> ['Gong']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['[', '70', ']', 'wang', ',', 'd.', ',', 'zhu', ',', 's.', ',', 'li', ',', 't.', ',', '&', 'gong', ',', '.']

 TOTAL PORTER STEM WORDS ==> 19



 ---- SNOWBALL STEMMING ----

['[', '70', ']', 'wang', ',', 'd.', ',', 'zhu', ',', 's.', ',', 'li', ',', 't.', ',', '&', 'gong', ',', '.']

 TOTAL SNOWBALL STEM WORDS ==> 19



 ---- LEMMATIZATION ----

['[', '70', ']', 'Wang', ',', 'D.', ',', 'Zhu', ',', 'S.', ',', 'Li', ',', 'T.', ',', '&', 'Gong', ',', '.']

 TOTAL LEMMATIZE WORDS ==> 19

************************************************************************************************************************

628 --> (2009, August). 


 ---- TOKENS ----

 ['(', '2009', ',', 'August', ')', '.'] 

 TOTAL TOKENS ==> 6

 ---- POST ----

 [('(', '('), ('2009', 'CD'), (',', ','), ('August', 'NNP'), (')', ')'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['(', '2009', ',', 'August', ')', '.']

 TOTAL FILTERED TOKENS ==>  6

 ---- POST FOR FILTERED TOKENS ----

 [('(', '('), ('2009', 'CD'), (',', ','), ('August', 'NNP'), (')', ')'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['( 2009', '2009 ,', ', August', 'August )', ') .'] 

 TOTAL BIGRAMS --> 5 



 ---- TRI-GRAMS ---- 

 ['( 2009 ,', '2009 , August', ', August )', 'August ) .'] 

 TOTAL TRIGRAMS --> 4 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['(', '2009', ',', 'august', ')', '.']

 TOTAL PORTER STEM WORDS ==> 6



 ---- SNOWBALL STEMMING ----

['(', '2009', ',', 'august', ')', '.']

 TOTAL SNOWBALL STEM WORDS ==> 6



 ---- LEMMATIZATION ----

['(', '2009', ',', 'August', ')', '.']

 TOTAL LEMMATIZE WORDS ==> 6

************************************************************************************************************************

629 --> Multi-document summarization  using sentence-based topic models. 


 ---- TOKENS ----

 ['Multi-document', 'summarization', 'using', 'sentence-based', 'topic', 'models', '.'] 

 TOTAL TOKENS ==> 7

 ---- POST ----

 [('Multi-document', 'JJ'), ('summarization', 'NN'), ('using', 'VBG'), ('sentence-based', 'JJ'), ('topic', 'NN'), ('models', 'NNS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Multi-document', 'summarization', 'using', 'sentence-based', 'topic', 'models', '.']

 TOTAL FILTERED TOKENS ==>  7

 ---- POST FOR FILTERED TOKENS ----

 [('Multi-document', 'JJ'), ('summarization', 'NN'), ('using', 'VBG'), ('sentence-based', 'JJ'), ('topic', 'NN'), ('models', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Multi-document summarization', 'summarization using', 'using sentence-based', 'sentence-based topic', 'topic models', 'models .'] 

 TOTAL BIGRAMS --> 6 



 ---- TRI-GRAMS ---- 

 ['Multi-document summarization using', 'summarization using sentence-based', 'using sentence-based topic', 'sentence-based topic models', 'topic models .'] 

 TOTAL TRIGRAMS --> 5 



 ---- NOUN PHRASES ---- 

 ['Multi-document summarization', 'sentence-based topic'] 

 TOTAL NOUN PHRASES --> 2 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['multi-docu', 'summar', 'use', 'sentence-bas', 'topic', 'model', '.']

 TOTAL PORTER STEM WORDS ==> 7



 ---- SNOWBALL STEMMING ----

['multi-docu', 'summar', 'use', 'sentence-bas', 'topic', 'model', '.']

 TOTAL SNOWBALL STEM WORDS ==> 7



 ---- LEMMATIZATION ----

['Multi-document', 'summarization', 'using', 'sentence-based', 'topic', 'model', '.']

 TOTAL LEMMATIZE WORDS ==> 7

************************************************************************************************************************

630 --> In Proceedings of the ACL-IJCNLP 2009 Conference  Short Papers (pp. 


 ---- TOKENS ----

 ['In', 'Proceedings', 'of', 'the', 'ACL-IJCNLP', '2009', 'Conference', 'Short', 'Papers', '(', 'pp', '.'] 

 TOTAL TOKENS ==> 12

 ---- POST ----

 [('In', 'IN'), ('Proceedings', 'NNP'), ('of', 'IN'), ('the', 'DT'), ('ACL-IJCNLP', 'JJ'), ('2009', 'CD'), ('Conference', 'NNP'), ('Short', 'NNP'), ('Papers', 'NNP'), ('(', '('), ('pp', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Proceedings', 'ACL-IJCNLP', '2009', 'Conference', 'Short', 'Papers', '(', 'pp', '.']

 TOTAL FILTERED TOKENS ==>  9

 ---- POST FOR FILTERED TOKENS ----

 [('Proceedings', 'NNS'), ('ACL-IJCNLP', 'NNP'), ('2009', 'CD'), ('Conference', 'NNP'), ('Short', 'NNP'), ('Papers', 'NNP'), ('(', '('), ('pp', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Proceedings ACL-IJCNLP', 'ACL-IJCNLP 2009', '2009 Conference', 'Conference Short', 'Short Papers', 'Papers (', '( pp', 'pp .'] 

 TOTAL BIGRAMS --> 8 



 ---- TRI-GRAMS ---- 

 ['Proceedings ACL-IJCNLP 2009', 'ACL-IJCNLP 2009 Conference', '2009 Conference Short', 'Conference Short Papers', 'Short Papers (', 'Papers ( pp', '( pp .'] 

 TOTAL TRIGRAMS --> 7 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 []

 ---- PORTER STEMMING ----

['proceed', 'acl-ijcnlp', '2009', 'confer', 'short', 'paper', '(', 'pp', '.']

 TOTAL PORTER STEM WORDS ==> 9



 ---- SNOWBALL STEMMING ----

['proceed', 'acl-ijcnlp', '2009', 'confer', 'short', 'paper', '(', 'pp', '.']

 TOTAL SNOWBALL STEM WORDS ==> 9



 ---- LEMMATIZATION ----

['Proceedings', 'ACL-IJCNLP', '2009', 'Conference', 'Short', 'Papers', '(', 'pp', '.']

 TOTAL LEMMATIZE WORDS ==> 9

************************************************************************************************************************

631 --> 297-300). 


 ---- TOKENS ----

 ['297-300', ')', '.'] 

 TOTAL TOKENS ==> 3

 ---- POST ----

 [('297-300', 'JJ'), (')', ')'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['297-300', ')', '.']

 TOTAL FILTERED TOKENS ==>  3

 ---- POST FOR FILTERED TOKENS ----

 [('297-300', 'JJ'), (')', ')'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['297-300 )', ') .'] 

 TOTAL BIGRAMS --> 2 



 ---- TRI-GRAMS ---- 

 ['297-300 ) .'] 

 TOTAL TRIGRAMS --> 1 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 []

 ---- PORTER STEMMING ----

['297-300', ')', '.']

 TOTAL PORTER STEM WORDS ==> 3



 ---- SNOWBALL STEMMING ----

['297-300', ')', '.']

 TOTAL SNOWBALL STEM WORDS ==> 3



 ---- LEMMATIZATION ----

['297-300', ')', '.']

 TOTAL LEMMATIZE WORDS ==> 3

************************************************************************************************************************

632 --> Association for Computational Linguistics. 


 ---- TOKENS ----

 ['Association', 'for', 'Computational', 'Linguistics', '.'] 

 TOTAL TOKENS ==> 5

 ---- POST ----

 [('Association', 'NNP'), ('for', 'IN'), ('Computational', 'NNP'), ('Linguistics', 'NNP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Association', 'Computational', 'Linguistics', '.']

 TOTAL FILTERED TOKENS ==>  4

 ---- POST FOR FILTERED TOKENS ----

 [('Association', 'NNP'), ('Computational', 'NNP'), ('Linguistics', 'NNP'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Association Computational', 'Computational Linguistics', 'Linguistics .'] 

 TOTAL BIGRAMS --> 3 



 ---- TRI-GRAMS ---- 

 ['Association Computational Linguistics', 'Computational Linguistics .'] 

 TOTAL TRIGRAMS --> 2 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['associ', 'comput', 'linguist', '.']

 TOTAL PORTER STEM WORDS ==> 4



 ---- SNOWBALL STEMMING ----

['associ', 'comput', 'linguist', '.']

 TOTAL SNOWBALL STEM WORDS ==> 4



 ---- LEMMATIZATION ----

['Association', 'Computational', 'Linguistics', '.']

 TOTAL LEMMATIZE WORDS ==> 4

************************************************************************************************************************

633 --> [71] Wang, D., Zhu, S., Li, T., Chi, Y., & Gong, Y. 


 ---- TOKENS ----

 ['[', '71', ']', 'Wang', ',', 'D.', ',', 'Zhu', ',', 'S.', ',', 'Li', ',', 'T.', ',', 'Chi', ',', 'Y.', ',', '&', 'Gong', ',', 'Y', '.'] 

 TOTAL TOKENS ==> 24

 ---- POST ----

 [('[', 'RB'), ('71', 'CD'), (']', 'NNP'), ('Wang', 'NNP'), (',', ','), ('D.', 'NNP'), (',', ','), ('Zhu', 'NNP'), (',', ','), ('S.', 'NNP'), (',', ','), ('Li', 'NNP'), (',', ','), ('T.', 'NNP'), (',', ','), ('Chi', 'NNP'), (',', ','), ('Y.', 'NNP'), (',', ','), ('&', 'CC'), ('Gong', 'NNP'), (',', ','), ('Y', 'NNP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['[', '71', ']', 'Wang', ',', 'D.', ',', 'Zhu', ',', 'S.', ',', 'Li', ',', 'T.', ',', 'Chi', ',', 'Y.', ',', '&', 'Gong', ',', '.']

 TOTAL FILTERED TOKENS ==>  23

 ---- POST FOR FILTERED TOKENS ----

 [('[', 'RB'), ('71', 'CD'), (']', 'NNP'), ('Wang', 'NNP'), (',', ','), ('D.', 'NNP'), (',', ','), ('Zhu', 'NNP'), (',', ','), ('S.', 'NNP'), (',', ','), ('Li', 'NNP'), (',', ','), ('T.', 'NNP'), (',', ','), ('Chi', 'NNP'), (',', ','), ('Y.', 'NNP'), (',', ','), ('&', 'CC'), ('Gong', 'NNP'), (',', ','), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['[ 71', '71 ]', '] Wang', 'Wang ,', ', D.', 'D. ,', ', Zhu', 'Zhu ,', ', S.', 'S. ,', ', Li', 'Li ,', ', T.', 'T. ,', ', Chi', 'Chi ,', ', Y.', 'Y. ,', ', &', '& Gong', 'Gong ,', ', .'] 

 TOTAL BIGRAMS --> 22 



 ---- TRI-GRAMS ---- 

 ['[ 71 ]', '71 ] Wang', '] Wang ,', 'Wang , D.', ', D. ,', 'D. , Zhu', ', Zhu ,', 'Zhu , S.', ', S. ,', 'S. , Li', ', Li ,', 'Li , T.', ', T. ,', 'T. , Chi', ', Chi ,', 'Chi , Y.', ', Y. ,', 'Y. , &', ', & Gong', '& Gong ,', 'Gong , .'] 

 TOTAL TRIGRAMS --> 21 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> ['Wang', 'Zhu', 'Li']
 TOTAL PERSON ENTITY --> 3 


 GPE ---> ['Chi', 'Gong']
 TOTAL GPE ENTITY --> 2 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['[', '71', ']', 'wang', ',', 'd.', ',', 'zhu', ',', 's.', ',', 'li', ',', 't.', ',', 'chi', ',', 'y.', ',', '&', 'gong', ',', '.']

 TOTAL PORTER STEM WORDS ==> 23



 ---- SNOWBALL STEMMING ----

['[', '71', ']', 'wang', ',', 'd.', ',', 'zhu', ',', 's.', ',', 'li', ',', 't.', ',', 'chi', ',', 'y.', ',', '&', 'gong', ',', '.']

 TOTAL SNOWBALL STEM WORDS ==> 23



 ---- LEMMATIZATION ----

['[', '71', ']', 'Wang', ',', 'D.', ',', 'Zhu', ',', 'S.', ',', 'Li', ',', 'T.', ',', 'Chi', ',', 'Y.', ',', '&', 'Gong', ',', '.']

 TOTAL LEMMATIZE WORDS ==> 23

************************************************************************************************************************

634 --> (2011). 


 ---- TOKENS ----

 ['(', '2011', ')', '.'] 

 TOTAL TOKENS ==> 4

 ---- POST ----

 [('(', '('), ('2011', 'CD'), (')', ')'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['(', '2011', ')', '.']

 TOTAL FILTERED TOKENS ==>  4

 ---- POST FOR FILTERED TOKENS ----

 [('(', '('), ('2011', 'CD'), (')', ')'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['( 2011', '2011 )', ') .'] 

 TOTAL BIGRAMS --> 3 



 ---- TRI-GRAMS ---- 

 ['( 2011 )', '2011 ) .'] 

 TOTAL TRIGRAMS --> 2 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['(', '2011', ')', '.']

 TOTAL PORTER STEM WORDS ==> 4



 ---- SNOWBALL STEMMING ----

['(', '2011', ')', '.']

 TOTAL SNOWBALL STEM WORDS ==> 4



 ---- LEMMATIZATION ----

['(', '2011', ')', '.']

 TOTAL LEMMATIZE WORDS ==> 4

************************************************************************************************************************

635 --> Integrating document clustering  and multidocument summarization. 


 ---- TOKENS ----

 ['Integrating', 'document', 'clustering', 'and', 'multidocument', 'summarization', '.'] 

 TOTAL TOKENS ==> 7

 ---- POST ----

 [('Integrating', 'VBG'), ('document', 'NN'), ('clustering', 'NN'), ('and', 'CC'), ('multidocument', 'JJ'), ('summarization', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Integrating', 'document', 'clustering', 'multidocument', 'summarization', '.']

 TOTAL FILTERED TOKENS ==>  6

 ---- POST FOR FILTERED TOKENS ----

 [('Integrating', 'VBG'), ('document', 'NN'), ('clustering', 'VBG'), ('multidocument', 'JJ'), ('summarization', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Integrating document', 'document clustering', 'clustering multidocument', 'multidocument summarization', 'summarization .'] 

 TOTAL BIGRAMS --> 5 



 ---- TRI-GRAMS ---- 

 ['Integrating document clustering', 'document clustering multidocument', 'clustering multidocument summarization', 'multidocument summarization .'] 

 TOTAL TRIGRAMS --> 4 



 ---- NOUN PHRASES ---- 

 ['document', 'multidocument summarization'] 

 TOTAL NOUN PHRASES --> 2 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['integr', 'document', 'cluster', 'multidocu', 'summar', '.']

 TOTAL PORTER STEM WORDS ==> 6



 ---- SNOWBALL STEMMING ----

['integr', 'document', 'cluster', 'multidocu', 'summar', '.']

 TOTAL SNOWBALL STEM WORDS ==> 6



 ---- LEMMATIZATION ----

['Integrating', 'document', 'clustering', 'multidocument', 'summarization', '.']

 TOTAL LEMMATIZE WORDS ==> 6

************************************************************************************************************************

636 --> ACM Transactions on Knowledge Discovery from Data  (TKDD), 5(3), 14. 


 ---- TOKENS ----

 ['ACM', 'Transactions', 'on', 'Knowledge', 'Discovery', 'from', 'Data', '(', 'TKDD', ')', ',', '5', '(', '3', ')', ',', '14', '.'] 

 TOTAL TOKENS ==> 18

 ---- POST ----

 [('ACM', 'NNP'), ('Transactions', 'NNP'), ('on', 'IN'), ('Knowledge', 'NNP'), ('Discovery', 'NNP'), ('from', 'IN'), ('Data', 'NNP'), ('(', '('), ('TKDD', 'NNP'), (')', ')'), (',', ','), ('5', 'CD'), ('(', '('), ('3', 'CD'), (')', ')'), (',', ','), ('14', 'CD'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['ACM', 'Transactions', 'Knowledge', 'Discovery', 'Data', '(', 'TKDD', ')', ',', '5', '(', '3', ')', ',', '14', '.']

 TOTAL FILTERED TOKENS ==>  16

 ---- POST FOR FILTERED TOKENS ----

 [('ACM', 'NNP'), ('Transactions', 'NNP'), ('Knowledge', 'NNP'), ('Discovery', 'NNP'), ('Data', 'NNP'), ('(', '('), ('TKDD', 'NNP'), (')', ')'), (',', ','), ('5', 'CD'), ('(', '('), ('3', 'CD'), (')', ')'), (',', ','), ('14', 'CD'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['ACM Transactions', 'Transactions Knowledge', 'Knowledge Discovery', 'Discovery Data', 'Data (', '( TKDD', 'TKDD )', ') ,', ', 5', '5 (', '( 3', '3 )', ') ,', ', 14', '14 .'] 

 TOTAL BIGRAMS --> 15 



 ---- TRI-GRAMS ---- 

 ['ACM Transactions Knowledge', 'Transactions Knowledge Discovery', 'Knowledge Discovery Data', 'Discovery Data (', 'Data ( TKDD', '( TKDD )', 'TKDD ) ,', ') , 5', ', 5 (', '5 ( 3', '( 3 )', '3 ) ,', ') , 14', ', 14 .'] 

 TOTAL TRIGRAMS --> 14 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> ['ACM Transactions']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['acm', 'transact', 'knowledg', 'discoveri', 'data', '(', 'tkdd', ')', ',', '5', '(', '3', ')', ',', '14', '.']

 TOTAL PORTER STEM WORDS ==> 16



 ---- SNOWBALL STEMMING ----

['acm', 'transact', 'knowledg', 'discoveri', 'data', '(', 'tkdd', ')', ',', '5', '(', '3', ')', ',', '14', '.']

 TOTAL SNOWBALL STEM WORDS ==> 16



 ---- LEMMATIZATION ----

['ACM', 'Transactions', 'Knowledge', 'Discovery', 'Data', '(', 'TKDD', ')', ',', '5', '(', '3', ')', ',', '14', '.']

 TOTAL LEMMATIZE WORDS ==> 16

************************************************************************************************************************

637 --> [72] Fang, H., Lu, W., Wu, F., Zhang, Y., Shang, X., Shao, J., & Zhuang, Y. 


 ---- TOKENS ----

 ['[', '72', ']', 'Fang', ',', 'H.', ',', 'Lu', ',', 'W.', ',', 'Wu', ',', 'F.', ',', 'Zhang', ',', 'Y.', ',', 'Shang', ',', 'X.', ',', 'Shao', ',', 'J.', ',', '&', 'Zhuang', ',', 'Y', '.'] 

 TOTAL TOKENS ==> 32

 ---- POST ----

 [('[', 'RB'), ('72', 'CD'), (']', 'NNP'), ('Fang', 'NNP'), (',', ','), ('H.', 'NNP'), (',', ','), ('Lu', 'NNP'), (',', ','), ('W.', 'NNP'), (',', ','), ('Wu', 'NNP'), (',', ','), ('F.', 'NNP'), (',', ','), ('Zhang', 'NNP'), (',', ','), ('Y.', 'NNP'), (',', ','), ('Shang', 'NNP'), (',', ','), ('X.', 'NNP'), (',', ','), ('Shao', 'NNP'), (',', ','), ('J.', 'NNP'), (',', ','), ('&', 'CC'), ('Zhuang', 'NNP'), (',', ','), ('Y', 'NNP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['[', '72', ']', 'Fang', ',', 'H.', ',', 'Lu', ',', 'W.', ',', 'Wu', ',', 'F.', ',', 'Zhang', ',', 'Y.', ',', 'Shang', ',', 'X.', ',', 'Shao', ',', 'J.', ',', '&', 'Zhuang', ',', '.']

 TOTAL FILTERED TOKENS ==>  31

 ---- POST FOR FILTERED TOKENS ----

 [('[', 'RB'), ('72', 'CD'), (']', 'NNP'), ('Fang', 'NNP'), (',', ','), ('H.', 'NNP'), (',', ','), ('Lu', 'NNP'), (',', ','), ('W.', 'NNP'), (',', ','), ('Wu', 'NNP'), (',', ','), ('F.', 'NNP'), (',', ','), ('Zhang', 'NNP'), (',', ','), ('Y.', 'NNP'), (',', ','), ('Shang', 'NNP'), (',', ','), ('X.', 'NNP'), (',', ','), ('Shao', 'NNP'), (',', ','), ('J.', 'NNP'), (',', ','), ('&', 'CC'), ('Zhuang', 'NNP'), (',', ','), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['[ 72', '72 ]', '] Fang', 'Fang ,', ', H.', 'H. ,', ', Lu', 'Lu ,', ', W.', 'W. ,', ', Wu', 'Wu ,', ', F.', 'F. ,', ', Zhang', 'Zhang ,', ', Y.', 'Y. ,', ', Shang', 'Shang ,', ', X.', 'X. ,', ', Shao', 'Shao ,', ', J.', 'J. ,', ', &', '& Zhuang', 'Zhuang ,', ', .'] 

 TOTAL BIGRAMS --> 30 



 ---- TRI-GRAMS ---- 

 ['[ 72 ]', '72 ] Fang', '] Fang ,', 'Fang , H.', ', H. ,', 'H. , Lu', ', Lu ,', 'Lu , W.', ', W. ,', 'W. , Wu', ', Wu ,', 'Wu , F.', ', F. ,', 'F. , Zhang', ', Zhang ,', 'Zhang , Y.', ', Y. ,', 'Y. , Shang', ', Shang ,', 'Shang , X.', ', X. ,', 'X. , Shao', ', Shao ,', 'Shao , J.', ', J. ,', 'J. , &', ', & Zhuang', '& Zhuang ,', 'Zhuang , .'] 

 TOTAL TRIGRAMS --> 29 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> ['Fang', 'Zhang', 'Shang', 'Shao', 'Zhuang']
 TOTAL PERSON ENTITY --> 5 


 GPE ---> ['Lu', 'Wu']
 TOTAL GPE ENTITY --> 2 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['[', '72', ']', 'fang', ',', 'h.', ',', 'lu', ',', 'w.', ',', 'wu', ',', 'f.', ',', 'zhang', ',', 'y.', ',', 'shang', ',', 'x.', ',', 'shao', ',', 'j.', ',', '&', 'zhuang', ',', '.']

 TOTAL PORTER STEM WORDS ==> 31



 ---- SNOWBALL STEMMING ----

['[', '72', ']', 'fang', ',', 'h.', ',', 'lu', ',', 'w.', ',', 'wu', ',', 'f.', ',', 'zhang', ',', 'y.', ',', 'shang', ',', 'x.', ',', 'shao', ',', 'j.', ',', '&', 'zhuang', ',', '.']

 TOTAL SNOWBALL STEM WORDS ==> 31



 ---- LEMMATIZATION ----

['[', '72', ']', 'Fang', ',', 'H.', ',', 'Lu', ',', 'W.', ',', 'Wu', ',', 'F.', ',', 'Zhang', ',', 'Y.', ',', 'Shang', ',', 'X.', ',', 'Shao', ',', 'J.', ',', '&', 'Zhuang', ',', '.']

 TOTAL LEMMATIZE WORDS ==> 31

************************************************************************************************************************

638 --> (2015). 


 ---- TOKENS ----

 ['(', '2015', ')', '.'] 

 TOTAL TOKENS ==> 4

 ---- POST ----

 [('(', '('), ('2015', 'CD'), (')', ')'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['(', '2015', ')', '.']

 TOTAL FILTERED TOKENS ==>  4

 ---- POST FOR FILTERED TOKENS ----

 [('(', '('), ('2015', 'CD'), (')', ')'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['( 2015', '2015 )', ') .'] 

 TOTAL BIGRAMS --> 3 



 ---- TRI-GRAMS ---- 

 ['( 2015 )', '2015 ) .'] 

 TOTAL TRIGRAMS --> 2 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['(', '2015', ')', '.']

 TOTAL PORTER STEM WORDS ==> 4



 ---- SNOWBALL STEMMING ----

['(', '2015', ')', '.']

 TOTAL SNOWBALL STEM WORDS ==> 4



 ---- LEMMATIZATION ----

['(', '2015', ')', '.']

 TOTAL LEMMATIZE WORDS ==> 4

************************************************************************************************************************

639 --> Topic  aspect-oriented summarization via group selection. 


 ---- TOKENS ----

 ['Topic', 'aspect-oriented', 'summarization', 'via', 'group', 'selection', '.'] 

 TOTAL TOKENS ==> 7

 ---- POST ----

 [('Topic', 'NNP'), ('aspect-oriented', 'JJ'), ('summarization', 'NN'), ('via', 'IN'), ('group', 'NN'), ('selection', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Topic', 'aspect-oriented', 'summarization', 'via', 'group', 'selection', '.']

 TOTAL FILTERED TOKENS ==>  7

 ---- POST FOR FILTERED TOKENS ----

 [('Topic', 'NNP'), ('aspect-oriented', 'JJ'), ('summarization', 'NN'), ('via', 'IN'), ('group', 'NN'), ('selection', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Topic aspect-oriented', 'aspect-oriented summarization', 'summarization via', 'via group', 'group selection', 'selection .'] 

 TOTAL BIGRAMS --> 6 



 ---- TRI-GRAMS ---- 

 ['Topic aspect-oriented summarization', 'aspect-oriented summarization via', 'summarization via group', 'via group selection', 'group selection .'] 

 TOTAL TRIGRAMS --> 5 



 ---- NOUN PHRASES ---- 

 ['aspect-oriented summarization', 'group', 'selection'] 

 TOTAL NOUN PHRASES --> 3 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> ['Topic']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['topic', 'aspect-ori', 'summar', 'via', 'group', 'select', '.']

 TOTAL PORTER STEM WORDS ==> 7



 ---- SNOWBALL STEMMING ----

['topic', 'aspect-ori', 'summar', 'via', 'group', 'select', '.']

 TOTAL SNOWBALL STEM WORDS ==> 7



 ---- LEMMATIZATION ----

['Topic', 'aspect-oriented', 'summarization', 'via', 'group', 'selection', '.']

 TOTAL LEMMATIZE WORDS ==> 7

************************************************************************************************************************

640 --> Neurocomputing, 149, 1613-1619. 


 ---- TOKENS ----

 ['Neurocomputing', ',', '149', ',', '1613-1619', '.'] 

 TOTAL TOKENS ==> 6

 ---- POST ----

 [('Neurocomputing', 'NNP'), (',', ','), ('149', 'CD'), (',', ','), ('1613-1619', 'CD'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Neurocomputing', ',', '149', ',', '1613-1619', '.']

 TOTAL FILTERED TOKENS ==>  6

 ---- POST FOR FILTERED TOKENS ----

 [('Neurocomputing', 'NNP'), (',', ','), ('149', 'CD'), (',', ','), ('1613-1619', 'CD'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Neurocomputing ,', ', 149', '149 ,', ', 1613-1619', '1613-1619 .'] 

 TOTAL BIGRAMS --> 5 



 ---- TRI-GRAMS ---- 

 ['Neurocomputing , 149', ', 149 ,', '149 , 1613-1619', ', 1613-1619 .'] 

 TOTAL TRIGRAMS --> 4 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['neurocomput', ',', '149', ',', '1613-1619', '.']

 TOTAL PORTER STEM WORDS ==> 6



 ---- SNOWBALL STEMMING ----

['neurocomput', ',', '149', ',', '1613-1619', '.']

 TOTAL SNOWBALL STEM WORDS ==> 6



 ---- LEMMATIZATION ----

['Neurocomputing', ',', '149', ',', '1613-1619', '.']

 TOTAL LEMMATIZE WORDS ==> 6

************************************************************************************************************************

641 --> [73] Sager, N., Lyman, M., Nhan, N. T., & Tick, L. J. 


 ---- TOKENS ----

 ['[', '73', ']', 'Sager', ',', 'N.', ',', 'Lyman', ',', 'M.', ',', 'Nhan', ',', 'N.', 'T.', ',', '&', 'Tick', ',', 'L.', 'J', '.'] 

 TOTAL TOKENS ==> 22

 ---- POST ----

 [('[', 'RB'), ('73', 'CD'), (']', 'JJ'), ('Sager', 'NNP'), (',', ','), ('N.', 'NNP'), (',', ','), ('Lyman', 'NNP'), (',', ','), ('M.', 'NNP'), (',', ','), ('Nhan', 'NNP'), (',', ','), ('N.', 'NNP'), ('T.', 'NNP'), (',', ','), ('&', 'CC'), ('Tick', 'NNP'), (',', ','), ('L.', 'NNP'), ('J', 'NNP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['[', '73', ']', 'Sager', ',', 'N.', ',', 'Lyman', ',', 'M.', ',', 'Nhan', ',', 'N.', 'T.', ',', '&', 'Tick', ',', 'L.', 'J', '.']

 TOTAL FILTERED TOKENS ==>  22

 ---- POST FOR FILTERED TOKENS ----

 [('[', 'RB'), ('73', 'CD'), (']', 'JJ'), ('Sager', 'NNP'), (',', ','), ('N.', 'NNP'), (',', ','), ('Lyman', 'NNP'), (',', ','), ('M.', 'NNP'), (',', ','), ('Nhan', 'NNP'), (',', ','), ('N.', 'NNP'), ('T.', 'NNP'), (',', ','), ('&', 'CC'), ('Tick', 'NNP'), (',', ','), ('L.', 'NNP'), ('J', 'NNP'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['[ 73', '73 ]', '] Sager', 'Sager ,', ', N.', 'N. ,', ', Lyman', 'Lyman ,', ', M.', 'M. ,', ', Nhan', 'Nhan ,', ', N.', 'N. T.', 'T. ,', ', &', '& Tick', 'Tick ,', ', L.', 'L. J', 'J .'] 

 TOTAL BIGRAMS --> 21 



 ---- TRI-GRAMS ---- 

 ['[ 73 ]', '73 ] Sager', '] Sager ,', 'Sager , N.', ', N. ,', 'N. , Lyman', ', Lyman ,', 'Lyman , M.', ', M. ,', 'M. , Nhan', ', Nhan ,', 'Nhan , N.', ', N. T.', 'N. T. ,', 'T. , &', ', & Tick', '& Tick ,', 'Tick , L.', ', L. J', 'L. J .'] 

 TOTAL TRIGRAMS --> 20 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> ['Lyman', 'Nhan', 'Tick']
 TOTAL PERSON ENTITY --> 3 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['[', '73', ']', 'sager', ',', 'n.', ',', 'lyman', ',', 'm.', ',', 'nhan', ',', 'n.', 't.', ',', '&', 'tick', ',', 'l.', 'j', '.']

 TOTAL PORTER STEM WORDS ==> 22



 ---- SNOWBALL STEMMING ----

['[', '73', ']', 'sager', ',', 'n.', ',', 'lyman', ',', 'm.', ',', 'nhan', ',', 'n.', 't.', ',', '&', 'tick', ',', 'l.', 'j', '.']

 TOTAL SNOWBALL STEM WORDS ==> 22



 ---- LEMMATIZATION ----

['[', '73', ']', 'Sager', ',', 'N.', ',', 'Lyman', ',', 'M.', ',', 'Nhan', ',', 'N.', 'T.', ',', '&', 'Tick', ',', 'L.', 'J', '.']

 TOTAL LEMMATIZE WORDS ==> 22

************************************************************************************************************************

642 --> (1995). 


 ---- TOKENS ----

 ['(', '1995', ')', '.'] 

 TOTAL TOKENS ==> 4

 ---- POST ----

 [('(', '('), ('1995', 'CD'), (')', ')'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['(', '1995', ')', '.']

 TOTAL FILTERED TOKENS ==>  4

 ---- POST FOR FILTERED TOKENS ----

 [('(', '('), ('1995', 'CD'), (')', ')'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['( 1995', '1995 )', ') .'] 

 TOTAL BIGRAMS --> 3 



 ---- TRI-GRAMS ---- 

 ['( 1995 )', '1995 ) .'] 

 TOTAL TRIGRAMS --> 2 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['(', '1995', ')', '.']

 TOTAL PORTER STEM WORDS ==> 4



 ---- SNOWBALL STEMMING ----

['(', '1995', ')', '.']

 TOTAL SNOWBALL STEM WORDS ==> 4



 ---- LEMMATIZATION ----

['(', '1995', ')', '.']

 TOTAL LEMMATIZE WORDS ==> 4

************************************************************************************************************************

643 --> Medical language processing:  applications to patient data representation and automatic encoding. 


 ---- TOKENS ----

 ['Medical', 'language', 'processing', ':', 'applications', 'to', 'patient', 'data', 'representation', 'and', 'automatic', 'encoding', '.'] 

 TOTAL TOKENS ==> 13

 ---- POST ----

 [('Medical', 'JJ'), ('language', 'NN'), ('processing', 'NN'), (':', ':'), ('applications', 'NNS'), ('to', 'TO'), ('patient', 'VB'), ('data', 'NNS'), ('representation', 'NN'), ('and', 'CC'), ('automatic', 'JJ'), ('encoding', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Medical', 'language', 'processing', ':', 'applications', 'patient', 'data', 'representation', 'automatic', 'encoding', '.']

 TOTAL FILTERED TOKENS ==>  11

 ---- POST FOR FILTERED TOKENS ----

 [('Medical', 'JJ'), ('language', 'NN'), ('processing', 'NN'), (':', ':'), ('applications', 'NNS'), ('patient', 'VBP'), ('data', 'NNS'), ('representation', 'NN'), ('automatic', 'JJ'), ('encoding', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Medical language', 'language processing', 'processing :', ': applications', 'applications patient', 'patient data', 'data representation', 'representation automatic', 'automatic encoding', 'encoding .'] 

 TOTAL BIGRAMS --> 10 



 ---- TRI-GRAMS ---- 

 ['Medical language processing', 'language processing :', 'processing : applications', ': applications patient', 'applications patient data', 'patient data representation', 'data representation automatic', 'representation automatic encoding', 'automatic encoding .'] 

 TOTAL TRIGRAMS --> 9 



 ---- NOUN PHRASES ---- 

 ['Medical language', 'processing', 'representation', 'automatic encoding'] 

 TOTAL NOUN PHRASES --> 4 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> ['Medical']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['medic', 'languag', 'process', ':', 'applic', 'patient', 'data', 'represent', 'automat', 'encod', '.']

 TOTAL PORTER STEM WORDS ==> 11



 ---- SNOWBALL STEMMING ----

['medic', 'languag', 'process', ':', 'applic', 'patient', 'data', 'represent', 'automat', 'encod', '.']

 TOTAL SNOWBALL STEM WORDS ==> 11



 ---- LEMMATIZATION ----

['Medical', 'language', 'processing', ':', 'application', 'patient', 'data', 'representation', 'automatic', 'encoding', '.']

 TOTAL LEMMATIZE WORDS ==> 11

************************************************************************************************************************

644 --> Methods of information in  medicine, 34(1-2), 140-146. 


 ---- TOKENS ----

 ['Methods', 'of', 'information', 'in', 'medicine', ',', '34', '(', '1-2', ')', ',', '140-146', '.'] 

 TOTAL TOKENS ==> 13

 ---- POST ----

 [('Methods', 'NNS'), ('of', 'IN'), ('information', 'NN'), ('in', 'IN'), ('medicine', 'NN'), (',', ','), ('34', 'CD'), ('(', '('), ('1-2', 'JJ'), (')', ')'), (',', ','), ('140-146', 'JJ'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Methods', 'information', 'medicine', ',', '34', '(', '1-2', ')', ',', '140-146', '.']

 TOTAL FILTERED TOKENS ==>  11

 ---- POST FOR FILTERED TOKENS ----

 [('Methods', 'NNS'), ('information', 'NN'), ('medicine', 'NN'), (',', ','), ('34', 'CD'), ('(', '('), ('1-2', 'JJ'), (')', ')'), (',', ','), ('140-146', 'JJ'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Methods information', 'information medicine', 'medicine ,', ', 34', '34 (', '( 1-2', '1-2 )', ') ,', ', 140-146', '140-146 .'] 

 TOTAL BIGRAMS --> 10 



 ---- TRI-GRAMS ---- 

 ['Methods information medicine', 'information medicine ,', 'medicine , 34', ', 34 (', '34 ( 1-2', '( 1-2 )', '1-2 ) ,', ') , 140-146', ', 140-146 .'] 

 TOTAL TRIGRAMS --> 9 



 ---- NOUN PHRASES ---- 

 ['information', 'medicine'] 

 TOTAL NOUN PHRASES --> 2 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> ['Methods']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['method', 'inform', 'medicin', ',', '34', '(', '1-2', ')', ',', '140-146', '.']

 TOTAL PORTER STEM WORDS ==> 11



 ---- SNOWBALL STEMMING ----

['method', 'inform', 'medicin', ',', '34', '(', '1-2', ')', ',', '140-146', '.']

 TOTAL SNOWBALL STEM WORDS ==> 11



 ---- LEMMATIZATION ----

['Methods', 'information', 'medicine', ',', '34', '(', '1-2', ')', ',', '140-146', '.']

 TOTAL LEMMATIZE WORDS ==> 11

************************************************************************************************************************

645 --> [74] Chi, E. C., Lyman, M. S., Sager, N., Friedman, C., & Macleod, C. (1985, November). 


 ---- TOKENS ----

 ['[', '74', ']', 'Chi', ',', 'E.', 'C.', ',', 'Lyman', ',', 'M.', 'S.', ',', 'Sager', ',', 'N.', ',', 'Friedman', ',', 'C.', ',', '&', 'Macleod', ',', 'C.', '(', '1985', ',', 'November', ')', '.'] 

 TOTAL TOKENS ==> 31

 ---- POST ----

 [('[', 'RB'), ('74', 'CD'), (']', 'JJ'), ('Chi', 'NNP'), (',', ','), ('E.', 'NNP'), ('C.', 'NNP'), (',', ','), ('Lyman', 'NNP'), (',', ','), ('M.', 'NNP'), ('S.', 'NNP'), (',', ','), ('Sager', 'NNP'), (',', ','), ('N.', 'NNP'), (',', ','), ('Friedman', 'NNP'), (',', ','), ('C.', 'NNP'), (',', ','), ('&', 'CC'), ('Macleod', 'NNP'), (',', ','), ('C.', 'NNP'), ('(', '('), ('1985', 'CD'), (',', ','), ('November', 'NNP'), (')', ')'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['[', '74', ']', 'Chi', ',', 'E.', 'C.', ',', 'Lyman', ',', 'M.', 'S.', ',', 'Sager', ',', 'N.', ',', 'Friedman', ',', 'C.', ',', '&', 'Macleod', ',', 'C.', '(', '1985', ',', 'November', ')', '.']

 TOTAL FILTERED TOKENS ==>  31

 ---- POST FOR FILTERED TOKENS ----

 [('[', 'RB'), ('74', 'CD'), (']', 'JJ'), ('Chi', 'NNP'), (',', ','), ('E.', 'NNP'), ('C.', 'NNP'), (',', ','), ('Lyman', 'NNP'), (',', ','), ('M.', 'NNP'), ('S.', 'NNP'), (',', ','), ('Sager', 'NNP'), (',', ','), ('N.', 'NNP'), (',', ','), ('Friedman', 'NNP'), (',', ','), ('C.', 'NNP'), (',', ','), ('&', 'CC'), ('Macleod', 'NNP'), (',', ','), ('C.', 'NNP'), ('(', '('), ('1985', 'CD'), (',', ','), ('November', 'NNP'), (')', ')'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['[ 74', '74 ]', '] Chi', 'Chi ,', ', E.', 'E. C.', 'C. ,', ', Lyman', 'Lyman ,', ', M.', 'M. S.', 'S. ,', ', Sager', 'Sager ,', ', N.', 'N. ,', ', Friedman', 'Friedman ,', ', C.', 'C. ,', ', &', '& Macleod', 'Macleod ,', ', C.', 'C. (', '( 1985', '1985 ,', ', November', 'November )', ') .'] 

 TOTAL BIGRAMS --> 30 



 ---- TRI-GRAMS ---- 

 ['[ 74 ]', '74 ] Chi', '] Chi ,', 'Chi , E.', ', E. C.', 'E. C. ,', 'C. , Lyman', ', Lyman ,', 'Lyman , M.', ', M. S.', 'M. S. ,', 'S. , Sager', ', Sager ,', 'Sager , N.', ', N. ,', 'N. , Friedman', ', Friedman ,', 'Friedman , C.', ', C. ,', 'C. , &', ', & Macleod', '& Macleod ,', 'Macleod , C.', ', C. (', 'C. ( 1985', '( 1985 ,', '1985 , November', ', November )', 'November ) .'] 

 TOTAL TRIGRAMS --> 29 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> ['Lyman', 'Sager', 'Friedman', 'Macleod']
 TOTAL PERSON ENTITY --> 4 


 GPE ---> ['Chi']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['[', '74', ']', 'chi', ',', 'e.', 'c.', ',', 'lyman', ',', 'm.', 's.', ',', 'sager', ',', 'n.', ',', 'friedman', ',', 'c.', ',', '&', 'macleod', ',', 'c.', '(', '1985', ',', 'novemb', ')', '.']

 TOTAL PORTER STEM WORDS ==> 31



 ---- SNOWBALL STEMMING ----

['[', '74', ']', 'chi', ',', 'e.', 'c.', ',', 'lyman', ',', 'm.', 's.', ',', 'sager', ',', 'n.', ',', 'friedman', ',', 'c.', ',', '&', 'macleod', ',', 'c.', '(', '1985', ',', 'novemb', ')', '.']

 TOTAL SNOWBALL STEM WORDS ==> 31



 ---- LEMMATIZATION ----

['[', '74', ']', 'Chi', ',', 'E.', 'C.', ',', 'Lyman', ',', 'M.', 'S.', ',', 'Sager', ',', 'N.', ',', 'Friedman', ',', 'C.', ',', '&', 'Macleod', ',', 'C.', '(', '1985', ',', 'November', ')', '.']

 TOTAL LEMMATIZE WORDS ==> 31

************************************************************************************************************************

646 --> A  database of computer-structured narrative: methods of computing complex relations. 


 ---- TOKENS ----

 ['A', 'database', 'of', 'computer-structured', 'narrative', ':', 'methods', 'of', 'computing', 'complex', 'relations', '.'] 

 TOTAL TOKENS ==> 12

 ---- POST ----

 [('A', 'DT'), ('database', 'NN'), ('of', 'IN'), ('computer-structured', 'JJ'), ('narrative', 'JJ'), (':', ':'), ('methods', 'NNS'), ('of', 'IN'), ('computing', 'VBG'), ('complex', 'JJ'), ('relations', 'NNS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['database', 'computer-structured', 'narrative', ':', 'methods', 'computing', 'complex', 'relations', '.']

 TOTAL FILTERED TOKENS ==>  9

 ---- POST FOR FILTERED TOKENS ----

 [('database', 'NN'), ('computer-structured', 'JJ'), ('narrative', 'JJ'), (':', ':'), ('methods', 'NNS'), ('computing', 'VBG'), ('complex', 'JJ'), ('relations', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['database computer-structured', 'computer-structured narrative', 'narrative :', ': methods', 'methods computing', 'computing complex', 'complex relations', 'relations .'] 

 TOTAL BIGRAMS --> 8 



 ---- TRI-GRAMS ---- 

 ['database computer-structured narrative', 'computer-structured narrative :', 'narrative : methods', ': methods computing', 'methods computing complex', 'computing complex relations', 'complex relations .'] 

 TOTAL TRIGRAMS --> 7 



 ---- NOUN PHRASES ---- 

 ['database'] 

 TOTAL NOUN PHRASES --> 1 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['databas', 'computer-structur', 'narr', ':', 'method', 'comput', 'complex', 'relat', '.']

 TOTAL PORTER STEM WORDS ==> 9



 ---- SNOWBALL STEMMING ----

['databas', 'computer-structur', 'narrat', ':', 'method', 'comput', 'complex', 'relat', '.']

 TOTAL SNOWBALL STEM WORDS ==> 9



 ---- LEMMATIZATION ----

['database', 'computer-structured', 'narrative', ':', 'method', 'computing', 'complex', 'relation', '.']

 TOTAL LEMMATIZE WORDS ==> 9

************************************************************************************************************************

647 --> In Proceedings of the Annual Symposium on Computer Application in Medical Care (p. 221). 


 ---- TOKENS ----

 ['In', 'Proceedings', 'of', 'the', 'Annual', 'Symposium', 'on', 'Computer', 'Application', 'in', 'Medical', 'Care', '(', 'p.', '221', ')', '.'] 

 TOTAL TOKENS ==> 17

 ---- POST ----

 [('In', 'IN'), ('Proceedings', 'NNP'), ('of', 'IN'), ('the', 'DT'), ('Annual', 'NNP'), ('Symposium', 'NNP'), ('on', 'IN'), ('Computer', 'NNP'), ('Application', 'NNP'), ('in', 'IN'), ('Medical', 'NNP'), ('Care', 'NNP'), ('(', '('), ('p.', 'VB'), ('221', 'CD'), (')', ')'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Proceedings', 'Annual', 'Symposium', 'Computer', 'Application', 'Medical', 'Care', '(', 'p.', '221', ')', '.']

 TOTAL FILTERED TOKENS ==>  12

 ---- POST FOR FILTERED TOKENS ----

 [('Proceedings', 'NNS'), ('Annual', 'NNP'), ('Symposium', 'NNP'), ('Computer', 'NNP'), ('Application', 'NNP'), ('Medical', 'NNP'), ('Care', 'NNP'), ('(', '('), ('p.', 'VB'), ('221', 'CD'), (')', ')'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Proceedings Annual', 'Annual Symposium', 'Symposium Computer', 'Computer Application', 'Application Medical', 'Medical Care', 'Care (', '( p.', 'p. 221', '221 )', ') .'] 

 TOTAL BIGRAMS --> 11 



 ---- TRI-GRAMS ---- 

 ['Proceedings Annual Symposium', 'Annual Symposium Computer', 'Symposium Computer Application', 'Computer Application Medical', 'Application Medical Care', 'Medical Care (', 'Care ( p.', '( p. 221', 'p. 221 )', '221 ) .'] 

 TOTAL TRIGRAMS --> 10 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> ['Annual Symposium Computer']
 TOTAL PERSON ENTITY --> 1 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['proceed', 'annual', 'symposium', 'comput', 'applic', 'medic', 'care', '(', 'p.', '221', ')', '.']

 TOTAL PORTER STEM WORDS ==> 12



 ---- SNOWBALL STEMMING ----

['proceed', 'annual', 'symposium', 'comput', 'applic', 'medic', 'care', '(', 'p.', '221', ')', '.']

 TOTAL SNOWBALL STEM WORDS ==> 12



 ---- LEMMATIZATION ----

['Proceedings', 'Annual', 'Symposium', 'Computer', 'Application', 'Medical', 'Care', '(', 'p.', '221', ')', '.']

 TOTAL LEMMATIZE WORDS ==> 12

************************************************************************************************************************

648 --> American Medical Informatics Association. 


 ---- TOKENS ----

 ['American', 'Medical', 'Informatics', 'Association', '.'] 

 TOTAL TOKENS ==> 5

 ---- POST ----

 [('American', 'NNP'), ('Medical', 'NNP'), ('Informatics', 'NNP'), ('Association', 'NNP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['American', 'Medical', 'Informatics', 'Association', '.']

 TOTAL FILTERED TOKENS ==>  5

 ---- POST FOR FILTERED TOKENS ----

 [('American', 'NNP'), ('Medical', 'NNP'), ('Informatics', 'NNP'), ('Association', 'NNP'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['American Medical', 'Medical Informatics', 'Informatics Association', 'Association .'] 

 TOTAL BIGRAMS --> 4 



 ---- TRI-GRAMS ---- 

 ['American Medical Informatics', 'Medical Informatics Association', 'Informatics Association .'] 

 TOTAL TRIGRAMS --> 3 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> ['Medical Informatics Association']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> ['American']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['american', 'medic', 'informat', 'associ', '.']

 TOTAL PORTER STEM WORDS ==> 5



 ---- SNOWBALL STEMMING ----

['american', 'medic', 'informat', 'associ', '.']

 TOTAL SNOWBALL STEM WORDS ==> 5



 ---- LEMMATIZATION ----

['American', 'Medical', 'Informatics', 'Association', '.']

 TOTAL LEMMATIZE WORDS ==> 5

************************************************************************************************************************

649 --> [75] Grishman, R., Sager, N., Raze, C., & Bookchin, B. 


 ---- TOKENS ----

 ['[', '75', ']', 'Grishman', ',', 'R.', ',', 'Sager', ',', 'N.', ',', 'Raze', ',', 'C.', ',', '&', 'Bookchin', ',', 'B', '.'] 

 TOTAL TOKENS ==> 20

 ---- POST ----

 [('[', 'RB'), ('75', 'CD'), (']', 'JJ'), ('Grishman', 'NNP'), (',', ','), ('R.', 'NNP'), (',', ','), ('Sager', 'NNP'), (',', ','), ('N.', 'NNP'), (',', ','), ('Raze', 'NNP'), (',', ','), ('C.', 'NNP'), (',', ','), ('&', 'CC'), ('Bookchin', 'NNP'), (',', ','), ('B', 'NNP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['[', '75', ']', 'Grishman', ',', 'R.', ',', 'Sager', ',', 'N.', ',', 'Raze', ',', 'C.', ',', '&', 'Bookchin', ',', 'B', '.']

 TOTAL FILTERED TOKENS ==>  20

 ---- POST FOR FILTERED TOKENS ----

 [('[', 'RB'), ('75', 'CD'), (']', 'JJ'), ('Grishman', 'NNP'), (',', ','), ('R.', 'NNP'), (',', ','), ('Sager', 'NNP'), (',', ','), ('N.', 'NNP'), (',', ','), ('Raze', 'NNP'), (',', ','), ('C.', 'NNP'), (',', ','), ('&', 'CC'), ('Bookchin', 'NNP'), (',', ','), ('B', 'NNP'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['[ 75', '75 ]', '] Grishman', 'Grishman ,', ', R.', 'R. ,', ', Sager', 'Sager ,', ', N.', 'N. ,', ', Raze', 'Raze ,', ', C.', 'C. ,', ', &', '& Bookchin', 'Bookchin ,', ', B', 'B .'] 

 TOTAL BIGRAMS --> 19 



 ---- TRI-GRAMS ---- 

 ['[ 75 ]', '75 ] Grishman', '] Grishman ,', 'Grishman , R.', ', R. ,', 'R. , Sager', ', Sager ,', 'Sager , N.', ', N. ,', 'N. , Raze', ', Raze ,', 'Raze , C.', ', C. ,', 'C. , &', ', & Bookchin', '& Bookchin ,', 'Bookchin , B', ', B .'] 

 TOTAL TRIGRAMS --> 18 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> ['Sager', 'Raze', 'Bookchin']
 TOTAL PERSON ENTITY --> 3 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['[', '75', ']', 'grishman', ',', 'r.', ',', 'sager', ',', 'n.', ',', 'raze', ',', 'c.', ',', '&', 'bookchin', ',', 'b', '.']

 TOTAL PORTER STEM WORDS ==> 20



 ---- SNOWBALL STEMMING ----

['[', '75', ']', 'grishman', ',', 'r.', ',', 'sager', ',', 'n.', ',', 'raze', ',', 'c.', ',', '&', 'bookchin', ',', 'b', '.']

 TOTAL SNOWBALL STEM WORDS ==> 20



 ---- LEMMATIZATION ----

['[', '75', ']', 'Grishman', ',', 'R.', ',', 'Sager', ',', 'N.', ',', 'Raze', ',', 'C.', ',', '&', 'Bookchin', ',', 'B', '.']

 TOTAL LEMMATIZE WORDS ==> 20

************************************************************************************************************************

650 --> (1973, June). 


 ---- TOKENS ----

 ['(', '1973', ',', 'June', ')', '.'] 

 TOTAL TOKENS ==> 6

 ---- POST ----

 [('(', '('), ('1973', 'CD'), (',', ','), ('June', 'NNP'), (')', ')'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['(', '1973', ',', 'June', ')', '.']

 TOTAL FILTERED TOKENS ==>  6

 ---- POST FOR FILTERED TOKENS ----

 [('(', '('), ('1973', 'CD'), (',', ','), ('June', 'NNP'), (')', ')'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['( 1973', '1973 ,', ', June', 'June )', ') .'] 

 TOTAL BIGRAMS --> 5 



 ---- TRI-GRAMS ---- 

 ['( 1973 ,', '1973 , June', ', June )', 'June ) .'] 

 TOTAL TRIGRAMS --> 4 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['(', '1973', ',', 'june', ')', '.']

 TOTAL PORTER STEM WORDS ==> 6



 ---- SNOWBALL STEMMING ----

['(', '1973', ',', 'june', ')', '.']

 TOTAL SNOWBALL STEM WORDS ==> 6



 ---- LEMMATIZATION ----

['(', '1973', ',', 'June', ')', '.']

 TOTAL LEMMATIZE WORDS ==> 6

************************************************************************************************************************

651 --> The linguistic string  parser. 


 ---- TOKENS ----

 ['The', 'linguistic', 'string', 'parser', '.'] 

 TOTAL TOKENS ==> 5

 ---- POST ----

 [('The', 'DT'), ('linguistic', 'JJ'), ('string', 'NN'), ('parser', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['linguistic', 'string', 'parser', '.']

 TOTAL FILTERED TOKENS ==>  4

 ---- POST FOR FILTERED TOKENS ----

 [('linguistic', 'JJ'), ('string', 'NN'), ('parser', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['linguistic string', 'string parser', 'parser .'] 

 TOTAL BIGRAMS --> 3 



 ---- TRI-GRAMS ---- 

 ['linguistic string parser', 'string parser .'] 

 TOTAL TRIGRAMS --> 2 



 ---- NOUN PHRASES ---- 

 ['linguistic string', 'parser'] 

 TOTAL NOUN PHRASES --> 2 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['linguist', 'string', 'parser', '.']

 TOTAL PORTER STEM WORDS ==> 4



 ---- SNOWBALL STEMMING ----

['linguist', 'string', 'parser', '.']

 TOTAL SNOWBALL STEM WORDS ==> 4



 ---- LEMMATIZATION ----

['linguistic', 'string', 'parser', '.']

 TOTAL LEMMATIZE WORDS ==> 4

************************************************************************************************************************

652 --> In Proceedings of the June 4-8, 1973, national computer conference and  exposition (pp. 


 ---- TOKENS ----

 ['In', 'Proceedings', 'of', 'the', 'June', '4-8', ',', '1973', ',', 'national', 'computer', 'conference', 'and', 'exposition', '(', 'pp', '.'] 

 TOTAL TOKENS ==> 17

 ---- POST ----

 [('In', 'IN'), ('Proceedings', 'NNP'), ('of', 'IN'), ('the', 'DT'), ('June', 'NNP'), ('4-8', 'CD'), (',', ','), ('1973', 'CD'), (',', ','), ('national', 'JJ'), ('computer', 'NN'), ('conference', 'NN'), ('and', 'CC'), ('exposition', 'NN'), ('(', '('), ('pp', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Proceedings', 'June', '4-8', ',', '1973', ',', 'national', 'computer', 'conference', 'exposition', '(', 'pp', '.']

 TOTAL FILTERED TOKENS ==>  13

 ---- POST FOR FILTERED TOKENS ----

 [('Proceedings', 'NNS'), ('June', 'NNP'), ('4-8', 'CD'), (',', ','), ('1973', 'CD'), (',', ','), ('national', 'JJ'), ('computer', 'NN'), ('conference', 'NN'), ('exposition', 'NN'), ('(', '('), ('pp', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Proceedings June', 'June 4-8', '4-8 ,', ', 1973', '1973 ,', ', national', 'national computer', 'computer conference', 'conference exposition', 'exposition (', '( pp', 'pp .'] 

 TOTAL BIGRAMS --> 12 



 ---- TRI-GRAMS ---- 

 ['Proceedings June 4-8', 'June 4-8 ,', '4-8 , 1973', ', 1973 ,', '1973 , national', ', national computer', 'national computer conference', 'computer conference exposition', 'conference exposition (', 'exposition ( pp', '( pp .'] 

 TOTAL TRIGRAMS --> 11 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 []

 ---- PORTER STEMMING ----

['proceed', 'june', '4-8', ',', '1973', ',', 'nation', 'comput', 'confer', 'exposit', '(', 'pp', '.']

 TOTAL PORTER STEM WORDS ==> 13



 ---- SNOWBALL STEMMING ----

['proceed', 'june', '4-8', ',', '1973', ',', 'nation', 'comput', 'confer', 'exposit', '(', 'pp', '.']

 TOTAL SNOWBALL STEM WORDS ==> 13



 ---- LEMMATIZATION ----

['Proceedings', 'June', '4-8', ',', '1973', ',', 'national', 'computer', 'conference', 'exposition', '(', 'pp', '.']

 TOTAL LEMMATIZE WORDS ==> 13

************************************************************************************************************************

653 --> 427-434). 


 ---- TOKENS ----

 ['427-434', ')', '.'] 

 TOTAL TOKENS ==> 3

 ---- POST ----

 [('427-434', 'JJ'), (')', ')'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['427-434', ')', '.']

 TOTAL FILTERED TOKENS ==>  3

 ---- POST FOR FILTERED TOKENS ----

 [('427-434', 'JJ'), (')', ')'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['427-434 )', ') .'] 

 TOTAL BIGRAMS --> 2 



 ---- TRI-GRAMS ---- 

 ['427-434 ) .'] 

 TOTAL TRIGRAMS --> 1 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 []

 ---- PORTER STEMMING ----

['427-434', ')', '.']

 TOTAL PORTER STEM WORDS ==> 3



 ---- SNOWBALL STEMMING ----

['427-434', ')', '.']

 TOTAL SNOWBALL STEM WORDS ==> 3



 ---- LEMMATIZATION ----

['427-434', ')', '.']

 TOTAL LEMMATIZE WORDS ==> 3

************************************************************************************************************************

654 --> ACM. 


 ---- TOKENS ----

 ['ACM', '.'] 

 TOTAL TOKENS ==> 2

 ---- POST ----

 [('ACM', 'NNP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['ACM', '.']

 TOTAL FILTERED TOKENS ==>  2

 ---- POST FOR FILTERED TOKENS ----

 [('ACM', 'NNP'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['ACM .'] 

 TOTAL BIGRAMS --> 1 



 ---- TRI-GRAMS ---- 

 [] 

 TOTAL TRIGRAMS --> 0 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['acm', '.']

 TOTAL PORTER STEM WORDS ==> 2



 ---- SNOWBALL STEMMING ----

['acm', '.']

 TOTAL SNOWBALL STEM WORDS ==> 2



 ---- LEMMATIZATION ----

['ACM', '.']

 TOTAL LEMMATIZE WORDS ==> 2

************************************************************************************************************************

655 --> [76] Hirschman, L., Grishman, R., & Sager, N. (1976, June). 


 ---- TOKENS ----

 ['[', '76', ']', 'Hirschman', ',', 'L.', ',', 'Grishman', ',', 'R.', ',', '&', 'Sager', ',', 'N.', '(', '1976', ',', 'June', ')', '.'] 

 TOTAL TOKENS ==> 21

 ---- POST ----

 [('[', 'RB'), ('76', 'CD'), (']', 'JJ'), ('Hirschman', 'NNP'), (',', ','), ('L.', 'NNP'), (',', ','), ('Grishman', 'NNP'), (',', ','), ('R.', 'NNP'), (',', ','), ('&', 'CC'), ('Sager', 'NNP'), (',', ','), ('N.', 'NNP'), ('(', '('), ('1976', 'CD'), (',', ','), ('June', 'NNP'), (')', ')'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['[', '76', ']', 'Hirschman', ',', 'L.', ',', 'Grishman', ',', 'R.', ',', '&', 'Sager', ',', 'N.', '(', '1976', ',', 'June', ')', '.']

 TOTAL FILTERED TOKENS ==>  21

 ---- POST FOR FILTERED TOKENS ----

 [('[', 'RB'), ('76', 'CD'), (']', 'JJ'), ('Hirschman', 'NNP'), (',', ','), ('L.', 'NNP'), (',', ','), ('Grishman', 'NNP'), (',', ','), ('R.', 'NNP'), (',', ','), ('&', 'CC'), ('Sager', 'NNP'), (',', ','), ('N.', 'NNP'), ('(', '('), ('1976', 'CD'), (',', ','), ('June', 'NNP'), (')', ')'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['[ 76', '76 ]', '] Hirschman', 'Hirschman ,', ', L.', 'L. ,', ', Grishman', 'Grishman ,', ', R.', 'R. ,', ', &', '& Sager', 'Sager ,', ', N.', 'N. (', '( 1976', '1976 ,', ', June', 'June )', ') .'] 

 TOTAL BIGRAMS --> 20 



 ---- TRI-GRAMS ---- 

 ['[ 76 ]', '76 ] Hirschman', '] Hirschman ,', 'Hirschman , L.', ', L. ,', 'L. , Grishman', ', Grishman ,', 'Grishman , R.', ', R. ,', 'R. , &', ', & Sager', '& Sager ,', 'Sager , N.', ', N. (', 'N. ( 1976', '( 1976 ,', '1976 , June', ', June )', 'June ) .'] 

 TOTAL TRIGRAMS --> 19 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> ['Grishman', 'Sager']
 TOTAL PERSON ENTITY --> 2 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['[', '76', ']', 'hirschman', ',', 'l.', ',', 'grishman', ',', 'r.', ',', '&', 'sager', ',', 'n.', '(', '1976', ',', 'june', ')', '.']

 TOTAL PORTER STEM WORDS ==> 21



 ---- SNOWBALL STEMMING ----

['[', '76', ']', 'hirschman', ',', 'l.', ',', 'grishman', ',', 'r.', ',', '&', 'sager', ',', 'n.', '(', '1976', ',', 'june', ')', '.']

 TOTAL SNOWBALL STEM WORDS ==> 21



 ---- LEMMATIZATION ----

['[', '76', ']', 'Hirschman', ',', 'L.', ',', 'Grishman', ',', 'R.', ',', '&', 'Sager', ',', 'N.', '(', '1976', ',', 'June', ')', '.']

 TOTAL LEMMATIZE WORDS ==> 21

************************************************************************************************************************

656 --> From text to structured  information: automatic processing of medical reports. 


 ---- TOKENS ----

 ['From', 'text', 'to', 'structured', 'information', ':', 'automatic', 'processing', 'of', 'medical', 'reports', '.'] 

 TOTAL TOKENS ==> 12

 ---- POST ----

 [('From', 'IN'), ('text', 'NN'), ('to', 'TO'), ('structured', 'VBN'), ('information', 'NN'), (':', ':'), ('automatic', 'JJ'), ('processing', 'NN'), ('of', 'IN'), ('medical', 'JJ'), ('reports', 'NNS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['text', 'structured', 'information', ':', 'automatic', 'processing', 'medical', 'reports', '.']

 TOTAL FILTERED TOKENS ==>  9

 ---- POST FOR FILTERED TOKENS ----

 [('text', 'RB'), ('structured', 'VBN'), ('information', 'NN'), (':', ':'), ('automatic', 'JJ'), ('processing', 'NN'), ('medical', 'JJ'), ('reports', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['text structured', 'structured information', 'information :', ': automatic', 'automatic processing', 'processing medical', 'medical reports', 'reports .'] 

 TOTAL BIGRAMS --> 8 



 ---- TRI-GRAMS ---- 

 ['text structured information', 'structured information :', 'information : automatic', ': automatic processing', 'automatic processing medical', 'processing medical reports', 'medical reports .'] 

 TOTAL TRIGRAMS --> 7 



 ---- NOUN PHRASES ---- 

 ['information', 'automatic processing'] 

 TOTAL NOUN PHRASES --> 2 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['text', 'structur', 'inform', ':', 'automat', 'process', 'medic', 'report', '.']

 TOTAL PORTER STEM WORDS ==> 9



 ---- SNOWBALL STEMMING ----

['text', 'structur', 'inform', ':', 'automat', 'process', 'medic', 'report', '.']

 TOTAL SNOWBALL STEM WORDS ==> 9



 ---- LEMMATIZATION ----

['text', 'structured', 'information', ':', 'automatic', 'processing', 'medical', 'report', '.']

 TOTAL LEMMATIZE WORDS ==> 9

************************************************************************************************************************

657 --> In Proceedings of the June 7-10, 1976,  national computer conference and exposition (pp. 


 ---- TOKENS ----

 ['In', 'Proceedings', 'of', 'the', 'June', '7-10', ',', '1976', ',', 'national', 'computer', 'conference', 'and', 'exposition', '(', 'pp', '.'] 

 TOTAL TOKENS ==> 17

 ---- POST ----

 [('In', 'IN'), ('Proceedings', 'NNP'), ('of', 'IN'), ('the', 'DT'), ('June', 'NNP'), ('7-10', 'CD'), (',', ','), ('1976', 'CD'), (',', ','), ('national', 'JJ'), ('computer', 'NN'), ('conference', 'NN'), ('and', 'CC'), ('exposition', 'NN'), ('(', '('), ('pp', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Proceedings', 'June', '7-10', ',', '1976', ',', 'national', 'computer', 'conference', 'exposition', '(', 'pp', '.']

 TOTAL FILTERED TOKENS ==>  13

 ---- POST FOR FILTERED TOKENS ----

 [('Proceedings', 'NNS'), ('June', 'NNP'), ('7-10', 'CD'), (',', ','), ('1976', 'CD'), (',', ','), ('national', 'JJ'), ('computer', 'NN'), ('conference', 'NN'), ('exposition', 'NN'), ('(', '('), ('pp', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Proceedings June', 'June 7-10', '7-10 ,', ', 1976', '1976 ,', ', national', 'national computer', 'computer conference', 'conference exposition', 'exposition (', '( pp', 'pp .'] 

 TOTAL BIGRAMS --> 12 



 ---- TRI-GRAMS ---- 

 ['Proceedings June 7-10', 'June 7-10 ,', '7-10 , 1976', ', 1976 ,', '1976 , national', ', national computer', 'national computer conference', 'computer conference exposition', 'conference exposition (', 'exposition ( pp', '( pp .'] 

 TOTAL TRIGRAMS --> 11 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 []

 ---- PORTER STEMMING ----

['proceed', 'june', '7-10', ',', '1976', ',', 'nation', 'comput', 'confer', 'exposit', '(', 'pp', '.']

 TOTAL PORTER STEM WORDS ==> 13



 ---- SNOWBALL STEMMING ----

['proceed', 'june', '7-10', ',', '1976', ',', 'nation', 'comput', 'confer', 'exposit', '(', 'pp', '.']

 TOTAL SNOWBALL STEM WORDS ==> 13



 ---- LEMMATIZATION ----

['Proceedings', 'June', '7-10', ',', '1976', ',', 'national', 'computer', 'conference', 'exposition', '(', 'pp', '.']

 TOTAL LEMMATIZE WORDS ==> 13

************************************************************************************************************************

658 --> 267-275). 


 ---- TOKENS ----

 ['267-275', ')', '.'] 

 TOTAL TOKENS ==> 3

 ---- POST ----

 [('267-275', 'JJ'), (')', ')'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['267-275', ')', '.']

 TOTAL FILTERED TOKENS ==>  3

 ---- POST FOR FILTERED TOKENS ----

 [('267-275', 'JJ'), (')', ')'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['267-275 )', ') .'] 

 TOTAL BIGRAMS --> 2 



 ---- TRI-GRAMS ---- 

 ['267-275 ) .'] 

 TOTAL TRIGRAMS --> 1 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 []

 ---- PORTER STEMMING ----

['267-275', ')', '.']

 TOTAL PORTER STEM WORDS ==> 3



 ---- SNOWBALL STEMMING ----

['267-275', ')', '.']

 TOTAL SNOWBALL STEM WORDS ==> 3



 ---- LEMMATIZATION ----

['267-275', ')', '.']

 TOTAL LEMMATIZE WORDS ==> 3

************************************************************************************************************************

659 --> ACM. 


 ---- TOKENS ----

 ['ACM', '.'] 

 TOTAL TOKENS ==> 2

 ---- POST ----

 [('ACM', 'NNP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['ACM', '.']

 TOTAL FILTERED TOKENS ==>  2

 ---- POST FOR FILTERED TOKENS ----

 [('ACM', 'NNP'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['ACM .'] 

 TOTAL BIGRAMS --> 1 



 ---- TRI-GRAMS ---- 

 [] 

 TOTAL TRIGRAMS --> 0 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['acm', '.']

 TOTAL PORTER STEM WORDS ==> 2



 ---- SNOWBALL STEMMING ----

['acm', '.']

 TOTAL SNOWBALL STEM WORDS ==> 2



 ---- LEMMATIZATION ----

['ACM', '.']

 TOTAL LEMMATIZE WORDS ==> 2

************************************************************************************************************************

660 --> [77] Sager, N. (1981). 


 ---- TOKENS ----

 ['[', '77', ']', 'Sager', ',', 'N.', '(', '1981', ')', '.'] 

 TOTAL TOKENS ==> 10

 ---- POST ----

 [('[', 'RB'), ('77', 'CD'), (']', 'JJ'), ('Sager', 'NNP'), (',', ','), ('N.', 'NNP'), ('(', '('), ('1981', 'CD'), (')', ')'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['[', '77', ']', 'Sager', ',', 'N.', '(', '1981', ')', '.']

 TOTAL FILTERED TOKENS ==>  10

 ---- POST FOR FILTERED TOKENS ----

 [('[', 'RB'), ('77', 'CD'), (']', 'JJ'), ('Sager', 'NNP'), (',', ','), ('N.', 'NNP'), ('(', '('), ('1981', 'CD'), (')', ')'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['[ 77', '77 ]', '] Sager', 'Sager ,', ', N.', 'N. (', '( 1981', '1981 )', ') .'] 

 TOTAL BIGRAMS --> 9 



 ---- TRI-GRAMS ---- 

 ['[ 77 ]', '77 ] Sager', '] Sager ,', 'Sager , N.', ', N. (', 'N. ( 1981', '( 1981 )', '1981 ) .'] 

 TOTAL TRIGRAMS --> 8 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['[', '77', ']', 'sager', ',', 'n.', '(', '1981', ')', '.']

 TOTAL PORTER STEM WORDS ==> 10



 ---- SNOWBALL STEMMING ----

['[', '77', ']', 'sager', ',', 'n.', '(', '1981', ')', '.']

 TOTAL SNOWBALL STEM WORDS ==> 10



 ---- LEMMATIZATION ----

['[', '77', ']', 'Sager', ',', 'N.', '(', '1981', ')', '.']

 TOTAL LEMMATIZE WORDS ==> 10

************************************************************************************************************************

661 --> Natural language information processing. 


 ---- TOKENS ----

 ['Natural', 'language', 'information', 'processing', '.'] 

 TOTAL TOKENS ==> 5

 ---- POST ----

 [('Natural', 'JJ'), ('language', 'NN'), ('information', 'NN'), ('processing', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Natural', 'language', 'information', 'processing', '.']

 TOTAL FILTERED TOKENS ==>  5

 ---- POST FOR FILTERED TOKENS ----

 [('Natural', 'JJ'), ('language', 'NN'), ('information', 'NN'), ('processing', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Natural language', 'language information', 'information processing', 'processing .'] 

 TOTAL BIGRAMS --> 4 



 ---- TRI-GRAMS ---- 

 ['Natural language information', 'language information processing', 'information processing .'] 

 TOTAL TRIGRAMS --> 3 



 ---- NOUN PHRASES ---- 

 ['Natural language', 'information', 'processing'] 

 TOTAL NOUN PHRASES --> 3 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['natur', 'languag', 'inform', 'process', '.']

 TOTAL PORTER STEM WORDS ==> 5



 ---- SNOWBALL STEMMING ----

['natur', 'languag', 'inform', 'process', '.']

 TOTAL SNOWBALL STEM WORDS ==> 5



 ---- LEMMATIZATION ----

['Natural', 'language', 'information', 'processing', '.']

 TOTAL LEMMATIZE WORDS ==> 5

************************************************************************************************************************

662 --> Addison-Wesley Publishing  Company, Advanced Book Program. 


 ---- TOKENS ----

 ['Addison-Wesley', 'Publishing', 'Company', ',', 'Advanced', 'Book', 'Program', '.'] 

 TOTAL TOKENS ==> 8

 ---- POST ----

 [('Addison-Wesley', 'JJ'), ('Publishing', 'NNP'), ('Company', 'NNP'), (',', ','), ('Advanced', 'NNP'), ('Book', 'NNP'), ('Program', 'NNP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Addison-Wesley', 'Publishing', 'Company', ',', 'Advanced', 'Book', 'Program', '.']

 TOTAL FILTERED TOKENS ==>  8

 ---- POST FOR FILTERED TOKENS ----

 [('Addison-Wesley', 'JJ'), ('Publishing', 'NNP'), ('Company', 'NNP'), (',', ','), ('Advanced', 'NNP'), ('Book', 'NNP'), ('Program', 'NNP'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Addison-Wesley Publishing', 'Publishing Company', 'Company ,', ', Advanced', 'Advanced Book', 'Book Program', 'Program .'] 

 TOTAL BIGRAMS --> 7 



 ---- TRI-GRAMS ---- 

 ['Addison-Wesley Publishing Company', 'Publishing Company ,', 'Company , Advanced', ', Advanced Book', 'Advanced Book Program', 'Book Program .'] 

 TOTAL TRIGRAMS --> 6 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> ['Advanced Book Program']
 TOTAL PERSON ENTITY --> 1 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['addison-wesley', 'publish', 'compani', ',', 'advanc', 'book', 'program', '.']

 TOTAL PORTER STEM WORDS ==> 8



 ---- SNOWBALL STEMMING ----

['addison-wesley', 'publish', 'compani', ',', 'advanc', 'book', 'program', '.']

 TOTAL SNOWBALL STEM WORDS ==> 8



 ---- LEMMATIZATION ----

['Addison-Wesley', 'Publishing', 'Company', ',', 'Advanced', 'Book', 'Program', '.']

 TOTAL LEMMATIZE WORDS ==> 8

************************************************************************************************************************

663 --> [78] Lyman, M., Sager, N., Friedman, C., & Chi, E. (1985, November). 


 ---- TOKENS ----

 ['[', '78', ']', 'Lyman', ',', 'M.', ',', 'Sager', ',', 'N.', ',', 'Friedman', ',', 'C.', ',', '&', 'Chi', ',', 'E.', '(', '1985', ',', 'November', ')', '.'] 

 TOTAL TOKENS ==> 25

 ---- POST ----

 [('[', 'RB'), ('78', 'CD'), (']', 'JJ'), ('Lyman', 'NNP'), (',', ','), ('M.', 'NNP'), (',', ','), ('Sager', 'NNP'), (',', ','), ('N.', 'NNP'), (',', ','), ('Friedman', 'NNP'), (',', ','), ('C.', 'NNP'), (',', ','), ('&', 'CC'), ('Chi', 'NNP'), (',', ','), ('E.', 'NNP'), ('(', '('), ('1985', 'CD'), (',', ','), ('November', 'NNP'), (')', ')'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['[', '78', ']', 'Lyman', ',', 'M.', ',', 'Sager', ',', 'N.', ',', 'Friedman', ',', 'C.', ',', '&', 'Chi', ',', 'E.', '(', '1985', ',', 'November', ')', '.']

 TOTAL FILTERED TOKENS ==>  25

 ---- POST FOR FILTERED TOKENS ----

 [('[', 'RB'), ('78', 'CD'), (']', 'JJ'), ('Lyman', 'NNP'), (',', ','), ('M.', 'NNP'), (',', ','), ('Sager', 'NNP'), (',', ','), ('N.', 'NNP'), (',', ','), ('Friedman', 'NNP'), (',', ','), ('C.', 'NNP'), (',', ','), ('&', 'CC'), ('Chi', 'NNP'), (',', ','), ('E.', 'NNP'), ('(', '('), ('1985', 'CD'), (',', ','), ('November', 'NNP'), (')', ')'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['[ 78', '78 ]', '] Lyman', 'Lyman ,', ', M.', 'M. ,', ', Sager', 'Sager ,', ', N.', 'N. ,', ', Friedman', 'Friedman ,', ', C.', 'C. ,', ', &', '& Chi', 'Chi ,', ', E.', 'E. (', '( 1985', '1985 ,', ', November', 'November )', ') .'] 

 TOTAL BIGRAMS --> 24 



 ---- TRI-GRAMS ---- 

 ['[ 78 ]', '78 ] Lyman', '] Lyman ,', 'Lyman , M.', ', M. ,', 'M. , Sager', ', Sager ,', 'Sager , N.', ', N. ,', 'N. , Friedman', ', Friedman ,', 'Friedman , C.', ', C. ,', 'C. , &', ', & Chi', '& Chi ,', 'Chi , E.', ', E. (', 'E. ( 1985', '( 1985 ,', '1985 , November', ', November )', 'November ) .'] 

 TOTAL TRIGRAMS --> 23 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> ['Lyman', 'Sager', 'Friedman']
 TOTAL PERSON ENTITY --> 3 


 GPE ---> ['Chi']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['[', '78', ']', 'lyman', ',', 'm.', ',', 'sager', ',', 'n.', ',', 'friedman', ',', 'c.', ',', '&', 'chi', ',', 'e.', '(', '1985', ',', 'novemb', ')', '.']

 TOTAL PORTER STEM WORDS ==> 25



 ---- SNOWBALL STEMMING ----

['[', '78', ']', 'lyman', ',', 'm.', ',', 'sager', ',', 'n.', ',', 'friedman', ',', 'c.', ',', '&', 'chi', ',', 'e.', '(', '1985', ',', 'novemb', ')', '.']

 TOTAL SNOWBALL STEM WORDS ==> 25



 ---- LEMMATIZATION ----

['[', '78', ']', 'Lyman', ',', 'M.', ',', 'Sager', ',', 'N.', ',', 'Friedman', ',', 'C.', ',', '&', 'Chi', ',', 'E.', '(', '1985', ',', 'November', ')', '.']

 TOTAL LEMMATIZE WORDS ==> 25

************************************************************************************************************************

664 --> Computer-structured  narrative in ambulatory care: its use in longitudinal review of clinical data. 


 ---- TOKENS ----

 ['Computer-structured', 'narrative', 'in', 'ambulatory', 'care', ':', 'its', 'use', 'in', 'longitudinal', 'review', 'of', 'clinical', 'data', '.'] 

 TOTAL TOKENS ==> 15

 ---- POST ----

 [('Computer-structured', 'JJ'), ('narrative', 'NN'), ('in', 'IN'), ('ambulatory', 'JJ'), ('care', 'NN'), (':', ':'), ('its', 'PRP$'), ('use', 'NN'), ('in', 'IN'), ('longitudinal', 'JJ'), ('review', 'NN'), ('of', 'IN'), ('clinical', 'JJ'), ('data', 'NNS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Computer-structured', 'narrative', 'ambulatory', 'care', ':', 'use', 'longitudinal', 'review', 'clinical', 'data', '.']

 TOTAL FILTERED TOKENS ==>  11

 ---- POST FOR FILTERED TOKENS ----

 [('Computer-structured', 'JJ'), ('narrative', 'JJ'), ('ambulatory', 'NN'), ('care', 'NN'), (':', ':'), ('use', 'NN'), ('longitudinal', 'JJ'), ('review', 'NN'), ('clinical', 'JJ'), ('data', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Computer-structured narrative', 'narrative ambulatory', 'ambulatory care', 'care :', ': use', 'use longitudinal', 'longitudinal review', 'review clinical', 'clinical data', 'data .'] 

 TOTAL BIGRAMS --> 10 



 ---- TRI-GRAMS ---- 

 ['Computer-structured narrative ambulatory', 'narrative ambulatory care', 'ambulatory care :', 'care : use', ': use longitudinal', 'use longitudinal review', 'longitudinal review clinical', 'review clinical data', 'clinical data .'] 

 TOTAL TRIGRAMS --> 9 



 ---- NOUN PHRASES ---- 

 ['Computer-structured narrative ambulatory', 'care', 'use', 'longitudinal review'] 

 TOTAL NOUN PHRASES --> 4 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['computer-structur', 'narr', 'ambulatori', 'care', ':', 'use', 'longitudin', 'review', 'clinic', 'data', '.']

 TOTAL PORTER STEM WORDS ==> 11



 ---- SNOWBALL STEMMING ----

['computer-structur', 'narrat', 'ambulatori', 'care', ':', 'use', 'longitudin', 'review', 'clinic', 'data', '.']

 TOTAL SNOWBALL STEM WORDS ==> 11



 ---- LEMMATIZATION ----

['Computer-structured', 'narrative', 'ambulatory', 'care', ':', 'use', 'longitudinal', 'review', 'clinical', 'data', '.']

 TOTAL LEMMATIZE WORDS ==> 11

************************************************************************************************************************

665 --> In Proceedings of  the Annual Symposium on Computer Application in Medical Care (p. 82). 


 ---- TOKENS ----

 ['In', 'Proceedings', 'of', 'the', 'Annual', 'Symposium', 'on', 'Computer', 'Application', 'in', 'Medical', 'Care', '(', 'p.', '82', ')', '.'] 

 TOTAL TOKENS ==> 17

 ---- POST ----

 [('In', 'IN'), ('Proceedings', 'NNP'), ('of', 'IN'), ('the', 'DT'), ('Annual', 'NNP'), ('Symposium', 'NNP'), ('on', 'IN'), ('Computer', 'NNP'), ('Application', 'NNP'), ('in', 'IN'), ('Medical', 'NNP'), ('Care', 'NNP'), ('(', '('), ('p.', 'VB'), ('82', 'CD'), (')', ')'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Proceedings', 'Annual', 'Symposium', 'Computer', 'Application', 'Medical', 'Care', '(', 'p.', '82', ')', '.']

 TOTAL FILTERED TOKENS ==>  12

 ---- POST FOR FILTERED TOKENS ----

 [('Proceedings', 'NNS'), ('Annual', 'NNP'), ('Symposium', 'NNP'), ('Computer', 'NNP'), ('Application', 'NNP'), ('Medical', 'NNP'), ('Care', 'NNP'), ('(', '('), ('p.', 'VB'), ('82', 'CD'), (')', ')'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Proceedings Annual', 'Annual Symposium', 'Symposium Computer', 'Computer Application', 'Application Medical', 'Medical Care', 'Care (', '( p.', 'p. 82', '82 )', ') .'] 

 TOTAL BIGRAMS --> 11 



 ---- TRI-GRAMS ---- 

 ['Proceedings Annual Symposium', 'Annual Symposium Computer', 'Symposium Computer Application', 'Computer Application Medical', 'Application Medical Care', 'Medical Care (', 'Care ( p.', '( p. 82', 'p. 82 )', '82 ) .'] 

 TOTAL TRIGRAMS --> 10 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> ['Annual Symposium Computer']
 TOTAL PERSON ENTITY --> 1 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['proceed', 'annual', 'symposium', 'comput', 'applic', 'medic', 'care', '(', 'p.', '82', ')', '.']

 TOTAL PORTER STEM WORDS ==> 12



 ---- SNOWBALL STEMMING ----

['proceed', 'annual', 'symposium', 'comput', 'applic', 'medic', 'care', '(', 'p.', '82', ')', '.']

 TOTAL SNOWBALL STEM WORDS ==> 12



 ---- LEMMATIZATION ----

['Proceedings', 'Annual', 'Symposium', 'Computer', 'Application', 'Medical', 'Care', '(', 'p.', '82', ')', '.']

 TOTAL LEMMATIZE WORDS ==> 12

************************************************************************************************************************

666 --> American Medical  Informatics Association. 


 ---- TOKENS ----

 ['American', 'Medical', 'Informatics', 'Association', '.'] 

 TOTAL TOKENS ==> 5

 ---- POST ----

 [('American', 'NNP'), ('Medical', 'NNP'), ('Informatics', 'NNP'), ('Association', 'NNP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['American', 'Medical', 'Informatics', 'Association', '.']

 TOTAL FILTERED TOKENS ==>  5

 ---- POST FOR FILTERED TOKENS ----

 [('American', 'NNP'), ('Medical', 'NNP'), ('Informatics', 'NNP'), ('Association', 'NNP'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['American Medical', 'Medical Informatics', 'Informatics Association', 'Association .'] 

 TOTAL BIGRAMS --> 4 



 ---- TRI-GRAMS ---- 

 ['American Medical Informatics', 'Medical Informatics Association', 'Informatics Association .'] 

 TOTAL TRIGRAMS --> 3 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> ['Medical Informatics Association']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> ['American']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['american', 'medic', 'informat', 'associ', '.']

 TOTAL PORTER STEM WORDS ==> 5



 ---- SNOWBALL STEMMING ----

['american', 'medic', 'informat', 'associ', '.']

 TOTAL SNOWBALL STEM WORDS ==> 5



 ---- LEMMATIZATION ----

['American', 'Medical', 'Informatics', 'Association', '.']

 TOTAL LEMMATIZE WORDS ==> 5

************************************************************************************************************************

667 --> [79] McCray, A. T., & Nelson, S. J. 


 ---- TOKENS ----

 ['[', '79', ']', 'McCray', ',', 'A.', 'T.', ',', '&', 'Nelson', ',', 'S.', 'J', '.'] 

 TOTAL TOKENS ==> 14

 ---- POST ----

 [('[', 'RB'), ('79', 'CD'), (']', 'JJ'), ('McCray', 'NNP'), (',', ','), ('A.', 'NNP'), ('T.', 'NNP'), (',', ','), ('&', 'CC'), ('Nelson', 'NNP'), (',', ','), ('S.', 'NNP'), ('J', 'NNP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['[', '79', ']', 'McCray', ',', 'A.', 'T.', ',', '&', 'Nelson', ',', 'S.', 'J', '.']

 TOTAL FILTERED TOKENS ==>  14

 ---- POST FOR FILTERED TOKENS ----

 [('[', 'RB'), ('79', 'CD'), (']', 'JJ'), ('McCray', 'NNP'), (',', ','), ('A.', 'NNP'), ('T.', 'NNP'), (',', ','), ('&', 'CC'), ('Nelson', 'NNP'), (',', ','), ('S.', 'NNP'), ('J', 'NNP'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['[ 79', '79 ]', '] McCray', 'McCray ,', ', A.', 'A. T.', 'T. ,', ', &', '& Nelson', 'Nelson ,', ', S.', 'S. J', 'J .'] 

 TOTAL BIGRAMS --> 13 



 ---- TRI-GRAMS ---- 

 ['[ 79 ]', '79 ] McCray', '] McCray ,', 'McCray , A.', ', A. T.', 'A. T. ,', 'T. , &', ', & Nelson', '& Nelson ,', 'Nelson , S.', ', S. J', 'S. J .'] 

 TOTAL TRIGRAMS --> 12 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> ['McCray']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> ['Nelson']
 TOTAL PERSON ENTITY --> 1 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['[', '79', ']', 'mccray', ',', 'a.', 't.', ',', '&', 'nelson', ',', 's.', 'j', '.']

 TOTAL PORTER STEM WORDS ==> 14



 ---- SNOWBALL STEMMING ----

['[', '79', ']', 'mccray', ',', 'a.', 't.', ',', '&', 'nelson', ',', 's.', 'j', '.']

 TOTAL SNOWBALL STEM WORDS ==> 14



 ---- LEMMATIZATION ----

['[', '79', ']', 'McCray', ',', 'A.', 'T.', ',', '&', 'Nelson', ',', 'S.', 'J', '.']

 TOTAL LEMMATIZE WORDS ==> 14

************************************************************************************************************************

668 --> (1995). 


 ---- TOKENS ----

 ['(', '1995', ')', '.'] 

 TOTAL TOKENS ==> 4

 ---- POST ----

 [('(', '('), ('1995', 'CD'), (')', ')'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['(', '1995', ')', '.']

 TOTAL FILTERED TOKENS ==>  4

 ---- POST FOR FILTERED TOKENS ----

 [('(', '('), ('1995', 'CD'), (')', ')'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['( 1995', '1995 )', ') .'] 

 TOTAL BIGRAMS --> 3 



 ---- TRI-GRAMS ---- 

 ['( 1995 )', '1995 ) .'] 

 TOTAL TRIGRAMS --> 2 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['(', '1995', ')', '.']

 TOTAL PORTER STEM WORDS ==> 4



 ---- SNOWBALL STEMMING ----

['(', '1995', ')', '.']

 TOTAL SNOWBALL STEM WORDS ==> 4



 ---- LEMMATIZATION ----

['(', '1995', ')', '.']

 TOTAL LEMMATIZE WORDS ==> 4

************************************************************************************************************************

669 --> The representation of meaning in the  UMLS. 


 ---- TOKENS ----

 ['The', 'representation', 'of', 'meaning', 'in', 'the', 'UMLS', '.'] 

 TOTAL TOKENS ==> 8

 ---- POST ----

 [('The', 'DT'), ('representation', 'NN'), ('of', 'IN'), ('meaning', 'NN'), ('in', 'IN'), ('the', 'DT'), ('UMLS', 'NNP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['representation', 'meaning', 'UMLS', '.']

 TOTAL FILTERED TOKENS ==>  4

 ---- POST FOR FILTERED TOKENS ----

 [('representation', 'NN'), ('meaning', 'NN'), ('UMLS', 'NNP'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['representation meaning', 'meaning UMLS', 'UMLS .'] 

 TOTAL BIGRAMS --> 3 



 ---- TRI-GRAMS ---- 

 ['representation meaning UMLS', 'meaning UMLS .'] 

 TOTAL TRIGRAMS --> 2 



 ---- NOUN PHRASES ---- 

 ['representation', 'meaning'] 

 TOTAL NOUN PHRASES --> 2 



 ---- NER ----

 
 ORGANIZATION ---> ['UMLS']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['represent', 'mean', 'uml', '.']

 TOTAL PORTER STEM WORDS ==> 4



 ---- SNOWBALL STEMMING ----

['represent', 'mean', 'uml', '.']

 TOTAL SNOWBALL STEM WORDS ==> 4



 ---- LEMMATIZATION ----

['representation', 'meaning', 'UMLS', '.']

 TOTAL LEMMATIZE WORDS ==> 4

************************************************************************************************************************

670 --> Methods of information in medicine, 34(1-2), 193-201. 


 ---- TOKENS ----

 ['Methods', 'of', 'information', 'in', 'medicine', ',', '34', '(', '1-2', ')', ',', '193-201', '.'] 

 TOTAL TOKENS ==> 13

 ---- POST ----

 [('Methods', 'NNS'), ('of', 'IN'), ('information', 'NN'), ('in', 'IN'), ('medicine', 'NN'), (',', ','), ('34', 'CD'), ('(', '('), ('1-2', 'JJ'), (')', ')'), (',', ','), ('193-201', 'JJ'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Methods', 'information', 'medicine', ',', '34', '(', '1-2', ')', ',', '193-201', '.']

 TOTAL FILTERED TOKENS ==>  11

 ---- POST FOR FILTERED TOKENS ----

 [('Methods', 'NNS'), ('information', 'NN'), ('medicine', 'NN'), (',', ','), ('34', 'CD'), ('(', '('), ('1-2', 'JJ'), (')', ')'), (',', ','), ('193-201', 'JJ'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Methods information', 'information medicine', 'medicine ,', ', 34', '34 (', '( 1-2', '1-2 )', ') ,', ', 193-201', '193-201 .'] 

 TOTAL BIGRAMS --> 10 



 ---- TRI-GRAMS ---- 

 ['Methods information medicine', 'information medicine ,', 'medicine , 34', ', 34 (', '34 ( 1-2', '( 1-2 )', '1-2 ) ,', ') , 193-201', ', 193-201 .'] 

 TOTAL TRIGRAMS --> 9 



 ---- NOUN PHRASES ---- 

 ['information', 'medicine'] 

 TOTAL NOUN PHRASES --> 2 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> ['Methods']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['method', 'inform', 'medicin', ',', '34', '(', '1-2', ')', ',', '193-201', '.']

 TOTAL PORTER STEM WORDS ==> 11



 ---- SNOWBALL STEMMING ----

['method', 'inform', 'medicin', ',', '34', '(', '1-2', ')', ',', '193-201', '.']

 TOTAL SNOWBALL STEM WORDS ==> 11



 ---- LEMMATIZATION ----

['Methods', 'information', 'medicine', ',', '34', '(', '1-2', ')', ',', '193-201', '.']

 TOTAL LEMMATIZE WORDS ==> 11

************************************************************************************************************************

671 --> [80] McGray, A. T., Sponsler, J. L., Brylawski, B., & Browne, A. C. (1987, November). 


 ---- TOKENS ----

 ['[', '80', ']', 'McGray', ',', 'A.', 'T.', ',', 'Sponsler', ',', 'J.', 'L.', ',', 'Brylawski', ',', 'B.', ',', '&', 'Browne', ',', 'A.', 'C.', '(', '1987', ',', 'November', ')', '.'] 

 TOTAL TOKENS ==> 28

 ---- POST ----

 [('[', 'RB'), ('80', 'CD'), (']', 'JJ'), ('McGray', 'NNP'), (',', ','), ('A.', 'NNP'), ('T.', 'NNP'), (',', ','), ('Sponsler', 'NNP'), (',', ','), ('J.', 'NNP'), ('L.', 'NNP'), (',', ','), ('Brylawski', 'NNP'), (',', ','), ('B.', 'NNP'), (',', ','), ('&', 'CC'), ('Browne', 'NNP'), (',', ','), ('A.', 'NNP'), ('C.', 'NNP'), ('(', '('), ('1987', 'CD'), (',', ','), ('November', 'NNP'), (')', ')'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['[', '80', ']', 'McGray', ',', 'A.', 'T.', ',', 'Sponsler', ',', 'J.', 'L.', ',', 'Brylawski', ',', 'B.', ',', '&', 'Browne', ',', 'A.', 'C.', '(', '1987', ',', 'November', ')', '.']

 TOTAL FILTERED TOKENS ==>  28

 ---- POST FOR FILTERED TOKENS ----

 [('[', 'RB'), ('80', 'CD'), (']', 'JJ'), ('McGray', 'NNP'), (',', ','), ('A.', 'NNP'), ('T.', 'NNP'), (',', ','), ('Sponsler', 'NNP'), (',', ','), ('J.', 'NNP'), ('L.', 'NNP'), (',', ','), ('Brylawski', 'NNP'), (',', ','), ('B.', 'NNP'), (',', ','), ('&', 'CC'), ('Browne', 'NNP'), (',', ','), ('A.', 'NNP'), ('C.', 'NNP'), ('(', '('), ('1987', 'CD'), (',', ','), ('November', 'NNP'), (')', ')'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['[ 80', '80 ]', '] McGray', 'McGray ,', ', A.', 'A. T.', 'T. ,', ', Sponsler', 'Sponsler ,', ', J.', 'J. L.', 'L. ,', ', Brylawski', 'Brylawski ,', ', B.', 'B. ,', ', &', '& Browne', 'Browne ,', ', A.', 'A. C.', 'C. (', '( 1987', '1987 ,', ', November', 'November )', ') .'] 

 TOTAL BIGRAMS --> 27 



 ---- TRI-GRAMS ---- 

 ['[ 80 ]', '80 ] McGray', '] McGray ,', 'McGray , A.', ', A. T.', 'A. T. ,', 'T. , Sponsler', ', Sponsler ,', 'Sponsler , J.', ', J. L.', 'J. L. ,', 'L. , Brylawski', ', Brylawski ,', 'Brylawski , B.', ', B. ,', 'B. , &', ', & Browne', '& Browne ,', 'Browne , A.', ', A. C.', 'A. C. (', 'C. ( 1987', '( 1987 ,', '1987 , November', ', November )', 'November ) .'] 

 TOTAL TRIGRAMS --> 26 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> ['McGray']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> ['Sponsler', 'J. L.', 'Brylawski', 'Browne']
 TOTAL PERSON ENTITY --> 4 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['[', '80', ']', 'mcgray', ',', 'a.', 't.', ',', 'sponsler', ',', 'j.', 'l.', ',', 'brylawski', ',', 'b.', ',', '&', 'brown', ',', 'a.', 'c.', '(', '1987', ',', 'novemb', ')', '.']

 TOTAL PORTER STEM WORDS ==> 28



 ---- SNOWBALL STEMMING ----

['[', '80', ']', 'mcgray', ',', 'a.', 't.', ',', 'sponsler', ',', 'j.', 'l.', ',', 'brylawski', ',', 'b.', ',', '&', 'brown', ',', 'a.', 'c.', '(', '1987', ',', 'novemb', ')', '.']

 TOTAL SNOWBALL STEM WORDS ==> 28



 ---- LEMMATIZATION ----

['[', '80', ']', 'McGray', ',', 'A.', 'T.', ',', 'Sponsler', ',', 'J.', 'L.', ',', 'Brylawski', ',', 'B.', ',', '&', 'Browne', ',', 'A.', 'C.', '(', '1987', ',', 'November', ')', '.']

 TOTAL LEMMATIZE WORDS ==> 28

************************************************************************************************************************

672 --> The  role of lexical knowledge in biomedical text understanding. 


 ---- TOKENS ----

 ['The', 'role', 'of', 'lexical', 'knowledge', 'in', 'biomedical', 'text', 'understanding', '.'] 

 TOTAL TOKENS ==> 10

 ---- POST ----

 [('The', 'DT'), ('role', 'NN'), ('of', 'IN'), ('lexical', 'JJ'), ('knowledge', 'NN'), ('in', 'IN'), ('biomedical', 'JJ'), ('text', 'NN'), ('understanding', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['role', 'lexical', 'knowledge', 'biomedical', 'text', 'understanding', '.']

 TOTAL FILTERED TOKENS ==>  7

 ---- POST FOR FILTERED TOKENS ----

 [('role', 'NN'), ('lexical', 'JJ'), ('knowledge', 'NN'), ('biomedical', 'JJ'), ('text', 'NN'), ('understanding', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['role lexical', 'lexical knowledge', 'knowledge biomedical', 'biomedical text', 'text understanding', 'understanding .'] 

 TOTAL BIGRAMS --> 6 



 ---- TRI-GRAMS ---- 

 ['role lexical knowledge', 'lexical knowledge biomedical', 'knowledge biomedical text', 'biomedical text understanding', 'text understanding .'] 

 TOTAL TRIGRAMS --> 5 



 ---- NOUN PHRASES ---- 

 ['role', 'lexical knowledge', 'biomedical text', 'understanding'] 

 TOTAL NOUN PHRASES --> 4 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['role', 'lexic', 'knowledg', 'biomed', 'text', 'understand', '.']

 TOTAL PORTER STEM WORDS ==> 7



 ---- SNOWBALL STEMMING ----

['role', 'lexic', 'knowledg', 'biomed', 'text', 'understand', '.']

 TOTAL SNOWBALL STEM WORDS ==> 7



 ---- LEMMATIZATION ----

['role', 'lexical', 'knowledge', 'biomedical', 'text', 'understanding', '.']

 TOTAL LEMMATIZE WORDS ==> 7

************************************************************************************************************************

673 --> In Proceedings of the Annual  Symposium on Computer Application in Medical Care (p. 103). 


 ---- TOKENS ----

 ['In', 'Proceedings', 'of', 'the', 'Annual', 'Symposium', 'on', 'Computer', 'Application', 'in', 'Medical', 'Care', '(', 'p.', '103', ')', '.'] 

 TOTAL TOKENS ==> 17

 ---- POST ----

 [('In', 'IN'), ('Proceedings', 'NNP'), ('of', 'IN'), ('the', 'DT'), ('Annual', 'NNP'), ('Symposium', 'NNP'), ('on', 'IN'), ('Computer', 'NNP'), ('Application', 'NNP'), ('in', 'IN'), ('Medical', 'NNP'), ('Care', 'NNP'), ('(', '('), ('p.', 'VB'), ('103', 'CD'), (')', ')'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Proceedings', 'Annual', 'Symposium', 'Computer', 'Application', 'Medical', 'Care', '(', 'p.', '103', ')', '.']

 TOTAL FILTERED TOKENS ==>  12

 ---- POST FOR FILTERED TOKENS ----

 [('Proceedings', 'NNS'), ('Annual', 'NNP'), ('Symposium', 'NNP'), ('Computer', 'NNP'), ('Application', 'NNP'), ('Medical', 'NNP'), ('Care', 'NNP'), ('(', '('), ('p.', 'VB'), ('103', 'CD'), (')', ')'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Proceedings Annual', 'Annual Symposium', 'Symposium Computer', 'Computer Application', 'Application Medical', 'Medical Care', 'Care (', '( p.', 'p. 103', '103 )', ') .'] 

 TOTAL BIGRAMS --> 11 



 ---- TRI-GRAMS ---- 

 ['Proceedings Annual Symposium', 'Annual Symposium Computer', 'Symposium Computer Application', 'Computer Application Medical', 'Application Medical Care', 'Medical Care (', 'Care ( p.', '( p. 103', 'p. 103 )', '103 ) .'] 

 TOTAL TRIGRAMS --> 10 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> ['Annual Symposium Computer']
 TOTAL PERSON ENTITY --> 1 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['proceed', 'annual', 'symposium', 'comput', 'applic', 'medic', 'care', '(', 'p.', '103', ')', '.']

 TOTAL PORTER STEM WORDS ==> 12



 ---- SNOWBALL STEMMING ----

['proceed', 'annual', 'symposium', 'comput', 'applic', 'medic', 'care', '(', 'p.', '103', ')', '.']

 TOTAL SNOWBALL STEM WORDS ==> 12



 ---- LEMMATIZATION ----

['Proceedings', 'Annual', 'Symposium', 'Computer', 'Application', 'Medical', 'Care', '(', 'p.', '103', ')', '.']

 TOTAL LEMMATIZE WORDS ==> 12

************************************************************************************************************************

674 --> American Medical  Informatics Association. 


 ---- TOKENS ----

 ['American', 'Medical', 'Informatics', 'Association', '.'] 

 TOTAL TOKENS ==> 5

 ---- POST ----

 [('American', 'NNP'), ('Medical', 'NNP'), ('Informatics', 'NNP'), ('Association', 'NNP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['American', 'Medical', 'Informatics', 'Association', '.']

 TOTAL FILTERED TOKENS ==>  5

 ---- POST FOR FILTERED TOKENS ----

 [('American', 'NNP'), ('Medical', 'NNP'), ('Informatics', 'NNP'), ('Association', 'NNP'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['American Medical', 'Medical Informatics', 'Informatics Association', 'Association .'] 

 TOTAL BIGRAMS --> 4 



 ---- TRI-GRAMS ---- 

 ['American Medical Informatics', 'Medical Informatics Association', 'Informatics Association .'] 

 TOTAL TRIGRAMS --> 3 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> ['Medical Informatics Association']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> ['American']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['american', 'medic', 'informat', 'associ', '.']

 TOTAL PORTER STEM WORDS ==> 5



 ---- SNOWBALL STEMMING ----

['american', 'medic', 'informat', 'associ', '.']

 TOTAL SNOWBALL STEM WORDS ==> 5



 ---- LEMMATIZATION ----

['American', 'Medical', 'Informatics', 'Association', '.']

 TOTAL LEMMATIZE WORDS ==> 5

************************************************************************************************************************

675 --> [81] McCray, A. T. (1991). 


 ---- TOKENS ----

 ['[', '81', ']', 'McCray', ',', 'A.', 'T.', '(', '1991', ')', '.'] 

 TOTAL TOKENS ==> 11

 ---- POST ----

 [('[', 'RB'), ('81', 'CD'), (']', 'JJ'), ('McCray', 'NNP'), (',', ','), ('A.', 'NNP'), ('T.', 'NNP'), ('(', '('), ('1991', 'CD'), (')', ')'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['[', '81', ']', 'McCray', ',', 'A.', 'T.', '(', '1991', ')', '.']

 TOTAL FILTERED TOKENS ==>  11

 ---- POST FOR FILTERED TOKENS ----

 [('[', 'RB'), ('81', 'CD'), (']', 'JJ'), ('McCray', 'NNP'), (',', ','), ('A.', 'NNP'), ('T.', 'NNP'), ('(', '('), ('1991', 'CD'), (')', ')'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['[ 81', '81 ]', '] McCray', 'McCray ,', ', A.', 'A. T.', 'T. (', '( 1991', '1991 )', ') .'] 

 TOTAL BIGRAMS --> 10 



 ---- TRI-GRAMS ---- 

 ['[ 81 ]', '81 ] McCray', '] McCray ,', 'McCray , A.', ', A. T.', 'A. T. (', 'T. ( 1991', '( 1991 )', '1991 ) .'] 

 TOTAL TRIGRAMS --> 9 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> ['McCray']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['[', '81', ']', 'mccray', ',', 'a.', 't.', '(', '1991', ')', '.']

 TOTAL PORTER STEM WORDS ==> 11



 ---- SNOWBALL STEMMING ----

['[', '81', ']', 'mccray', ',', 'a.', 't.', '(', '1991', ')', '.']

 TOTAL SNOWBALL STEM WORDS ==> 11



 ---- LEMMATIZATION ----

['[', '81', ']', 'McCray', ',', 'A.', 'T.', '(', '1991', ')', '.']

 TOTAL LEMMATIZE WORDS ==> 11

************************************************************************************************************************

676 --> Natural language processing for intelligent information retrieval. 


 ---- TOKENS ----

 ['Natural', 'language', 'processing', 'for', 'intelligent', 'information', 'retrieval', '.'] 

 TOTAL TOKENS ==> 8

 ---- POST ----

 [('Natural', 'JJ'), ('language', 'NN'), ('processing', 'NN'), ('for', 'IN'), ('intelligent', 'JJ'), ('information', 'NN'), ('retrieval', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Natural', 'language', 'processing', 'intelligent', 'information', 'retrieval', '.']

 TOTAL FILTERED TOKENS ==>  7

 ---- POST FOR FILTERED TOKENS ----

 [('Natural', 'JJ'), ('language', 'NN'), ('processing', 'NN'), ('intelligent', 'JJ'), ('information', 'NN'), ('retrieval', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Natural language', 'language processing', 'processing intelligent', 'intelligent information', 'information retrieval', 'retrieval .'] 

 TOTAL BIGRAMS --> 6 



 ---- TRI-GRAMS ---- 

 ['Natural language processing', 'language processing intelligent', 'processing intelligent information', 'intelligent information retrieval', 'information retrieval .'] 

 TOTAL TRIGRAMS --> 5 



 ---- NOUN PHRASES ---- 

 ['Natural language', 'processing', 'intelligent information', 'retrieval'] 

 TOTAL NOUN PHRASES --> 4 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['natur', 'languag', 'process', 'intellig', 'inform', 'retriev', '.']

 TOTAL PORTER STEM WORDS ==> 7



 ---- SNOWBALL STEMMING ----

['natur', 'languag', 'process', 'intellig', 'inform', 'retriev', '.']

 TOTAL SNOWBALL STEM WORDS ==> 7



 ---- LEMMATIZATION ----

['Natural', 'language', 'processing', 'intelligent', 'information', 'retrieval', '.']

 TOTAL LEMMATIZE WORDS ==> 7

************************************************************************************************************************

677 --> In Engineering in Medicine and Biology Society, 1991. 


 ---- TOKENS ----

 ['In', 'Engineering', 'in', 'Medicine', 'and', 'Biology', 'Society', ',', '1991', '.'] 

 TOTAL TOKENS ==> 10

 ---- POST ----

 [('In', 'IN'), ('Engineering', 'NNP'), ('in', 'IN'), ('Medicine', 'NNP'), ('and', 'CC'), ('Biology', 'NNP'), ('Society', 'NNP'), (',', ','), ('1991', 'CD'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Engineering', 'Medicine', 'Biology', 'Society', ',', '1991', '.']

 TOTAL FILTERED TOKENS ==>  7

 ---- POST FOR FILTERED TOKENS ----

 [('Engineering', 'VBG'), ('Medicine', 'NNP'), ('Biology', 'NNP'), ('Society', 'NNP'), (',', ','), ('1991', 'CD'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Engineering Medicine', 'Medicine Biology', 'Biology Society', 'Society ,', ', 1991', '1991 .'] 

 TOTAL BIGRAMS --> 6 



 ---- TRI-GRAMS ---- 

 ['Engineering Medicine Biology', 'Medicine Biology Society', 'Biology Society ,', 'Society , 1991', ', 1991 .'] 

 TOTAL TRIGRAMS --> 5 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> ['Medicine Biology Society']
 TOTAL PERSON ENTITY --> 1 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['engin', 'medicin', 'biolog', 'societi', ',', '1991', '.']

 TOTAL PORTER STEM WORDS ==> 7



 ---- SNOWBALL STEMMING ----

['engin', 'medicin', 'biolog', 'societi', ',', '1991', '.']

 TOTAL SNOWBALL STEM WORDS ==> 7



 ---- LEMMATIZATION ----

['Engineering', 'Medicine', 'Biology', 'Society', ',', '1991', '.']

 TOTAL LEMMATIZE WORDS ==> 7

************************************************************************************************************************

678 --> Vol. 


 ---- TOKENS ----

 ['Vol', '.'] 

 TOTAL TOKENS ==> 2

 ---- POST ----

 [('Vol', 'NNP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Vol', '.']

 TOTAL FILTERED TOKENS ==>  2

 ---- POST FOR FILTERED TOKENS ----

 [('Vol', 'NNP'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Vol .'] 

 TOTAL BIGRAMS --> 1 



 ---- TRI-GRAMS ---- 

 [] 

 TOTAL TRIGRAMS --> 0 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> ['Vol']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['vol', '.']

 TOTAL PORTER STEM WORDS ==> 2



 ---- SNOWBALL STEMMING ----

['vol', '.']

 TOTAL SNOWBALL STEM WORDS ==> 2



 ---- LEMMATIZATION ----

['Vol', '.']

 TOTAL LEMMATIZE WORDS ==> 2

************************************************************************************************************************

679 --> 13: 1991., Proceedings of the  Annual International Conference of the IEEE (pp. 


 ---- TOKENS ----

 ['13', ':', '1991.', ',', 'Proceedings', 'of', 'the', 'Annual', 'International', 'Conference', 'of', 'the', 'IEEE', '(', 'pp', '.'] 

 TOTAL TOKENS ==> 16

 ---- POST ----

 [('13', 'CD'), (':', ':'), ('1991.', 'CD'), (',', ','), ('Proceedings', 'NNS'), ('of', 'IN'), ('the', 'DT'), ('Annual', 'NNP'), ('International', 'NNP'), ('Conference', 'NNP'), ('of', 'IN'), ('the', 'DT'), ('IEEE', 'NNP'), ('(', '('), ('pp', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['13', ':', '1991.', ',', 'Proceedings', 'Annual', 'International', 'Conference', 'IEEE', '(', 'pp', '.']

 TOTAL FILTERED TOKENS ==>  12

 ---- POST FOR FILTERED TOKENS ----

 [('13', 'CD'), (':', ':'), ('1991.', 'CD'), (',', ','), ('Proceedings', 'NNP'), ('Annual', 'NNP'), ('International', 'NNP'), ('Conference', 'NNP'), ('IEEE', 'NNP'), ('(', '('), ('pp', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['13 :', ': 1991.', '1991. ,', ', Proceedings', 'Proceedings Annual', 'Annual International', 'International Conference', 'Conference IEEE', 'IEEE (', '( pp', 'pp .'] 

 TOTAL BIGRAMS --> 11 



 ---- TRI-GRAMS ---- 

 ['13 : 1991.', ': 1991. ,', '1991. , Proceedings', ', Proceedings Annual', 'Proceedings Annual International', 'Annual International Conference', 'International Conference IEEE', 'Conference IEEE (', 'IEEE ( pp', '( pp .'] 

 TOTAL TRIGRAMS --> 10 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 []

 ---- PORTER STEMMING ----

['13', ':', '1991.', ',', 'proceed', 'annual', 'intern', 'confer', 'ieee', '(', 'pp', '.']

 TOTAL PORTER STEM WORDS ==> 12



 ---- SNOWBALL STEMMING ----

['13', ':', '1991.', ',', 'proceed', 'annual', 'intern', 'confer', 'ieee', '(', 'pp', '.']

 TOTAL SNOWBALL STEM WORDS ==> 12



 ---- LEMMATIZATION ----

['13', ':', '1991.', ',', 'Proceedings', 'Annual', 'International', 'Conference', 'IEEE', '(', 'pp', '.']

 TOTAL LEMMATIZE WORDS ==> 12

************************************************************************************************************************

680 --> 1160-1161). 


 ---- TOKENS ----

 ['1160-1161', ')', '.'] 

 TOTAL TOKENS ==> 3

 ---- POST ----

 [('1160-1161', 'JJ'), (')', ')'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['1160-1161', ')', '.']

 TOTAL FILTERED TOKENS ==>  3

 ---- POST FOR FILTERED TOKENS ----

 [('1160-1161', 'JJ'), (')', ')'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['1160-1161 )', ') .'] 

 TOTAL BIGRAMS --> 2 



 ---- TRI-GRAMS ---- 

 ['1160-1161 ) .'] 

 TOTAL TRIGRAMS --> 1 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 []

 ---- PORTER STEMMING ----

['1160-1161', ')', '.']

 TOTAL PORTER STEM WORDS ==> 3



 ---- SNOWBALL STEMMING ----

['1160-1161', ')', '.']

 TOTAL SNOWBALL STEM WORDS ==> 3



 ---- LEMMATIZATION ----

['1160-1161', ')', '.']

 TOTAL LEMMATIZE WORDS ==> 3

************************************************************************************************************************

681 --> IEEE. 


 ---- TOKENS ----

 ['IEEE', '.'] 

 TOTAL TOKENS ==> 2

 ---- POST ----

 [('IEEE', 'NNP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['IEEE', '.']

 TOTAL FILTERED TOKENS ==>  2

 ---- POST FOR FILTERED TOKENS ----

 [('IEEE', 'NNP'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['IEEE .'] 

 TOTAL BIGRAMS --> 1 



 ---- TRI-GRAMS ---- 

 [] 

 TOTAL TRIGRAMS --> 0 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['ieee', '.']

 TOTAL PORTER STEM WORDS ==> 2



 ---- SNOWBALL STEMMING ----

['ieee', '.']

 TOTAL SNOWBALL STEM WORDS ==> 2



 ---- LEMMATIZATION ----

['IEEE', '.']

 TOTAL LEMMATIZE WORDS ==> 2

************************************************************************************************************************

682 --> [82] McCray, A. T. (1991). 


 ---- TOKENS ----

 ['[', '82', ']', 'McCray', ',', 'A.', 'T.', '(', '1991', ')', '.'] 

 TOTAL TOKENS ==> 11

 ---- POST ----

 [('[', 'RB'), ('82', 'CD'), (']', 'JJ'), ('McCray', 'NNP'), (',', ','), ('A.', 'NNP'), ('T.', 'NNP'), ('(', '('), ('1991', 'CD'), (')', ')'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['[', '82', ']', 'McCray', ',', 'A.', 'T.', '(', '1991', ')', '.']

 TOTAL FILTERED TOKENS ==>  11

 ---- POST FOR FILTERED TOKENS ----

 [('[', 'RB'), ('82', 'CD'), (']', 'JJ'), ('McCray', 'NNP'), (',', ','), ('A.', 'NNP'), ('T.', 'NNP'), ('(', '('), ('1991', 'CD'), (')', ')'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['[ 82', '82 ]', '] McCray', 'McCray ,', ', A.', 'A. T.', 'T. (', '( 1991', '1991 )', ') .'] 

 TOTAL BIGRAMS --> 10 



 ---- TRI-GRAMS ---- 

 ['[ 82 ]', '82 ] McCray', '] McCray ,', 'McCray , A.', ', A. T.', 'A. T. (', 'T. ( 1991', '( 1991 )', '1991 ) .'] 

 TOTAL TRIGRAMS --> 9 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> ['McCray']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['[', '82', ']', 'mccray', ',', 'a.', 't.', '(', '1991', ')', '.']

 TOTAL PORTER STEM WORDS ==> 11



 ---- SNOWBALL STEMMING ----

['[', '82', ']', 'mccray', ',', 'a.', 't.', '(', '1991', ')', '.']

 TOTAL SNOWBALL STEM WORDS ==> 11



 ---- LEMMATIZATION ----

['[', '82', ']', 'McCray', ',', 'A.', 'T.', '(', '1991', ')', '.']

 TOTAL LEMMATIZE WORDS ==> 11

************************************************************************************************************************

683 --> Extending a natural language parser with UMLS knowledge. 


 ---- TOKENS ----

 ['Extending', 'a', 'natural', 'language', 'parser', 'with', 'UMLS', 'knowledge', '.'] 

 TOTAL TOKENS ==> 9

 ---- POST ----

 [('Extending', 'VBG'), ('a', 'DT'), ('natural', 'JJ'), ('language', 'NN'), ('parser', 'NN'), ('with', 'IN'), ('UMLS', 'NNP'), ('knowledge', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Extending', 'natural', 'language', 'parser', 'UMLS', 'knowledge', '.']

 TOTAL FILTERED TOKENS ==>  7

 ---- POST FOR FILTERED TOKENS ----

 [('Extending', 'VBG'), ('natural', 'JJ'), ('language', 'NN'), ('parser', 'NN'), ('UMLS', 'NNP'), ('knowledge', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Extending natural', 'natural language', 'language parser', 'parser UMLS', 'UMLS knowledge', 'knowledge .'] 

 TOTAL BIGRAMS --> 6 



 ---- TRI-GRAMS ---- 

 ['Extending natural language', 'natural language parser', 'language parser UMLS', 'parser UMLS knowledge', 'UMLS knowledge .'] 

 TOTAL TRIGRAMS --> 5 



 ---- NOUN PHRASES ---- 

 ['natural language', 'parser', 'knowledge'] 

 TOTAL NOUN PHRASES --> 3 



 ---- NER ----

 
 ORGANIZATION ---> ['UMLS']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['extend', 'natur', 'languag', 'parser', 'uml', 'knowledg', '.']

 TOTAL PORTER STEM WORDS ==> 7



 ---- SNOWBALL STEMMING ----

['extend', 'natur', 'languag', 'parser', 'uml', 'knowledg', '.']

 TOTAL SNOWBALL STEM WORDS ==> 7



 ---- LEMMATIZATION ----

['Extending', 'natural', 'language', 'parser', 'UMLS', 'knowledge', '.']

 TOTAL LEMMATIZE WORDS ==> 7

************************************************************************************************************************

684 --> In Proceedings of the Annual Symposium on Computer Application in Medical Care (p. 194). 


 ---- TOKENS ----

 ['In', 'Proceedings', 'of', 'the', 'Annual', 'Symposium', 'on', 'Computer', 'Application', 'in', 'Medical', 'Care', '(', 'p.', '194', ')', '.'] 

 TOTAL TOKENS ==> 17

 ---- POST ----

 [('In', 'IN'), ('Proceedings', 'NNP'), ('of', 'IN'), ('the', 'DT'), ('Annual', 'NNP'), ('Symposium', 'NNP'), ('on', 'IN'), ('Computer', 'NNP'), ('Application', 'NNP'), ('in', 'IN'), ('Medical', 'NNP'), ('Care', 'NNP'), ('(', '('), ('p.', 'VB'), ('194', 'CD'), (')', ')'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Proceedings', 'Annual', 'Symposium', 'Computer', 'Application', 'Medical', 'Care', '(', 'p.', '194', ')', '.']

 TOTAL FILTERED TOKENS ==>  12

 ---- POST FOR FILTERED TOKENS ----

 [('Proceedings', 'NNS'), ('Annual', 'NNP'), ('Symposium', 'NNP'), ('Computer', 'NNP'), ('Application', 'NNP'), ('Medical', 'NNP'), ('Care', 'NNP'), ('(', '('), ('p.', 'VB'), ('194', 'CD'), (')', ')'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Proceedings Annual', 'Annual Symposium', 'Symposium Computer', 'Computer Application', 'Application Medical', 'Medical Care', 'Care (', '( p.', 'p. 194', '194 )', ') .'] 

 TOTAL BIGRAMS --> 11 



 ---- TRI-GRAMS ---- 

 ['Proceedings Annual Symposium', 'Annual Symposium Computer', 'Symposium Computer Application', 'Computer Application Medical', 'Application Medical Care', 'Medical Care (', 'Care ( p.', '( p. 194', 'p. 194 )', '194 ) .'] 

 TOTAL TRIGRAMS --> 10 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> ['Annual Symposium Computer']
 TOTAL PERSON ENTITY --> 1 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['proceed', 'annual', 'symposium', 'comput', 'applic', 'medic', 'care', '(', 'p.', '194', ')', '.']

 TOTAL PORTER STEM WORDS ==> 12



 ---- SNOWBALL STEMMING ----

['proceed', 'annual', 'symposium', 'comput', 'applic', 'medic', 'care', '(', 'p.', '194', ')', '.']

 TOTAL SNOWBALL STEM WORDS ==> 12



 ---- LEMMATIZATION ----

['Proceedings', 'Annual', 'Symposium', 'Computer', 'Application', 'Medical', 'Care', '(', 'p.', '194', ')', '.']

 TOTAL LEMMATIZE WORDS ==> 12

************************************************************************************************************************

685 --> American Medical Informatics Association. 


 ---- TOKENS ----

 ['American', 'Medical', 'Informatics', 'Association', '.'] 

 TOTAL TOKENS ==> 5

 ---- POST ----

 [('American', 'NNP'), ('Medical', 'NNP'), ('Informatics', 'NNP'), ('Association', 'NNP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['American', 'Medical', 'Informatics', 'Association', '.']

 TOTAL FILTERED TOKENS ==>  5

 ---- POST FOR FILTERED TOKENS ----

 [('American', 'NNP'), ('Medical', 'NNP'), ('Informatics', 'NNP'), ('Association', 'NNP'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['American Medical', 'Medical Informatics', 'Informatics Association', 'Association .'] 

 TOTAL BIGRAMS --> 4 



 ---- TRI-GRAMS ---- 

 ['American Medical Informatics', 'Medical Informatics Association', 'Informatics Association .'] 

 TOTAL TRIGRAMS --> 3 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> ['Medical Informatics Association']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> ['American']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['american', 'medic', 'informat', 'associ', '.']

 TOTAL PORTER STEM WORDS ==> 5



 ---- SNOWBALL STEMMING ----

['american', 'medic', 'informat', 'associ', '.']

 TOTAL SNOWBALL STEM WORDS ==> 5



 ---- LEMMATIZATION ----

['American', 'Medical', 'Informatics', 'Association', '.']

 TOTAL LEMMATIZE WORDS ==> 5

************************************************************************************************************************

686 --> [83] McCray, A. T., Srinivasan, S., & Browne, A. C. (1994). 


 ---- TOKENS ----

 ['[', '83', ']', 'McCray', ',', 'A.', 'T.', ',', 'Srinivasan', ',', 'S.', ',', '&', 'Browne', ',', 'A.', 'C.', '(', '1994', ')', '.'] 

 TOTAL TOKENS ==> 21

 ---- POST ----

 [('[', 'RB'), ('83', 'CD'), (']', 'JJ'), ('McCray', 'NNP'), (',', ','), ('A.', 'NNP'), ('T.', 'NNP'), (',', ','), ('Srinivasan', 'NNP'), (',', ','), ('S.', 'NNP'), (',', ','), ('&', 'CC'), ('Browne', 'NNP'), (',', ','), ('A.', 'NNP'), ('C.', 'NNP'), ('(', '('), ('1994', 'CD'), (')', ')'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['[', '83', ']', 'McCray', ',', 'A.', 'T.', ',', 'Srinivasan', ',', 'S.', ',', '&', 'Browne', ',', 'A.', 'C.', '(', '1994', ')', '.']

 TOTAL FILTERED TOKENS ==>  21

 ---- POST FOR FILTERED TOKENS ----

 [('[', 'RB'), ('83', 'CD'), (']', 'JJ'), ('McCray', 'NNP'), (',', ','), ('A.', 'NNP'), ('T.', 'NNP'), (',', ','), ('Srinivasan', 'NNP'), (',', ','), ('S.', 'NNP'), (',', ','), ('&', 'CC'), ('Browne', 'NNP'), (',', ','), ('A.', 'NNP'), ('C.', 'NNP'), ('(', '('), ('1994', 'CD'), (')', ')'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['[ 83', '83 ]', '] McCray', 'McCray ,', ', A.', 'A. T.', 'T. ,', ', Srinivasan', 'Srinivasan ,', ', S.', 'S. ,', ', &', '& Browne', 'Browne ,', ', A.', 'A. C.', 'C. (', '( 1994', '1994 )', ') .'] 

 TOTAL BIGRAMS --> 20 



 ---- TRI-GRAMS ---- 

 ['[ 83 ]', '83 ] McCray', '] McCray ,', 'McCray , A.', ', A. T.', 'A. T. ,', 'T. , Srinivasan', ', Srinivasan ,', 'Srinivasan , S.', ', S. ,', 'S. , &', ', & Browne', '& Browne ,', 'Browne , A.', ', A. C.', 'A. C. (', 'C. ( 1994', '( 1994 )', '1994 ) .'] 

 TOTAL TRIGRAMS --> 19 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> ['McCray']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> ['Browne']
 TOTAL PERSON ENTITY --> 1 


 GPE ---> ['Srinivasan']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['[', '83', ']', 'mccray', ',', 'a.', 't.', ',', 'srinivasan', ',', 's.', ',', '&', 'brown', ',', 'a.', 'c.', '(', '1994', ')', '.']

 TOTAL PORTER STEM WORDS ==> 21



 ---- SNOWBALL STEMMING ----

['[', '83', ']', 'mccray', ',', 'a.', 't.', ',', 'srinivasan', ',', 's.', ',', '&', 'brown', ',', 'a.', 'c.', '(', '1994', ')', '.']

 TOTAL SNOWBALL STEM WORDS ==> 21



 ---- LEMMATIZATION ----

['[', '83', ']', 'McCray', ',', 'A.', 'T.', ',', 'Srinivasan', ',', 'S.', ',', '&', 'Browne', ',', 'A.', 'C.', '(', '1994', ')', '.']

 TOTAL LEMMATIZE WORDS ==> 21

************************************************************************************************************************

687 --> Lexical methods for managing  variation in biomedical terminologies. 


 ---- TOKENS ----

 ['Lexical', 'methods', 'for', 'managing', 'variation', 'in', 'biomedical', 'terminologies', '.'] 

 TOTAL TOKENS ==> 9

 ---- POST ----

 [('Lexical', 'JJ'), ('methods', 'NNS'), ('for', 'IN'), ('managing', 'VBG'), ('variation', 'NN'), ('in', 'IN'), ('biomedical', 'JJ'), ('terminologies', 'NNS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Lexical', 'methods', 'managing', 'variation', 'biomedical', 'terminologies', '.']

 TOTAL FILTERED TOKENS ==>  7

 ---- POST FOR FILTERED TOKENS ----

 [('Lexical', 'JJ'), ('methods', 'NNS'), ('managing', 'VBG'), ('variation', 'NN'), ('biomedical', 'JJ'), ('terminologies', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Lexical methods', 'methods managing', 'managing variation', 'variation biomedical', 'biomedical terminologies', 'terminologies .'] 

 TOTAL BIGRAMS --> 6 



 ---- TRI-GRAMS ---- 

 ['Lexical methods managing', 'methods managing variation', 'managing variation biomedical', 'variation biomedical terminologies', 'biomedical terminologies .'] 

 TOTAL TRIGRAMS --> 5 



 ---- NOUN PHRASES ---- 

 ['variation'] 

 TOTAL NOUN PHRASES --> 1 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> ['Lexical']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['lexic', 'method', 'manag', 'variat', 'biomed', 'terminolog', '.']

 TOTAL PORTER STEM WORDS ==> 7



 ---- SNOWBALL STEMMING ----

['lexic', 'method', 'manag', 'variat', 'biomed', 'terminolog', '.']

 TOTAL SNOWBALL STEM WORDS ==> 7



 ---- LEMMATIZATION ----

['Lexical', 'method', 'managing', 'variation', 'biomedical', 'terminology', '.']

 TOTAL LEMMATIZE WORDS ==> 7

************************************************************************************************************************

688 --> In Proceedings of the Annual Symposium on  Computer Application in Medical Care (p. 235). 


 ---- TOKENS ----

 ['In', 'Proceedings', 'of', 'the', 'Annual', 'Symposium', 'on', 'Computer', 'Application', 'in', 'Medical', 'Care', '(', 'p.', '235', ')', '.'] 

 TOTAL TOKENS ==> 17

 ---- POST ----

 [('In', 'IN'), ('Proceedings', 'NNP'), ('of', 'IN'), ('the', 'DT'), ('Annual', 'NNP'), ('Symposium', 'NNP'), ('on', 'IN'), ('Computer', 'NNP'), ('Application', 'NNP'), ('in', 'IN'), ('Medical', 'NNP'), ('Care', 'NNP'), ('(', '('), ('p.', 'VB'), ('235', 'CD'), (')', ')'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Proceedings', 'Annual', 'Symposium', 'Computer', 'Application', 'Medical', 'Care', '(', 'p.', '235', ')', '.']

 TOTAL FILTERED TOKENS ==>  12

 ---- POST FOR FILTERED TOKENS ----

 [('Proceedings', 'NNS'), ('Annual', 'NNP'), ('Symposium', 'NNP'), ('Computer', 'NNP'), ('Application', 'NNP'), ('Medical', 'NNP'), ('Care', 'NNP'), ('(', '('), ('p.', 'VB'), ('235', 'CD'), (')', ')'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Proceedings Annual', 'Annual Symposium', 'Symposium Computer', 'Computer Application', 'Application Medical', 'Medical Care', 'Care (', '( p.', 'p. 235', '235 )', ') .'] 

 TOTAL BIGRAMS --> 11 



 ---- TRI-GRAMS ---- 

 ['Proceedings Annual Symposium', 'Annual Symposium Computer', 'Symposium Computer Application', 'Computer Application Medical', 'Application Medical Care', 'Medical Care (', 'Care ( p.', '( p. 235', 'p. 235 )', '235 ) .'] 

 TOTAL TRIGRAMS --> 10 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> ['Annual Symposium Computer']
 TOTAL PERSON ENTITY --> 1 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['proceed', 'annual', 'symposium', 'comput', 'applic', 'medic', 'care', '(', 'p.', '235', ')', '.']

 TOTAL PORTER STEM WORDS ==> 12



 ---- SNOWBALL STEMMING ----

['proceed', 'annual', 'symposium', 'comput', 'applic', 'medic', 'care', '(', 'p.', '235', ')', '.']

 TOTAL SNOWBALL STEM WORDS ==> 12



 ---- LEMMATIZATION ----

['Proceedings', 'Annual', 'Symposium', 'Computer', 'Application', 'Medical', 'Care', '(', 'p.', '235', ')', '.']

 TOTAL LEMMATIZE WORDS ==> 12

************************************************************************************************************************

689 --> American Medical Informatics Association. 


 ---- TOKENS ----

 ['American', 'Medical', 'Informatics', 'Association', '.'] 

 TOTAL TOKENS ==> 5

 ---- POST ----

 [('American', 'NNP'), ('Medical', 'NNP'), ('Informatics', 'NNP'), ('Association', 'NNP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['American', 'Medical', 'Informatics', 'Association', '.']

 TOTAL FILTERED TOKENS ==>  5

 ---- POST FOR FILTERED TOKENS ----

 [('American', 'NNP'), ('Medical', 'NNP'), ('Informatics', 'NNP'), ('Association', 'NNP'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['American Medical', 'Medical Informatics', 'Informatics Association', 'Association .'] 

 TOTAL BIGRAMS --> 4 



 ---- TRI-GRAMS ---- 

 ['American Medical Informatics', 'Medical Informatics Association', 'Informatics Association .'] 

 TOTAL TRIGRAMS --> 3 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> ['Medical Informatics Association']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> ['American']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['american', 'medic', 'informat', 'associ', '.']

 TOTAL PORTER STEM WORDS ==> 5



 ---- SNOWBALL STEMMING ----

['american', 'medic', 'informat', 'associ', '.']

 TOTAL SNOWBALL STEM WORDS ==> 5



 ---- LEMMATIZATION ----

['American', 'Medical', 'Informatics', 'Association', '.']

 TOTAL LEMMATIZE WORDS ==> 5

************************************************************************************************************************

690 --> [84] McCray, A. T., & Razi, A. 


 ---- TOKENS ----

 ['[', '84', ']', 'McCray', ',', 'A.', 'T.', ',', '&', 'Razi', ',', 'A', '.'] 

 TOTAL TOKENS ==> 13

 ---- POST ----

 [('[', 'RB'), ('84', 'CD'), (']', 'JJ'), ('McCray', 'NNP'), (',', ','), ('A.', 'NNP'), ('T.', 'NNP'), (',', ','), ('&', 'CC'), ('Razi', 'NNP'), (',', ','), ('A', 'NNP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['[', '84', ']', 'McCray', ',', 'A.', 'T.', ',', '&', 'Razi', ',', '.']

 TOTAL FILTERED TOKENS ==>  12

 ---- POST FOR FILTERED TOKENS ----

 [('[', 'RB'), ('84', 'CD'), (']', 'JJ'), ('McCray', 'NNP'), (',', ','), ('A.', 'NNP'), ('T.', 'NNP'), (',', ','), ('&', 'CC'), ('Razi', 'NNP'), (',', ','), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['[ 84', '84 ]', '] McCray', 'McCray ,', ', A.', 'A. T.', 'T. ,', ', &', '& Razi', 'Razi ,', ', .'] 

 TOTAL BIGRAMS --> 11 



 ---- TRI-GRAMS ---- 

 ['[ 84 ]', '84 ] McCray', '] McCray ,', 'McCray , A.', ', A. T.', 'A. T. ,', 'T. , &', ', & Razi', '& Razi ,', 'Razi , .'] 

 TOTAL TRIGRAMS --> 10 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> ['McCray']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> ['Razi']
 TOTAL PERSON ENTITY --> 1 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['[', '84', ']', 'mccray', ',', 'a.', 't.', ',', '&', 'razi', ',', '.']

 TOTAL PORTER STEM WORDS ==> 12



 ---- SNOWBALL STEMMING ----

['[', '84', ']', 'mccray', ',', 'a.', 't.', ',', '&', 'razi', ',', '.']

 TOTAL SNOWBALL STEM WORDS ==> 12



 ---- LEMMATIZATION ----

['[', '84', ']', 'McCray', ',', 'A.', 'T.', ',', '&', 'Razi', ',', '.']

 TOTAL LEMMATIZE WORDS ==> 12

************************************************************************************************************************

691 --> (1994). 


 ---- TOKENS ----

 ['(', '1994', ')', '.'] 

 TOTAL TOKENS ==> 4

 ---- POST ----

 [('(', '('), ('1994', 'CD'), (')', ')'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['(', '1994', ')', '.']

 TOTAL FILTERED TOKENS ==>  4

 ---- POST FOR FILTERED TOKENS ----

 [('(', '('), ('1994', 'CD'), (')', ')'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['( 1994', '1994 )', ') .'] 

 TOTAL BIGRAMS --> 3 



 ---- TRI-GRAMS ---- 

 ['( 1994 )', '1994 ) .'] 

 TOTAL TRIGRAMS --> 2 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['(', '1994', ')', '.']

 TOTAL PORTER STEM WORDS ==> 4



 ---- SNOWBALL STEMMING ----

['(', '1994', ')', '.']

 TOTAL SNOWBALL STEM WORDS ==> 4



 ---- LEMMATIZATION ----

['(', '1994', ')', '.']

 TOTAL LEMMATIZE WORDS ==> 4

************************************************************************************************************************

692 --> The UMLS Knowledge Source server. 


 ---- TOKENS ----

 ['The', 'UMLS', 'Knowledge', 'Source', 'server', '.'] 

 TOTAL TOKENS ==> 6

 ---- POST ----

 [('The', 'DT'), ('UMLS', 'NNP'), ('Knowledge', 'NNP'), ('Source', 'NNP'), ('server', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['UMLS', 'Knowledge', 'Source', 'server', '.']

 TOTAL FILTERED TOKENS ==>  5

 ---- POST FOR FILTERED TOKENS ----

 [('UMLS', 'NNP'), ('Knowledge', 'NNP'), ('Source', 'NNP'), ('server', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['UMLS Knowledge', 'Knowledge Source', 'Source server', 'server .'] 

 TOTAL BIGRAMS --> 4 



 ---- TRI-GRAMS ---- 

 ['UMLS Knowledge Source', 'Knowledge Source server', 'Source server .'] 

 TOTAL TRIGRAMS --> 3 



 ---- NOUN PHRASES ---- 

 ['server'] 

 TOTAL NOUN PHRASES --> 1 



 ---- NER ----

 
 ORGANIZATION ---> ['UMLS']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['uml', 'knowledg', 'sourc', 'server', '.']

 TOTAL PORTER STEM WORDS ==> 5



 ---- SNOWBALL STEMMING ----

['uml', 'knowledg', 'sourc', 'server', '.']

 TOTAL SNOWBALL STEM WORDS ==> 5



 ---- LEMMATIZATION ----

['UMLS', 'Knowledge', 'Source', 'server', '.']

 TOTAL LEMMATIZE WORDS ==> 5

************************************************************************************************************************

693 --> Medinfo. 


 ---- TOKENS ----

 ['Medinfo', '.'] 

 TOTAL TOKENS ==> 2

 ---- POST ----

 [('Medinfo', 'NNP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Medinfo', '.']

 TOTAL FILTERED TOKENS ==>  2

 ---- POST FOR FILTERED TOKENS ----

 [('Medinfo', 'NNP'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Medinfo .'] 

 TOTAL BIGRAMS --> 1 



 ---- TRI-GRAMS ---- 

 [] 

 TOTAL TRIGRAMS --> 0 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> ['Medinfo']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['medinfo', '.']

 TOTAL PORTER STEM WORDS ==> 2



 ---- SNOWBALL STEMMING ----

['medinfo', '.']

 TOTAL SNOWBALL STEM WORDS ==> 2



 ---- LEMMATIZATION ----

['Medinfo', '.']

 TOTAL LEMMATIZE WORDS ==> 2

************************************************************************************************************************

694 --> MEDINFO, 8, 144-147. 


 ---- TOKENS ----

 ['MEDINFO', ',', '8', ',', '144-147', '.'] 

 TOTAL TOKENS ==> 6

 ---- POST ----

 [('MEDINFO', 'NNP'), (',', ','), ('8', 'CD'), (',', ','), ('144-147', 'CD'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['MEDINFO', ',', '8', ',', '144-147', '.']

 TOTAL FILTERED TOKENS ==>  6

 ---- POST FOR FILTERED TOKENS ----

 [('MEDINFO', 'NNP'), (',', ','), ('8', 'CD'), (',', ','), ('144-147', 'CD'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['MEDINFO ,', ', 8', '8 ,', ', 144-147', '144-147 .'] 

 TOTAL BIGRAMS --> 5 



 ---- TRI-GRAMS ---- 

 ['MEDINFO , 8', ', 8 ,', '8 , 144-147', ', 144-147 .'] 

 TOTAL TRIGRAMS --> 4 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> ['MEDINFO']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['medinfo', ',', '8', ',', '144-147', '.']

 TOTAL PORTER STEM WORDS ==> 6



 ---- SNOWBALL STEMMING ----

['medinfo', ',', '8', ',', '144-147', '.']

 TOTAL SNOWBALL STEM WORDS ==> 6



 ---- LEMMATIZATION ----

['MEDINFO', ',', '8', ',', '144-147', '.']

 TOTAL LEMMATIZE WORDS ==> 6

************************************************************************************************************************

695 --> [85] Scherrer, J. R., Revillard, C., Borst, F., Berthoud, M., & Lovis, C. (1994). 


 ---- TOKENS ----

 ['[', '85', ']', 'Scherrer', ',', 'J.', 'R.', ',', 'Revillard', ',', 'C.', ',', 'Borst', ',', 'F.', ',', 'Berthoud', ',', 'M.', ',', '&', 'Lovis', ',', 'C.', '(', '1994', ')', '.'] 

 TOTAL TOKENS ==> 28

 ---- POST ----

 [('[', 'RB'), ('85', 'CD'), (']', 'JJ'), ('Scherrer', 'NNP'), (',', ','), ('J.', 'NNP'), ('R.', 'NNP'), (',', ','), ('Revillard', 'NNP'), (',', ','), ('C.', 'NNP'), (',', ','), ('Borst', 'NNP'), (',', ','), ('F.', 'NNP'), (',', ','), ('Berthoud', 'NNP'), (',', ','), ('M.', 'NNP'), (',', ','), ('&', 'CC'), ('Lovis', 'NNP'), (',', ','), ('C.', 'NNP'), ('(', '('), ('1994', 'CD'), (')', ')'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['[', '85', ']', 'Scherrer', ',', 'J.', 'R.', ',', 'Revillard', ',', 'C.', ',', 'Borst', ',', 'F.', ',', 'Berthoud', ',', 'M.', ',', '&', 'Lovis', ',', 'C.', '(', '1994', ')', '.']

 TOTAL FILTERED TOKENS ==>  28

 ---- POST FOR FILTERED TOKENS ----

 [('[', 'RB'), ('85', 'CD'), (']', 'JJ'), ('Scherrer', 'NNP'), (',', ','), ('J.', 'NNP'), ('R.', 'NNP'), (',', ','), ('Revillard', 'NNP'), (',', ','), ('C.', 'NNP'), (',', ','), ('Borst', 'NNP'), (',', ','), ('F.', 'NNP'), (',', ','), ('Berthoud', 'NNP'), (',', ','), ('M.', 'NNP'), (',', ','), ('&', 'CC'), ('Lovis', 'NNP'), (',', ','), ('C.', 'NNP'), ('(', '('), ('1994', 'CD'), (')', ')'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['[ 85', '85 ]', '] Scherrer', 'Scherrer ,', ', J.', 'J. R.', 'R. ,', ', Revillard', 'Revillard ,', ', C.', 'C. ,', ', Borst', 'Borst ,', ', F.', 'F. ,', ', Berthoud', 'Berthoud ,', ', M.', 'M. ,', ', &', '& Lovis', 'Lovis ,', ', C.', 'C. (', '( 1994', '1994 )', ') .'] 

 TOTAL BIGRAMS --> 27 



 ---- TRI-GRAMS ---- 

 ['[ 85 ]', '85 ] Scherrer', '] Scherrer ,', 'Scherrer , J.', ', J. R.', 'J. R. ,', 'R. , Revillard', ', Revillard ,', 'Revillard , C.', ', C. ,', 'C. , Borst', ', Borst ,', 'Borst , F.', ', F. ,', 'F. , Berthoud', ', Berthoud ,', 'Berthoud , M.', ', M. ,', 'M. , &', ', & Lovis', '& Lovis ,', 'Lovis , C.', ', C. (', 'C. ( 1994', '( 1994 )', '1994 ) .'] 

 TOTAL TRIGRAMS --> 26 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> ['J. R.', 'Revillard', 'Borst', 'Berthoud', 'Lovis']
 TOTAL PERSON ENTITY --> 5 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['[', '85', ']', 'scherrer', ',', 'j.', 'r.', ',', 'revillard', ',', 'c.', ',', 'borst', ',', 'f.', ',', 'berthoud', ',', 'm.', ',', '&', 'lovi', ',', 'c.', '(', '1994', ')', '.']

 TOTAL PORTER STEM WORDS ==> 28



 ---- SNOWBALL STEMMING ----

['[', '85', ']', 'scherrer', ',', 'j.', 'r.', ',', 'revillard', ',', 'c.', ',', 'borst', ',', 'f.', ',', 'berthoud', ',', 'm.', ',', '&', 'lovi', ',', 'c.', '(', '1994', ')', '.']

 TOTAL SNOWBALL STEM WORDS ==> 28



 ---- LEMMATIZATION ----

['[', '85', ']', 'Scherrer', ',', 'J.', 'R.', ',', 'Revillard', ',', 'C.', ',', 'Borst', ',', 'F.', ',', 'Berthoud', ',', 'M.', ',', '&', 'Lovis', ',', 'C.', '(', '1994', ')', '.']

 TOTAL LEMMATIZE WORDS ==> 28

************************************************************************************************************************

696 --> Medical office  automation integrated into the distributed architecture of a hospital information  system. 


 ---- TOKENS ----

 ['Medical', 'office', 'automation', 'integrated', 'into', 'the', 'distributed', 'architecture', 'of', 'a', 'hospital', 'information', 'system', '.'] 

 TOTAL TOKENS ==> 14

 ---- POST ----

 [('Medical', 'JJ'), ('office', 'NN'), ('automation', 'NN'), ('integrated', 'VBN'), ('into', 'IN'), ('the', 'DT'), ('distributed', 'JJ'), ('architecture', 'NN'), ('of', 'IN'), ('a', 'DT'), ('hospital', 'NN'), ('information', 'NN'), ('system', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Medical', 'office', 'automation', 'integrated', 'distributed', 'architecture', 'hospital', 'information', 'system', '.']

 TOTAL FILTERED TOKENS ==>  10

 ---- POST FOR FILTERED TOKENS ----

 [('Medical', 'JJ'), ('office', 'NN'), ('automation', 'NN'), ('integrated', 'VBN'), ('distributed', 'JJ'), ('architecture', 'NN'), ('hospital', 'NN'), ('information', 'NN'), ('system', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Medical office', 'office automation', 'automation integrated', 'integrated distributed', 'distributed architecture', 'architecture hospital', 'hospital information', 'information system', 'system .'] 

 TOTAL BIGRAMS --> 9 



 ---- TRI-GRAMS ---- 

 ['Medical office automation', 'office automation integrated', 'automation integrated distributed', 'integrated distributed architecture', 'distributed architecture hospital', 'architecture hospital information', 'hospital information system', 'information system .'] 

 TOTAL TRIGRAMS --> 8 



 ---- NOUN PHRASES ---- 

 ['Medical office', 'automation', 'distributed architecture', 'hospital', 'information', 'system'] 

 TOTAL NOUN PHRASES --> 6 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> ['Medical']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['medic', 'offic', 'autom', 'integr', 'distribut', 'architectur', 'hospit', 'inform', 'system', '.']

 TOTAL PORTER STEM WORDS ==> 10



 ---- SNOWBALL STEMMING ----

['medic', 'offic', 'autom', 'integr', 'distribut', 'architectur', 'hospit', 'inform', 'system', '.']

 TOTAL SNOWBALL STEM WORDS ==> 10



 ---- LEMMATIZATION ----

['Medical', 'office', 'automation', 'integrated', 'distributed', 'architecture', 'hospital', 'information', 'system', '.']

 TOTAL LEMMATIZE WORDS ==> 10

************************************************************************************************************************

697 --> Methods of information in medicine, 33(2), 174-179. 


 ---- TOKENS ----

 ['Methods', 'of', 'information', 'in', 'medicine', ',', '33', '(', '2', ')', ',', '174-179', '.'] 

 TOTAL TOKENS ==> 13

 ---- POST ----

 [('Methods', 'NNS'), ('of', 'IN'), ('information', 'NN'), ('in', 'IN'), ('medicine', 'NN'), (',', ','), ('33', 'CD'), ('(', '('), ('2', 'CD'), (')', ')'), (',', ','), ('174-179', 'JJ'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Methods', 'information', 'medicine', ',', '33', '(', '2', ')', ',', '174-179', '.']

 TOTAL FILTERED TOKENS ==>  11

 ---- POST FOR FILTERED TOKENS ----

 [('Methods', 'NNS'), ('information', 'NN'), ('medicine', 'NN'), (',', ','), ('33', 'CD'), ('(', '('), ('2', 'CD'), (')', ')'), (',', ','), ('174-179', 'JJ'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Methods information', 'information medicine', 'medicine ,', ', 33', '33 (', '( 2', '2 )', ') ,', ', 174-179', '174-179 .'] 

 TOTAL BIGRAMS --> 10 



 ---- TRI-GRAMS ---- 

 ['Methods information medicine', 'information medicine ,', 'medicine , 33', ', 33 (', '33 ( 2', '( 2 )', '2 ) ,', ') , 174-179', ', 174-179 .'] 

 TOTAL TRIGRAMS --> 9 



 ---- NOUN PHRASES ---- 

 ['information', 'medicine'] 

 TOTAL NOUN PHRASES --> 2 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> ['Methods']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['method', 'inform', 'medicin', ',', '33', '(', '2', ')', ',', '174-179', '.']

 TOTAL PORTER STEM WORDS ==> 11



 ---- SNOWBALL STEMMING ----

['method', 'inform', 'medicin', ',', '33', '(', '2', ')', ',', '174-179', '.']

 TOTAL SNOWBALL STEM WORDS ==> 11



 ---- LEMMATIZATION ----

['Methods', 'information', 'medicine', ',', '33', '(', '2', ')', ',', '174-179', '.']

 TOTAL LEMMATIZE WORDS ==> 11

************************************************************************************************************************

698 --> [86] Baud, R. H., Rassinoux, A. M., & Scherrer, J. R. (1992). 


 ---- TOKENS ----

 ['[', '86', ']', 'Baud', ',', 'R.', 'H.', ',', 'Rassinoux', ',', 'A.', 'M.', ',', '&', 'Scherrer', ',', 'J.', 'R.', '(', '1992', ')', '.'] 

 TOTAL TOKENS ==> 22

 ---- POST ----

 [('[', 'RB'), ('86', 'CD'), (']', 'JJ'), ('Baud', 'NNP'), (',', ','), ('R.', 'NNP'), ('H.', 'NNP'), (',', ','), ('Rassinoux', 'NNP'), (',', ','), ('A.', 'NNP'), ('M.', 'NNP'), (',', ','), ('&', 'CC'), ('Scherrer', 'NNP'), (',', ','), ('J.', 'NNP'), ('R.', 'NNP'), ('(', '('), ('1992', 'CD'), (')', ')'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['[', '86', ']', 'Baud', ',', 'R.', 'H.', ',', 'Rassinoux', ',', 'A.', 'M.', ',', '&', 'Scherrer', ',', 'J.', 'R.', '(', '1992', ')', '.']

 TOTAL FILTERED TOKENS ==>  22

 ---- POST FOR FILTERED TOKENS ----

 [('[', 'RB'), ('86', 'CD'), (']', 'JJ'), ('Baud', 'NNP'), (',', ','), ('R.', 'NNP'), ('H.', 'NNP'), (',', ','), ('Rassinoux', 'NNP'), (',', ','), ('A.', 'NNP'), ('M.', 'NNP'), (',', ','), ('&', 'CC'), ('Scherrer', 'NNP'), (',', ','), ('J.', 'NNP'), ('R.', 'NNP'), ('(', '('), ('1992', 'CD'), (')', ')'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['[ 86', '86 ]', '] Baud', 'Baud ,', ', R.', 'R. H.', 'H. ,', ', Rassinoux', 'Rassinoux ,', ', A.', 'A. M.', 'M. ,', ', &', '& Scherrer', 'Scherrer ,', ', J.', 'J. R.', 'R. (', '( 1992', '1992 )', ') .'] 

 TOTAL BIGRAMS --> 21 



 ---- TRI-GRAMS ---- 

 ['[ 86 ]', '86 ] Baud', '] Baud ,', 'Baud , R.', ', R. H.', 'R. H. ,', 'H. , Rassinoux', ', Rassinoux ,', 'Rassinoux , A.', ', A. M.', 'A. M. ,', 'M. , &', ', & Scherrer', '& Scherrer ,', 'Scherrer , J.', ', J. R.', 'J. R. (', 'R. ( 1992', '( 1992 )', '1992 ) .'] 

 TOTAL TRIGRAMS --> 20 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> ['Rassinoux', 'Scherrer', 'J. R.']
 TOTAL PERSON ENTITY --> 3 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['[', '86', ']', 'baud', ',', 'r.', 'h.', ',', 'rassinoux', ',', 'a.', 'm.', ',', '&', 'scherrer', ',', 'j.', 'r.', '(', '1992', ')', '.']

 TOTAL PORTER STEM WORDS ==> 22



 ---- SNOWBALL STEMMING ----

['[', '86', ']', 'baud', ',', 'r.', 'h.', ',', 'rassinoux', ',', 'a.', 'm.', ',', '&', 'scherrer', ',', 'j.', 'r.', '(', '1992', ')', '.']

 TOTAL SNOWBALL STEM WORDS ==> 22



 ---- LEMMATIZATION ----

['[', '86', ']', 'Baud', ',', 'R.', 'H.', ',', 'Rassinoux', ',', 'A.', 'M.', ',', '&', 'Scherrer', ',', 'J.', 'R.', '(', '1992', ')', '.']

 TOTAL LEMMATIZE WORDS ==> 22

************************************************************************************************************************

699 --> Natural language processing  and semantical representation of medical texts. 


 ---- TOKENS ----

 ['Natural', 'language', 'processing', 'and', 'semantical', 'representation', 'of', 'medical', 'texts', '.'] 

 TOTAL TOKENS ==> 10

 ---- POST ----

 [('Natural', 'JJ'), ('language', 'NN'), ('processing', 'NN'), ('and', 'CC'), ('semantical', 'JJ'), ('representation', 'NN'), ('of', 'IN'), ('medical', 'JJ'), ('texts', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Natural', 'language', 'processing', 'semantical', 'representation', 'medical', 'texts', '.']

 TOTAL FILTERED TOKENS ==>  8

 ---- POST FOR FILTERED TOKENS ----

 [('Natural', 'JJ'), ('language', 'NN'), ('processing', 'VBG'), ('semantical', 'JJ'), ('representation', 'NN'), ('medical', 'JJ'), ('texts', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Natural language', 'language processing', 'processing semantical', 'semantical representation', 'representation medical', 'medical texts', 'texts .'] 

 TOTAL BIGRAMS --> 7 



 ---- TRI-GRAMS ---- 

 ['Natural language processing', 'language processing semantical', 'processing semantical representation', 'semantical representation medical', 'representation medical texts', 'medical texts .'] 

 TOTAL TRIGRAMS --> 6 



 ---- NOUN PHRASES ---- 

 ['Natural language', 'semantical representation', 'medical texts'] 

 TOTAL NOUN PHRASES --> 3 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['natur', 'languag', 'process', 'semant', 'represent', 'medic', 'text', '.']

 TOTAL PORTER STEM WORDS ==> 8



 ---- SNOWBALL STEMMING ----

['natur', 'languag', 'process', 'semant', 'represent', 'medic', 'text', '.']

 TOTAL SNOWBALL STEM WORDS ==> 8



 ---- LEMMATIZATION ----

['Natural', 'language', 'processing', 'semantical', 'representation', 'medical', 'text', '.']

 TOTAL LEMMATIZE WORDS ==> 8

************************************************************************************************************************

700 --> Methods of information in medicine, 31(2),  117-125. 


 ---- TOKENS ----

 ['Methods', 'of', 'information', 'in', 'medicine', ',', '31', '(', '2', ')', ',', '117-125', '.'] 

 TOTAL TOKENS ==> 13

 ---- POST ----

 [('Methods', 'NNS'), ('of', 'IN'), ('information', 'NN'), ('in', 'IN'), ('medicine', 'NN'), (',', ','), ('31', 'CD'), ('(', '('), ('2', 'CD'), (')', ')'), (',', ','), ('117-125', 'JJ'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Methods', 'information', 'medicine', ',', '31', '(', '2', ')', ',', '117-125', '.']

 TOTAL FILTERED TOKENS ==>  11

 ---- POST FOR FILTERED TOKENS ----

 [('Methods', 'NNS'), ('information', 'NN'), ('medicine', 'NN'), (',', ','), ('31', 'CD'), ('(', '('), ('2', 'CD'), (')', ')'), (',', ','), ('117-125', 'JJ'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Methods information', 'information medicine', 'medicine ,', ', 31', '31 (', '( 2', '2 )', ') ,', ', 117-125', '117-125 .'] 

 TOTAL BIGRAMS --> 10 



 ---- TRI-GRAMS ---- 

 ['Methods information medicine', 'information medicine ,', 'medicine , 31', ', 31 (', '31 ( 2', '( 2 )', '2 ) ,', ') , 117-125', ', 117-125 .'] 

 TOTAL TRIGRAMS --> 9 



 ---- NOUN PHRASES ---- 

 ['information', 'medicine'] 

 TOTAL NOUN PHRASES --> 2 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> ['Methods']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['method', 'inform', 'medicin', ',', '31', '(', '2', ')', ',', '117-125', '.']

 TOTAL PORTER STEM WORDS ==> 11



 ---- SNOWBALL STEMMING ----

['method', 'inform', 'medicin', ',', '31', '(', '2', ')', ',', '117-125', '.']

 TOTAL SNOWBALL STEM WORDS ==> 11



 ---- LEMMATIZATION ----

['Methods', 'information', 'medicine', ',', '31', '(', '2', ')', ',', '117-125', '.']

 TOTAL LEMMATIZE WORDS ==> 11

************************************************************************************************************************

701 --> [87] Lyman, M., Sager, N., Chi, E. C., Tick, L. J., Nhan, N. T., Su, Y., ... & Scherrer, J. 


 ---- TOKENS ----

 ['[', '87', ']', 'Lyman', ',', 'M.', ',', 'Sager', ',', 'N.', ',', 'Chi', ',', 'E.', 'C.', ',', 'Tick', ',', 'L.', 'J.', ',', 'Nhan', ',', 'N.', 'T.', ',', 'Su', ',', 'Y.', ',', '...', '&', 'Scherrer', ',', 'J', '.'] 

 TOTAL TOKENS ==> 36

 ---- POST ----

 [('[', 'RB'), ('87', 'CD'), (']', 'JJ'), ('Lyman', 'NNP'), (',', ','), ('M.', 'NNP'), (',', ','), ('Sager', 'NNP'), (',', ','), ('N.', 'NNP'), (',', ','), ('Chi', 'NNP'), (',', ','), ('E.', 'NNP'), ('C.', 'NNP'), (',', ','), ('Tick', 'NNP'), (',', ','), ('L.', 'NNP'), ('J.', 'NNP'), (',', ','), ('Nhan', 'NNP'), (',', ','), ('N.', 'NNP'), ('T.', 'NNP'), (',', ','), ('Su', 'NNP'), (',', ','), ('Y.', 'NNP'), (',', ','), ('...', ':'), ('&', 'CC'), ('Scherrer', 'NNP'), (',', ','), ('J', 'NNP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['[', '87', ']', 'Lyman', ',', 'M.', ',', 'Sager', ',', 'N.', ',', 'Chi', ',', 'E.', 'C.', ',', 'Tick', ',', 'L.', 'J.', ',', 'Nhan', ',', 'N.', 'T.', ',', 'Su', ',', 'Y.', ',', '...', '&', 'Scherrer', ',', 'J', '.']

 TOTAL FILTERED TOKENS ==>  36

 ---- POST FOR FILTERED TOKENS ----

 [('[', 'RB'), ('87', 'CD'), (']', 'JJ'), ('Lyman', 'NNP'), (',', ','), ('M.', 'NNP'), (',', ','), ('Sager', 'NNP'), (',', ','), ('N.', 'NNP'), (',', ','), ('Chi', 'NNP'), (',', ','), ('E.', 'NNP'), ('C.', 'NNP'), (',', ','), ('Tick', 'NNP'), (',', ','), ('L.', 'NNP'), ('J.', 'NNP'), (',', ','), ('Nhan', 'NNP'), (',', ','), ('N.', 'NNP'), ('T.', 'NNP'), (',', ','), ('Su', 'NNP'), (',', ','), ('Y.', 'NNP'), (',', ','), ('...', ':'), ('&', 'CC'), ('Scherrer', 'NNP'), (',', ','), ('J', 'NNP'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['[ 87', '87 ]', '] Lyman', 'Lyman ,', ', M.', 'M. ,', ', Sager', 'Sager ,', ', N.', 'N. ,', ', Chi', 'Chi ,', ', E.', 'E. C.', 'C. ,', ', Tick', 'Tick ,', ', L.', 'L. J.', 'J. ,', ', Nhan', 'Nhan ,', ', N.', 'N. T.', 'T. ,', ', Su', 'Su ,', ', Y.', 'Y. ,', ', ...', '... &', '& Scherrer', 'Scherrer ,', ', J', 'J .'] 

 TOTAL BIGRAMS --> 35 



 ---- TRI-GRAMS ---- 

 ['[ 87 ]', '87 ] Lyman', '] Lyman ,', 'Lyman , M.', ', M. ,', 'M. , Sager', ', Sager ,', 'Sager , N.', ', N. ,', 'N. , Chi', ', Chi ,', 'Chi , E.', ', E. C.', 'E. C. ,', 'C. , Tick', ', Tick ,', 'Tick , L.', ', L. J.', 'L. J. ,', 'J. , Nhan', ', Nhan ,', 'Nhan , N.', ', N. T.', 'N. T. ,', 'T. , Su', ', Su ,', 'Su , Y.', ', Y. ,', 'Y. , ...', ', ... &', '... & Scherrer', '& Scherrer ,', 'Scherrer , J', ', J .'] 

 TOTAL TRIGRAMS --> 34 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> ['Lyman', 'Sager', 'Tick', 'Nhan', 'Scherrer']
 TOTAL PERSON ENTITY --> 5 


 GPE ---> ['Chi', 'Su']
 TOTAL GPE ENTITY --> 2 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['[', '87', ']', 'lyman', ',', 'm.', ',', 'sager', ',', 'n.', ',', 'chi', ',', 'e.', 'c.', ',', 'tick', ',', 'l.', 'j.', ',', 'nhan', ',', 'n.', 't.', ',', 'su', ',', 'y.', ',', '...', '&', 'scherrer', ',', 'j', '.']

 TOTAL PORTER STEM WORDS ==> 36



 ---- SNOWBALL STEMMING ----

['[', '87', ']', 'lyman', ',', 'm.', ',', 'sager', ',', 'n.', ',', 'chi', ',', 'e.', 'c.', ',', 'tick', ',', 'l.', 'j.', ',', 'nhan', ',', 'n.', 't.', ',', 'su', ',', 'y.', ',', '...', '&', 'scherrer', ',', 'j', '.']

 TOTAL SNOWBALL STEM WORDS ==> 36



 ---- LEMMATIZATION ----

['[', '87', ']', 'Lyman', ',', 'M.', ',', 'Sager', ',', 'N.', ',', 'Chi', ',', 'E.', 'C.', ',', 'Tick', ',', 'L.', 'J.', ',', 'Nhan', ',', 'N.', 'T.', ',', 'Su', ',', 'Y.', ',', '...', '&', 'Scherrer', ',', 'J', '.']

 TOTAL LEMMATIZE WORDS ==> 36

************************************************************************************************************************

702 --> (1989, November). 


 ---- TOKENS ----

 ['(', '1989', ',', 'November', ')', '.'] 

 TOTAL TOKENS ==> 6

 ---- POST ----

 [('(', '('), ('1989', 'CD'), (',', ','), ('November', 'NNP'), (')', ')'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['(', '1989', ',', 'November', ')', '.']

 TOTAL FILTERED TOKENS ==>  6

 ---- POST FOR FILTERED TOKENS ----

 [('(', '('), ('1989', 'CD'), (',', ','), ('November', 'NNP'), (')', ')'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['( 1989', '1989 ,', ', November', 'November )', ') .'] 

 TOTAL BIGRAMS --> 5 



 ---- TRI-GRAMS ---- 

 ['( 1989 ,', '1989 , November', ', November )', 'November ) .'] 

 TOTAL TRIGRAMS --> 4 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['(', '1989', ',', 'novemb', ')', '.']

 TOTAL PORTER STEM WORDS ==> 6



 ---- SNOWBALL STEMMING ----

['(', '1989', ',', 'novemb', ')', '.']

 TOTAL SNOWBALL STEM WORDS ==> 6



 ---- LEMMATIZATION ----

['(', '1989', ',', 'November', ')', '.']

 TOTAL LEMMATIZE WORDS ==> 6

************************************************************************************************************************

703 --> Medical Language Processing for Knowledge Representation and  Retrievals. 


 ---- TOKENS ----

 ['Medical', 'Language', 'Processing', 'for', 'Knowledge', 'Representation', 'and', 'Retrievals', '.'] 

 TOTAL TOKENS ==> 9

 ---- POST ----

 [('Medical', 'JJ'), ('Language', 'NNP'), ('Processing', 'NNP'), ('for', 'IN'), ('Knowledge', 'NNP'), ('Representation', 'NNP'), ('and', 'CC'), ('Retrievals', 'NNP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Medical', 'Language', 'Processing', 'Knowledge', 'Representation', 'Retrievals', '.']

 TOTAL FILTERED TOKENS ==>  7

 ---- POST FOR FILTERED TOKENS ----

 [('Medical', 'JJ'), ('Language', 'NNP'), ('Processing', 'NNP'), ('Knowledge', 'NNP'), ('Representation', 'NNP'), ('Retrievals', 'NNP'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Medical Language', 'Language Processing', 'Processing Knowledge', 'Knowledge Representation', 'Representation Retrievals', 'Retrievals .'] 

 TOTAL BIGRAMS --> 6 



 ---- TRI-GRAMS ---- 

 ['Medical Language Processing', 'Language Processing Knowledge', 'Processing Knowledge Representation', 'Knowledge Representation Retrievals', 'Representation Retrievals .'] 

 TOTAL TRIGRAMS --> 5 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> ['Language']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> ['Medical']
 TOTAL PERSON ENTITY --> 1 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['medic', 'languag', 'process', 'knowledg', 'represent', 'retriev', '.']

 TOTAL PORTER STEM WORDS ==> 7



 ---- SNOWBALL STEMMING ----

['medic', 'languag', 'process', 'knowledg', 'represent', 'retriev', '.']

 TOTAL SNOWBALL STEM WORDS ==> 7



 ---- LEMMATIZATION ----

['Medical', 'Language', 'Processing', 'Knowledge', 'Representation', 'Retrievals', '.']

 TOTAL LEMMATIZE WORDS ==> 7

************************************************************************************************************************

704 --> In Proceedings. 


 ---- TOKENS ----

 ['In', 'Proceedings', '.'] 

 TOTAL TOKENS ==> 3

 ---- POST ----

 [('In', 'IN'), ('Proceedings', 'NNP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Proceedings', '.']

 TOTAL FILTERED TOKENS ==>  2

 ---- POST FOR FILTERED TOKENS ----

 [('Proceedings', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Proceedings .'] 

 TOTAL BIGRAMS --> 1 



 ---- TRI-GRAMS ---- 

 [] 

 TOTAL TRIGRAMS --> 0 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['proceed', '.']

 TOTAL PORTER STEM WORDS ==> 2



 ---- SNOWBALL STEMMING ----

['proceed', '.']

 TOTAL SNOWBALL STEM WORDS ==> 2



 ---- LEMMATIZATION ----

['Proceedings', '.']

 TOTAL LEMMATIZE WORDS ==> 2

************************************************************************************************************************

705 --> Symposium on Computer Applications in Medical Care (pp. 


 ---- TOKENS ----

 ['Symposium', 'on', 'Computer', 'Applications', 'in', 'Medical', 'Care', '(', 'pp', '.'] 

 TOTAL TOKENS ==> 10

 ---- POST ----

 [('Symposium', 'NN'), ('on', 'IN'), ('Computer', 'NNP'), ('Applications', 'NNPS'), ('in', 'IN'), ('Medical', 'NNP'), ('Care', 'NNP'), ('(', '('), ('pp', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Symposium', 'Computer', 'Applications', 'Medical', 'Care', '(', 'pp', '.']

 TOTAL FILTERED TOKENS ==>  8

 ---- POST FOR FILTERED TOKENS ----

 [('Symposium', 'NNP'), ('Computer', 'NNP'), ('Applications', 'NNP'), ('Medical', 'NNP'), ('Care', 'NNP'), ('(', '('), ('pp', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Symposium Computer', 'Computer Applications', 'Applications Medical', 'Medical Care', 'Care (', '( pp', 'pp .'] 

 TOTAL BIGRAMS --> 7 



 ---- TRI-GRAMS ---- 

 ['Symposium Computer Applications', 'Computer Applications Medical', 'Applications Medical Care', 'Medical Care (', 'Care ( pp', '( pp .'] 

 TOTAL TRIGRAMS --> 6 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 []

 ---- PORTER STEMMING ----

['symposium', 'comput', 'applic', 'medic', 'care', '(', 'pp', '.']

 TOTAL PORTER STEM WORDS ==> 8



 ---- SNOWBALL STEMMING ----

['symposium', 'comput', 'applic', 'medic', 'care', '(', 'pp', '.']

 TOTAL SNOWBALL STEM WORDS ==> 8



 ---- LEMMATIZATION ----

['Symposium', 'Computer', 'Applications', 'Medical', 'Care', '(', 'pp', '.']

 TOTAL LEMMATIZE WORDS ==> 8

************************************************************************************************************************

706 --> 548- 553). 


 ---- TOKENS ----

 ['548-', '553', ')', '.'] 

 TOTAL TOKENS ==> 4

 ---- POST ----

 [('548-', 'JJ'), ('553', 'CD'), (')', ')'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['548-', '553', ')', '.']

 TOTAL FILTERED TOKENS ==>  4

 ---- POST FOR FILTERED TOKENS ----

 [('548-', 'JJ'), ('553', 'CD'), (')', ')'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['548- 553', '553 )', ') .'] 

 TOTAL BIGRAMS --> 3 



 ---- TRI-GRAMS ---- 

 ['548- 553 )', '553 ) .'] 

 TOTAL TRIGRAMS --> 2 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 []

 ---- PORTER STEMMING ----

['548-', '553', ')', '.']

 TOTAL PORTER STEM WORDS ==> 4



 ---- SNOWBALL STEMMING ----

['548-', '553', ')', '.']

 TOTAL SNOWBALL STEM WORDS ==> 4



 ---- LEMMATIZATION ----

['548-', '553', ')', '.']

 TOTAL LEMMATIZE WORDS ==> 4

************************************************************************************************************************

707 --> American Medical Informatics Association. 


 ---- TOKENS ----

 ['American', 'Medical', 'Informatics', 'Association', '.'] 

 TOTAL TOKENS ==> 5

 ---- POST ----

 [('American', 'NNP'), ('Medical', 'NNP'), ('Informatics', 'NNP'), ('Association', 'NNP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['American', 'Medical', 'Informatics', 'Association', '.']

 TOTAL FILTERED TOKENS ==>  5

 ---- POST FOR FILTERED TOKENS ----

 [('American', 'NNP'), ('Medical', 'NNP'), ('Informatics', 'NNP'), ('Association', 'NNP'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['American Medical', 'Medical Informatics', 'Informatics Association', 'Association .'] 

 TOTAL BIGRAMS --> 4 



 ---- TRI-GRAMS ---- 

 ['American Medical Informatics', 'Medical Informatics Association', 'Informatics Association .'] 

 TOTAL TRIGRAMS --> 3 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> ['Medical Informatics Association']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> ['American']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['american', 'medic', 'informat', 'associ', '.']

 TOTAL PORTER STEM WORDS ==> 5



 ---- SNOWBALL STEMMING ----

['american', 'medic', 'informat', 'associ', '.']

 TOTAL SNOWBALL STEM WORDS ==> 5



 ---- LEMMATIZATION ----

['American', 'Medical', 'Informatics', 'Association', '.']

 TOTAL LEMMATIZE WORDS ==> 5

************************************************************************************************************************

708 --> [88] Nhàn, N. T., Sager, N., Lyman, M., Tick, L. J., Borst, F., & Su, Y. 


 ---- TOKENS ----

 ['[', '88', ']', 'Nhàn', ',', 'N.', 'T.', ',', 'Sager', ',', 'N.', ',', 'Lyman', ',', 'M.', ',', 'Tick', ',', 'L.', 'J.', ',', 'Borst', ',', 'F.', ',', '&', 'Su', ',', 'Y', '.'] 

 TOTAL TOKENS ==> 30

 ---- POST ----

 [('[', 'RB'), ('88', 'CD'), (']', 'JJ'), ('Nhàn', 'NNP'), (',', ','), ('N.', 'NNP'), ('T.', 'NNP'), (',', ','), ('Sager', 'NNP'), (',', ','), ('N.', 'NNP'), (',', ','), ('Lyman', 'NNP'), (',', ','), ('M.', 'NNP'), (',', ','), ('Tick', 'NNP'), (',', ','), ('L.', 'NNP'), ('J.', 'NNP'), (',', ','), ('Borst', 'NNP'), (',', ','), ('F.', 'NNP'), (',', ','), ('&', 'CC'), ('Su', 'NNP'), (',', ','), ('Y', 'NNP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['[', '88', ']', 'Nhàn', ',', 'N.', 'T.', ',', 'Sager', ',', 'N.', ',', 'Lyman', ',', 'M.', ',', 'Tick', ',', 'L.', 'J.', ',', 'Borst', ',', 'F.', ',', '&', 'Su', ',', '.']

 TOTAL FILTERED TOKENS ==>  29

 ---- POST FOR FILTERED TOKENS ----

 [('[', 'RB'), ('88', 'CD'), (']', 'JJ'), ('Nhàn', 'NNP'), (',', ','), ('N.', 'NNP'), ('T.', 'NNP'), (',', ','), ('Sager', 'NNP'), (',', ','), ('N.', 'NNP'), (',', ','), ('Lyman', 'NNP'), (',', ','), ('M.', 'NNP'), (',', ','), ('Tick', 'NNP'), (',', ','), ('L.', 'NNP'), ('J.', 'NNP'), (',', ','), ('Borst', 'NNP'), (',', ','), ('F.', 'NNP'), (',', ','), ('&', 'CC'), ('Su', 'NNP'), (',', ','), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['[ 88', '88 ]', '] Nhàn', 'Nhàn ,', ', N.', 'N. T.', 'T. ,', ', Sager', 'Sager ,', ', N.', 'N. ,', ', Lyman', 'Lyman ,', ', M.', 'M. ,', ', Tick', 'Tick ,', ', L.', 'L. J.', 'J. ,', ', Borst', 'Borst ,', ', F.', 'F. ,', ', &', '& Su', 'Su ,', ', .'] 

 TOTAL BIGRAMS --> 28 



 ---- TRI-GRAMS ---- 

 ['[ 88 ]', '88 ] Nhàn', '] Nhàn ,', 'Nhàn , N.', ', N. T.', 'N. T. ,', 'T. , Sager', ', Sager ,', 'Sager , N.', ', N. ,', 'N. , Lyman', ', Lyman ,', 'Lyman , M.', ', M. ,', 'M. , Tick', ', Tick ,', 'Tick , L.', ', L. J.', 'L. J. ,', 'J. , Borst', ', Borst ,', 'Borst , F.', ', F. ,', 'F. , &', ', & Su', '& Su ,', 'Su , .'] 

 TOTAL TRIGRAMS --> 27 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> ['Sager', 'Lyman', 'Tick', 'Borst']
 TOTAL PERSON ENTITY --> 4 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['[', '88', ']', 'nhàn', ',', 'n.', 't.', ',', 'sager', ',', 'n.', ',', 'lyman', ',', 'm.', ',', 'tick', ',', 'l.', 'j.', ',', 'borst', ',', 'f.', ',', '&', 'su', ',', '.']

 TOTAL PORTER STEM WORDS ==> 29



 ---- SNOWBALL STEMMING ----

['[', '88', ']', 'nhàn', ',', 'n.', 't.', ',', 'sager', ',', 'n.', ',', 'lyman', ',', 'm.', ',', 'tick', ',', 'l.', 'j.', ',', 'borst', ',', 'f.', ',', '&', 'su', ',', '.']

 TOTAL SNOWBALL STEM WORDS ==> 29



 ---- LEMMATIZATION ----

['[', '88', ']', 'Nhàn', ',', 'N.', 'T.', ',', 'Sager', ',', 'N.', ',', 'Lyman', ',', 'M.', ',', 'Tick', ',', 'L.', 'J.', ',', 'Borst', ',', 'F.', ',', '&', 'Su', ',', '.']

 TOTAL LEMMATIZE WORDS ==> 29

************************************************************************************************************************

709 --> (1989, November). 


 ---- TOKENS ----

 ['(', '1989', ',', 'November', ')', '.'] 

 TOTAL TOKENS ==> 6

 ---- POST ----

 [('(', '('), ('1989', 'CD'), (',', ','), ('November', 'NNP'), (')', ')'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['(', '1989', ',', 'November', ')', '.']

 TOTAL FILTERED TOKENS ==>  6

 ---- POST FOR FILTERED TOKENS ----

 [('(', '('), ('1989', 'CD'), (',', ','), ('November', 'NNP'), (')', ')'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['( 1989', '1989 ,', ', November', 'November )', ') .'] 

 TOTAL BIGRAMS --> 5 



 ---- TRI-GRAMS ---- 

 ['( 1989 ,', '1989 , November', ', November )', 'November ) .'] 

 TOTAL TRIGRAMS --> 4 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['(', '1989', ',', 'novemb', ')', '.']

 TOTAL PORTER STEM WORDS ==> 6



 ---- SNOWBALL STEMMING ----

['(', '1989', ',', 'novemb', ')', '.']

 TOTAL SNOWBALL STEM WORDS ==> 6



 ---- LEMMATIZATION ----

['(', '1989', ',', 'November', ')', '.']

 TOTAL LEMMATIZE WORDS ==> 6

************************************************************************************************************************

710 --> A  Medical Language Processor for Two Indo-European Languages. 


 ---- TOKENS ----

 ['A', 'Medical', 'Language', 'Processor', 'for', 'Two', 'Indo-European', 'Languages', '.'] 

 TOTAL TOKENS ==> 9

 ---- POST ----

 [('A', 'DT'), ('Medical', 'JJ'), ('Language', 'NN'), ('Processor', 'NNP'), ('for', 'IN'), ('Two', 'CD'), ('Indo-European', 'JJ'), ('Languages', 'NNS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Medical', 'Language', 'Processor', 'Two', 'Indo-European', 'Languages', '.']

 TOTAL FILTERED TOKENS ==>  7

 ---- POST FOR FILTERED TOKENS ----

 [('Medical', 'JJ'), ('Language', 'NNP'), ('Processor', 'NNP'), ('Two', 'CD'), ('Indo-European', 'JJ'), ('Languages', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Medical Language', 'Language Processor', 'Processor Two', 'Two Indo-European', 'Indo-European Languages', 'Languages .'] 

 TOTAL BIGRAMS --> 6 



 ---- TRI-GRAMS ---- 

 ['Medical Language Processor', 'Language Processor Two', 'Processor Two Indo-European', 'Two Indo-European Languages', 'Indo-European Languages .'] 

 TOTAL TRIGRAMS --> 5 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> ['Language']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> ['Medical']
 TOTAL PERSON ENTITY --> 1 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['medic', 'languag', 'processor', 'two', 'indo-european', 'languag', '.']

 TOTAL PORTER STEM WORDS ==> 7



 ---- SNOWBALL STEMMING ----

['medic', 'languag', 'processor', 'two', 'indo-european', 'languag', '.']

 TOTAL SNOWBALL STEM WORDS ==> 7



 ---- LEMMATIZATION ----

['Medical', 'Language', 'Processor', 'Two', 'Indo-European', 'Languages', '.']

 TOTAL LEMMATIZE WORDS ==> 7

************************************************************************************************************************

711 --> In Proceedings. 


 ---- TOKENS ----

 ['In', 'Proceedings', '.'] 

 TOTAL TOKENS ==> 3

 ---- POST ----

 [('In', 'IN'), ('Proceedings', 'NNP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Proceedings', '.']

 TOTAL FILTERED TOKENS ==>  2

 ---- POST FOR FILTERED TOKENS ----

 [('Proceedings', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Proceedings .'] 

 TOTAL BIGRAMS --> 1 



 ---- TRI-GRAMS ---- 

 [] 

 TOTAL TRIGRAMS --> 0 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['proceed', '.']

 TOTAL PORTER STEM WORDS ==> 2



 ---- SNOWBALL STEMMING ----

['proceed', '.']

 TOTAL SNOWBALL STEM WORDS ==> 2



 ---- LEMMATIZATION ----

['Proceedings', '.']

 TOTAL LEMMATIZE WORDS ==> 2

************************************************************************************************************************

712 --> Symposium on Computer Applications in Medical Care (pp. 


 ---- TOKENS ----

 ['Symposium', 'on', 'Computer', 'Applications', 'in', 'Medical', 'Care', '(', 'pp', '.'] 

 TOTAL TOKENS ==> 10

 ---- POST ----

 [('Symposium', 'NN'), ('on', 'IN'), ('Computer', 'NNP'), ('Applications', 'NNPS'), ('in', 'IN'), ('Medical', 'NNP'), ('Care', 'NNP'), ('(', '('), ('pp', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Symposium', 'Computer', 'Applications', 'Medical', 'Care', '(', 'pp', '.']

 TOTAL FILTERED TOKENS ==>  8

 ---- POST FOR FILTERED TOKENS ----

 [('Symposium', 'NNP'), ('Computer', 'NNP'), ('Applications', 'NNP'), ('Medical', 'NNP'), ('Care', 'NNP'), ('(', '('), ('pp', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Symposium Computer', 'Computer Applications', 'Applications Medical', 'Medical Care', 'Care (', '( pp', 'pp .'] 

 TOTAL BIGRAMS --> 7 



 ---- TRI-GRAMS ---- 

 ['Symposium Computer Applications', 'Computer Applications Medical', 'Applications Medical Care', 'Medical Care (', 'Care ( pp', '( pp .'] 

 TOTAL TRIGRAMS --> 6 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 []

 ---- PORTER STEMMING ----

['symposium', 'comput', 'applic', 'medic', 'care', '(', 'pp', '.']

 TOTAL PORTER STEM WORDS ==> 8



 ---- SNOWBALL STEMMING ----

['symposium', 'comput', 'applic', 'medic', 'care', '(', 'pp', '.']

 TOTAL SNOWBALL STEM WORDS ==> 8



 ---- LEMMATIZATION ----

['Symposium', 'Computer', 'Applications', 'Medical', 'Care', '(', 'pp', '.']

 TOTAL LEMMATIZE WORDS ==> 8

************************************************************************************************************************

713 --> 554-558). 


 ---- TOKENS ----

 ['554-558', ')', '.'] 

 TOTAL TOKENS ==> 3

 ---- POST ----

 [('554-558', 'JJ'), (')', ')'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['554-558', ')', '.']

 TOTAL FILTERED TOKENS ==>  3

 ---- POST FOR FILTERED TOKENS ----

 [('554-558', 'JJ'), (')', ')'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['554-558 )', ') .'] 

 TOTAL BIGRAMS --> 2 



 ---- TRI-GRAMS ---- 

 ['554-558 ) .'] 

 TOTAL TRIGRAMS --> 1 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 []

 ---- PORTER STEMMING ----

['554-558', ')', '.']

 TOTAL PORTER STEM WORDS ==> 3



 ---- SNOWBALL STEMMING ----

['554-558', ')', '.']

 TOTAL SNOWBALL STEM WORDS ==> 3



 ---- LEMMATIZATION ----

['554-558', ')', '.']

 TOTAL LEMMATIZE WORDS ==> 3

************************************************************************************************************************

714 --> American Medical  Informatics Association. 


 ---- TOKENS ----

 ['American', 'Medical', 'Informatics', 'Association', '.'] 

 TOTAL TOKENS ==> 5

 ---- POST ----

 [('American', 'NNP'), ('Medical', 'NNP'), ('Informatics', 'NNP'), ('Association', 'NNP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['American', 'Medical', 'Informatics', 'Association', '.']

 TOTAL FILTERED TOKENS ==>  5

 ---- POST FOR FILTERED TOKENS ----

 [('American', 'NNP'), ('Medical', 'NNP'), ('Informatics', 'NNP'), ('Association', 'NNP'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['American Medical', 'Medical Informatics', 'Informatics Association', 'Association .'] 

 TOTAL BIGRAMS --> 4 



 ---- TRI-GRAMS ---- 

 ['American Medical Informatics', 'Medical Informatics Association', 'Informatics Association .'] 

 TOTAL TRIGRAMS --> 3 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> ['Medical Informatics Association']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> ['American']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['american', 'medic', 'informat', 'associ', '.']

 TOTAL PORTER STEM WORDS ==> 5



 ---- SNOWBALL STEMMING ----

['american', 'medic', 'informat', 'associ', '.']

 TOTAL SNOWBALL STEM WORDS ==> 5



 ---- LEMMATIZATION ----

['American', 'Medical', 'Informatics', 'Association', '.']

 TOTAL LEMMATIZE WORDS ==> 5

************************************************************************************************************************

715 --> [89] Sager, N., Lyman, M., Tick, L. J., Borst, F., Nhan, N. T., Revillard, C., ... & Scherrer, J.  R. (1989). 


 ---- TOKENS ----

 ['[', '89', ']', 'Sager', ',', 'N.', ',', 'Lyman', ',', 'M.', ',', 'Tick', ',', 'L.', 'J.', ',', 'Borst', ',', 'F.', ',', 'Nhan', ',', 'N.', 'T.', ',', 'Revillard', ',', 'C.', ',', '...', '&', 'Scherrer', ',', 'J.', 'R.', '(', '1989', ')', '.'] 

 TOTAL TOKENS ==> 39

 ---- POST ----

 [('[', 'RB'), ('89', 'CD'), (']', 'JJ'), ('Sager', 'NNP'), (',', ','), ('N.', 'NNP'), (',', ','), ('Lyman', 'NNP'), (',', ','), ('M.', 'NNP'), (',', ','), ('Tick', 'NNP'), (',', ','), ('L.', 'NNP'), ('J.', 'NNP'), (',', ','), ('Borst', 'NNP'), (',', ','), ('F.', 'NNP'), (',', ','), ('Nhan', 'NNP'), (',', ','), ('N.', 'NNP'), ('T.', 'NNP'), (',', ','), ('Revillard', 'NNP'), (',', ','), ('C.', 'NNP'), (',', ','), ('...', ':'), ('&', 'CC'), ('Scherrer', 'NNP'), (',', ','), ('J.', 'NNP'), ('R.', 'NNP'), ('(', '('), ('1989', 'CD'), (')', ')'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['[', '89', ']', 'Sager', ',', 'N.', ',', 'Lyman', ',', 'M.', ',', 'Tick', ',', 'L.', 'J.', ',', 'Borst', ',', 'F.', ',', 'Nhan', ',', 'N.', 'T.', ',', 'Revillard', ',', 'C.', ',', '...', '&', 'Scherrer', ',', 'J.', 'R.', '(', '1989', ')', '.']

 TOTAL FILTERED TOKENS ==>  39

 ---- POST FOR FILTERED TOKENS ----

 [('[', 'RB'), ('89', 'CD'), (']', 'JJ'), ('Sager', 'NNP'), (',', ','), ('N.', 'NNP'), (',', ','), ('Lyman', 'NNP'), (',', ','), ('M.', 'NNP'), (',', ','), ('Tick', 'NNP'), (',', ','), ('L.', 'NNP'), ('J.', 'NNP'), (',', ','), ('Borst', 'NNP'), (',', ','), ('F.', 'NNP'), (',', ','), ('Nhan', 'NNP'), (',', ','), ('N.', 'NNP'), ('T.', 'NNP'), (',', ','), ('Revillard', 'NNP'), (',', ','), ('C.', 'NNP'), (',', ','), ('...', ':'), ('&', 'CC'), ('Scherrer', 'NNP'), (',', ','), ('J.', 'NNP'), ('R.', 'NNP'), ('(', '('), ('1989', 'CD'), (')', ')'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['[ 89', '89 ]', '] Sager', 'Sager ,', ', N.', 'N. ,', ', Lyman', 'Lyman ,', ', M.', 'M. ,', ', Tick', 'Tick ,', ', L.', 'L. J.', 'J. ,', ', Borst', 'Borst ,', ', F.', 'F. ,', ', Nhan', 'Nhan ,', ', N.', 'N. T.', 'T. ,', ', Revillard', 'Revillard ,', ', C.', 'C. ,', ', ...', '... &', '& Scherrer', 'Scherrer ,', ', J.', 'J. R.', 'R. (', '( 1989', '1989 )', ') .'] 

 TOTAL BIGRAMS --> 38 



 ---- TRI-GRAMS ---- 

 ['[ 89 ]', '89 ] Sager', '] Sager ,', 'Sager , N.', ', N. ,', 'N. , Lyman', ', Lyman ,', 'Lyman , M.', ', M. ,', 'M. , Tick', ', Tick ,', 'Tick , L.', ', L. J.', 'L. J. ,', 'J. , Borst', ', Borst ,', 'Borst , F.', ', F. ,', 'F. , Nhan', ', Nhan ,', 'Nhan , N.', ', N. T.', 'N. T. ,', 'T. , Revillard', ', Revillard ,', 'Revillard , C.', ', C. ,', 'C. , ...', ', ... &', '... & Scherrer', '& Scherrer ,', 'Scherrer , J.', ', J. R.', 'J. R. (', 'R. ( 1989', '( 1989 )', '1989 ) .'] 

 TOTAL TRIGRAMS --> 37 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> ['Lyman', 'Tick', 'Borst', 'Nhan', 'Revillard', 'Scherrer', 'J. R.']
 TOTAL PERSON ENTITY --> 7 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['[', '89', ']', 'sager', ',', 'n.', ',', 'lyman', ',', 'm.', ',', 'tick', ',', 'l.', 'j.', ',', 'borst', ',', 'f.', ',', 'nhan', ',', 'n.', 't.', ',', 'revillard', ',', 'c.', ',', '...', '&', 'scherrer', ',', 'j.', 'r.', '(', '1989', ')', '.']

 TOTAL PORTER STEM WORDS ==> 39



 ---- SNOWBALL STEMMING ----

['[', '89', ']', 'sager', ',', 'n.', ',', 'lyman', ',', 'm.', ',', 'tick', ',', 'l.', 'j.', ',', 'borst', ',', 'f.', ',', 'nhan', ',', 'n.', 't.', ',', 'revillard', ',', 'c.', ',', '...', '&', 'scherrer', ',', 'j.', 'r.', '(', '1989', ')', '.']

 TOTAL SNOWBALL STEM WORDS ==> 39



 ---- LEMMATIZATION ----

['[', '89', ']', 'Sager', ',', 'N.', ',', 'Lyman', ',', 'M.', ',', 'Tick', ',', 'L.', 'J.', ',', 'Borst', ',', 'F.', ',', 'Nhan', ',', 'N.', 'T.', ',', 'Revillard', ',', 'C.', ',', '...', '&', 'Scherrer', ',', 'J.', 'R.', '(', '1989', ')', '.']

 TOTAL LEMMATIZE WORDS ==> 39

************************************************************************************************************************

716 --> Adapting a medical language processor from English to French. 


 ---- TOKENS ----

 ['Adapting', 'a', 'medical', 'language', 'processor', 'from', 'English', 'to', 'French', '.'] 

 TOTAL TOKENS ==> 10

 ---- POST ----

 [('Adapting', 'VBG'), ('a', 'DT'), ('medical', 'JJ'), ('language', 'NN'), ('processor', 'NN'), ('from', 'IN'), ('English', 'NNP'), ('to', 'TO'), ('French', 'NNP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Adapting', 'medical', 'language', 'processor', 'English', 'French', '.']

 TOTAL FILTERED TOKENS ==>  7

 ---- POST FOR FILTERED TOKENS ----

 [('Adapting', 'NNP'), ('medical', 'JJ'), ('language', 'NN'), ('processor', 'NN'), ('English', 'NNP'), ('French', 'NNP'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Adapting medical', 'medical language', 'language processor', 'processor English', 'English French', 'French .'] 

 TOTAL BIGRAMS --> 6 



 ---- TRI-GRAMS ---- 

 ['Adapting medical language', 'medical language processor', 'language processor English', 'processor English French', 'English French .'] 

 TOTAL TRIGRAMS --> 5 



 ---- NOUN PHRASES ---- 

 ['medical language', 'processor'] 

 TOTAL NOUN PHRASES --> 2 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> ['English French']
 TOTAL PERSON ENTITY --> 1 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['adapt', 'medic', 'languag', 'processor', 'english', 'french', '.']

 TOTAL PORTER STEM WORDS ==> 7



 ---- SNOWBALL STEMMING ----

['adapt', 'medic', 'languag', 'processor', 'english', 'french', '.']

 TOTAL SNOWBALL STEM WORDS ==> 7



 ---- LEMMATIZATION ----

['Adapting', 'medical', 'language', 'processor', 'English', 'French', '.']

 TOTAL LEMMATIZE WORDS ==> 7

************************************************************************************************************************

717 --> Medinfo, 89, 795- 799. 


 ---- TOKENS ----

 ['Medinfo', ',', '89', ',', '795-', '799', '.'] 

 TOTAL TOKENS ==> 7

 ---- POST ----

 [('Medinfo', 'NNP'), (',', ','), ('89', 'CD'), (',', ','), ('795-', 'JJ'), ('799', 'CD'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Medinfo', ',', '89', ',', '795-', '799', '.']

 TOTAL FILTERED TOKENS ==>  7

 ---- POST FOR FILTERED TOKENS ----

 [('Medinfo', 'NNP'), (',', ','), ('89', 'CD'), (',', ','), ('795-', 'JJ'), ('799', 'CD'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Medinfo ,', ', 89', '89 ,', ', 795-', '795- 799', '799 .'] 

 TOTAL BIGRAMS --> 6 



 ---- TRI-GRAMS ---- 

 ['Medinfo , 89', ', 89 ,', '89 , 795-', ', 795- 799', '795- 799 .'] 

 TOTAL TRIGRAMS --> 5 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> ['Medinfo']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['medinfo', ',', '89', ',', '795-', '799', '.']

 TOTAL PORTER STEM WORDS ==> 7



 ---- SNOWBALL STEMMING ----

['medinfo', ',', '89', ',', '795-', '799', '.']

 TOTAL SNOWBALL STEM WORDS ==> 7



 ---- LEMMATIZATION ----

['Medinfo', ',', '89', ',', '795-', '799', '.']

 TOTAL LEMMATIZE WORDS ==> 7

************************************************************************************************************************

718 --> [90] Borst, F., Sager, N., Nhàn, N. T., Su, Y., Lyman, M., Tick, L. J., ... & Scherrer, J. R.  (1989). 


 ---- TOKENS ----

 ['[', '90', ']', 'Borst', ',', 'F.', ',', 'Sager', ',', 'N.', ',', 'Nhàn', ',', 'N.', 'T.', ',', 'Su', ',', 'Y.', ',', 'Lyman', ',', 'M.', ',', 'Tick', ',', 'L.', 'J.', ',', '...', '&', 'Scherrer', ',', 'J.', 'R.', '(', '1989', ')', '.'] 

 TOTAL TOKENS ==> 39

 ---- POST ----

 [('[', 'RB'), ('90', 'CD'), (']', 'JJ'), ('Borst', 'NNP'), (',', ','), ('F.', 'NNP'), (',', ','), ('Sager', 'NNP'), (',', ','), ('N.', 'NNP'), (',', ','), ('Nhàn', 'NNP'), (',', ','), ('N.', 'NNP'), ('T.', 'NNP'), (',', ','), ('Su', 'NNP'), (',', ','), ('Y.', 'NNP'), (',', ','), ('Lyman', 'NNP'), (',', ','), ('M.', 'NNP'), (',', ','), ('Tick', 'NNP'), (',', ','), ('L.', 'NNP'), ('J.', 'NNP'), (',', ','), ('...', ':'), ('&', 'CC'), ('Scherrer', 'NNP'), (',', ','), ('J.', 'NNP'), ('R.', 'NNP'), ('(', '('), ('1989', 'CD'), (')', ')'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['[', '90', ']', 'Borst', ',', 'F.', ',', 'Sager', ',', 'N.', ',', 'Nhàn', ',', 'N.', 'T.', ',', 'Su', ',', 'Y.', ',', 'Lyman', ',', 'M.', ',', 'Tick', ',', 'L.', 'J.', ',', '...', '&', 'Scherrer', ',', 'J.', 'R.', '(', '1989', ')', '.']

 TOTAL FILTERED TOKENS ==>  39

 ---- POST FOR FILTERED TOKENS ----

 [('[', 'RB'), ('90', 'CD'), (']', 'JJ'), ('Borst', 'NNP'), (',', ','), ('F.', 'NNP'), (',', ','), ('Sager', 'NNP'), (',', ','), ('N.', 'NNP'), (',', ','), ('Nhàn', 'NNP'), (',', ','), ('N.', 'NNP'), ('T.', 'NNP'), (',', ','), ('Su', 'NNP'), (',', ','), ('Y.', 'NNP'), (',', ','), ('Lyman', 'NNP'), (',', ','), ('M.', 'NNP'), (',', ','), ('Tick', 'NNP'), (',', ','), ('L.', 'NNP'), ('J.', 'NNP'), (',', ','), ('...', ':'), ('&', 'CC'), ('Scherrer', 'NNP'), (',', ','), ('J.', 'NNP'), ('R.', 'NNP'), ('(', '('), ('1989', 'CD'), (')', ')'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['[ 90', '90 ]', '] Borst', 'Borst ,', ', F.', 'F. ,', ', Sager', 'Sager ,', ', N.', 'N. ,', ', Nhàn', 'Nhàn ,', ', N.', 'N. T.', 'T. ,', ', Su', 'Su ,', ', Y.', 'Y. ,', ', Lyman', 'Lyman ,', ', M.', 'M. ,', ', Tick', 'Tick ,', ', L.', 'L. J.', 'J. ,', ', ...', '... &', '& Scherrer', 'Scherrer ,', ', J.', 'J. R.', 'R. (', '( 1989', '1989 )', ') .'] 

 TOTAL BIGRAMS --> 38 



 ---- TRI-GRAMS ---- 

 ['[ 90 ]', '90 ] Borst', '] Borst ,', 'Borst , F.', ', F. ,', 'F. , Sager', ', Sager ,', 'Sager , N.', ', N. ,', 'N. , Nhàn', ', Nhàn ,', 'Nhàn , N.', ', N. T.', 'N. T. ,', 'T. , Su', ', Su ,', 'Su , Y.', ', Y. ,', 'Y. , Lyman', ', Lyman ,', 'Lyman , M.', ', M. ,', 'M. , Tick', ', Tick ,', 'Tick , L.', ', L. J.', 'L. J. ,', 'J. , ...', ', ... &', '... & Scherrer', '& Scherrer ,', 'Scherrer , J.', ', J. R.', 'J. R. (', 'R. ( 1989', '( 1989 )', '1989 ) .'] 

 TOTAL TRIGRAMS --> 37 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> ['Sager', 'Nhàn', 'Lyman', 'Tick', 'Scherrer', 'J. R.']
 TOTAL PERSON ENTITY --> 6 


 GPE ---> ['Su']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['[', '90', ']', 'borst', ',', 'f.', ',', 'sager', ',', 'n.', ',', 'nhàn', ',', 'n.', 't.', ',', 'su', ',', 'y.', ',', 'lyman', ',', 'm.', ',', 'tick', ',', 'l.', 'j.', ',', '...', '&', 'scherrer', ',', 'j.', 'r.', '(', '1989', ')', '.']

 TOTAL PORTER STEM WORDS ==> 39



 ---- SNOWBALL STEMMING ----

['[', '90', ']', 'borst', ',', 'f.', ',', 'sager', ',', 'n.', ',', 'nhàn', ',', 'n.', 't.', ',', 'su', ',', 'y.', ',', 'lyman', ',', 'm.', ',', 'tick', ',', 'l.', 'j.', ',', '...', '&', 'scherrer', ',', 'j.', 'r.', '(', '1989', ')', '.']

 TOTAL SNOWBALL STEM WORDS ==> 39



 ---- LEMMATIZATION ----

['[', '90', ']', 'Borst', ',', 'F.', ',', 'Sager', ',', 'N.', ',', 'Nhàn', ',', 'N.', 'T.', ',', 'Su', ',', 'Y.', ',', 'Lyman', ',', 'M.', ',', 'Tick', ',', 'L.', 'J.', ',', '...', '&', 'Scherrer', ',', 'J.', 'R.', '(', '1989', ')', '.']

 TOTAL LEMMATIZE WORDS ==> 39

************************************************************************************************************************

719 --> Analyse automatique de comptes rendus d'hospitalisation. 


 ---- TOKENS ----

 ['Analyse', 'automatique', 'de', 'comptes', 'rendus', "d'hospitalisation", '.'] 

 TOTAL TOKENS ==> 7

 ---- POST ----

 [('Analyse', 'NNP'), ('automatique', 'NN'), ('de', 'FW'), ('comptes', 'VBZ'), ('rendus', 'JJ'), ("d'hospitalisation", 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Analyse', 'automatique', 'de', 'comptes', 'rendus', "d'hospitalisation", '.']

 TOTAL FILTERED TOKENS ==>  7

 ---- POST FOR FILTERED TOKENS ----

 [('Analyse', 'NNP'), ('automatique', 'NN'), ('de', 'FW'), ('comptes', 'VBZ'), ('rendus', 'JJ'), ("d'hospitalisation", 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Analyse automatique', 'automatique de', 'de comptes', 'comptes rendus', "rendus d'hospitalisation", "d'hospitalisation ."] 

 TOTAL BIGRAMS --> 6 



 ---- TRI-GRAMS ---- 

 ['Analyse automatique de', 'automatique de comptes', 'de comptes rendus', "comptes rendus d'hospitalisation", "rendus d'hospitalisation ."] 

 TOTAL TRIGRAMS --> 5 



 ---- NOUN PHRASES ---- 

 ['automatique', "rendus d'hospitalisation"] 

 TOTAL NOUN PHRASES --> 2 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> ['Analyse']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['analys', 'automatiqu', 'de', 'compt', 'rendu', "d'hospitalis", '.']

 TOTAL PORTER STEM WORDS ==> 7



 ---- SNOWBALL STEMMING ----

['analys', 'automatiqu', 'de', 'compt', 'rendus', "d'hospitalis", '.']

 TOTAL SNOWBALL STEM WORDS ==> 7



 ---- LEMMATIZATION ----

['Analyse', 'automatique', 'de', 'comptes', 'rendus', "d'hospitalisation", '.']

 TOTAL LEMMATIZE WORDS ==> 7

************************************************************************************************************************

720 --> In Degoulet P, Stephan JC,  Venot A, Yvon PJ, rédacteurs. 


 ---- TOKENS ----

 ['In', 'Degoulet', 'P', ',', 'Stephan', 'JC', ',', 'Venot', 'A', ',', 'Yvon', 'PJ', ',', 'rédacteurs', '.'] 

 TOTAL TOKENS ==> 15

 ---- POST ----

 [('In', 'IN'), ('Degoulet', 'NNP'), ('P', 'NNP'), (',', ','), ('Stephan', 'NNP'), ('JC', 'NNP'), (',', ','), ('Venot', 'NNP'), ('A', 'NNP'), (',', ','), ('Yvon', 'NNP'), ('PJ', 'NNP'), (',', ','), ('rédacteurs', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Degoulet', 'P', ',', 'Stephan', 'JC', ',', 'Venot', ',', 'Yvon', 'PJ', ',', 'rédacteurs', '.']

 TOTAL FILTERED TOKENS ==>  13

 ---- POST FOR FILTERED TOKENS ----

 [('Degoulet', 'NNP'), ('P', 'NNP'), (',', ','), ('Stephan', 'NNP'), ('JC', 'NNP'), (',', ','), ('Venot', 'NNP'), (',', ','), ('Yvon', 'NNP'), ('PJ', 'NNP'), (',', ','), ('rédacteurs', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Degoulet P', 'P ,', ', Stephan', 'Stephan JC', 'JC ,', ', Venot', 'Venot ,', ', Yvon', 'Yvon PJ', 'PJ ,', ', rédacteurs', 'rédacteurs .'] 

 TOTAL BIGRAMS --> 12 



 ---- TRI-GRAMS ---- 

 ['Degoulet P ,', 'P , Stephan', ', Stephan JC', 'Stephan JC ,', 'JC , Venot', ', Venot ,', 'Venot , Yvon', ', Yvon PJ', 'Yvon PJ ,', 'PJ , rédacteurs', ', rédacteurs .'] 

 TOTAL TRIGRAMS --> 11 



 ---- NOUN PHRASES ---- 

 ['rédacteurs'] 

 TOTAL NOUN PHRASES --> 1 



 ---- NER ----

 
 ORGANIZATION ---> ['P']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> ['Degoulet', 'Stephan JC', 'Yvon PJ']
 TOTAL PERSON ENTITY --> 3 


 GPE ---> ['Venot']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['degoulet', 'p', ',', 'stephan', 'jc', ',', 'venot', ',', 'yvon', 'pj', ',', 'rédacteur', '.']

 TOTAL PORTER STEM WORDS ==> 13



 ---- SNOWBALL STEMMING ----

['degoulet', 'p', ',', 'stephan', 'jc', ',', 'venot', ',', 'yvon', 'pj', ',', 'rédacteur', '.']

 TOTAL SNOWBALL STEM WORDS ==> 13



 ---- LEMMATIZATION ----

['Degoulet', 'P', ',', 'Stephan', 'JC', ',', 'Venot', ',', 'Yvon', 'PJ', ',', 'rédacteurs', '.']

 TOTAL LEMMATIZE WORDS ==> 13

************************************************************************************************************************

721 --> Informatique et Santé, Informatique et Gestion des Unités de  Soins, Comptes Rendus du Colloque AIM-IF, Paris (pp. 


 ---- TOKENS ----

 ['Informatique', 'et', 'Santé', ',', 'Informatique', 'et', 'Gestion', 'des', 'Unités', 'de', 'Soins', ',', 'Comptes', 'Rendus', 'du', 'Colloque', 'AIM-IF', ',', 'Paris', '(', 'pp', '.'] 

 TOTAL TOKENS ==> 22

 ---- POST ----

 [('Informatique', 'NNP'), ('et', 'CC'), ('Santé', 'NNP'), (',', ','), ('Informatique', 'NNP'), ('et', 'CC'), ('Gestion', 'NNP'), ('des', 'FW'), ('Unités', 'NNP'), ('de', 'FW'), ('Soins', 'NNP'), (',', ','), ('Comptes', 'NNP'), ('Rendus', 'NNP'), ('du', 'NN'), ('Colloque', 'NNP'), ('AIM-IF', 'NNP'), (',', ','), ('Paris', 'NNP'), ('(', '('), ('pp', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Informatique', 'et', 'Santé', ',', 'Informatique', 'et', 'Gestion', 'des', 'Unités', 'de', 'Soins', ',', 'Comptes', 'Rendus', 'du', 'Colloque', 'AIM-IF', ',', 'Paris', '(', 'pp', '.']

 TOTAL FILTERED TOKENS ==>  22

 ---- POST FOR FILTERED TOKENS ----

 [('Informatique', 'NNP'), ('et', 'CC'), ('Santé', 'NNP'), (',', ','), ('Informatique', 'NNP'), ('et', 'CC'), ('Gestion', 'NNP'), ('des', 'FW'), ('Unités', 'NNP'), ('de', 'FW'), ('Soins', 'NNP'), (',', ','), ('Comptes', 'NNP'), ('Rendus', 'NNP'), ('du', 'NN'), ('Colloque', 'NNP'), ('AIM-IF', 'NNP'), (',', ','), ('Paris', 'NNP'), ('(', '('), ('pp', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Informatique et', 'et Santé', 'Santé ,', ', Informatique', 'Informatique et', 'et Gestion', 'Gestion des', 'des Unités', 'Unités de', 'de Soins', 'Soins ,', ', Comptes', 'Comptes Rendus', 'Rendus du', 'du Colloque', 'Colloque AIM-IF', 'AIM-IF ,', ', Paris', 'Paris (', '( pp', 'pp .'] 

 TOTAL BIGRAMS --> 21 



 ---- TRI-GRAMS ---- 

 ['Informatique et Santé', 'et Santé ,', 'Santé , Informatique', ', Informatique et', 'Informatique et Gestion', 'et Gestion des', 'Gestion des Unités', 'des Unités de', 'Unités de Soins', 'de Soins ,', 'Soins , Comptes', ', Comptes Rendus', 'Comptes Rendus du', 'Rendus du Colloque', 'du Colloque AIM-IF', 'Colloque AIM-IF ,', 'AIM-IF , Paris', ', Paris (', 'Paris ( pp', '( pp .'] 

 TOTAL TRIGRAMS --> 20 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 []

 ---- PORTER STEMMING ----

['informatiqu', 'et', 'santé', ',', 'informatiqu', 'et', 'gestion', 'de', 'unité', 'de', 'soin', ',', 'compt', 'rendu', 'du', 'colloqu', 'aim-if', ',', 'pari', '(', 'pp', '.']

 TOTAL PORTER STEM WORDS ==> 22



 ---- SNOWBALL STEMMING ----

['informatiqu', 'et', 'santé', ',', 'informatiqu', 'et', 'gestion', 'des', 'unité', 'de', 'soin', ',', 'compt', 'rendus', 'du', 'colloqu', 'aim-if', ',', 'pari', '(', 'pp', '.']

 TOTAL SNOWBALL STEM WORDS ==> 22



 ---- LEMMATIZATION ----

['Informatique', 'et', 'Santé', ',', 'Informatique', 'et', 'Gestion', 'de', 'Unités', 'de', 'Soins', ',', 'Comptes', 'Rendus', 'du', 'Colloque', 'AIM-IF', ',', 'Paris', '(', 'pp', '.']

 TOTAL LEMMATIZE WORDS ==> 22

************************************************************************************************************************

722 --> 246-56). 


 ---- TOKENS ----

 ['246-56', ')', '.'] 

 TOTAL TOKENS ==> 3

 ---- POST ----

 [('246-56', 'JJ'), (')', ')'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['246-56', ')', '.']

 TOTAL FILTERED TOKENS ==>  3

 ---- POST FOR FILTERED TOKENS ----

 [('246-56', 'JJ'), (')', ')'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['246-56 )', ') .'] 

 TOTAL BIGRAMS --> 2 



 ---- TRI-GRAMS ---- 

 ['246-56 ) .'] 

 TOTAL TRIGRAMS --> 1 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 []

 ---- PORTER STEMMING ----

['246-56', ')', '.']

 TOTAL PORTER STEM WORDS ==> 3



 ---- SNOWBALL STEMMING ----

['246-56', ')', '.']

 TOTAL SNOWBALL STEM WORDS ==> 3



 ---- LEMMATIZATION ----

['246-56', ')', '.']

 TOTAL LEMMATIZE WORDS ==> 3

************************************************************************************************************************

723 --> [5]  [91] Baud, R. H., Rassinoux, A. M., & Scherrer, J. R. (1991). 


 ---- TOKENS ----

 ['[', '5', ']', '[', '91', ']', 'Baud', ',', 'R.', 'H.', ',', 'Rassinoux', ',', 'A.', 'M.', ',', '&', 'Scherrer', ',', 'J.', 'R.', '(', '1991', ')', '.'] 

 TOTAL TOKENS ==> 25

 ---- POST ----

 [('[', 'RB'), ('5', 'CD'), (']', 'JJ'), ('[', '$'), ('91', 'CD'), (']', 'NNP'), ('Baud', 'NNP'), (',', ','), ('R.', 'NNP'), ('H.', 'NNP'), (',', ','), ('Rassinoux', 'NNP'), (',', ','), ('A.', 'NNP'), ('M.', 'NNP'), (',', ','), ('&', 'CC'), ('Scherrer', 'NNP'), (',', ','), ('J.', 'NNP'), ('R.', 'NNP'), ('(', '('), ('1991', 'CD'), (')', ')'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['[', '5', ']', '[', '91', ']', 'Baud', ',', 'R.', 'H.', ',', 'Rassinoux', ',', 'A.', 'M.', ',', '&', 'Scherrer', ',', 'J.', 'R.', '(', '1991', ')', '.']

 TOTAL FILTERED TOKENS ==>  25

 ---- POST FOR FILTERED TOKENS ----

 [('[', 'RB'), ('5', 'CD'), (']', 'JJ'), ('[', '$'), ('91', 'CD'), (']', 'NNP'), ('Baud', 'NNP'), (',', ','), ('R.', 'NNP'), ('H.', 'NNP'), (',', ','), ('Rassinoux', 'NNP'), (',', ','), ('A.', 'NNP'), ('M.', 'NNP'), (',', ','), ('&', 'CC'), ('Scherrer', 'NNP'), (',', ','), ('J.', 'NNP'), ('R.', 'NNP'), ('(', '('), ('1991', 'CD'), (')', ')'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['[ 5', '5 ]', '] [', '[ 91', '91 ]', '] Baud', 'Baud ,', ', R.', 'R. H.', 'H. ,', ', Rassinoux', 'Rassinoux ,', ', A.', 'A. M.', 'M. ,', ', &', '& Scherrer', 'Scherrer ,', ', J.', 'J. R.', 'R. (', '( 1991', '1991 )', ') .'] 

 TOTAL BIGRAMS --> 24 



 ---- TRI-GRAMS ---- 

 ['[ 5 ]', '5 ] [', '] [ 91', '[ 91 ]', '91 ] Baud', '] Baud ,', 'Baud , R.', ', R. H.', 'R. H. ,', 'H. , Rassinoux', ', Rassinoux ,', 'Rassinoux , A.', ', A. M.', 'A. M. ,', 'M. , &', ', & Scherrer', '& Scherrer ,', 'Scherrer , J.', ', J. R.', 'J. R. (', 'R. ( 1991', '( 1991 )', '1991 ) .'] 

 TOTAL TRIGRAMS --> 23 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> ['Rassinoux', 'Scherrer', 'J. R.']
 TOTAL PERSON ENTITY --> 3 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['[', '5', ']', '[', '91', ']', 'baud', ',', 'r.', 'h.', ',', 'rassinoux', ',', 'a.', 'm.', ',', '&', 'scherrer', ',', 'j.', 'r.', '(', '1991', ')', '.']

 TOTAL PORTER STEM WORDS ==> 25



 ---- SNOWBALL STEMMING ----

['[', '5', ']', '[', '91', ']', 'baud', ',', 'r.', 'h.', ',', 'rassinoux', ',', 'a.', 'm.', ',', '&', 'scherrer', ',', 'j.', 'r.', '(', '1991', ')', '.']

 TOTAL SNOWBALL STEM WORDS ==> 25



 ---- LEMMATIZATION ----

['[', '5', ']', '[', '91', ']', 'Baud', ',', 'R.', 'H.', ',', 'Rassinoux', ',', 'A.', 'M.', ',', '&', 'Scherrer', ',', 'J.', 'R.', '(', '1991', ')', '.']

 TOTAL LEMMATIZE WORDS ==> 25

************************************************************************************************************************

724 --> Knowledge representation of  discharge summaries. 


 ---- TOKENS ----

 ['Knowledge', 'representation', 'of', 'discharge', 'summaries', '.'] 

 TOTAL TOKENS ==> 6

 ---- POST ----

 [('Knowledge', 'NNP'), ('representation', 'NN'), ('of', 'IN'), ('discharge', 'NN'), ('summaries', 'NNS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Knowledge', 'representation', 'discharge', 'summaries', '.']

 TOTAL FILTERED TOKENS ==>  5

 ---- POST FOR FILTERED TOKENS ----

 [('Knowledge', 'NNP'), ('representation', 'NN'), ('discharge', 'NN'), ('summaries', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Knowledge representation', 'representation discharge', 'discharge summaries', 'summaries .'] 

 TOTAL BIGRAMS --> 4 



 ---- TRI-GRAMS ---- 

 ['Knowledge representation discharge', 'representation discharge summaries', 'discharge summaries .'] 

 TOTAL TRIGRAMS --> 3 



 ---- NOUN PHRASES ---- 

 ['representation', 'discharge'] 

 TOTAL NOUN PHRASES --> 2 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> ['Knowledge']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['knowledg', 'represent', 'discharg', 'summari', '.']

 TOTAL PORTER STEM WORDS ==> 5



 ---- SNOWBALL STEMMING ----

['knowledg', 'represent', 'discharg', 'summari', '.']

 TOTAL SNOWBALL STEM WORDS ==> 5



 ---- LEMMATIZATION ----

['Knowledge', 'representation', 'discharge', 'summary', '.']

 TOTAL LEMMATIZE WORDS ==> 5

************************************************************************************************************************

725 --> In AIME 91 (pp. 


 ---- TOKENS ----

 ['In', 'AIME', '91', '(', 'pp', '.'] 

 TOTAL TOKENS ==> 6

 ---- POST ----

 [('In', 'IN'), ('AIME', 'NNP'), ('91', 'CD'), ('(', '('), ('pp', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['AIME', '91', '(', 'pp', '.']

 TOTAL FILTERED TOKENS ==>  5

 ---- POST FOR FILTERED TOKENS ----

 [('AIME', '$'), ('91', 'CD'), ('(', '('), ('pp', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['AIME 91', '91 (', '( pp', 'pp .'] 

 TOTAL BIGRAMS --> 4 



 ---- TRI-GRAMS ---- 

 ['AIME 91 (', '91 ( pp', '( pp .'] 

 TOTAL TRIGRAMS --> 3 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 []

 ---- PORTER STEMMING ----

['aim', '91', '(', 'pp', '.']

 TOTAL PORTER STEM WORDS ==> 5



 ---- SNOWBALL STEMMING ----

['aim', '91', '(', 'pp', '.']

 TOTAL SNOWBALL STEM WORDS ==> 5



 ---- LEMMATIZATION ----

['AIME', '91', '(', 'pp', '.']

 TOTAL LEMMATIZE WORDS ==> 5

************************************************************************************************************************

726 --> 173-182). 


 ---- TOKENS ----

 ['173-182', ')', '.'] 

 TOTAL TOKENS ==> 3

 ---- POST ----

 [('173-182', 'JJ'), (')', ')'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['173-182', ')', '.']

 TOTAL FILTERED TOKENS ==>  3

 ---- POST FOR FILTERED TOKENS ----

 [('173-182', 'JJ'), (')', ')'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['173-182 )', ') .'] 

 TOTAL BIGRAMS --> 2 



 ---- TRI-GRAMS ---- 

 ['173-182 ) .'] 

 TOTAL TRIGRAMS --> 1 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 []

 ---- PORTER STEMMING ----

['173-182', ')', '.']

 TOTAL PORTER STEM WORDS ==> 3



 ---- SNOWBALL STEMMING ----

['173-182', ')', '.']

 TOTAL SNOWBALL STEM WORDS ==> 3



 ---- LEMMATIZATION ----

['173-182', ')', '.']

 TOTAL LEMMATIZE WORDS ==> 3

************************************************************************************************************************

727 --> Springer Berlin Heidelberg. 


 ---- TOKENS ----

 ['Springer', 'Berlin', 'Heidelberg', '.'] 

 TOTAL TOKENS ==> 4

 ---- POST ----

 [('Springer', 'NNP'), ('Berlin', 'NNP'), ('Heidelberg', 'NNP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Springer', 'Berlin', 'Heidelberg', '.']

 TOTAL FILTERED TOKENS ==>  4

 ---- POST FOR FILTERED TOKENS ----

 [('Springer', 'NNP'), ('Berlin', 'NNP'), ('Heidelberg', 'NNP'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Springer Berlin', 'Berlin Heidelberg', 'Heidelberg .'] 

 TOTAL BIGRAMS --> 3 



 ---- TRI-GRAMS ---- 

 ['Springer Berlin Heidelberg', 'Berlin Heidelberg .'] 

 TOTAL TRIGRAMS --> 2 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> ['Springer', 'Berlin Heidelberg']
 TOTAL PERSON ENTITY --> 2 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['springer', 'berlin', 'heidelberg', '.']

 TOTAL PORTER STEM WORDS ==> 4



 ---- SNOWBALL STEMMING ----

['springer', 'berlin', 'heidelberg', '.']

 TOTAL SNOWBALL STEM WORDS ==> 4



 ---- LEMMATIZATION ----

['Springer', 'Berlin', 'Heidelberg', '.']

 TOTAL LEMMATIZE WORDS ==> 4

************************************************************************************************************************

728 --> [92] Baud, R. H., Alpay, L., & Lovis, C. (1994). 


 ---- TOKENS ----

 ['[', '92', ']', 'Baud', ',', 'R.', 'H.', ',', 'Alpay', ',', 'L.', ',', '&', 'Lovis', ',', 'C.', '(', '1994', ')', '.'] 

 TOTAL TOKENS ==> 20

 ---- POST ----

 [('[', 'RB'), ('92', 'CD'), (']', 'JJ'), ('Baud', 'NNP'), (',', ','), ('R.', 'NNP'), ('H.', 'NNP'), (',', ','), ('Alpay', 'NNP'), (',', ','), ('L.', 'NNP'), (',', ','), ('&', 'CC'), ('Lovis', 'NNP'), (',', ','), ('C.', 'NNP'), ('(', '('), ('1994', 'CD'), (')', ')'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['[', '92', ']', 'Baud', ',', 'R.', 'H.', ',', 'Alpay', ',', 'L.', ',', '&', 'Lovis', ',', 'C.', '(', '1994', ')', '.']

 TOTAL FILTERED TOKENS ==>  20

 ---- POST FOR FILTERED TOKENS ----

 [('[', 'RB'), ('92', 'CD'), (']', 'JJ'), ('Baud', 'NNP'), (',', ','), ('R.', 'NNP'), ('H.', 'NNP'), (',', ','), ('Alpay', 'NNP'), (',', ','), ('L.', 'NNP'), (',', ','), ('&', 'CC'), ('Lovis', 'NNP'), (',', ','), ('C.', 'NNP'), ('(', '('), ('1994', 'CD'), (')', ')'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['[ 92', '92 ]', '] Baud', 'Baud ,', ', R.', 'R. H.', 'H. ,', ', Alpay', 'Alpay ,', ', L.', 'L. ,', ', &', '& Lovis', 'Lovis ,', ', C.', 'C. (', '( 1994', '1994 )', ') .'] 

 TOTAL BIGRAMS --> 19 



 ---- TRI-GRAMS ---- 

 ['[ 92 ]', '92 ] Baud', '] Baud ,', 'Baud , R.', ', R. H.', 'R. H. ,', 'H. , Alpay', ', Alpay ,', 'Alpay , L.', ', L. ,', 'L. , &', ', & Lovis', '& Lovis ,', 'Lovis , C.', ', C. (', 'C. ( 1994', '( 1994 )', '1994 ) .'] 

 TOTAL TRIGRAMS --> 18 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> ['Alpay', 'Lovis']
 TOTAL PERSON ENTITY --> 2 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['[', '92', ']', 'baud', ',', 'r.', 'h.', ',', 'alpay', ',', 'l.', ',', '&', 'lovi', ',', 'c.', '(', '1994', ')', '.']

 TOTAL PORTER STEM WORDS ==> 20



 ---- SNOWBALL STEMMING ----

['[', '92', ']', 'baud', ',', 'r.', 'h.', ',', 'alpay', ',', 'l.', ',', '&', 'lovi', ',', 'c.', '(', '1994', ')', '.']

 TOTAL SNOWBALL STEM WORDS ==> 20



 ---- LEMMATIZATION ----

['[', '92', ']', 'Baud', ',', 'R.', 'H.', ',', 'Alpay', ',', 'L.', ',', '&', 'Lovis', ',', 'C.', '(', '1994', ')', '.']

 TOTAL LEMMATIZE WORDS ==> 20

************************************************************************************************************************

729 --> Let’s Meet the Users with Natural Language  Understanding. 


 ---- TOKENS ----

 ['Let', '’', 's', 'Meet', 'the', 'Users', 'with', 'Natural', 'Language', 'Understanding', '.'] 

 TOTAL TOKENS ==> 11

 ---- POST ----

 [('Let', 'VB'), ('’', 'NNP'), ('s', 'VB'), ('Meet', 'NNP'), ('the', 'DT'), ('Users', 'NNP'), ('with', 'IN'), ('Natural', 'NNP'), ('Language', 'NNP'), ('Understanding', 'NNP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Let', '’', 'Meet', 'Users', 'Natural', 'Language', 'Understanding', '.']

 TOTAL FILTERED TOKENS ==>  8

 ---- POST FOR FILTERED TOKENS ----

 [('Let', 'VB'), ('’', 'NNP'), ('Meet', 'NNP'), ('Users', 'NNP'), ('Natural', 'NNP'), ('Language', 'NNP'), ('Understanding', 'NNP'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Let ’', '’ Meet', 'Meet Users', 'Users Natural', 'Natural Language', 'Language Understanding', 'Understanding .'] 

 TOTAL BIGRAMS --> 7 



 ---- TRI-GRAMS ---- 

 ['Let ’ Meet', '’ Meet Users', 'Meet Users Natural', 'Users Natural Language', 'Natural Language Understanding', 'Language Understanding .'] 

 TOTAL TRIGRAMS --> 6 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['let', '’', 'meet', 'user', 'natur', 'languag', 'understand', '.']

 TOTAL PORTER STEM WORDS ==> 8



 ---- SNOWBALL STEMMING ----

['let', '’', 'meet', 'user', 'natur', 'languag', 'understand', '.']

 TOTAL SNOWBALL STEM WORDS ==> 8



 ---- LEMMATIZATION ----

['Let', '’', 'Meet', 'Users', 'Natural', 'Language', 'Understanding', '.']

 TOTAL LEMMATIZE WORDS ==> 8

************************************************************************************************************************

730 --> Knowledge and Decisions in Health Telematics: The Next Decade, 12, 103. 


 ---- TOKENS ----

 ['Knowledge', 'and', 'Decisions', 'in', 'Health', 'Telematics', ':', 'The', 'Next', 'Decade', ',', '12', ',', '103', '.'] 

 TOTAL TOKENS ==> 15

 ---- POST ----

 [('Knowledge', 'NNP'), ('and', 'CC'), ('Decisions', 'NNP'), ('in', 'IN'), ('Health', 'NNP'), ('Telematics', 'NNS'), (':', ':'), ('The', 'DT'), ('Next', 'NNP'), ('Decade', 'NNP'), (',', ','), ('12', 'CD'), (',', ','), ('103', 'CD'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Knowledge', 'Decisions', 'Health', 'Telematics', ':', 'Next', 'Decade', ',', '12', ',', '103', '.']

 TOTAL FILTERED TOKENS ==>  12

 ---- POST FOR FILTERED TOKENS ----

 [('Knowledge', 'NNP'), ('Decisions', 'NNP'), ('Health', 'NNP'), ('Telematics', 'NNS'), (':', ':'), ('Next', 'JJ'), ('Decade', 'NNP'), (',', ','), ('12', 'CD'), (',', ','), ('103', 'CD'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Knowledge Decisions', 'Decisions Health', 'Health Telematics', 'Telematics :', ': Next', 'Next Decade', 'Decade ,', ', 12', '12 ,', ', 103', '103 .'] 

 TOTAL BIGRAMS --> 11 



 ---- TRI-GRAMS ---- 

 ['Knowledge Decisions Health', 'Decisions Health Telematics', 'Health Telematics :', 'Telematics : Next', ': Next Decade', 'Next Decade ,', 'Decade , 12', ', 12 ,', '12 , 103', ', 103 .'] 

 TOTAL TRIGRAMS --> 10 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> ['Knowledge', 'Decisions Health']
 TOTAL PERSON ENTITY --> 2 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['knowledg', 'decis', 'health', 'telemat', ':', 'next', 'decad', ',', '12', ',', '103', '.']

 TOTAL PORTER STEM WORDS ==> 12



 ---- SNOWBALL STEMMING ----

['knowledg', 'decis', 'health', 'telemat', ':', 'next', 'decad', ',', '12', ',', '103', '.']

 TOTAL SNOWBALL STEM WORDS ==> 12



 ---- LEMMATIZATION ----

['Knowledge', 'Decisions', 'Health', 'Telematics', ':', 'Next', 'Decade', ',', '12', ',', '103', '.']

 TOTAL LEMMATIZE WORDS ==> 12

************************************************************************************************************************

731 --> [93] Rassinoux, A. M., Baud, R. H., & Scherrer, J. R. (1992). 


 ---- TOKENS ----

 ['[', '93', ']', 'Rassinoux', ',', 'A.', 'M.', ',', 'Baud', ',', 'R.', 'H.', ',', '&', 'Scherrer', ',', 'J.', 'R.', '(', '1992', ')', '.'] 

 TOTAL TOKENS ==> 22

 ---- POST ----

 [('[', 'RB'), ('93', 'CD'), (']', 'JJ'), ('Rassinoux', 'NNP'), (',', ','), ('A.', 'NNP'), ('M.', 'NNP'), (',', ','), ('Baud', 'NNP'), (',', ','), ('R.', 'NNP'), ('H.', 'NNP'), (',', ','), ('&', 'CC'), ('Scherrer', 'NNP'), (',', ','), ('J.', 'NNP'), ('R.', 'NNP'), ('(', '('), ('1992', 'CD'), (')', ')'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['[', '93', ']', 'Rassinoux', ',', 'A.', 'M.', ',', 'Baud', ',', 'R.', 'H.', ',', '&', 'Scherrer', ',', 'J.', 'R.', '(', '1992', ')', '.']

 TOTAL FILTERED TOKENS ==>  22

 ---- POST FOR FILTERED TOKENS ----

 [('[', 'RB'), ('93', 'CD'), (']', 'JJ'), ('Rassinoux', 'NNP'), (',', ','), ('A.', 'NNP'), ('M.', 'NNP'), (',', ','), ('Baud', 'NNP'), (',', ','), ('R.', 'NNP'), ('H.', 'NNP'), (',', ','), ('&', 'CC'), ('Scherrer', 'NNP'), (',', ','), ('J.', 'NNP'), ('R.', 'NNP'), ('(', '('), ('1992', 'CD'), (')', ')'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['[ 93', '93 ]', '] Rassinoux', 'Rassinoux ,', ', A.', 'A. M.', 'M. ,', ', Baud', 'Baud ,', ', R.', 'R. H.', 'H. ,', ', &', '& Scherrer', 'Scherrer ,', ', J.', 'J. R.', 'R. (', '( 1992', '1992 )', ') .'] 

 TOTAL BIGRAMS --> 21 



 ---- TRI-GRAMS ---- 

 ['[ 93 ]', '93 ] Rassinoux', '] Rassinoux ,', 'Rassinoux , A.', ', A. M.', 'A. M. ,', 'M. , Baud', ', Baud ,', 'Baud , R.', ', R. H.', 'R. H. ,', 'H. , &', ', & Scherrer', '& Scherrer ,', 'Scherrer , J.', ', J. R.', 'J. R. (', 'R. ( 1992', '( 1992 )', '1992 ) .'] 

 TOTAL TRIGRAMS --> 20 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> ['Baud', 'Scherrer', 'J. R.']
 TOTAL PERSON ENTITY --> 3 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['[', '93', ']', 'rassinoux', ',', 'a.', 'm.', ',', 'baud', ',', 'r.', 'h.', ',', '&', 'scherrer', ',', 'j.', 'r.', '(', '1992', ')', '.']

 TOTAL PORTER STEM WORDS ==> 22



 ---- SNOWBALL STEMMING ----

['[', '93', ']', 'rassinoux', ',', 'a.', 'm.', ',', 'baud', ',', 'r.', 'h.', ',', '&', 'scherrer', ',', 'j.', 'r.', '(', '1992', ')', '.']

 TOTAL SNOWBALL STEM WORDS ==> 22



 ---- LEMMATIZATION ----

['[', '93', ']', 'Rassinoux', ',', 'A.', 'M.', ',', 'Baud', ',', 'R.', 'H.', ',', '&', 'Scherrer', ',', 'J.', 'R.', '(', '1992', ')', '.']

 TOTAL LEMMATIZE WORDS ==> 22

************************************************************************************************************************

732 --> Conceptual graphs model  extension for knowledge representation of medical texts. 


 ---- TOKENS ----

 ['Conceptual', 'graphs', 'model', 'extension', 'for', 'knowledge', 'representation', 'of', 'medical', 'texts', '.'] 

 TOTAL TOKENS ==> 11

 ---- POST ----

 [('Conceptual', 'JJ'), ('graphs', 'NN'), ('model', 'NN'), ('extension', 'NN'), ('for', 'IN'), ('knowledge', 'NN'), ('representation', 'NN'), ('of', 'IN'), ('medical', 'JJ'), ('texts', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Conceptual', 'graphs', 'model', 'extension', 'knowledge', 'representation', 'medical', 'texts', '.']

 TOTAL FILTERED TOKENS ==>  9

 ---- POST FOR FILTERED TOKENS ----

 [('Conceptual', 'JJ'), ('graphs', 'NN'), ('model', 'NN'), ('extension', 'NN'), ('knowledge', 'NN'), ('representation', 'NN'), ('medical', 'JJ'), ('texts', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Conceptual graphs', 'graphs model', 'model extension', 'extension knowledge', 'knowledge representation', 'representation medical', 'medical texts', 'texts .'] 

 TOTAL BIGRAMS --> 8 



 ---- TRI-GRAMS ---- 

 ['Conceptual graphs model', 'graphs model extension', 'model extension knowledge', 'extension knowledge representation', 'knowledge representation medical', 'representation medical texts', 'medical texts .'] 

 TOTAL TRIGRAMS --> 7 



 ---- NOUN PHRASES ---- 

 ['Conceptual graphs', 'model', 'extension', 'knowledge', 'representation', 'medical texts'] 

 TOTAL NOUN PHRASES --> 6 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['conceptu', 'graph', 'model', 'extens', 'knowledg', 'represent', 'medic', 'text', '.']

 TOTAL PORTER STEM WORDS ==> 9



 ---- SNOWBALL STEMMING ----

['conceptu', 'graph', 'model', 'extens', 'knowledg', 'represent', 'medic', 'text', '.']

 TOTAL SNOWBALL STEM WORDS ==> 9



 ---- LEMMATIZATION ----

['Conceptual', 'graph', 'model', 'extension', 'knowledge', 'representation', 'medical', 'text', '.']

 TOTAL LEMMATIZE WORDS ==> 9

************************************************************************************************************************

733 --> MEDINFO, 92, 1368-1374. 


 ---- TOKENS ----

 ['MEDINFO', ',', '92', ',', '1368-1374', '.'] 

 TOTAL TOKENS ==> 6

 ---- POST ----

 [('MEDINFO', 'NNP'), (',', ','), ('92', 'CD'), (',', ','), ('1368-1374', 'CD'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['MEDINFO', ',', '92', ',', '1368-1374', '.']

 TOTAL FILTERED TOKENS ==>  6

 ---- POST FOR FILTERED TOKENS ----

 [('MEDINFO', 'NNP'), (',', ','), ('92', 'CD'), (',', ','), ('1368-1374', 'CD'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['MEDINFO ,', ', 92', '92 ,', ', 1368-1374', '1368-1374 .'] 

 TOTAL BIGRAMS --> 5 



 ---- TRI-GRAMS ---- 

 ['MEDINFO , 92', ', 92 ,', '92 , 1368-1374', ', 1368-1374 .'] 

 TOTAL TRIGRAMS --> 4 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> ['MEDINFO']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['medinfo', ',', '92', ',', '1368-1374', '.']

 TOTAL PORTER STEM WORDS ==> 6



 ---- SNOWBALL STEMMING ----

['medinfo', ',', '92', ',', '1368-1374', '.']

 TOTAL SNOWBALL STEM WORDS ==> 6



 ---- LEMMATIZATION ----

['MEDINFO', ',', '92', ',', '1368-1374', '.']

 TOTAL LEMMATIZE WORDS ==> 6

************************************************************************************************************************

734 --> [94] Morel-Guillemaz, A. M., Baud, R. H., & Scherrer, J. R. (1990). 


 ---- TOKENS ----

 ['[', '94', ']', 'Morel-Guillemaz', ',', 'A.', 'M.', ',', 'Baud', ',', 'R.', 'H.', ',', '&', 'Scherrer', ',', 'J.', 'R.', '(', '1990', ')', '.'] 

 TOTAL TOKENS ==> 22

 ---- POST ----

 [('[', 'RB'), ('94', 'CD'), (']', 'JJ'), ('Morel-Guillemaz', 'NNP'), (',', ','), ('A.', 'NNP'), ('M.', 'NNP'), (',', ','), ('Baud', 'NNP'), (',', ','), ('R.', 'NNP'), ('H.', 'NNP'), (',', ','), ('&', 'CC'), ('Scherrer', 'NNP'), (',', ','), ('J.', 'NNP'), ('R.', 'NNP'), ('(', '('), ('1990', 'CD'), (')', ')'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['[', '94', ']', 'Morel-Guillemaz', ',', 'A.', 'M.', ',', 'Baud', ',', 'R.', 'H.', ',', '&', 'Scherrer', ',', 'J.', 'R.', '(', '1990', ')', '.']

 TOTAL FILTERED TOKENS ==>  22

 ---- POST FOR FILTERED TOKENS ----

 [('[', 'RB'), ('94', 'CD'), (']', 'JJ'), ('Morel-Guillemaz', 'NNP'), (',', ','), ('A.', 'NNP'), ('M.', 'NNP'), (',', ','), ('Baud', 'NNP'), (',', ','), ('R.', 'NNP'), ('H.', 'NNP'), (',', ','), ('&', 'CC'), ('Scherrer', 'NNP'), (',', ','), ('J.', 'NNP'), ('R.', 'NNP'), ('(', '('), ('1990', 'CD'), (')', ')'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['[ 94', '94 ]', '] Morel-Guillemaz', 'Morel-Guillemaz ,', ', A.', 'A. M.', 'M. ,', ', Baud', 'Baud ,', ', R.', 'R. H.', 'H. ,', ', &', '& Scherrer', 'Scherrer ,', ', J.', 'J. R.', 'R. (', '( 1990', '1990 )', ') .'] 

 TOTAL BIGRAMS --> 21 



 ---- TRI-GRAMS ---- 

 ['[ 94 ]', '94 ] Morel-Guillemaz', '] Morel-Guillemaz ,', 'Morel-Guillemaz , A.', ', A. M.', 'A. M. ,', 'M. , Baud', ', Baud ,', 'Baud , R.', ', R. H.', 'R. H. ,', 'H. , &', ', & Scherrer', '& Scherrer ,', 'Scherrer , J.', ', J. R.', 'J. R. (', 'R. ( 1990', '( 1990 )', '1990 ) .'] 

 TOTAL TRIGRAMS --> 20 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> ['Baud', 'Scherrer', 'J. R.']
 TOTAL PERSON ENTITY --> 3 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['[', '94', ']', 'morel-guillemaz', ',', 'a.', 'm.', ',', 'baud', ',', 'r.', 'h.', ',', '&', 'scherrer', ',', 'j.', 'r.', '(', '1990', ')', '.']

 TOTAL PORTER STEM WORDS ==> 22



 ---- SNOWBALL STEMMING ----

['[', '94', ']', 'morel-guillemaz', ',', 'a.', 'm.', ',', 'baud', ',', 'r.', 'h.', ',', '&', 'scherrer', ',', 'j.', 'r.', '(', '1990', ')', '.']

 TOTAL SNOWBALL STEM WORDS ==> 22



 ---- LEMMATIZATION ----

['[', '94', ']', 'Morel-Guillemaz', ',', 'A.', 'M.', ',', 'Baud', ',', 'R.', 'H.', ',', '&', 'Scherrer', ',', 'J.', 'R.', '(', '1990', ')', '.']

 TOTAL LEMMATIZE WORDS ==> 22

************************************************************************************************************************

735 --> Proximity Processing of  Medical Text. 


 ---- TOKENS ----

 ['Proximity', 'Processing', 'of', 'Medical', 'Text', '.'] 

 TOTAL TOKENS ==> 6

 ---- POST ----

 [('Proximity', 'NNP'), ('Processing', 'NNP'), ('of', 'IN'), ('Medical', 'NNP'), ('Text', 'NNP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Proximity', 'Processing', 'Medical', 'Text', '.']

 TOTAL FILTERED TOKENS ==>  5

 ---- POST FOR FILTERED TOKENS ----

 [('Proximity', 'NN'), ('Processing', 'NNP'), ('Medical', 'NNP'), ('Text', 'NNP'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Proximity Processing', 'Processing Medical', 'Medical Text', 'Text .'] 

 TOTAL BIGRAMS --> 4 



 ---- TRI-GRAMS ---- 

 ['Proximity Processing Medical', 'Processing Medical Text', 'Medical Text .'] 

 TOTAL TRIGRAMS --> 3 



 ---- NOUN PHRASES ---- 

 ['Proximity'] 

 TOTAL NOUN PHRASES --> 1 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['proxim', 'process', 'medic', 'text', '.']

 TOTAL PORTER STEM WORDS ==> 5



 ---- SNOWBALL STEMMING ----

['proxim', 'process', 'medic', 'text', '.']

 TOTAL SNOWBALL STEM WORDS ==> 5



 ---- LEMMATIZATION ----

['Proximity', 'Processing', 'Medical', 'Text', '.']

 TOTAL LEMMATIZE WORDS ==> 5

************************************************************************************************************************

736 --> In Medical Informatics Europe’90 (pp. 


 ---- TOKENS ----

 ['In', 'Medical', 'Informatics', 'Europe', '’', '90', '(', 'pp', '.'] 

 TOTAL TOKENS ==> 9

 ---- POST ----

 [('In', 'IN'), ('Medical', 'NNP'), ('Informatics', 'NNP'), ('Europe', 'NNP'), ('’', 'VBZ'), ('90', 'CD'), ('(', '('), ('pp', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Medical', 'Informatics', 'Europe', '’', '90', '(', 'pp', '.']

 TOTAL FILTERED TOKENS ==>  8

 ---- POST FOR FILTERED TOKENS ----

 [('Medical', 'JJ'), ('Informatics', 'NNP'), ('Europe', 'NNP'), ('’', 'VBZ'), ('90', 'CD'), ('(', '('), ('pp', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Medical Informatics', 'Informatics Europe', 'Europe ’', '’ 90', '90 (', '( pp', 'pp .'] 

 TOTAL BIGRAMS --> 7 



 ---- TRI-GRAMS ---- 

 ['Medical Informatics Europe', 'Informatics Europe ’', 'Europe ’ 90', '’ 90 (', '90 ( pp', '( pp .'] 

 TOTAL TRIGRAMS --> 6 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 []

 ---- PORTER STEMMING ----

['medic', 'informat', 'europ', '’', '90', '(', 'pp', '.']

 TOTAL PORTER STEM WORDS ==> 8



 ---- SNOWBALL STEMMING ----

['medic', 'informat', 'europ', '’', '90', '(', 'pp', '.']

 TOTAL SNOWBALL STEM WORDS ==> 8



 ---- LEMMATIZATION ----

['Medical', 'Informatics', 'Europe', '’', '90', '(', 'pp', '.']

 TOTAL LEMMATIZE WORDS ==> 8

************************************************************************************************************************

737 --> 625-630). 


 ---- TOKENS ----

 ['625-630', ')', '.'] 

 TOTAL TOKENS ==> 3

 ---- POST ----

 [('625-630', 'JJ'), (')', ')'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['625-630', ')', '.']

 TOTAL FILTERED TOKENS ==>  3

 ---- POST FOR FILTERED TOKENS ----

 [('625-630', 'JJ'), (')', ')'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['625-630 )', ') .'] 

 TOTAL BIGRAMS --> 2 



 ---- TRI-GRAMS ---- 

 ['625-630 ) .'] 

 TOTAL TRIGRAMS --> 1 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 []

 ---- PORTER STEMMING ----

['625-630', ')', '.']

 TOTAL PORTER STEM WORDS ==> 3



 ---- SNOWBALL STEMMING ----

['625-630', ')', '.']

 TOTAL SNOWBALL STEM WORDS ==> 3



 ---- LEMMATIZATION ----

['625-630', ')', '.']

 TOTAL LEMMATIZE WORDS ==> 3

************************************************************************************************************************

738 --> Springer Berlin Heidelberg. 


 ---- TOKENS ----

 ['Springer', 'Berlin', 'Heidelberg', '.'] 

 TOTAL TOKENS ==> 4

 ---- POST ----

 [('Springer', 'NNP'), ('Berlin', 'NNP'), ('Heidelberg', 'NNP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Springer', 'Berlin', 'Heidelberg', '.']

 TOTAL FILTERED TOKENS ==>  4

 ---- POST FOR FILTERED TOKENS ----

 [('Springer', 'NNP'), ('Berlin', 'NNP'), ('Heidelberg', 'NNP'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Springer Berlin', 'Berlin Heidelberg', 'Heidelberg .'] 

 TOTAL BIGRAMS --> 3 



 ---- TRI-GRAMS ---- 

 ['Springer Berlin Heidelberg', 'Berlin Heidelberg .'] 

 TOTAL TRIGRAMS --> 2 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> ['Springer', 'Berlin Heidelberg']
 TOTAL PERSON ENTITY --> 2 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['springer', 'berlin', 'heidelberg', '.']

 TOTAL PORTER STEM WORDS ==> 4



 ---- SNOWBALL STEMMING ----

['springer', 'berlin', 'heidelberg', '.']

 TOTAL SNOWBALL STEM WORDS ==> 4



 ---- LEMMATIZATION ----

['Springer', 'Berlin', 'Heidelberg', '.']

 TOTAL LEMMATIZE WORDS ==> 4

************************************************************************************************************************

739 --> [95] Rassinoux, A. M., Michel, P. A., Juge, C., Baud, R., & Scherrer, J. R. (1994). 


 ---- TOKENS ----

 ['[', '95', ']', 'Rassinoux', ',', 'A.', 'M.', ',', 'Michel', ',', 'P.', 'A.', ',', 'Juge', ',', 'C.', ',', 'Baud', ',', 'R.', ',', '&', 'Scherrer', ',', 'J.', 'R.', '(', '1994', ')', '.'] 

 TOTAL TOKENS ==> 30

 ---- POST ----

 [('[', 'RB'), ('95', 'CD'), (']', 'JJ'), ('Rassinoux', 'NNP'), (',', ','), ('A.', 'NNP'), ('M.', 'NNP'), (',', ','), ('Michel', 'NNP'), (',', ','), ('P.', 'NNP'), ('A.', 'NN'), (',', ','), ('Juge', 'NNP'), (',', ','), ('C.', 'NNP'), (',', ','), ('Baud', 'NNP'), (',', ','), ('R.', 'NNP'), (',', ','), ('&', 'CC'), ('Scherrer', 'NNP'), (',', ','), ('J.', 'NNP'), ('R.', 'NNP'), ('(', '('), ('1994', 'CD'), (')', ')'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['[', '95', ']', 'Rassinoux', ',', 'A.', 'M.', ',', 'Michel', ',', 'P.', 'A.', ',', 'Juge', ',', 'C.', ',', 'Baud', ',', 'R.', ',', '&', 'Scherrer', ',', 'J.', 'R.', '(', '1994', ')', '.']

 TOTAL FILTERED TOKENS ==>  30

 ---- POST FOR FILTERED TOKENS ----

 [('[', 'RB'), ('95', 'CD'), (']', 'JJ'), ('Rassinoux', 'NNP'), (',', ','), ('A.', 'NNP'), ('M.', 'NNP'), (',', ','), ('Michel', 'NNP'), (',', ','), ('P.', 'NNP'), ('A.', 'NN'), (',', ','), ('Juge', 'NNP'), (',', ','), ('C.', 'NNP'), (',', ','), ('Baud', 'NNP'), (',', ','), ('R.', 'NNP'), (',', ','), ('&', 'CC'), ('Scherrer', 'NNP'), (',', ','), ('J.', 'NNP'), ('R.', 'NNP'), ('(', '('), ('1994', 'CD'), (')', ')'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['[ 95', '95 ]', '] Rassinoux', 'Rassinoux ,', ', A.', 'A. M.', 'M. ,', ', Michel', 'Michel ,', ', P.', 'P. A.', 'A. ,', ', Juge', 'Juge ,', ', C.', 'C. ,', ', Baud', 'Baud ,', ', R.', 'R. ,', ', &', '& Scherrer', 'Scherrer ,', ', J.', 'J. R.', 'R. (', '( 1994', '1994 )', ') .'] 

 TOTAL BIGRAMS --> 29 



 ---- TRI-GRAMS ---- 

 ['[ 95 ]', '95 ] Rassinoux', '] Rassinoux ,', 'Rassinoux , A.', ', A. M.', 'A. M. ,', 'M. , Michel', ', Michel ,', 'Michel , P.', ', P. A.', 'P. A. ,', 'A. , Juge', ', Juge ,', 'Juge , C.', ', C. ,', 'C. , Baud', ', Baud ,', 'Baud , R.', ', R. ,', 'R. , &', ', & Scherrer', '& Scherrer ,', 'Scherrer , J.', ', J. R.', 'J. R. (', 'R. ( 1994', '( 1994 )', '1994 ) .'] 

 TOTAL TRIGRAMS --> 28 



 ---- NOUN PHRASES ---- 

 ['A.'] 

 TOTAL NOUN PHRASES --> 1 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> ['Michel', 'Juge', 'Baud', 'Scherrer', 'J. R.']
 TOTAL PERSON ENTITY --> 5 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['[', '95', ']', 'rassinoux', ',', 'a.', 'm.', ',', 'michel', ',', 'p.', 'a.', ',', 'juge', ',', 'c.', ',', 'baud', ',', 'r.', ',', '&', 'scherrer', ',', 'j.', 'r.', '(', '1994', ')', '.']

 TOTAL PORTER STEM WORDS ==> 30



 ---- SNOWBALL STEMMING ----

['[', '95', ']', 'rassinoux', ',', 'a.', 'm.', ',', 'michel', ',', 'p.', 'a.', ',', 'juge', ',', 'c.', ',', 'baud', ',', 'r.', ',', '&', 'scherrer', ',', 'j.', 'r.', '(', '1994', ')', '.']

 TOTAL SNOWBALL STEM WORDS ==> 30



 ---- LEMMATIZATION ----

['[', '95', ']', 'Rassinoux', ',', 'A.', 'M.', ',', 'Michel', ',', 'P.', 'A.', ',', 'Juge', ',', 'C.', ',', 'Baud', ',', 'R.', ',', '&', 'Scherrer', ',', 'J.', 'R.', '(', '1994', ')', '.']

 TOTAL LEMMATIZE WORDS ==> 30

************************************************************************************************************************

740 --> Natural  language processing of medical texts within the HELIOS environment. 


 ---- TOKENS ----

 ['Natural', 'language', 'processing', 'of', 'medical', 'texts', 'within', 'the', 'HELIOS', 'environment', '.'] 

 TOTAL TOKENS ==> 11

 ---- POST ----

 [('Natural', 'JJ'), ('language', 'NN'), ('processing', 'NN'), ('of', 'IN'), ('medical', 'JJ'), ('texts', 'NN'), ('within', 'IN'), ('the', 'DT'), ('HELIOS', 'NNP'), ('environment', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Natural', 'language', 'processing', 'medical', 'texts', 'within', 'HELIOS', 'environment', '.']

 TOTAL FILTERED TOKENS ==>  9

 ---- POST FOR FILTERED TOKENS ----

 [('Natural', 'JJ'), ('language', 'NN'), ('processing', 'NN'), ('medical', 'JJ'), ('texts', 'NN'), ('within', 'IN'), ('HELIOS', 'NNP'), ('environment', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Natural language', 'language processing', 'processing medical', 'medical texts', 'texts within', 'within HELIOS', 'HELIOS environment', 'environment .'] 

 TOTAL BIGRAMS --> 8 



 ---- TRI-GRAMS ---- 

 ['Natural language processing', 'language processing medical', 'processing medical texts', 'medical texts within', 'texts within HELIOS', 'within HELIOS environment', 'HELIOS environment .'] 

 TOTAL TRIGRAMS --> 7 



 ---- NOUN PHRASES ---- 

 ['Natural language', 'processing', 'medical texts', 'environment'] 

 TOTAL NOUN PHRASES --> 4 



 ---- NER ----

 
 ORGANIZATION ---> ['HELIOS']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['natur', 'languag', 'process', 'medic', 'text', 'within', 'helio', 'environ', '.']

 TOTAL PORTER STEM WORDS ==> 9



 ---- SNOWBALL STEMMING ----

['natur', 'languag', 'process', 'medic', 'text', 'within', 'helio', 'environ', '.']

 TOTAL SNOWBALL STEM WORDS ==> 9



 ---- LEMMATIZATION ----

['Natural', 'language', 'processing', 'medical', 'text', 'within', 'HELIOS', 'environment', '.']

 TOTAL LEMMATIZE WORDS ==> 9

************************************************************************************************************************

741 --> Computer methods  and programs in biomedicine, 45, S79-96. 


 ---- TOKENS ----

 ['Computer', 'methods', 'and', 'programs', 'in', 'biomedicine', ',', '45', ',', 'S79-96', '.'] 

 TOTAL TOKENS ==> 11

 ---- POST ----

 [('Computer', 'NNP'), ('methods', 'NNS'), ('and', 'CC'), ('programs', 'NNS'), ('in', 'IN'), ('biomedicine', 'NN'), (',', ','), ('45', 'CD'), (',', ','), ('S79-96', 'NNP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Computer', 'methods', 'programs', 'biomedicine', ',', '45', ',', 'S79-96', '.']

 TOTAL FILTERED TOKENS ==>  9

 ---- POST FOR FILTERED TOKENS ----

 [('Computer', 'NNP'), ('methods', 'NNS'), ('programs', 'NNS'), ('biomedicine', 'VBP'), (',', ','), ('45', 'CD'), (',', ','), ('S79-96', 'NNP'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Computer methods', 'methods programs', 'programs biomedicine', 'biomedicine ,', ', 45', '45 ,', ', S79-96', 'S79-96 .'] 

 TOTAL BIGRAMS --> 8 



 ---- TRI-GRAMS ---- 

 ['Computer methods programs', 'methods programs biomedicine', 'programs biomedicine ,', 'biomedicine , 45', ', 45 ,', '45 , S79-96', ', S79-96 .'] 

 TOTAL TRIGRAMS --> 7 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['comput', 'method', 'program', 'biomedicin', ',', '45', ',', 's79-96', '.']

 TOTAL PORTER STEM WORDS ==> 9



 ---- SNOWBALL STEMMING ----

['comput', 'method', 'program', 'biomedicin', ',', '45', ',', 's79-96', '.']

 TOTAL SNOWBALL STEM WORDS ==> 9



 ---- LEMMATIZATION ----

['Computer', 'method', 'program', 'biomedicine', ',', '45', ',', 'S79-96', '.']

 TOTAL LEMMATIZE WORDS ==> 9

************************************************************************************************************************

742 --> [96] Rassinoux, A. M., Juge, C., Michel, P. A., Baud, R. H., Lemaitre, D., Jean, F. C., ... &  Scherrer, J. R. (1995, June). 


 ---- TOKENS ----

 ['[', '96', ']', 'Rassinoux', ',', 'A.', 'M.', ',', 'Juge', ',', 'C.', ',', 'Michel', ',', 'P.', 'A.', ',', 'Baud', ',', 'R.', 'H.', ',', 'Lemaitre', ',', 'D.', ',', 'Jean', ',', 'F.', 'C.', ',', '...', '&', 'Scherrer', ',', 'J.', 'R.', '(', '1995', ',', 'June', ')', '.'] 

 TOTAL TOKENS ==> 43

 ---- POST ----

 [('[', 'RB'), ('96', 'CD'), (']', 'JJ'), ('Rassinoux', 'NNP'), (',', ','), ('A.', 'NNP'), ('M.', 'NNP'), (',', ','), ('Juge', 'NNP'), (',', ','), ('C.', 'NNP'), (',', ','), ('Michel', 'NNP'), (',', ','), ('P.', 'NNP'), ('A.', 'NN'), (',', ','), ('Baud', 'NNP'), (',', ','), ('R.', 'NNP'), ('H.', 'NNP'), (',', ','), ('Lemaitre', 'NNP'), (',', ','), ('D.', 'NNP'), (',', ','), ('Jean', 'NNP'), (',', ','), ('F.', 'NNP'), ('C.', 'NNP'), (',', ','), ('...', ':'), ('&', 'CC'), ('Scherrer', 'NNP'), (',', ','), ('J.', 'NNP'), ('R.', 'NNP'), ('(', '('), ('1995', 'CD'), (',', ','), ('June', 'NNP'), (')', ')'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['[', '96', ']', 'Rassinoux', ',', 'A.', 'M.', ',', 'Juge', ',', 'C.', ',', 'Michel', ',', 'P.', 'A.', ',', 'Baud', ',', 'R.', 'H.', ',', 'Lemaitre', ',', 'D.', ',', 'Jean', ',', 'F.', 'C.', ',', '...', '&', 'Scherrer', ',', 'J.', 'R.', '(', '1995', ',', 'June', ')', '.']

 TOTAL FILTERED TOKENS ==>  43

 ---- POST FOR FILTERED TOKENS ----

 [('[', 'RB'), ('96', 'CD'), (']', 'JJ'), ('Rassinoux', 'NNP'), (',', ','), ('A.', 'NNP'), ('M.', 'NNP'), (',', ','), ('Juge', 'NNP'), (',', ','), ('C.', 'NNP'), (',', ','), ('Michel', 'NNP'), (',', ','), ('P.', 'NNP'), ('A.', 'NN'), (',', ','), ('Baud', 'NNP'), (',', ','), ('R.', 'NNP'), ('H.', 'NNP'), (',', ','), ('Lemaitre', 'NNP'), (',', ','), ('D.', 'NNP'), (',', ','), ('Jean', 'NNP'), (',', ','), ('F.', 'NNP'), ('C.', 'NNP'), (',', ','), ('...', ':'), ('&', 'CC'), ('Scherrer', 'NNP'), (',', ','), ('J.', 'NNP'), ('R.', 'NNP'), ('(', '('), ('1995', 'CD'), (',', ','), ('June', 'NNP'), (')', ')'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['[ 96', '96 ]', '] Rassinoux', 'Rassinoux ,', ', A.', 'A. M.', 'M. ,', ', Juge', 'Juge ,', ', C.', 'C. ,', ', Michel', 'Michel ,', ', P.', 'P. A.', 'A. ,', ', Baud', 'Baud ,', ', R.', 'R. H.', 'H. ,', ', Lemaitre', 'Lemaitre ,', ', D.', 'D. ,', ', Jean', 'Jean ,', ', F.', 'F. C.', 'C. ,', ', ...', '... &', '& Scherrer', 'Scherrer ,', ', J.', 'J. R.', 'R. (', '( 1995', '1995 ,', ', June', 'June )', ') .'] 

 TOTAL BIGRAMS --> 42 



 ---- TRI-GRAMS ---- 

 ['[ 96 ]', '96 ] Rassinoux', '] Rassinoux ,', 'Rassinoux , A.', ', A. M.', 'A. M. ,', 'M. , Juge', ', Juge ,', 'Juge , C.', ', C. ,', 'C. , Michel', ', Michel ,', 'Michel , P.', ', P. A.', 'P. A. ,', 'A. , Baud', ', Baud ,', 'Baud , R.', ', R. H.', 'R. H. ,', 'H. , Lemaitre', ', Lemaitre ,', 'Lemaitre , D.', ', D. ,', 'D. , Jean', ', Jean ,', 'Jean , F.', ', F. C.', 'F. C. ,', 'C. , ...', ', ... &', '... & Scherrer', '& Scherrer ,', 'Scherrer , J.', ', J. R.', 'J. R. (', 'R. ( 1995', '( 1995 ,', '1995 , June', ', June )', 'June ) .'] 

 TOTAL TRIGRAMS --> 41 



 ---- NOUN PHRASES ---- 

 ['A.'] 

 TOTAL NOUN PHRASES --> 1 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> ['Juge', 'Michel', 'Baud', 'Scherrer', 'J. R.']
 TOTAL PERSON ENTITY --> 5 


 GPE ---> ['Lemaitre', 'Jean']
 TOTAL GPE ENTITY --> 2 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['[', '96', ']', 'rassinoux', ',', 'a.', 'm.', ',', 'juge', ',', 'c.', ',', 'michel', ',', 'p.', 'a.', ',', 'baud', ',', 'r.', 'h.', ',', 'lemaitr', ',', 'd.', ',', 'jean', ',', 'f.', 'c.', ',', '...', '&', 'scherrer', ',', 'j.', 'r.', '(', '1995', ',', 'june', ')', '.']

 TOTAL PORTER STEM WORDS ==> 43



 ---- SNOWBALL STEMMING ----

['[', '96', ']', 'rassinoux', ',', 'a.', 'm.', ',', 'juge', ',', 'c.', ',', 'michel', ',', 'p.', 'a.', ',', 'baud', ',', 'r.', 'h.', ',', 'lemaitr', ',', 'd.', ',', 'jean', ',', 'f.', 'c.', ',', '...', '&', 'scherrer', ',', 'j.', 'r.', '(', '1995', ',', 'june', ')', '.']

 TOTAL SNOWBALL STEM WORDS ==> 43



 ---- LEMMATIZATION ----

['[', '96', ']', 'Rassinoux', ',', 'A.', 'M.', ',', 'Juge', ',', 'C.', ',', 'Michel', ',', 'P.', 'A.', ',', 'Baud', ',', 'R.', 'H.', ',', 'Lemaitre', ',', 'D.', ',', 'Jean', ',', 'F.', 'C.', ',', '...', '&', 'Scherrer', ',', 'J.', 'R.', '(', '1995', ',', 'June', ')', '.']

 TOTAL LEMMATIZE WORDS ==> 43

************************************************************************************************************************

743 --> Analysis of medical jargon: The RECIT system. 


 ---- TOKENS ----

 ['Analysis', 'of', 'medical', 'jargon', ':', 'The', 'RECIT', 'system', '.'] 

 TOTAL TOKENS ==> 9

 ---- POST ----

 [('Analysis', 'NN'), ('of', 'IN'), ('medical', 'JJ'), ('jargon', 'NN'), (':', ':'), ('The', 'DT'), ('RECIT', 'NNP'), ('system', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Analysis', 'medical', 'jargon', ':', 'RECIT', 'system', '.']

 TOTAL FILTERED TOKENS ==>  7

 ---- POST FOR FILTERED TOKENS ----

 [('Analysis', 'NNP'), ('medical', 'JJ'), ('jargon', 'NN'), (':', ':'), ('RECIT', 'NNP'), ('system', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Analysis medical', 'medical jargon', 'jargon :', ': RECIT', 'RECIT system', 'system .'] 

 TOTAL BIGRAMS --> 6 



 ---- TRI-GRAMS ---- 

 ['Analysis medical jargon', 'medical jargon :', 'jargon : RECIT', ': RECIT system', 'RECIT system .'] 

 TOTAL TRIGRAMS --> 5 



 ---- NOUN PHRASES ---- 

 ['medical jargon', 'system'] 

 TOTAL NOUN PHRASES --> 2 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> ['Analysis']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['analysi', 'medic', 'jargon', ':', 'recit', 'system', '.']

 TOTAL PORTER STEM WORDS ==> 7



 ---- SNOWBALL STEMMING ----

['analysi', 'medic', 'jargon', ':', 'recit', 'system', '.']

 TOTAL SNOWBALL STEM WORDS ==> 7



 ---- LEMMATIZATION ----

['Analysis', 'medical', 'jargon', ':', 'RECIT', 'system', '.']

 TOTAL LEMMATIZE WORDS ==> 7

************************************************************************************************************************

744 --> In Conference  on Artificial Intelligence in Medicine in Europe (pp. 


 ---- TOKENS ----

 ['In', 'Conference', 'on', 'Artificial', 'Intelligence', 'in', 'Medicine', 'in', 'Europe', '(', 'pp', '.'] 

 TOTAL TOKENS ==> 12

 ---- POST ----

 [('In', 'IN'), ('Conference', 'NNP'), ('on', 'IN'), ('Artificial', 'NNP'), ('Intelligence', 'NNP'), ('in', 'IN'), ('Medicine', 'NNP'), ('in', 'IN'), ('Europe', 'NNP'), ('(', '('), ('pp', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Conference', 'Artificial', 'Intelligence', 'Medicine', 'Europe', '(', 'pp', '.']

 TOTAL FILTERED TOKENS ==>  8

 ---- POST FOR FILTERED TOKENS ----

 [('Conference', 'NNP'), ('Artificial', 'NNP'), ('Intelligence', 'NNP'), ('Medicine', 'NNP'), ('Europe', 'NNP'), ('(', '('), ('pp', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Conference Artificial', 'Artificial Intelligence', 'Intelligence Medicine', 'Medicine Europe', 'Europe (', '( pp', 'pp .'] 

 TOTAL BIGRAMS --> 7 



 ---- TRI-GRAMS ---- 

 ['Conference Artificial Intelligence', 'Artificial Intelligence Medicine', 'Intelligence Medicine Europe', 'Medicine Europe (', 'Europe ( pp', '( pp .'] 

 TOTAL TRIGRAMS --> 6 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 []

 ---- PORTER STEMMING ----

['confer', 'artifici', 'intellig', 'medicin', 'europ', '(', 'pp', '.']

 TOTAL PORTER STEM WORDS ==> 8



 ---- SNOWBALL STEMMING ----

['confer', 'artifici', 'intellig', 'medicin', 'europ', '(', 'pp', '.']

 TOTAL SNOWBALL STEM WORDS ==> 8



 ---- LEMMATIZATION ----

['Conference', 'Artificial', 'Intelligence', 'Medicine', 'Europe', '(', 'pp', '.']

 TOTAL LEMMATIZE WORDS ==> 8

************************************************************************************************************************

745 --> 42-52). 


 ---- TOKENS ----

 ['42-52', ')', '.'] 

 TOTAL TOKENS ==> 3

 ---- POST ----

 [('42-52', 'JJ'), (')', ')'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['42-52', ')', '.']

 TOTAL FILTERED TOKENS ==>  3

 ---- POST FOR FILTERED TOKENS ----

 [('42-52', 'JJ'), (')', ')'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['42-52 )', ') .'] 

 TOTAL BIGRAMS --> 2 



 ---- TRI-GRAMS ---- 

 ['42-52 ) .'] 

 TOTAL TRIGRAMS --> 1 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 []

 ---- PORTER STEMMING ----

['42-52', ')', '.']

 TOTAL PORTER STEM WORDS ==> 3



 ---- SNOWBALL STEMMING ----

['42-52', ')', '.']

 TOTAL SNOWBALL STEM WORDS ==> 3



 ---- LEMMATIZATION ----

['42-52', ')', '.']

 TOTAL LEMMATIZE WORDS ==> 3

************************************************************************************************************************

746 --> Springer Berlin Heidelberg. 


 ---- TOKENS ----

 ['Springer', 'Berlin', 'Heidelberg', '.'] 

 TOTAL TOKENS ==> 4

 ---- POST ----

 [('Springer', 'NNP'), ('Berlin', 'NNP'), ('Heidelberg', 'NNP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Springer', 'Berlin', 'Heidelberg', '.']

 TOTAL FILTERED TOKENS ==>  4

 ---- POST FOR FILTERED TOKENS ----

 [('Springer', 'NNP'), ('Berlin', 'NNP'), ('Heidelberg', 'NNP'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Springer Berlin', 'Berlin Heidelberg', 'Heidelberg .'] 

 TOTAL BIGRAMS --> 3 



 ---- TRI-GRAMS ---- 

 ['Springer Berlin Heidelberg', 'Berlin Heidelberg .'] 

 TOTAL TRIGRAMS --> 2 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> ['Springer', 'Berlin Heidelberg']
 TOTAL PERSON ENTITY --> 2 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['springer', 'berlin', 'heidelberg', '.']

 TOTAL PORTER STEM WORDS ==> 4



 ---- SNOWBALL STEMMING ----

['springer', 'berlin', 'heidelberg', '.']

 TOTAL SNOWBALL STEM WORDS ==> 4



 ---- LEMMATIZATION ----

['Springer', 'Berlin', 'Heidelberg', '.']

 TOTAL LEMMATIZE WORDS ==> 4

************************************************************************************************************************

747 --> [97] Friedman, C., Cimino, J. J., & Johnson, S. B. 


 ---- TOKENS ----

 ['[', '97', ']', 'Friedman', ',', 'C.', ',', 'Cimino', ',', 'J.', 'J.', ',', '&', 'Johnson', ',', 'S.', 'B', '.'] 

 TOTAL TOKENS ==> 18

 ---- POST ----

 [('[', 'RB'), ('97', 'CD'), (']', 'JJ'), ('Friedman', 'NNP'), (',', ','), ('C.', 'NNP'), (',', ','), ('Cimino', 'NNP'), (',', ','), ('J.', 'NNP'), ('J.', 'NNP'), (',', ','), ('&', 'CC'), ('Johnson', 'NNP'), (',', ','), ('S.', 'NNP'), ('B', 'NNP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['[', '97', ']', 'Friedman', ',', 'C.', ',', 'Cimino', ',', 'J.', 'J.', ',', '&', 'Johnson', ',', 'S.', 'B', '.']

 TOTAL FILTERED TOKENS ==>  18

 ---- POST FOR FILTERED TOKENS ----

 [('[', 'RB'), ('97', 'CD'), (']', 'JJ'), ('Friedman', 'NNP'), (',', ','), ('C.', 'NNP'), (',', ','), ('Cimino', 'NNP'), (',', ','), ('J.', 'NNP'), ('J.', 'NNP'), (',', ','), ('&', 'CC'), ('Johnson', 'NNP'), (',', ','), ('S.', 'NNP'), ('B', 'NNP'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['[ 97', '97 ]', '] Friedman', 'Friedman ,', ', C.', 'C. ,', ', Cimino', 'Cimino ,', ', J.', 'J. J.', 'J. ,', ', &', '& Johnson', 'Johnson ,', ', S.', 'S. B', 'B .'] 

 TOTAL BIGRAMS --> 17 



 ---- TRI-GRAMS ---- 

 ['[ 97 ]', '97 ] Friedman', '] Friedman ,', 'Friedman , C.', ', C. ,', 'C. , Cimino', ', Cimino ,', 'Cimino , J.', ', J. J.', 'J. J. ,', 'J. , &', ', & Johnson', '& Johnson ,', 'Johnson , S.', ', S. B', 'S. B .'] 

 TOTAL TRIGRAMS --> 16 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> ['J. J.', 'Johnson']
 TOTAL PERSON ENTITY --> 2 


 GPE ---> ['Cimino']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['[', '97', ']', 'friedman', ',', 'c.', ',', 'cimino', ',', 'j.', 'j.', ',', '&', 'johnson', ',', 's.', 'b', '.']

 TOTAL PORTER STEM WORDS ==> 18



 ---- SNOWBALL STEMMING ----

['[', '97', ']', 'friedman', ',', 'c.', ',', 'cimino', ',', 'j.', 'j.', ',', '&', 'johnson', ',', 's.', 'b', '.']

 TOTAL SNOWBALL STEM WORDS ==> 18



 ---- LEMMATIZATION ----

['[', '97', ']', 'Friedman', ',', 'C.', ',', 'Cimino', ',', 'J.', 'J.', ',', '&', 'Johnson', ',', 'S.', 'B', '.']

 TOTAL LEMMATIZE WORDS ==> 18

************************************************************************************************************************

748 --> (1993). 


 ---- TOKENS ----

 ['(', '1993', ')', '.'] 

 TOTAL TOKENS ==> 4

 ---- POST ----

 [('(', '('), ('1993', 'CD'), (')', ')'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['(', '1993', ')', '.']

 TOTAL FILTERED TOKENS ==>  4

 ---- POST FOR FILTERED TOKENS ----

 [('(', '('), ('1993', 'CD'), (')', ')'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['( 1993', '1993 )', ') .'] 

 TOTAL BIGRAMS --> 3 



 ---- TRI-GRAMS ---- 

 ['( 1993 )', '1993 ) .'] 

 TOTAL TRIGRAMS --> 2 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['(', '1993', ')', '.']

 TOTAL PORTER STEM WORDS ==> 4



 ---- SNOWBALL STEMMING ----

['(', '1993', ')', '.']

 TOTAL SNOWBALL STEM WORDS ==> 4



 ---- LEMMATIZATION ----

['(', '1993', ')', '.']

 TOTAL LEMMATIZE WORDS ==> 4

************************************************************************************************************************

749 --> A conceptual model for clinical  radiology reports. 


 ---- TOKENS ----

 ['A', 'conceptual', 'model', 'for', 'clinical', 'radiology', 'reports', '.'] 

 TOTAL TOKENS ==> 8

 ---- POST ----

 [('A', 'DT'), ('conceptual', 'JJ'), ('model', 'NN'), ('for', 'IN'), ('clinical', 'JJ'), ('radiology', 'NN'), ('reports', 'NNS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['conceptual', 'model', 'clinical', 'radiology', 'reports', '.']

 TOTAL FILTERED TOKENS ==>  6

 ---- POST FOR FILTERED TOKENS ----

 [('conceptual', 'JJ'), ('model', 'NN'), ('clinical', 'JJ'), ('radiology', 'NN'), ('reports', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['conceptual model', 'model clinical', 'clinical radiology', 'radiology reports', 'reports .'] 

 TOTAL BIGRAMS --> 5 



 ---- TRI-GRAMS ---- 

 ['conceptual model clinical', 'model clinical radiology', 'clinical radiology reports', 'radiology reports .'] 

 TOTAL TRIGRAMS --> 4 



 ---- NOUN PHRASES ---- 

 ['conceptual model', 'clinical radiology'] 

 TOTAL NOUN PHRASES --> 2 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['conceptu', 'model', 'clinic', 'radiolog', 'report', '.']

 TOTAL PORTER STEM WORDS ==> 6



 ---- SNOWBALL STEMMING ----

['conceptu', 'model', 'clinic', 'radiolog', 'report', '.']

 TOTAL SNOWBALL STEM WORDS ==> 6



 ---- LEMMATIZATION ----

['conceptual', 'model', 'clinical', 'radiology', 'report', '.']

 TOTAL LEMMATIZE WORDS ==> 6

************************************************************************************************************************

750 --> In Proceedings of the Annual Symposium on Computer Application in  Medical Care (p. 829). 


 ---- TOKENS ----

 ['In', 'Proceedings', 'of', 'the', 'Annual', 'Symposium', 'on', 'Computer', 'Application', 'in', 'Medical', 'Care', '(', 'p.', '829', ')', '.'] 

 TOTAL TOKENS ==> 17

 ---- POST ----

 [('In', 'IN'), ('Proceedings', 'NNP'), ('of', 'IN'), ('the', 'DT'), ('Annual', 'NNP'), ('Symposium', 'NNP'), ('on', 'IN'), ('Computer', 'NNP'), ('Application', 'NNP'), ('in', 'IN'), ('Medical', 'NNP'), ('Care', 'NNP'), ('(', '('), ('p.', 'VB'), ('829', 'CD'), (')', ')'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Proceedings', 'Annual', 'Symposium', 'Computer', 'Application', 'Medical', 'Care', '(', 'p.', '829', ')', '.']

 TOTAL FILTERED TOKENS ==>  12

 ---- POST FOR FILTERED TOKENS ----

 [('Proceedings', 'NNS'), ('Annual', 'NNP'), ('Symposium', 'NNP'), ('Computer', 'NNP'), ('Application', 'NNP'), ('Medical', 'NNP'), ('Care', 'NNP'), ('(', '('), ('p.', 'VB'), ('829', 'CD'), (')', ')'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Proceedings Annual', 'Annual Symposium', 'Symposium Computer', 'Computer Application', 'Application Medical', 'Medical Care', 'Care (', '( p.', 'p. 829', '829 )', ') .'] 

 TOTAL BIGRAMS --> 11 



 ---- TRI-GRAMS ---- 

 ['Proceedings Annual Symposium', 'Annual Symposium Computer', 'Symposium Computer Application', 'Computer Application Medical', 'Application Medical Care', 'Medical Care (', 'Care ( p.', '( p. 829', 'p. 829 )', '829 ) .'] 

 TOTAL TRIGRAMS --> 10 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> ['Annual Symposium Computer']
 TOTAL PERSON ENTITY --> 1 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['proceed', 'annual', 'symposium', 'comput', 'applic', 'medic', 'care', '(', 'p.', '829', ')', '.']

 TOTAL PORTER STEM WORDS ==> 12



 ---- SNOWBALL STEMMING ----

['proceed', 'annual', 'symposium', 'comput', 'applic', 'medic', 'care', '(', 'p.', '829', ')', '.']

 TOTAL SNOWBALL STEM WORDS ==> 12



 ---- LEMMATIZATION ----

['Proceedings', 'Annual', 'Symposium', 'Computer', 'Application', 'Medical', 'Care', '(', 'p.', '829', ')', '.']

 TOTAL LEMMATIZE WORDS ==> 12

************************************************************************************************************************

751 --> American Medical Informatics Association. 


 ---- TOKENS ----

 ['American', 'Medical', 'Informatics', 'Association', '.'] 

 TOTAL TOKENS ==> 5

 ---- POST ----

 [('American', 'NNP'), ('Medical', 'NNP'), ('Informatics', 'NNP'), ('Association', 'NNP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['American', 'Medical', 'Informatics', 'Association', '.']

 TOTAL FILTERED TOKENS ==>  5

 ---- POST FOR FILTERED TOKENS ----

 [('American', 'NNP'), ('Medical', 'NNP'), ('Informatics', 'NNP'), ('Association', 'NNP'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['American Medical', 'Medical Informatics', 'Informatics Association', 'Association .'] 

 TOTAL BIGRAMS --> 4 



 ---- TRI-GRAMS ---- 

 ['American Medical Informatics', 'Medical Informatics Association', 'Informatics Association .'] 

 TOTAL TRIGRAMS --> 3 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> ['Medical Informatics Association']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> ['American']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['american', 'medic', 'informat', 'associ', '.']

 TOTAL PORTER STEM WORDS ==> 5



 ---- SNOWBALL STEMMING ----

['american', 'medic', 'informat', 'associ', '.']

 TOTAL SNOWBALL STEM WORDS ==> 5



 ---- LEMMATIZATION ----

['American', 'Medical', 'Informatics', 'Association', '.']

 TOTAL LEMMATIZE WORDS ==> 5

************************************************************************************************************************

752 --> [98] "Natural Language Processing." 


 ---- TOKENS ----

 ['[', '98', ']', '``', 'Natural', 'Language', 'Processing', '.', "''"] 

 TOTAL TOKENS ==> 9

 ---- POST ----

 [('[', 'RB'), ('98', 'CD'), (']', 'NN'), ('``', '``'), ('Natural', 'JJ'), ('Language', 'NN'), ('Processing', 'NN'), ('.', '.'), ("''", "''")] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['[', '98', ']', '``', 'Natural', 'Language', 'Processing', '.', "''"]

 TOTAL FILTERED TOKENS ==>  9

 ---- POST FOR FILTERED TOKENS ----

 [('[', 'RB'), ('98', 'CD'), (']', 'NN'), ('``', '``'), ('Natural', 'JJ'), ('Language', 'NN'), ('Processing', 'NN'), ('.', '.'), ("''", "''")] 



 ---- BI-GRAMS ---- 

 ['[ 98', '98 ]', '] ``', '`` Natural', 'Natural Language', 'Language Processing', 'Processing .', ". ''"] 

 TOTAL BIGRAMS --> 8 



 ---- TRI-GRAMS ---- 

 ['[ 98 ]', '98 ] ``', '] `` Natural', '`` Natural Language', 'Natural Language Processing', 'Language Processing .', "Processing . ''"] 

 TOTAL TRIGRAMS --> 7 



 ---- NOUN PHRASES ---- 

 [']', 'Natural Language', 'Processing'] 

 TOTAL NOUN PHRASES --> 3 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['[', '98', ']', '``', 'natur', 'languag', 'process', '.', "''"]

 TOTAL PORTER STEM WORDS ==> 9



 ---- SNOWBALL STEMMING ----

['[', '98', ']', '``', 'natur', 'languag', 'process', '.', "''"]

 TOTAL SNOWBALL STEM WORDS ==> 9



 ---- LEMMATIZATION ----

['[', '98', ']', '``', 'Natural', 'Language', 'Processing', '.', "''"]

 TOTAL LEMMATIZE WORDS ==> 9

************************************************************************************************************************

753 --> Natural Language Processing RSS. 


 ---- TOKENS ----

 ['Natural', 'Language', 'Processing', 'RSS', '.'] 

 TOTAL TOKENS ==> 5

 ---- POST ----

 [('Natural', 'JJ'), ('Language', 'NNP'), ('Processing', 'NNP'), ('RSS', 'NNP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Natural', 'Language', 'Processing', 'RSS', '.']

 TOTAL FILTERED TOKENS ==>  5

 ---- POST FOR FILTERED TOKENS ----

 [('Natural', 'JJ'), ('Language', 'NNP'), ('Processing', 'NNP'), ('RSS', 'NNP'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Natural Language', 'Language Processing', 'Processing RSS', 'RSS .'] 

 TOTAL BIGRAMS --> 4 



 ---- TRI-GRAMS ---- 

 ['Natural Language Processing', 'Language Processing RSS', 'Processing RSS .'] 

 TOTAL TRIGRAMS --> 3 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['natur', 'languag', 'process', 'rss', '.']

 TOTAL PORTER STEM WORDS ==> 5



 ---- SNOWBALL STEMMING ----

['natur', 'languag', 'process', 'rss', '.']

 TOTAL SNOWBALL STEM WORDS ==> 5



 ---- LEMMATIZATION ----

['Natural', 'Language', 'Processing', 'RSS', '.']

 TOTAL LEMMATIZE WORDS ==> 5

************************************************************************************************************************

754 --> N.p., n.d. 


 ---- TOKENS ----

 ['N.p.', ',', 'n.d', '.'] 

 TOTAL TOKENS ==> 4

 ---- POST ----

 [('N.p.', 'NNP'), (',', ','), ('n.d', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['N.p.', ',', 'n.d', '.']

 TOTAL FILTERED TOKENS ==>  4

 ---- POST FOR FILTERED TOKENS ----

 [('N.p.', 'NNP'), (',', ','), ('n.d', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['N.p. ,', ', n.d', 'n.d .'] 

 TOTAL BIGRAMS --> 3 



 ---- TRI-GRAMS ---- 

 ['N.p. , n.d', ', n.d .'] 

 TOTAL TRIGRAMS --> 2 



 ---- NOUN PHRASES ---- 

 ['n.d'] 

 TOTAL NOUN PHRASES --> 1 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['n.p.', ',', 'n.d', '.']

 TOTAL PORTER STEM WORDS ==> 4



 ---- SNOWBALL STEMMING ----

['n.p.', ',', 'n.d', '.']

 TOTAL SNOWBALL STEM WORDS ==> 4



 ---- LEMMATIZATION ----

['N.p.', ',', 'n.d', '.']

 TOTAL LEMMATIZE WORDS ==> 4

************************************************************************************************************************

755 --> Web. 


 ---- TOKENS ----

 ['Web', '.'] 

 TOTAL TOKENS ==> 2

 ---- POST ----

 [('Web', 'NNP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Web', '.']

 TOTAL FILTERED TOKENS ==>  2

 ---- POST FOR FILTERED TOKENS ----

 [('Web', 'NNP'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Web .'] 

 TOTAL BIGRAMS --> 1 



 ---- TRI-GRAMS ---- 

 [] 

 TOTAL TRIGRAMS --> 0 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['web', '.']

 TOTAL PORTER STEM WORDS ==> 2



 ---- SNOWBALL STEMMING ----

['web', '.']

 TOTAL SNOWBALL STEM WORDS ==> 2



 ---- LEMMATIZATION ----

['Web', '.']

 TOTAL LEMMATIZE WORDS ==> 2

************************************************************************************************************************

756 --> 23  Mar. 


 ---- TOKENS ----

 ['23', 'Mar', '.'] 

 TOTAL TOKENS ==> 3

 ---- POST ----

 [('23', 'CD'), ('Mar', 'NNP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['23', 'Mar', '.']

 TOTAL FILTERED TOKENS ==>  3

 ---- POST FOR FILTERED TOKENS ----

 [('23', 'CD'), ('Mar', 'NNP'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['23 Mar', 'Mar .'] 

 TOTAL BIGRAMS --> 2 



 ---- TRI-GRAMS ---- 

 ['23 Mar .'] 

 TOTAL TRIGRAMS --> 1 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['23', 'mar', '.']

 TOTAL PORTER STEM WORDS ==> 3



 ---- SNOWBALL STEMMING ----

['23', 'mar', '.']

 TOTAL SNOWBALL STEM WORDS ==> 3



 ---- LEMMATIZATION ----

['23', 'Mar', '.']

 TOTAL LEMMATIZE WORDS ==> 3

************************************************************************************************************************

757 --> 2017. 


 ---- TOKENS ----

 ['2017', '.'] 

 TOTAL TOKENS ==> 2

 ---- POST ----

 [('2017', 'CD'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['2017', '.']

 TOTAL FILTERED TOKENS ==>  2

 ---- POST FOR FILTERED TOKENS ----

 [('2017', 'CD'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['2017 .'] 

 TOTAL BIGRAMS --> 1 



 ---- TRI-GRAMS ---- 

 [] 

 TOTAL TRIGRAMS --> 0 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['2017', '.']

 TOTAL PORTER STEM WORDS ==> 2



 ---- SNOWBALL STEMMING ----

['2017', '.']

 TOTAL SNOWBALL STEM WORDS ==> 2



 ---- LEMMATIZATION ----

['2017', '.']

 TOTAL LEMMATIZE WORDS ==> 2

************************************************************************************************************************

758 --> [99] [Srihari S. Machine Learning: Generative and Discriminative Models. 


 ---- TOKENS ----

 ['[', '99', ']', '[', 'Srihari', 'S.', 'Machine', 'Learning', ':', 'Generative', 'and', 'Discriminative', 'Models', '.'] 

 TOTAL TOKENS ==> 14

 ---- POST ----

 [('[', 'RB'), ('99', 'CD'), (']', 'JJ'), ('[', 'NNP'), ('Srihari', 'NNP'), ('S.', 'NNP'), ('Machine', 'NNP'), ('Learning', 'NNP'), (':', ':'), ('Generative', 'JJ'), ('and', 'CC'), ('Discriminative', 'JJ'), ('Models', 'NNP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['[', '99', ']', '[', 'Srihari', 'S.', 'Machine', 'Learning', ':', 'Generative', 'Discriminative', 'Models', '.']

 TOTAL FILTERED TOKENS ==>  13

 ---- POST FOR FILTERED TOKENS ----

 [('[', 'RB'), ('99', 'CD'), (']', 'JJ'), ('[', 'NNP'), ('Srihari', 'NNP'), ('S.', 'NNP'), ('Machine', 'NNP'), ('Learning', 'NNP'), (':', ':'), ('Generative', 'JJ'), ('Discriminative', 'NNP'), ('Models', 'NNP'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['[ 99', '99 ]', '] [', '[ Srihari', 'Srihari S.', 'S. Machine', 'Machine Learning', 'Learning :', ': Generative', 'Generative Discriminative', 'Discriminative Models', 'Models .'] 

 TOTAL BIGRAMS --> 12 



 ---- TRI-GRAMS ---- 

 ['[ 99 ]', '99 ] [', '] [ Srihari', '[ Srihari S.', 'Srihari S. Machine', 'S. Machine Learning', 'Machine Learning :', 'Learning : Generative', ': Generative Discriminative', 'Generative Discriminative Models', 'Discriminative Models .'] 

 TOTAL TRIGRAMS --> 11 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> ['Srihari S. Machine']
 TOTAL PERSON ENTITY --> 1 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['[', '99', ']', '[', 'srihari', 's.', 'machin', 'learn', ':', 'gener', 'discrimin', 'model', '.']

 TOTAL PORTER STEM WORDS ==> 13



 ---- SNOWBALL STEMMING ----

['[', '99', ']', '[', 'srihari', 's.', 'machin', 'learn', ':', 'generat', 'discrimin', 'model', '.']

 TOTAL SNOWBALL STEM WORDS ==> 13



 ---- LEMMATIZATION ----

['[', '99', ']', '[', 'Srihari', 'S.', 'Machine', 'Learning', ':', 'Generative', 'Discriminative', 'Models', '.']

 TOTAL LEMMATIZE WORDS ==> 13

************************************************************************************************************************

759 --> 2010. http://  www.cedar.buffalo.edu/wsrihari/CSE574/Discriminative-Generative.pdf (accessed 31 May  2011).] 


 ---- TOKENS ----

 ['2010.', 'http', ':', '//', 'www.cedar.buffalo.edu/wsrihari/CSE574/Discriminative-Generative.pdf', '(', 'accessed', '31', 'May', '2011', ')', '.', ']'] 

 TOTAL TOKENS ==> 13

 ---- POST ----

 [('2010.', 'CD'), ('http', 'NN'), (':', ':'), ('//', 'JJ'), ('www.cedar.buffalo.edu/wsrihari/CSE574/Discriminative-Generative.pdf', 'NN'), ('(', '('), ('accessed', 'JJ'), ('31', 'CD'), ('May', 'NNP'), ('2011', 'CD'), (')', ')'), ('.', '.'), (']', 'NN')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['2010.', 'http', ':', '//', 'www.cedar.buffalo.edu/wsrihari/CSE574/Discriminative-Generative.pdf', '(', 'accessed', '31', 'May', '2011', ')', '.', ']']

 TOTAL FILTERED TOKENS ==>  13

 ---- POST FOR FILTERED TOKENS ----

 [('2010.', 'CD'), ('http', 'NN'), (':', ':'), ('//', 'JJ'), ('www.cedar.buffalo.edu/wsrihari/CSE574/Discriminative-Generative.pdf', 'NN'), ('(', '('), ('accessed', 'JJ'), ('31', 'CD'), ('May', 'NNP'), ('2011', 'CD'), (')', ')'), ('.', '.'), (']', 'NN')] 



 ---- BI-GRAMS ---- 

 ['2010. http', 'http :', ': //', '// www.cedar.buffalo.edu/wsrihari/CSE574/Discriminative-Generative.pdf', 'www.cedar.buffalo.edu/wsrihari/CSE574/Discriminative-Generative.pdf (', '( accessed', 'accessed 31', '31 May', 'May 2011', '2011 )', ') .', '. ]'] 

 TOTAL BIGRAMS --> 12 



 ---- TRI-GRAMS ---- 

 ['2010. http :', 'http : //', ': // www.cedar.buffalo.edu/wsrihari/CSE574/Discriminative-Generative.pdf', '// www.cedar.buffalo.edu/wsrihari/CSE574/Discriminative-Generative.pdf (', 'www.cedar.buffalo.edu/wsrihari/CSE574/Discriminative-Generative.pdf ( accessed', '( accessed 31', 'accessed 31 May', '31 May 2011', 'May 2011 )', '2011 ) .', ') . ]'] 

 TOTAL TRIGRAMS --> 11 



 ---- NOUN PHRASES ---- 

 ['http', ' www.cedar.buffalo.edu', ']'] 

 TOTAL NOUN PHRASES --> 3 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['2010.', 'http', ':', '//', 'www.cedar.buffalo.edu/wsrihari/cse574/discriminative-generative.pdf', '(', 'access', '31', 'may', '2011', ')', '.', ']']

 TOTAL PORTER STEM WORDS ==> 13



 ---- SNOWBALL STEMMING ----

['2010.', 'http', ':', '//', 'www.cedar.buffalo.edu/wsrihari/cse574/discriminative-generative.pdf', '(', 'access', '31', 'may', '2011', ')', '.', ']']

 TOTAL SNOWBALL STEM WORDS ==> 13



 ---- LEMMATIZATION ----

['2010.', 'http', ':', '//', 'www.cedar.buffalo.edu/wsrihari/CSE574/Discriminative-Generative.pdf', '(', 'accessed', '31', 'May', '2011', ')', '.', ']']

 TOTAL LEMMATIZE WORDS ==> 13

************************************************************************************************************************

760 --> [100] [Elkan C. Log-Linear Models and Conditional Random Fields. 


 ---- TOKENS ----

 ['[', '100', ']', '[', 'Elkan', 'C.', 'Log-Linear', 'Models', 'and', 'Conditional', 'Random', 'Fields', '.'] 

 TOTAL TOKENS ==> 13

 ---- POST ----

 [('[', 'RB'), ('100', 'CD'), (']', 'JJ'), ('[', 'NNP'), ('Elkan', 'NNP'), ('C.', 'NNP'), ('Log-Linear', 'NNP'), ('Models', 'NNP'), ('and', 'CC'), ('Conditional', 'NNP'), ('Random', 'NNP'), ('Fields', 'NNP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['[', '100', ']', '[', 'Elkan', 'C.', 'Log-Linear', 'Models', 'Conditional', 'Random', 'Fields', '.']

 TOTAL FILTERED TOKENS ==>  12

 ---- POST FOR FILTERED TOKENS ----

 [('[', 'RB'), ('100', 'CD'), (']', 'JJ'), ('[', 'NNP'), ('Elkan', 'NNP'), ('C.', 'NNP'), ('Log-Linear', 'NNP'), ('Models', 'NNP'), ('Conditional', 'NNP'), ('Random', 'NNP'), ('Fields', 'NNP'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['[ 100', '100 ]', '] [', '[ Elkan', 'Elkan C.', 'C. Log-Linear', 'Log-Linear Models', 'Models Conditional', 'Conditional Random', 'Random Fields', 'Fields .'] 

 TOTAL BIGRAMS --> 11 



 ---- TRI-GRAMS ---- 

 ['[ 100 ]', '100 ] [', '] [ Elkan', '[ Elkan C.', 'Elkan C. Log-Linear', 'C. Log-Linear Models', 'Log-Linear Models Conditional', 'Models Conditional Random', 'Conditional Random Fields', 'Random Fields .'] 

 TOTAL TRIGRAMS --> 10 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> ['Elkan C.', 'Random Fields']
 TOTAL PERSON ENTITY --> 2 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['[', '100', ']', '[', 'elkan', 'c.', 'log-linear', 'model', 'condit', 'random', 'field', '.']

 TOTAL PORTER STEM WORDS ==> 12



 ---- SNOWBALL STEMMING ----

['[', '100', ']', '[', 'elkan', 'c.', 'log-linear', 'model', 'condit', 'random', 'field', '.']

 TOTAL SNOWBALL STEM WORDS ==> 12



 ---- LEMMATIZATION ----

['[', '100', ']', '[', 'Elkan', 'C.', 'Log-Linear', 'Models', 'Conditional', 'Random', 'Fields', '.']

 TOTAL LEMMATIZE WORDS ==> 12

************************************************************************************************************************

761 --> 2008. http://cseweb. 


 ---- TOKENS ----

 ['2008.', 'http', ':', '//cseweb', '.'] 

 TOTAL TOKENS ==> 5

 ---- POST ----

 [('2008.', 'CD'), ('http', 'NN'), (':', ':'), ('//cseweb', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['2008.', 'http', ':', '//cseweb', '.']

 TOTAL FILTERED TOKENS ==>  5

 ---- POST FOR FILTERED TOKENS ----

 [('2008.', 'CD'), ('http', 'NN'), (':', ':'), ('//cseweb', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['2008. http', 'http :', ': //cseweb', '//cseweb .'] 

 TOTAL BIGRAMS --> 4 



 ---- TRI-GRAMS ---- 

 ['2008. http :', 'http : //cseweb', ': //cseweb .'] 

 TOTAL TRIGRAMS --> 3 



 ---- NOUN PHRASES ---- 

 ['http', ''] 

 TOTAL NOUN PHRASES --> 2 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['2008.', 'http', ':', '//cseweb', '.']

 TOTAL PORTER STEM WORDS ==> 5



 ---- SNOWBALL STEMMING ----

['2008.', 'http', ':', '//cseweb', '.']

 TOTAL SNOWBALL STEM WORDS ==> 5



 ---- LEMMATIZATION ----

['2008.', 'http', ':', '//cseweb', '.']

 TOTAL LEMMATIZE WORDS ==> 5

************************************************************************************************************************

762 --> ucsd.edu/welkan/250B/cikmtutorial.pdf (accessed 28 Jun 2011). 


 ---- TOKENS ----

 ['ucsd.edu/welkan/250B/cikmtutorial.pdf', '(', 'accessed', '28', 'Jun', '2011', ')', '.'] 

 TOTAL TOKENS ==> 8

 ---- POST ----

 [('ucsd.edu/welkan/250B/cikmtutorial.pdf', 'NN'), ('(', '('), ('accessed', 'JJ'), ('28', 'CD'), ('Jun', 'NNP'), ('2011', 'CD'), (')', ')'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['ucsd.edu/welkan/250B/cikmtutorial.pdf', '(', 'accessed', '28', 'Jun', '2011', ')', '.']

 TOTAL FILTERED TOKENS ==>  8

 ---- POST FOR FILTERED TOKENS ----

 [('ucsd.edu/welkan/250B/cikmtutorial.pdf', 'NN'), ('(', '('), ('accessed', 'JJ'), ('28', 'CD'), ('Jun', 'NNP'), ('2011', 'CD'), (')', ')'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['ucsd.edu/welkan/250B/cikmtutorial.pdf (', '( accessed', 'accessed 28', '28 Jun', 'Jun 2011', '2011 )', ') .'] 

 TOTAL BIGRAMS --> 7 



 ---- TRI-GRAMS ---- 

 ['ucsd.edu/welkan/250B/cikmtutorial.pdf ( accessed', '( accessed 28', 'accessed 28 Jun', '28 Jun 2011', 'Jun 2011 )', '2011 ) .'] 

 TOTAL TRIGRAMS --> 6 



 ---- NOUN PHRASES ---- 

 ['ucsd.edu'] 

 TOTAL NOUN PHRASES --> 1 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['ucsd.edu/welkan/250b/cikmtutorial.pdf', '(', 'access', '28', 'jun', '2011', ')', '.']

 TOTAL PORTER STEM WORDS ==> 8



 ---- SNOWBALL STEMMING ----

['ucsd.edu/welkan/250b/cikmtutorial.pdf', '(', 'access', '28', 'jun', '2011', ')', '.']

 TOTAL SNOWBALL STEM WORDS ==> 8



 ---- LEMMATIZATION ----

['ucsd.edu/welkan/250B/cikmtutorial.pdf', '(', 'accessed', '28', 'Jun', '2011', ')', '.']

 TOTAL LEMMATIZE WORDS ==> 8

************************************************************************************************************************

763 --> 62. 


 ---- TOKENS ----

 ['62', '.'] 

 TOTAL TOKENS ==> 2

 ---- POST ----

 [('62', 'CD'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['62', '.']

 TOTAL FILTERED TOKENS ==>  2

 ---- POST FOR FILTERED TOKENS ----

 [('62', 'CD'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['62 .'] 

 TOTAL BIGRAMS --> 1 



 ---- TRI-GRAMS ---- 

 [] 

 TOTAL TRIGRAMS --> 0 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['62', '.']

 TOTAL PORTER STEM WORDS ==> 2



 ---- SNOWBALL STEMMING ----

['62', '.']

 TOTAL SNOWBALL STEM WORDS ==> 2



 ---- LEMMATIZATION ----

['62', '.']

 TOTAL LEMMATIZE WORDS ==> 2

************************************************************************************************************************

764 --> Hearst MA, Dumais ST,  Osman E, et al. 


 ---- TOKENS ----

 ['Hearst', 'MA', ',', 'Dumais', 'ST', ',', 'Osman', 'E', ',', 'et', 'al', '.'] 

 TOTAL TOKENS ==> 12

 ---- POST ----

 [('Hearst', 'NNP'), ('MA', 'NNP'), (',', ','), ('Dumais', 'NNP'), ('ST', 'NNP'), (',', ','), ('Osman', 'NNP'), ('E', 'NNP'), (',', ','), ('et', 'RB'), ('al', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Hearst', ',', 'Dumais', 'ST', ',', 'Osman', 'E', ',', 'et', 'al', '.']

 TOTAL FILTERED TOKENS ==>  11

 ---- POST FOR FILTERED TOKENS ----

 [('Hearst', 'NNP'), (',', ','), ('Dumais', 'NNP'), ('ST', 'NNP'), (',', ','), ('Osman', 'NNP'), ('E', 'NNP'), (',', ','), ('et', 'RB'), ('al', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Hearst ,', ', Dumais', 'Dumais ST', 'ST ,', ', Osman', 'Osman E', 'E ,', ', et', 'et al', 'al .'] 

 TOTAL BIGRAMS --> 10 



 ---- TRI-GRAMS ---- 

 ['Hearst , Dumais', ', Dumais ST', 'Dumais ST ,', 'ST , Osman', ', Osman E', 'Osman E ,', 'E , et', ', et al', 'et al .'] 

 TOTAL TRIGRAMS --> 9 



 ---- NOUN PHRASES ---- 

 ['al'] 

 TOTAL NOUN PHRASES --> 1 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> ['Dumais ST', 'Osman E']
 TOTAL PERSON ENTITY --> 2 


 GPE ---> ['Hearst']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['hearst', ',', 'dumai', 'st', ',', 'osman', 'e', ',', 'et', 'al', '.']

 TOTAL PORTER STEM WORDS ==> 11



 ---- SNOWBALL STEMMING ----

['hearst', ',', 'dumai', 'st', ',', 'osman', 'e', ',', 'et', 'al', '.']

 TOTAL SNOWBALL STEM WORDS ==> 11



 ---- LEMMATIZATION ----

['Hearst', ',', 'Dumais', 'ST', ',', 'Osman', 'E', ',', 'et', 'al', '.']

 TOTAL LEMMATIZE WORDS ==> 11

************************************************************************************************************************

765 --> Support vector machines]  [101] [Jurafsky D, Martin JH. 


 ---- TOKENS ----

 ['Support', 'vector', 'machines', ']', '[', '101', ']', '[', 'Jurafsky', 'D', ',', 'Martin', 'JH', '.'] 

 TOTAL TOKENS ==> 14

 ---- POST ----

 [('Support', 'NNP'), ('vector', 'NN'), ('machines', 'NNS'), (']', 'NNP'), ('[', 'VBD'), ('101', 'CD'), (']', 'NNP'), ('[', 'NNP'), ('Jurafsky', 'NNP'), ('D', 'NNP'), (',', ','), ('Martin', 'NNP'), ('JH', 'NNP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Support', 'vector', 'machines', ']', '[', '101', ']', '[', 'Jurafsky', ',', 'Martin', 'JH', '.']

 TOTAL FILTERED TOKENS ==>  13

 ---- POST FOR FILTERED TOKENS ----

 [('Support', 'NNP'), ('vector', 'NN'), ('machines', 'NNS'), (']', 'NNP'), ('[', 'VBD'), ('101', 'CD'), (']', 'NNP'), ('[', 'NNP'), ('Jurafsky', 'NNP'), (',', ','), ('Martin', 'NNP'), ('JH', 'NNP'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Support vector', 'vector machines', 'machines ]', '] [', '[ 101', '101 ]', '] [', '[ Jurafsky', 'Jurafsky ,', ', Martin', 'Martin JH', 'JH .'] 

 TOTAL BIGRAMS --> 12 



 ---- TRI-GRAMS ---- 

 ['Support vector machines', 'vector machines ]', 'machines ] [', '] [ 101', '[ 101 ]', '101 ] [', '] [ Jurafsky', '[ Jurafsky ,', 'Jurafsky , Martin', ', Martin JH', 'Martin JH .'] 

 TOTAL TRIGRAMS --> 11 



 ---- NOUN PHRASES ---- 

 ['vector'] 

 TOTAL NOUN PHRASES --> 1 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> ['Martin JH']
 TOTAL PERSON ENTITY --> 1 


 GPE ---> ['Support']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['support', 'vector', 'machin', ']', '[', '101', ']', '[', 'jurafski', ',', 'martin', 'jh', '.']

 TOTAL PORTER STEM WORDS ==> 13



 ---- SNOWBALL STEMMING ----

['support', 'vector', 'machin', ']', '[', '101', ']', '[', 'jurafski', ',', 'martin', 'jh', '.']

 TOTAL SNOWBALL STEM WORDS ==> 13



 ---- LEMMATIZATION ----

['Support', 'vector', 'machine', ']', '[', '101', ']', '[', 'Jurafsky', ',', 'Martin', 'JH', '.']

 TOTAL LEMMATIZE WORDS ==> 13

************************************************************************************************************************

766 --> Speech and Language Processing. 


 ---- TOKENS ----

 ['Speech', 'and', 'Language', 'Processing', '.'] 

 TOTAL TOKENS ==> 5

 ---- POST ----

 [('Speech', 'NN'), ('and', 'CC'), ('Language', 'NNP'), ('Processing', 'NNP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Speech', 'Language', 'Processing', '.']

 TOTAL FILTERED TOKENS ==>  4

 ---- POST FOR FILTERED TOKENS ----

 [('Speech', 'NNP'), ('Language', 'NNP'), ('Processing', 'NNP'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Speech Language', 'Language Processing', 'Processing .'] 

 TOTAL BIGRAMS --> 3 



 ---- TRI-GRAMS ---- 

 ['Speech Language Processing', 'Language Processing .'] 

 TOTAL TRIGRAMS --> 2 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> ['Speech', 'Language Processing']
 TOTAL PERSON ENTITY --> 2 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['speech', 'languag', 'process', '.']

 TOTAL PORTER STEM WORDS ==> 4



 ---- SNOWBALL STEMMING ----

['speech', 'languag', 'process', '.']

 TOTAL SNOWBALL STEM WORDS ==> 4



 ---- LEMMATIZATION ----

['Speech', 'Language', 'Processing', '.']

 TOTAL LEMMATIZE WORDS ==> 4

************************************************************************************************************************

767 --> 2nd edn. 


 ---- TOKENS ----

 ['2nd', 'edn', '.'] 

 TOTAL TOKENS ==> 3

 ---- POST ----

 [('2nd', 'CD'), ('edn', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['2nd', 'edn', '.']

 TOTAL FILTERED TOKENS ==>  3

 ---- POST FOR FILTERED TOKENS ----

 [('2nd', 'CD'), ('edn', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['2nd edn', 'edn .'] 

 TOTAL BIGRAMS --> 2 



 ---- TRI-GRAMS ---- 

 ['2nd edn .'] 

 TOTAL TRIGRAMS --> 1 



 ---- NOUN PHRASES ---- 

 ['edn'] 

 TOTAL NOUN PHRASES --> 1 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['2nd', 'edn', '.']

 TOTAL PORTER STEM WORDS ==> 3



 ---- SNOWBALL STEMMING ----

['2nd', 'edn', '.']

 TOTAL SNOWBALL STEM WORDS ==> 3



 ---- LEMMATIZATION ----

['2nd', 'edn', '.']

 TOTAL LEMMATIZE WORDS ==> 3

************************************************************************************************************************

768 --> Englewood Cliffs,  NJ: Prentice-Hall, 2008.] 


 ---- TOKENS ----

 ['Englewood', 'Cliffs', ',', 'NJ', ':', 'Prentice-Hall', ',', '2008', '.', ']'] 

 TOTAL TOKENS ==> 10

 ---- POST ----

 [('Englewood', 'NNP'), ('Cliffs', 'NNP'), (',', ','), ('NJ', 'NNP'), (':', ':'), ('Prentice-Hall', 'NN'), (',', ','), ('2008', 'CD'), ('.', '.'), (']', 'VB')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Englewood', 'Cliffs', ',', 'NJ', ':', 'Prentice-Hall', ',', '2008', '.', ']']

 TOTAL FILTERED TOKENS ==>  10

 ---- POST FOR FILTERED TOKENS ----

 [('Englewood', 'NNP'), ('Cliffs', 'NNP'), (',', ','), ('NJ', 'NNP'), (':', ':'), ('Prentice-Hall', 'NN'), (',', ','), ('2008', 'CD'), ('.', '.'), (']', 'VB')] 



 ---- BI-GRAMS ---- 

 ['Englewood Cliffs', 'Cliffs ,', ', NJ', 'NJ :', ': Prentice-Hall', 'Prentice-Hall ,', ', 2008', '2008 .', '. ]'] 

 TOTAL BIGRAMS --> 9 



 ---- TRI-GRAMS ---- 

 ['Englewood Cliffs ,', 'Cliffs , NJ', ', NJ :', 'NJ : Prentice-Hall', ': Prentice-Hall ,', 'Prentice-Hall , 2008', ', 2008 .', '2008 . ]'] 

 TOTAL TRIGRAMS --> 8 



 ---- NOUN PHRASES ---- 

 ['Prentice-Hall'] 

 TOTAL NOUN PHRASES --> 1 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> ['Englewood', 'Cliffs']
 TOTAL PERSON ENTITY --> 2 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['englewood', 'cliff', ',', 'nj', ':', 'prentice-hal', ',', '2008', '.', ']']

 TOTAL PORTER STEM WORDS ==> 10



 ---- SNOWBALL STEMMING ----

['englewood', 'cliff', ',', 'nj', ':', 'prentice-hal', ',', '2008', '.', ']']

 TOTAL SNOWBALL STEM WORDS ==> 10



 ---- LEMMATIZATION ----

['Englewood', 'Cliffs', ',', 'NJ', ':', 'Prentice-Hall', ',', '2008', '.', ']']

 TOTAL LEMMATIZE WORDS ==> 10

************************************************************************************************************************

769 --> [102] [Sonnhammer ELL, Eddy SR, Birney E, et al. 


 ---- TOKENS ----

 ['[', '102', ']', '[', 'Sonnhammer', 'ELL', ',', 'Eddy', 'SR', ',', 'Birney', 'E', ',', 'et', 'al', '.'] 

 TOTAL TOKENS ==> 16

 ---- POST ----

 [('[', 'RB'), ('102', 'CD'), (']', 'JJ'), ('[', 'NNP'), ('Sonnhammer', 'NNP'), ('ELL', 'NNP'), (',', ','), ('Eddy', 'NNP'), ('SR', 'NNP'), (',', ','), ('Birney', 'NNP'), ('E', 'NNP'), (',', ','), ('et', 'RB'), ('al', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['[', '102', ']', '[', 'Sonnhammer', 'ELL', ',', 'Eddy', 'SR', ',', 'Birney', 'E', ',', 'et', 'al', '.']

 TOTAL FILTERED TOKENS ==>  16

 ---- POST FOR FILTERED TOKENS ----

 [('[', 'RB'), ('102', 'CD'), (']', 'JJ'), ('[', 'NNP'), ('Sonnhammer', 'NNP'), ('ELL', 'NNP'), (',', ','), ('Eddy', 'NNP'), ('SR', 'NNP'), (',', ','), ('Birney', 'NNP'), ('E', 'NNP'), (',', ','), ('et', 'RB'), ('al', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['[ 102', '102 ]', '] [', '[ Sonnhammer', 'Sonnhammer ELL', 'ELL ,', ', Eddy', 'Eddy SR', 'SR ,', ', Birney', 'Birney E', 'E ,', ', et', 'et al', 'al .'] 

 TOTAL BIGRAMS --> 15 



 ---- TRI-GRAMS ---- 

 ['[ 102 ]', '102 ] [', '] [ Sonnhammer', '[ Sonnhammer ELL', 'Sonnhammer ELL ,', 'ELL , Eddy', ', Eddy SR', 'Eddy SR ,', 'SR , Birney', ', Birney E', 'Birney E ,', 'E , et', ', et al', 'et al .'] 

 TOTAL TRIGRAMS --> 14 



 ---- NOUN PHRASES ---- 

 ['al'] 

 TOTAL NOUN PHRASES --> 1 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> ['Eddy SR', 'Birney E']
 TOTAL PERSON ENTITY --> 2 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['[', '102', ']', '[', 'sonnhamm', 'ell', ',', 'eddi', 'sr', ',', 'birney', 'e', ',', 'et', 'al', '.']

 TOTAL PORTER STEM WORDS ==> 16



 ---- SNOWBALL STEMMING ----

['[', '102', ']', '[', 'sonnhamm', 'ell', ',', 'eddi', 'sr', ',', 'birney', 'e', ',', 'et', 'al', '.']

 TOTAL SNOWBALL STEM WORDS ==> 16



 ---- LEMMATIZATION ----

['[', '102', ']', '[', 'Sonnhammer', 'ELL', ',', 'Eddy', 'SR', ',', 'Birney', 'E', ',', 'et', 'al', '.']

 TOTAL LEMMATIZE WORDS ==> 16

************************************************************************************************************************

770 --> Pfam: Multiple sequence alignments and  HMM-profiles of protein domains. 


 ---- TOKENS ----

 ['Pfam', ':', 'Multiple', 'sequence', 'alignments', 'and', 'HMM-profiles', 'of', 'protein', 'domains', '.'] 

 TOTAL TOKENS ==> 11

 ---- POST ----

 [('Pfam', 'NN'), (':', ':'), ('Multiple', 'JJ'), ('sequence', 'NN'), ('alignments', 'NNS'), ('and', 'CC'), ('HMM-profiles', 'NNS'), ('of', 'IN'), ('protein', 'NN'), ('domains', 'NNS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Pfam', ':', 'Multiple', 'sequence', 'alignments', 'HMM-profiles', 'protein', 'domains', '.']

 TOTAL FILTERED TOKENS ==>  9

 ---- POST FOR FILTERED TOKENS ----

 [('Pfam', 'NN'), (':', ':'), ('Multiple', 'JJ'), ('sequence', 'NN'), ('alignments', 'NNS'), ('HMM-profiles', 'NNP'), ('protein', 'NN'), ('domains', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Pfam :', ': Multiple', 'Multiple sequence', 'sequence alignments', 'alignments HMM-profiles', 'HMM-profiles protein', 'protein domains', 'domains .'] 

 TOTAL BIGRAMS --> 8 



 ---- TRI-GRAMS ---- 

 ['Pfam : Multiple', ': Multiple sequence', 'Multiple sequence alignments', 'sequence alignments HMM-profiles', 'alignments HMM-profiles protein', 'HMM-profiles protein domains', 'protein domains .'] 

 TOTAL TRIGRAMS --> 7 



 ---- NOUN PHRASES ---- 

 ['Pfam', 'Multiple sequence', 'protein'] 

 TOTAL NOUN PHRASES --> 3 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> ['Pfam']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['pfam', ':', 'multipl', 'sequenc', 'align', 'hmm-profil', 'protein', 'domain', '.']

 TOTAL PORTER STEM WORDS ==> 9



 ---- SNOWBALL STEMMING ----

['pfam', ':', 'multipl', 'sequenc', 'align', 'hmm-profil', 'protein', 'domain', '.']

 TOTAL SNOWBALL STEM WORDS ==> 9



 ---- LEMMATIZATION ----

['Pfam', ':', 'Multiple', 'sequence', 'alignment', 'HMM-profiles', 'protein', 'domain', '.']

 TOTAL LEMMATIZE WORDS ==> 9

************************************************************************************************************************

771 --> Nucleic Acids Res 1998;26:320]  [103] [Sonnhammer, E. L., Eddy, S. R., Birney, E., Bateman, A., & Durbin, R. (1998). 


 ---- TOKENS ----

 ['Nucleic', 'Acids', 'Res', '1998', ';', '26:320', ']', '[', '103', ']', '[', 'Sonnhammer', ',', 'E.', 'L.', ',', 'Eddy', ',', 'S.', 'R.', ',', 'Birney', ',', 'E.', ',', 'Bateman', ',', 'A.', ',', '&', 'Durbin', ',', 'R.', '(', '1998', ')', '.'] 

 TOTAL TOKENS ==> 37

 ---- POST ----

 [('Nucleic', 'NNP'), ('Acids', 'NNP'), ('Res', 'NNP'), ('1998', 'CD'), (';', ':'), ('26:320', 'CD'), (']', 'NNP'), ('[', 'VBD'), ('103', 'CD'), (']', 'NNP'), ('[', 'NNP'), ('Sonnhammer', 'NNP'), (',', ','), ('E.', 'NNP'), ('L.', 'NNP'), (',', ','), ('Eddy', 'NNP'), (',', ','), ('S.', 'NNP'), ('R.', 'NNP'), (',', ','), ('Birney', 'NNP'), (',', ','), ('E.', 'NNP'), (',', ','), ('Bateman', 'NNP'), (',', ','), ('A.', 'NNP'), (',', ','), ('&', 'CC'), ('Durbin', 'NNP'), (',', ','), ('R.', 'NNP'), ('(', '('), ('1998', 'CD'), (')', ')'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Nucleic', 'Acids', 'Res', '1998', ';', '26:320', ']', '[', '103', ']', '[', 'Sonnhammer', ',', 'E.', 'L.', ',', 'Eddy', ',', 'S.', 'R.', ',', 'Birney', ',', 'E.', ',', 'Bateman', ',', 'A.', ',', '&', 'Durbin', ',', 'R.', '(', '1998', ')', '.']

 TOTAL FILTERED TOKENS ==>  37

 ---- POST FOR FILTERED TOKENS ----

 [('Nucleic', 'NNP'), ('Acids', 'NNP'), ('Res', 'NNP'), ('1998', 'CD'), (';', ':'), ('26:320', 'CD'), (']', 'NNP'), ('[', 'VBD'), ('103', 'CD'), (']', 'NNP'), ('[', 'NNP'), ('Sonnhammer', 'NNP'), (',', ','), ('E.', 'NNP'), ('L.', 'NNP'), (',', ','), ('Eddy', 'NNP'), (',', ','), ('S.', 'NNP'), ('R.', 'NNP'), (',', ','), ('Birney', 'NNP'), (',', ','), ('E.', 'NNP'), (',', ','), ('Bateman', 'NNP'), (',', ','), ('A.', 'NNP'), (',', ','), ('&', 'CC'), ('Durbin', 'NNP'), (',', ','), ('R.', 'NNP'), ('(', '('), ('1998', 'CD'), (')', ')'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Nucleic Acids', 'Acids Res', 'Res 1998', '1998 ;', '; 26:320', '26:320 ]', '] [', '[ 103', '103 ]', '] [', '[ Sonnhammer', 'Sonnhammer ,', ', E.', 'E. L.', 'L. ,', ', Eddy', 'Eddy ,', ', S.', 'S. R.', 'R. ,', ', Birney', 'Birney ,', ', E.', 'E. ,', ', Bateman', 'Bateman ,', ', A.', 'A. ,', ', &', '& Durbin', 'Durbin ,', ', R.', 'R. (', '( 1998', '1998 )', ') .'] 

 TOTAL BIGRAMS --> 36 



 ---- TRI-GRAMS ---- 

 ['Nucleic Acids Res', 'Acids Res 1998', 'Res 1998 ;', '1998 ; 26:320', '; 26:320 ]', '26:320 ] [', '] [ 103', '[ 103 ]', '103 ] [', '] [ Sonnhammer', '[ Sonnhammer ,', 'Sonnhammer , E.', ', E. L.', 'E. L. ,', 'L. , Eddy', ', Eddy ,', 'Eddy , S.', ', S. R.', 'S. R. ,', 'R. , Birney', ', Birney ,', 'Birney , E.', ', E. ,', 'E. , Bateman', ', Bateman ,', 'Bateman , A.', ', A. ,', 'A. , &', ', & Durbin', '& Durbin ,', 'Durbin , R.', ', R. (', 'R. ( 1998', '( 1998 )', '1998 ) .'] 

 TOTAL TRIGRAMS --> 35 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> ['Nucleic', 'Acids Res', 'Eddy', 'Bateman', 'Durbin']
 TOTAL PERSON ENTITY --> 5 


 GPE ---> ['Birney']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['nucleic', 'acid', 're', '1998', ';', '26:320', ']', '[', '103', ']', '[', 'sonnhamm', ',', 'e.', 'l.', ',', 'eddi', ',', 's.', 'r.', ',', 'birney', ',', 'e.', ',', 'bateman', ',', 'a.', ',', '&', 'durbin', ',', 'r.', '(', '1998', ')', '.']

 TOTAL PORTER STEM WORDS ==> 37



 ---- SNOWBALL STEMMING ----

['nucleic', 'acid', 'res', '1998', ';', '26:320', ']', '[', '103', ']', '[', 'sonnhamm', ',', 'e.', 'l.', ',', 'eddi', ',', 's.', 'r.', ',', 'birney', ',', 'e.', ',', 'bateman', ',', 'a.', ',', '&', 'durbin', ',', 'r.', '(', '1998', ')', '.']

 TOTAL SNOWBALL STEM WORDS ==> 37



 ---- LEMMATIZATION ----

['Nucleic', 'Acids', 'Res', '1998', ';', '26:320', ']', '[', '103', ']', '[', 'Sonnhammer', ',', 'E.', 'L.', ',', 'Eddy', ',', 'S.', 'R.', ',', 'Birney', ',', 'E.', ',', 'Bateman', ',', 'A.', ',', '&', 'Durbin', ',', 'R.', '(', '1998', ')', '.']

 TOTAL LEMMATIZE WORDS ==> 37

************************************************************************************************************************

772 --> Pfam:  multiple sequence alignments and HMM-profiles of protein domains. 


 ---- TOKENS ----

 ['Pfam', ':', 'multiple', 'sequence', 'alignments', 'and', 'HMM-profiles', 'of', 'protein', 'domains', '.'] 

 TOTAL TOKENS ==> 11

 ---- POST ----

 [('Pfam', 'NN'), (':', ':'), ('multiple', 'JJ'), ('sequence', 'NN'), ('alignments', 'NNS'), ('and', 'CC'), ('HMM-profiles', 'NNS'), ('of', 'IN'), ('protein', 'NN'), ('domains', 'NNS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Pfam', ':', 'multiple', 'sequence', 'alignments', 'HMM-profiles', 'protein', 'domains', '.']

 TOTAL FILTERED TOKENS ==>  9

 ---- POST FOR FILTERED TOKENS ----

 [('Pfam', 'NN'), (':', ':'), ('multiple', 'JJ'), ('sequence', 'NN'), ('alignments', 'NNS'), ('HMM-profiles', 'NNP'), ('protein', 'NN'), ('domains', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Pfam :', ': multiple', 'multiple sequence', 'sequence alignments', 'alignments HMM-profiles', 'HMM-profiles protein', 'protein domains', 'domains .'] 

 TOTAL BIGRAMS --> 8 



 ---- TRI-GRAMS ---- 

 ['Pfam : multiple', ': multiple sequence', 'multiple sequence alignments', 'sequence alignments HMM-profiles', 'alignments HMM-profiles protein', 'HMM-profiles protein domains', 'protein domains .'] 

 TOTAL TRIGRAMS --> 7 



 ---- NOUN PHRASES ---- 

 ['Pfam', 'multiple sequence', 'protein'] 

 TOTAL NOUN PHRASES --> 3 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> ['Pfam']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['pfam', ':', 'multipl', 'sequenc', 'align', 'hmm-profil', 'protein', 'domain', '.']

 TOTAL PORTER STEM WORDS ==> 9



 ---- SNOWBALL STEMMING ----

['pfam', ':', 'multipl', 'sequenc', 'align', 'hmm-profil', 'protein', 'domain', '.']

 TOTAL SNOWBALL STEM WORDS ==> 9



 ---- LEMMATIZATION ----

['Pfam', ':', 'multiple', 'sequence', 'alignment', 'HMM-profiles', 'protein', 'domain', '.']

 TOTAL LEMMATIZE WORDS ==> 9

************************************************************************************************************************

773 --> Nucleic acids  research, 26(1), 320-322]  [104] Systems, RAVN. 


 ---- TOKENS ----

 ['Nucleic', 'acids', 'research', ',', '26', '(', '1', ')', ',', '320-322', ']', '[', '104', ']', 'Systems', ',', 'RAVN', '.'] 

 TOTAL TOKENS ==> 18

 ---- POST ----

 [('Nucleic', 'NNP'), ('acids', 'NNS'), ('research', 'NN'), (',', ','), ('26', 'CD'), ('(', '('), ('1', 'CD'), (')', ')'), (',', ','), ('320-322', 'JJ'), (']', 'NNP'), ('[', 'NNP'), ('104', 'CD'), (']', 'NNP'), ('Systems', 'NNPS'), (',', ','), ('RAVN', 'NNP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Nucleic', 'acids', 'research', ',', '26', '(', '1', ')', ',', '320-322', ']', '[', '104', ']', 'Systems', ',', 'RAVN', '.']

 TOTAL FILTERED TOKENS ==>  18

 ---- POST FOR FILTERED TOKENS ----

 [('Nucleic', 'NNP'), ('acids', 'NNS'), ('research', 'NN'), (',', ','), ('26', 'CD'), ('(', '('), ('1', 'CD'), (')', ')'), (',', ','), ('320-322', 'JJ'), (']', 'NNP'), ('[', 'NNP'), ('104', 'CD'), (']', 'NNP'), ('Systems', 'NNPS'), (',', ','), ('RAVN', 'NNP'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Nucleic acids', 'acids research', 'research ,', ', 26', '26 (', '( 1', '1 )', ') ,', ', 320-322', '320-322 ]', '] [', '[ 104', '104 ]', '] Systems', 'Systems ,', ', RAVN', 'RAVN .'] 

 TOTAL BIGRAMS --> 17 



 ---- TRI-GRAMS ---- 

 ['Nucleic acids research', 'acids research ,', 'research , 26', ', 26 (', '26 ( 1', '( 1 )', '1 ) ,', ') , 320-322', ', 320-322 ]', '320-322 ] [', '] [ 104', '[ 104 ]', '104 ] Systems', '] Systems ,', 'Systems , RAVN', ', RAVN .'] 

 TOTAL TRIGRAMS --> 16 



 ---- NOUN PHRASES ---- 

 ['research'] 

 TOTAL NOUN PHRASES --> 1 



 ---- NER ----

 
 ORGANIZATION ---> ['RAVN']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> ['Nucleic']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['nucleic', 'acid', 'research', ',', '26', '(', '1', ')', ',', '320-322', ']', '[', '104', ']', 'system', ',', 'ravn', '.']

 TOTAL PORTER STEM WORDS ==> 18



 ---- SNOWBALL STEMMING ----

['nucleic', 'acid', 'research', ',', '26', '(', '1', ')', ',', '320-322', ']', '[', '104', ']', 'system', ',', 'ravn', '.']

 TOTAL SNOWBALL STEM WORDS ==> 18



 ---- LEMMATIZATION ----

['Nucleic', 'acid', 'research', ',', '26', '(', '1', ')', ',', '320-322', ']', '[', '104', ']', 'Systems', ',', 'RAVN', '.']

 TOTAL LEMMATIZE WORDS ==> 18

************************************************************************************************************************

774 --> "RAVN Systems Launch the ACE Powered GDPR Robot - Artificial  Intelligence to Expedite GDPR Compliance." 


 ---- TOKENS ----

 ['``', 'RAVN', 'Systems', 'Launch', 'the', 'ACE', 'Powered', 'GDPR', 'Robot', '-', 'Artificial', 'Intelligence', 'to', 'Expedite', 'GDPR', 'Compliance', '.', "''"] 

 TOTAL TOKENS ==> 18

 ---- POST ----

 [('``', '``'), ('RAVN', 'JJ'), ('Systems', 'NNP'), ('Launch', 'NNP'), ('the', 'DT'), ('ACE', 'NNP'), ('Powered', 'NNP'), ('GDPR', 'NNP'), ('Robot', 'NNP'), ('-', ':'), ('Artificial', 'JJ'), ('Intelligence', 'NN'), ('to', 'TO'), ('Expedite', 'NNP'), ('GDPR', 'NNP'), ('Compliance', 'NNP'), ('.', '.'), ("''", "''")] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['``', 'RAVN', 'Systems', 'Launch', 'ACE', 'Powered', 'GDPR', 'Robot', '-', 'Artificial', 'Intelligence', 'Expedite', 'GDPR', 'Compliance', '.', "''"]

 TOTAL FILTERED TOKENS ==>  16

 ---- POST FOR FILTERED TOKENS ----

 [('``', '``'), ('RAVN', 'JJ'), ('Systems', 'NNP'), ('Launch', 'NNP'), ('ACE', 'NNP'), ('Powered', 'NNP'), ('GDPR', 'NNP'), ('Robot', 'NNP'), ('-', ':'), ('Artificial', 'JJ'), ('Intelligence', 'NNP'), ('Expedite', 'NNP'), ('GDPR', 'NNP'), ('Compliance', 'NNP'), ('.', '.'), ("''", "''")] 



 ---- BI-GRAMS ---- 

 ['`` RAVN', 'RAVN Systems', 'Systems Launch', 'Launch ACE', 'ACE Powered', 'Powered GDPR', 'GDPR Robot', 'Robot -', '- Artificial', 'Artificial Intelligence', 'Intelligence Expedite', 'Expedite GDPR', 'GDPR Compliance', 'Compliance .', ". ''"] 

 TOTAL BIGRAMS --> 15 



 ---- TRI-GRAMS ---- 

 ['`` RAVN Systems', 'RAVN Systems Launch', 'Systems Launch ACE', 'Launch ACE Powered', 'ACE Powered GDPR', 'Powered GDPR Robot', 'GDPR Robot -', 'Robot - Artificial', '- Artificial Intelligence', 'Artificial Intelligence Expedite', 'Intelligence Expedite GDPR', 'Expedite GDPR Compliance', 'GDPR Compliance .', "Compliance . ''"] 

 TOTAL TRIGRAMS --> 14 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> ['RAVN Systems Launch', 'Artificial Intelligence Expedite']
 TOTAL ORGANIZATION ENTITY --> 2 


 PERSON ---> ['Robot']
 TOTAL PERSON ENTITY --> 1 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['``', 'ravn', 'system', 'launch', 'ace', 'power', 'gdpr', 'robot', '-', 'artifici', 'intellig', 'expedit', 'gdpr', 'complianc', '.', "''"]

 TOTAL PORTER STEM WORDS ==> 16



 ---- SNOWBALL STEMMING ----

['``', 'ravn', 'system', 'launch', 'ace', 'power', 'gdpr', 'robot', '-', 'artifici', 'intellig', 'expedit', 'gdpr', 'complianc', '.', "''"]

 TOTAL SNOWBALL STEM WORDS ==> 16



 ---- LEMMATIZATION ----

['``', 'RAVN', 'Systems', 'Launch', 'ACE', 'Powered', 'GDPR', 'Robot', '-', 'Artificial', 'Intelligence', 'Expedite', 'GDPR', 'Compliance', '.', "''"]

 TOTAL LEMMATIZE WORDS ==> 16

************************************************************************************************************************

775 --> Stock Market. 


 ---- TOKENS ----

 ['Stock', 'Market', '.'] 

 TOTAL TOKENS ==> 3

 ---- POST ----

 [('Stock', 'NN'), ('Market', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Stock', 'Market', '.']

 TOTAL FILTERED TOKENS ==>  3

 ---- POST FOR FILTERED TOKENS ----

 [('Stock', 'NN'), ('Market', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Stock Market', 'Market .'] 

 TOTAL BIGRAMS --> 2 



 ---- TRI-GRAMS ---- 

 ['Stock Market .'] 

 TOTAL TRIGRAMS --> 1 



 ---- NOUN PHRASES ---- 

 ['Stock', 'Market'] 

 TOTAL NOUN PHRASES --> 2 



 ---- NER ----

 
 ORGANIZATION ---> ['Market']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> ['Stock']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['stock', 'market', '.']

 TOTAL PORTER STEM WORDS ==> 3



 ---- SNOWBALL STEMMING ----

['stock', 'market', '.']

 TOTAL SNOWBALL STEM WORDS ==> 3



 ---- LEMMATIZATION ----

['Stock', 'Market', '.']

 TOTAL LEMMATIZE WORDS ==> 3

************************************************************************************************************************

776 --> PR Newswire, n.d. 


 ---- TOKENS ----

 ['PR', 'Newswire', ',', 'n.d', '.'] 

 TOTAL TOKENS ==> 5

 ---- POST ----

 [('PR', 'NNP'), ('Newswire', 'NNP'), (',', ','), ('n.d', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['PR', 'Newswire', ',', 'n.d', '.']

 TOTAL FILTERED TOKENS ==>  5

 ---- POST FOR FILTERED TOKENS ----

 [('PR', 'NNP'), ('Newswire', 'NNP'), (',', ','), ('n.d', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['PR Newswire', 'Newswire ,', ', n.d', 'n.d .'] 

 TOTAL BIGRAMS --> 4 



 ---- TRI-GRAMS ---- 

 ['PR Newswire ,', 'Newswire , n.d', ', n.d .'] 

 TOTAL TRIGRAMS --> 3 



 ---- NOUN PHRASES ---- 

 ['n.d'] 

 TOTAL NOUN PHRASES --> 1 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['pr', 'newswir', ',', 'n.d', '.']

 TOTAL PORTER STEM WORDS ==> 5



 ---- SNOWBALL STEMMING ----

['pr', 'newswir', ',', 'n.d', '.']

 TOTAL SNOWBALL STEM WORDS ==> 5



 ---- LEMMATIZATION ----

['PR', 'Newswire', ',', 'n.d', '.']

 TOTAL LEMMATIZE WORDS ==> 5

************************************************************************************************************************

777 --> Web. 


 ---- TOKENS ----

 ['Web', '.'] 

 TOTAL TOKENS ==> 2

 ---- POST ----

 [('Web', 'NNP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Web', '.']

 TOTAL FILTERED TOKENS ==>  2

 ---- POST FOR FILTERED TOKENS ----

 [('Web', 'NNP'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Web .'] 

 TOTAL BIGRAMS --> 1 



 ---- TRI-GRAMS ---- 

 [] 

 TOTAL TRIGRAMS --> 0 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['web', '.']

 TOTAL PORTER STEM WORDS ==> 2



 ---- SNOWBALL STEMMING ----

['web', '.']

 TOTAL SNOWBALL STEM WORDS ==> 2



 ---- LEMMATIZATION ----

['Web', '.']

 TOTAL LEMMATIZE WORDS ==> 2

************************************************************************************************************************

778 --> 19  Mar. 


 ---- TOKENS ----

 ['19', 'Mar', '.'] 

 TOTAL TOKENS ==> 3

 ---- POST ----

 [('19', 'CD'), ('Mar', 'NNP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['19', 'Mar', '.']

 TOTAL FILTERED TOKENS ==>  3

 ---- POST FOR FILTERED TOKENS ----

 [('19', 'CD'), ('Mar', 'NNP'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['19 Mar', 'Mar .'] 

 TOTAL BIGRAMS --> 2 



 ---- TRI-GRAMS ---- 

 ['19 Mar .'] 

 TOTAL TRIGRAMS --> 1 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['19', 'mar', '.']

 TOTAL PORTER STEM WORDS ==> 3



 ---- SNOWBALL STEMMING ----

['19', 'mar', '.']

 TOTAL SNOWBALL STEM WORDS ==> 3



 ---- LEMMATIZATION ----

['19', 'Mar', '.']

 TOTAL LEMMATIZE WORDS ==> 3

************************************************************************************************************************

779 --> 2017. 


 ---- TOKENS ----

 ['2017', '.'] 

 TOTAL TOKENS ==> 2

 ---- POST ----

 [('2017', 'CD'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['2017', '.']

 TOTAL FILTERED TOKENS ==>  2

 ---- POST FOR FILTERED TOKENS ----

 [('2017', 'CD'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['2017 .'] 

 TOTAL BIGRAMS --> 1 



 ---- TRI-GRAMS ---- 

 [] 

 TOTAL TRIGRAMS --> 0 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['2017', '.']

 TOTAL PORTER STEM WORDS ==> 2



 ---- SNOWBALL STEMMING ----

['2017', '.']

 TOTAL SNOWBALL STEM WORDS ==> 2



 ---- LEMMATIZATION ----

['2017', '.']

 TOTAL LEMMATIZE WORDS ==> 2

************************************************************************************************************************

780 --> [105] "Here's Why Natural Language Processing is the Future of BI." 


 ---- TOKENS ----

 ['[', '105', ']', '``', 'Here', "'s", 'Why', 'Natural', 'Language', 'Processing', 'is', 'the', 'Future', 'of', 'BI', '.', "''"] 

 TOTAL TOKENS ==> 17

 ---- POST ----

 [('[', 'RB'), ('105', 'CD'), (']', 'JJ'), ('``', '``'), ('Here', 'RB'), ("'s", 'VBZ'), ('Why', 'WRB'), ('Natural', 'JJ'), ('Language', 'NN'), ('Processing', 'NN'), ('is', 'VBZ'), ('the', 'DT'), ('Future', 'NN'), ('of', 'IN'), ('BI', 'NNP'), ('.', '.'), ("''", "''")] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['[', '105', ']', '``', "'s", 'Natural', 'Language', 'Processing', 'Future', 'BI', '.', "''"]

 TOTAL FILTERED TOKENS ==>  12

 ---- POST FOR FILTERED TOKENS ----

 [('[', 'RB'), ('105', 'CD'), (']', 'NNPS'), ('``', '``'), ("'s", 'POS'), ('Natural', 'JJ'), ('Language', 'NNP'), ('Processing', 'NNP'), ('Future', 'NNP'), ('BI', 'NNP'), ('.', '.'), ("''", "''")] 



 ---- BI-GRAMS ---- 

 ['[ 105', '105 ]', '] ``', "`` 's", "'s Natural", 'Natural Language', 'Language Processing', 'Processing Future', 'Future BI', 'BI .', ". ''"] 

 TOTAL BIGRAMS --> 11 



 ---- TRI-GRAMS ---- 

 ['[ 105 ]', '105 ] ``', "] `` 's", "`` 's Natural", "'s Natural Language", 'Natural Language Processing', 'Language Processing Future', 'Processing Future BI', 'Future BI .', "BI . ''"] 

 TOTAL TRIGRAMS --> 10 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> ['Natural Language']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['[', '105', ']', '``', "'s", 'natur', 'languag', 'process', 'futur', 'bi', '.', "''"]

 TOTAL PORTER STEM WORDS ==> 12



 ---- SNOWBALL STEMMING ----

['[', '105', ']', '``', "'s", 'natur', 'languag', 'process', 'futur', 'bi', '.', "''"]

 TOTAL SNOWBALL STEM WORDS ==> 12



 ---- LEMMATIZATION ----

['[', '105', ']', '``', "'s", 'Natural', 'Language', 'Processing', 'Future', 'BI', '.', "''"]

 TOTAL LEMMATIZE WORDS ==> 12

************************************************************************************************************************

781 --> SmartData Collective. 


 ---- TOKENS ----

 ['SmartData', 'Collective', '.'] 

 TOTAL TOKENS ==> 3

 ---- POST ----

 [('SmartData', 'NNP'), ('Collective', 'NNP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['SmartData', 'Collective', '.']

 TOTAL FILTERED TOKENS ==>  3

 ---- POST FOR FILTERED TOKENS ----

 [('SmartData', 'NNP'), ('Collective', 'NNP'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['SmartData Collective', 'Collective .'] 

 TOTAL BIGRAMS --> 2 



 ---- TRI-GRAMS ---- 

 ['SmartData Collective .'] 

 TOTAL TRIGRAMS --> 1 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> ['SmartData Collective']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['smartdata', 'collect', '.']

 TOTAL PORTER STEM WORDS ==> 3



 ---- SNOWBALL STEMMING ----

['smartdata', 'collect', '.']

 TOTAL SNOWBALL STEM WORDS ==> 3



 ---- LEMMATIZATION ----

['SmartData', 'Collective', '.']

 TOTAL LEMMATIZE WORDS ==> 3

************************************************************************************************************************

782 --> N.p., n.d. 


 ---- TOKENS ----

 ['N.p.', ',', 'n.d', '.'] 

 TOTAL TOKENS ==> 4

 ---- POST ----

 [('N.p.', 'NNP'), (',', ','), ('n.d', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['N.p.', ',', 'n.d', '.']

 TOTAL FILTERED TOKENS ==>  4

 ---- POST FOR FILTERED TOKENS ----

 [('N.p.', 'NNP'), (',', ','), ('n.d', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['N.p. ,', ', n.d', 'n.d .'] 

 TOTAL BIGRAMS --> 3 



 ---- TRI-GRAMS ---- 

 ['N.p. , n.d', ', n.d .'] 

 TOTAL TRIGRAMS --> 2 



 ---- NOUN PHRASES ---- 

 ['n.d'] 

 TOTAL NOUN PHRASES --> 1 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['n.p.', ',', 'n.d', '.']

 TOTAL PORTER STEM WORDS ==> 4



 ---- SNOWBALL STEMMING ----

['n.p.', ',', 'n.d', '.']

 TOTAL SNOWBALL STEM WORDS ==> 4



 ---- LEMMATIZATION ----

['N.p.', ',', 'n.d', '.']

 TOTAL LEMMATIZE WORDS ==> 4

************************************************************************************************************************

783 --> Web. 


 ---- TOKENS ----

 ['Web', '.'] 

 TOTAL TOKENS ==> 2

 ---- POST ----

 [('Web', 'NNP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Web', '.']

 TOTAL FILTERED TOKENS ==>  2

 ---- POST FOR FILTERED TOKENS ----

 [('Web', 'NNP'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Web .'] 

 TOTAL BIGRAMS --> 1 



 ---- TRI-GRAMS ---- 

 [] 

 TOTAL TRIGRAMS --> 0 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['web', '.']

 TOTAL PORTER STEM WORDS ==> 2



 ---- SNOWBALL STEMMING ----

['web', '.']

 TOTAL SNOWBALL STEM WORDS ==> 2



 ---- LEMMATIZATION ----

['Web', '.']

 TOTAL LEMMATIZE WORDS ==> 2

************************************************************************************************************************

784 --> 19 Mar. 


 ---- TOKENS ----

 ['19', 'Mar', '.'] 

 TOTAL TOKENS ==> 3

 ---- POST ----

 [('19', 'CD'), ('Mar', 'NNP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['19', 'Mar', '.']

 TOTAL FILTERED TOKENS ==>  3

 ---- POST FOR FILTERED TOKENS ----

 [('19', 'CD'), ('Mar', 'NNP'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['19 Mar', 'Mar .'] 

 TOTAL BIGRAMS --> 2 



 ---- TRI-GRAMS ---- 

 ['19 Mar .'] 

 TOTAL TRIGRAMS --> 1 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['19', 'mar', '.']

 TOTAL PORTER STEM WORDS ==> 3



 ---- SNOWBALL STEMMING ----

['19', 'mar', '.']

 TOTAL SNOWBALL STEM WORDS ==> 3



 ---- LEMMATIZATION ----

['19', 'Mar', '.']

 TOTAL LEMMATIZE WORDS ==> 3

************************************************************************************************************************

785 --> 2017   [106] "Using Natural Language Processing and Network Analysis to Develop a Conceptual  Framework for Medication Therapy Management Research." 


 ---- TOKENS ----

 ['2017', '[', '106', ']', '``', 'Using', 'Natural', 'Language', 'Processing', 'and', 'Network', 'Analysis', 'to', 'Develop', 'a', 'Conceptual', 'Framework', 'for', 'Medication', 'Therapy', 'Management', 'Research', '.', "''"] 

 TOTAL TOKENS ==> 24

 ---- POST ----

 [('2017', 'CD'), ('[', '$'), ('106', 'CD'), (']', 'NNP'), ('``', '``'), ('Using', 'NNP'), ('Natural', 'NNP'), ('Language', 'NNP'), ('Processing', 'NNP'), ('and', 'CC'), ('Network', 'NNP'), ('Analysis', 'NNP'), ('to', 'TO'), ('Develop', 'VB'), ('a', 'DT'), ('Conceptual', 'NNP'), ('Framework', 'NNP'), ('for', 'IN'), ('Medication', 'NNP'), ('Therapy', 'NNP'), ('Management', 'NNP'), ('Research', 'NNP'), ('.', '.'), ("''", "''")] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['2017', '[', '106', ']', '``', 'Using', 'Natural', 'Language', 'Processing', 'Network', 'Analysis', 'Develop', 'Conceptual', 'Framework', 'Medication', 'Therapy', 'Management', 'Research', '.', "''"]

 TOTAL FILTERED TOKENS ==>  20

 ---- POST FOR FILTERED TOKENS ----

 [('2017', 'CD'), ('[', '$'), ('106', 'CD'), (']', 'NNP'), ('``', '``'), ('Using', 'NNP'), ('Natural', 'NNP'), ('Language', 'NNP'), ('Processing', 'NNP'), ('Network', 'NNP'), ('Analysis', 'NNP'), ('Develop', 'NNP'), ('Conceptual', 'NNP'), ('Framework', 'NNP'), ('Medication', 'NNP'), ('Therapy', 'NNP'), ('Management', 'NNP'), ('Research', 'NNP'), ('.', '.'), ("''", "''")] 



 ---- BI-GRAMS ---- 

 ['2017 [', '[ 106', '106 ]', '] ``', '`` Using', 'Using Natural', 'Natural Language', 'Language Processing', 'Processing Network', 'Network Analysis', 'Analysis Develop', 'Develop Conceptual', 'Conceptual Framework', 'Framework Medication', 'Medication Therapy', 'Therapy Management', 'Management Research', 'Research .', ". ''"] 

 TOTAL BIGRAMS --> 19 



 ---- TRI-GRAMS ---- 

 ['2017 [ 106', '[ 106 ]', '106 ] ``', '] `` Using', '`` Using Natural', 'Using Natural Language', 'Natural Language Processing', 'Language Processing Network', 'Processing Network Analysis', 'Network Analysis Develop', 'Analysis Develop Conceptual', 'Develop Conceptual Framework', 'Conceptual Framework Medication', 'Framework Medication Therapy', 'Medication Therapy Management', 'Therapy Management Research', 'Management Research .', "Research . ''"] 

 TOTAL TRIGRAMS --> 18 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> ['Network Analysis Develop Conceptual Framework Medication Therapy']
 TOTAL PERSON ENTITY --> 1 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['2017', '[', '106', ']', '``', 'use', 'natur', 'languag', 'process', 'network', 'analysi', 'develop', 'conceptu', 'framework', 'medic', 'therapi', 'manag', 'research', '.', "''"]

 TOTAL PORTER STEM WORDS ==> 20



 ---- SNOWBALL STEMMING ----

['2017', '[', '106', ']', '``', 'use', 'natur', 'languag', 'process', 'network', 'analysi', 'develop', 'conceptu', 'framework', 'medic', 'therapi', 'manag', 'research', '.', "''"]

 TOTAL SNOWBALL STEM WORDS ==> 20



 ---- LEMMATIZATION ----

['2017', '[', '106', ']', '``', 'Using', 'Natural', 'Language', 'Processing', 'Network', 'Analysis', 'Develop', 'Conceptual', 'Framework', 'Medication', 'Therapy', 'Management', 'Research', '.', "''"]

 TOTAL LEMMATIZE WORDS ==> 20

************************************************************************************************************************

786 --> AMIA ... 


 ---- TOKENS ----

 ['AMIA', '...'] 

 TOTAL TOKENS ==> 2

 ---- POST ----

 [('AMIA', 'NNS'), ('...', ':')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['AMIA', '...']

 TOTAL FILTERED TOKENS ==>  2

 ---- POST FOR FILTERED TOKENS ----

 [('AMIA', 'NNS'), ('...', ':')] 



 ---- BI-GRAMS ---- 

 ['AMIA ...'] 

 TOTAL BIGRAMS --> 1 



 ---- TRI-GRAMS ---- 

 [] 

 TOTAL TRIGRAMS --> 0 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['amia', '...']

 TOTAL PORTER STEM WORDS ==> 2



 ---- SNOWBALL STEMMING ----

['amia', '...']

 TOTAL SNOWBALL STEM WORDS ==> 2



 ---- LEMMATIZATION ----

['AMIA', '...']

 TOTAL LEMMATIZE WORDS ==> 2

************************************************************************************************************************

787 --> Annual Symposium  proceedings. 


 ---- TOKENS ----

 ['Annual', 'Symposium', 'proceedings', '.'] 

 TOTAL TOKENS ==> 4

 ---- POST ----

 [('Annual', 'JJ'), ('Symposium', 'NNP'), ('proceedings', 'NNS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Annual', 'Symposium', 'proceedings', '.']

 TOTAL FILTERED TOKENS ==>  4

 ---- POST FOR FILTERED TOKENS ----

 [('Annual', 'JJ'), ('Symposium', 'NNP'), ('proceedings', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Annual Symposium', 'Symposium proceedings', 'proceedings .'] 

 TOTAL BIGRAMS --> 3 



 ---- TRI-GRAMS ---- 

 ['Annual Symposium proceedings', 'Symposium proceedings .'] 

 TOTAL TRIGRAMS --> 2 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> ['Symposium']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> ['Annual']
 TOTAL PERSON ENTITY --> 1 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['annual', 'symposium', 'proceed', '.']

 TOTAL PORTER STEM WORDS ==> 4



 ---- SNOWBALL STEMMING ----

['annual', 'symposium', 'proceed', '.']

 TOTAL SNOWBALL STEM WORDS ==> 4



 ---- LEMMATIZATION ----

['Annual', 'Symposium', 'proceeding', '.']

 TOTAL LEMMATIZE WORDS ==> 4

************************************************************************************************************************

788 --> AMIA Symposium. 


 ---- TOKENS ----

 ['AMIA', 'Symposium', '.'] 

 TOTAL TOKENS ==> 3

 ---- POST ----

 [('AMIA', 'NNP'), ('Symposium', 'NNP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['AMIA', 'Symposium', '.']

 TOTAL FILTERED TOKENS ==>  3

 ---- POST FOR FILTERED TOKENS ----

 [('AMIA', 'NNP'), ('Symposium', 'NNP'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['AMIA Symposium', 'Symposium .'] 

 TOTAL BIGRAMS --> 2 



 ---- TRI-GRAMS ---- 

 ['AMIA Symposium .'] 

 TOTAL TRIGRAMS --> 1 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> ['AMIA Symposium']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['amia', 'symposium', '.']

 TOTAL PORTER STEM WORDS ==> 3



 ---- SNOWBALL STEMMING ----

['amia', 'symposium', '.']

 TOTAL SNOWBALL STEM WORDS ==> 3



 ---- LEMMATIZATION ----

['AMIA', 'Symposium', '.']

 TOTAL LEMMATIZE WORDS ==> 3

************************************************************************************************************************

789 --> U.S. National Library of Medicine, n.d. 


 ---- TOKENS ----

 ['U.S.', 'National', 'Library', 'of', 'Medicine', ',', 'n.d', '.'] 

 TOTAL TOKENS ==> 8

 ---- POST ----

 [('U.S.', 'NNP'), ('National', 'NNP'), ('Library', 'NNP'), ('of', 'IN'), ('Medicine', 'NNP'), (',', ','), ('n.d', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['U.S.', 'National', 'Library', 'Medicine', ',', 'n.d', '.']

 TOTAL FILTERED TOKENS ==>  7

 ---- POST FOR FILTERED TOKENS ----

 [('U.S.', 'NNP'), ('National', 'NNP'), ('Library', 'NNP'), ('Medicine', 'NNP'), (',', ','), ('n.d', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['U.S. National', 'National Library', 'Library Medicine', 'Medicine ,', ', n.d', 'n.d .'] 

 TOTAL BIGRAMS --> 6 



 ---- TRI-GRAMS ---- 

 ['U.S. National Library', 'National Library Medicine', 'Library Medicine ,', 'Medicine , n.d', ', n.d .'] 

 TOTAL TRIGRAMS --> 5 



 ---- NOUN PHRASES ---- 

 ['n.d'] 

 TOTAL NOUN PHRASES --> 1 



 ---- NER ----

 
 ORGANIZATION ---> ['National Library Medicine']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> ['U.S.']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['u.s.', 'nation', 'librari', 'medicin', ',', 'n.d', '.']

 TOTAL PORTER STEM WORDS ==> 7



 ---- SNOWBALL STEMMING ----

['u.s.', 'nation', 'librari', 'medicin', ',', 'n.d', '.']

 TOTAL SNOWBALL STEM WORDS ==> 7



 ---- LEMMATIZATION ----

['U.S.', 'National', 'Library', 'Medicine', ',', 'n.d', '.']

 TOTAL LEMMATIZE WORDS ==> 7

************************************************************************************************************************

790 --> Web. 


 ---- TOKENS ----

 ['Web', '.'] 

 TOTAL TOKENS ==> 2

 ---- POST ----

 [('Web', 'NNP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Web', '.']

 TOTAL FILTERED TOKENS ==>  2

 ---- POST FOR FILTERED TOKENS ----

 [('Web', 'NNP'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Web .'] 

 TOTAL BIGRAMS --> 1 



 ---- TRI-GRAMS ---- 

 [] 

 TOTAL TRIGRAMS --> 0 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['web', '.']

 TOTAL PORTER STEM WORDS ==> 2



 ---- SNOWBALL STEMMING ----

['web', '.']

 TOTAL SNOWBALL STEM WORDS ==> 2



 ---- LEMMATIZATION ----

['Web', '.']

 TOTAL LEMMATIZE WORDS ==> 2

************************************************************************************************************************

791 --> 19 Mar. 


 ---- TOKENS ----

 ['19', 'Mar', '.'] 

 TOTAL TOKENS ==> 3

 ---- POST ----

 [('19', 'CD'), ('Mar', 'NNP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['19', 'Mar', '.']

 TOTAL FILTERED TOKENS ==>  3

 ---- POST FOR FILTERED TOKENS ----

 [('19', 'CD'), ('Mar', 'NNP'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['19 Mar', 'Mar .'] 

 TOTAL BIGRAMS --> 2 



 ---- TRI-GRAMS ---- 

 ['19 Mar .'] 

 TOTAL TRIGRAMS --> 1 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['19', 'mar', '.']

 TOTAL PORTER STEM WORDS ==> 3



 ---- SNOWBALL STEMMING ----

['19', 'mar', '.']

 TOTAL SNOWBALL STEM WORDS ==> 3



 ---- LEMMATIZATION ----

['19', 'Mar', '.']

 TOTAL LEMMATIZE WORDS ==> 3

************************************************************************************************************************

792 --> 2017  [107] Ogallo, W., & Kanter, A. S. (2017, February 10). 


 ---- TOKENS ----

 ['2017', '[', '107', ']', 'Ogallo', ',', 'W.', ',', '&', 'Kanter', ',', 'A.', 'S.', '(', '2017', ',', 'February', '10', ')', '.'] 

 TOTAL TOKENS ==> 20

 ---- POST ----

 [('2017', 'CD'), ('[', '$'), ('107', 'CD'), (']', 'NNP'), ('Ogallo', 'NNP'), (',', ','), ('W.', 'NNP'), (',', ','), ('&', 'CC'), ('Kanter', 'NNP'), (',', ','), ('A.', 'NNP'), ('S.', 'NNP'), ('(', '('), ('2017', 'CD'), (',', ','), ('February', 'NNP'), ('10', 'CD'), (')', ')'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['2017', '[', '107', ']', 'Ogallo', ',', 'W.', ',', '&', 'Kanter', ',', 'A.', 'S.', '(', '2017', ',', 'February', '10', ')', '.']

 TOTAL FILTERED TOKENS ==>  20

 ---- POST FOR FILTERED TOKENS ----

 [('2017', 'CD'), ('[', '$'), ('107', 'CD'), (']', 'NNP'), ('Ogallo', 'NNP'), (',', ','), ('W.', 'NNP'), (',', ','), ('&', 'CC'), ('Kanter', 'NNP'), (',', ','), ('A.', 'NNP'), ('S.', 'NNP'), ('(', '('), ('2017', 'CD'), (',', ','), ('February', 'NNP'), ('10', 'CD'), (')', ')'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['2017 [', '[ 107', '107 ]', '] Ogallo', 'Ogallo ,', ', W.', 'W. ,', ', &', '& Kanter', 'Kanter ,', ', A.', 'A. S.', 'S. (', '( 2017', '2017 ,', ', February', 'February 10', '10 )', ') .'] 

 TOTAL BIGRAMS --> 19 



 ---- TRI-GRAMS ---- 

 ['2017 [ 107', '[ 107 ]', '107 ] Ogallo', '] Ogallo ,', 'Ogallo , W.', ', W. ,', 'W. , &', ', & Kanter', '& Kanter ,', 'Kanter , A.', ', A. S.', 'A. S. (', 'S. ( 2017', '( 2017 ,', '2017 , February', ', February 10', 'February 10 )', '10 ) .'] 

 TOTAL TRIGRAMS --> 18 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> ['Kanter']
 TOTAL PERSON ENTITY --> 1 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['2017', '[', '107', ']', 'ogallo', ',', 'w.', ',', '&', 'kanter', ',', 'a.', 's.', '(', '2017', ',', 'februari', '10', ')', '.']

 TOTAL PORTER STEM WORDS ==> 20



 ---- SNOWBALL STEMMING ----

['2017', '[', '107', ']', 'ogallo', ',', 'w.', ',', '&', 'kanter', ',', 'a.', 's.', '(', '2017', ',', 'februari', '10', ')', '.']

 TOTAL SNOWBALL STEM WORDS ==> 20



 ---- LEMMATIZATION ----

['2017', '[', '107', ']', 'Ogallo', ',', 'W.', ',', '&', 'Kanter', ',', 'A.', 'S.', '(', '2017', ',', 'February', '10', ')', '.']

 TOTAL LEMMATIZE WORDS ==> 20

************************************************************************************************************************

793 --> Using Natural Language Processing  and Network Analysis to Develop a Conceptual Framework for Medication Therapy  Management Research. 


 ---- TOKENS ----

 ['Using', 'Natural', 'Language', 'Processing', 'and', 'Network', 'Analysis', 'to', 'Develop', 'a', 'Conceptual', 'Framework', 'for', 'Medication', 'Therapy', 'Management', 'Research', '.'] 

 TOTAL TOKENS ==> 18

 ---- POST ----

 [('Using', 'VBG'), ('Natural', 'NNP'), ('Language', 'NNP'), ('Processing', 'NNP'), ('and', 'CC'), ('Network', 'NNP'), ('Analysis', 'NNP'), ('to', 'TO'), ('Develop', 'VB'), ('a', 'DT'), ('Conceptual', 'NNP'), ('Framework', 'NNP'), ('for', 'IN'), ('Medication', 'NNP'), ('Therapy', 'NNP'), ('Management', 'NNP'), ('Research', 'NNP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Using', 'Natural', 'Language', 'Processing', 'Network', 'Analysis', 'Develop', 'Conceptual', 'Framework', 'Medication', 'Therapy', 'Management', 'Research', '.']

 TOTAL FILTERED TOKENS ==>  14

 ---- POST FOR FILTERED TOKENS ----

 [('Using', 'VBG'), ('Natural', 'NNP'), ('Language', 'NNP'), ('Processing', 'NNP'), ('Network', 'NNP'), ('Analysis', 'NNP'), ('Develop', 'NNP'), ('Conceptual', 'NNP'), ('Framework', 'NNP'), ('Medication', 'NNP'), ('Therapy', 'NNP'), ('Management', 'NNP'), ('Research', 'NNP'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Using Natural', 'Natural Language', 'Language Processing', 'Processing Network', 'Network Analysis', 'Analysis Develop', 'Develop Conceptual', 'Conceptual Framework', 'Framework Medication', 'Medication Therapy', 'Therapy Management', 'Management Research', 'Research .'] 

 TOTAL BIGRAMS --> 13 



 ---- TRI-GRAMS ---- 

 ['Using Natural Language', 'Natural Language Processing', 'Language Processing Network', 'Processing Network Analysis', 'Network Analysis Develop', 'Analysis Develop Conceptual', 'Develop Conceptual Framework', 'Conceptual Framework Medication', 'Framework Medication Therapy', 'Medication Therapy Management', 'Therapy Management Research', 'Management Research .'] 

 TOTAL TRIGRAMS --> 12 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> ['Natural Language']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> ['Network Analysis Develop Conceptual Framework Medication Therapy']
 TOTAL PERSON ENTITY --> 1 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['use', 'natur', 'languag', 'process', 'network', 'analysi', 'develop', 'conceptu', 'framework', 'medic', 'therapi', 'manag', 'research', '.']

 TOTAL PORTER STEM WORDS ==> 14



 ---- SNOWBALL STEMMING ----

['use', 'natur', 'languag', 'process', 'network', 'analysi', 'develop', 'conceptu', 'framework', 'medic', 'therapi', 'manag', 'research', '.']

 TOTAL SNOWBALL STEM WORDS ==> 14



 ---- LEMMATIZATION ----

['Using', 'Natural', 'Language', 'Processing', 'Network', 'Analysis', 'Develop', 'Conceptual', 'Framework', 'Medication', 'Therapy', 'Management', 'Research', '.']

 TOTAL LEMMATIZE WORDS ==> 14

************************************************************************************************************************

794 --> Retrieved April 10, 2017, from  https://www.ncbi.nlm.nih.gov/pubmed/28269895?dopt=Abstract  [108] Ochoa, A. 


 ---- TOKENS ----

 ['Retrieved', 'April', '10', ',', '2017', ',', 'from', 'https', ':', '//www.ncbi.nlm.nih.gov/pubmed/28269895', '?', 'dopt=Abstract', '[', '108', ']', 'Ochoa', ',', 'A', '.'] 

 TOTAL TOKENS ==> 19

 ---- POST ----

 [('Retrieved', 'VBN'), ('April', 'NNP'), ('10', 'CD'), (',', ','), ('2017', 'CD'), (',', ','), ('from', 'IN'), ('https', 'NN'), (':', ':'), ('//www.ncbi.nlm.nih.gov/pubmed/28269895', 'NN'), ('?', '.'), ('dopt=Abstract', 'JJ'), ('[', '$'), ('108', 'CD'), (']', 'NNP'), ('Ochoa', 'NNP'), (',', ','), ('A', 'NNP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Retrieved', 'April', '10', ',', '2017', ',', 'https', ':', '//www.ncbi.nlm.nih.gov/pubmed/28269895', '?', 'dopt=Abstract', '[', '108', ']', 'Ochoa', ',', '.']

 TOTAL FILTERED TOKENS ==>  17

 ---- POST FOR FILTERED TOKENS ----

 [('Retrieved', 'VBN'), ('April', 'NNP'), ('10', 'CD'), (',', ','), ('2017', 'CD'), (',', ','), ('https', 'NN'), (':', ':'), ('//www.ncbi.nlm.nih.gov/pubmed/28269895', 'NN'), ('?', '.'), ('dopt=Abstract', 'JJ'), ('[', '$'), ('108', 'CD'), (']', 'NNP'), ('Ochoa', 'NNP'), (',', ','), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Retrieved April', 'April 10', '10 ,', ', 2017', '2017 ,', ', https', 'https :', ': //www.ncbi.nlm.nih.gov/pubmed/28269895', '//www.ncbi.nlm.nih.gov/pubmed/28269895 ?', '? dopt=Abstract', 'dopt=Abstract [', '[ 108', '108 ]', '] Ochoa', 'Ochoa ,', ', .'] 

 TOTAL BIGRAMS --> 16 



 ---- TRI-GRAMS ---- 

 ['Retrieved April 10', 'April 10 ,', '10 , 2017', ', 2017 ,', '2017 , https', ', https :', 'https : //www.ncbi.nlm.nih.gov/pubmed/28269895', ': //www.ncbi.nlm.nih.gov/pubmed/28269895 ?', '//www.ncbi.nlm.nih.gov/pubmed/28269895 ? dopt=Abstract', '? dopt=Abstract [', 'dopt=Abstract [ 108', '[ 108 ]', '108 ] Ochoa', '] Ochoa ,', 'Ochoa , .'] 

 TOTAL TRIGRAMS --> 15 



 ---- NOUN PHRASES ---- 

 ['https', ''] 

 TOTAL NOUN PHRASES --> 2 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['retriev', 'april', '10', ',', '2017', ',', 'http', ':', '//www.ncbi.nlm.nih.gov/pubmed/28269895', '?', 'dopt=abstract', '[', '108', ']', 'ochoa', ',', '.']

 TOTAL PORTER STEM WORDS ==> 17



 ---- SNOWBALL STEMMING ----

['retriev', 'april', '10', ',', '2017', ',', 'https', ':', '//www.ncbi.nlm.nih.gov/pubmed/28269895', '?', 'dopt=abstract', '[', '108', ']', 'ochoa', ',', '.']

 TOTAL SNOWBALL STEM WORDS ==> 17



 ---- LEMMATIZATION ----

['Retrieved', 'April', '10', ',', '2017', ',', 'http', ':', '//www.ncbi.nlm.nih.gov/pubmed/28269895', '?', 'dopt=Abstract', '[', '108', ']', 'Ochoa', ',', '.']

 TOTAL LEMMATIZE WORDS ==> 17

************************************************************************************************************************

795 --> (2016, May 25). 


 ---- TOKENS ----

 ['(', '2016', ',', 'May', '25', ')', '.'] 

 TOTAL TOKENS ==> 7

 ---- POST ----

 [('(', '('), ('2016', 'CD'), (',', ','), ('May', 'NNP'), ('25', 'CD'), (')', ')'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['(', '2016', ',', 'May', '25', ')', '.']

 TOTAL FILTERED TOKENS ==>  7

 ---- POST FOR FILTERED TOKENS ----

 [('(', '('), ('2016', 'CD'), (',', ','), ('May', 'NNP'), ('25', 'CD'), (')', ')'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['( 2016', '2016 ,', ', May', 'May 25', '25 )', ') .'] 

 TOTAL BIGRAMS --> 6 



 ---- TRI-GRAMS ---- 

 ['( 2016 ,', '2016 , May', ', May 25', 'May 25 )', '25 ) .'] 

 TOTAL TRIGRAMS --> 5 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['(', '2016', ',', 'may', '25', ')', '.']

 TOTAL PORTER STEM WORDS ==> 7



 ---- SNOWBALL STEMMING ----

['(', '2016', ',', 'may', '25', ')', '.']

 TOTAL SNOWBALL STEM WORDS ==> 7



 ---- LEMMATIZATION ----

['(', '2016', ',', 'May', '25', ')', '.']

 TOTAL LEMMATIZE WORDS ==> 7

************************************************************************************************************************

796 --> Meet the Pilot: Smart Earpiece Language Translator. 


 ---- TOKENS ----

 ['Meet', 'the', 'Pilot', ':', 'Smart', 'Earpiece', 'Language', 'Translator', '.'] 

 TOTAL TOKENS ==> 9

 ---- POST ----

 [('Meet', 'NNP'), ('the', 'DT'), ('Pilot', 'NNP'), (':', ':'), ('Smart', 'NNP'), ('Earpiece', 'NNP'), ('Language', 'NNP'), ('Translator', 'NNP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Meet', 'Pilot', ':', 'Smart', 'Earpiece', 'Language', 'Translator', '.']

 TOTAL FILTERED TOKENS ==>  8

 ---- POST FOR FILTERED TOKENS ----

 [('Meet', 'NNP'), ('Pilot', 'NNP'), (':', ':'), ('Smart', 'NNP'), ('Earpiece', 'NNP'), ('Language', 'NNP'), ('Translator', 'NNP'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Meet Pilot', 'Pilot :', ': Smart', 'Smart Earpiece', 'Earpiece Language', 'Language Translator', 'Translator .'] 

 TOTAL BIGRAMS --> 7 



 ---- TRI-GRAMS ---- 

 ['Meet Pilot :', 'Pilot : Smart', ': Smart Earpiece', 'Smart Earpiece Language', 'Earpiece Language Translator', 'Language Translator .'] 

 TOTAL TRIGRAMS --> 6 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> ['Meet', 'Pilot', 'Smart Earpiece Language Translator']
 TOTAL PERSON ENTITY --> 3 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['meet', 'pilot', ':', 'smart', 'earpiec', 'languag', 'translat', '.']

 TOTAL PORTER STEM WORDS ==> 8



 ---- SNOWBALL STEMMING ----

['meet', 'pilot', ':', 'smart', 'earpiec', 'languag', 'translat', '.']

 TOTAL SNOWBALL STEM WORDS ==> 8



 ---- LEMMATIZATION ----

['Meet', 'Pilot', ':', 'Smart', 'Earpiece', 'Language', 'Translator', '.']

 TOTAL LEMMATIZE WORDS ==> 8

************************************************************************************************************************

797 --> Retrieved April 10, 2017, from https://www.indiegogo.com/projects/meet-the-pilot-smart- earpiece-language-translator-headphones-travel  View publication statsView publication stats https://www.ncbi.nlm.nih.gov/pubmed/28269895?dopt=Abstract https://www.researchgate.net/publication/319164243 


 ---- TOKENS ----

 ['Retrieved', 'April', '10', ',', '2017', ',', 'from', 'https', ':', '//www.indiegogo.com/projects/meet-the-pilot-smart-', 'earpiece-language-translator-headphones-travel', 'View', 'publication', 'statsView', 'publication', 'stats', 'https', ':', '//www.ncbi.nlm.nih.gov/pubmed/28269895', '?', 'dopt=Abstract', 'https', ':', '//www.researchgate.net/publication/319164243'] 

 TOTAL TOKENS ==> 24

 ---- POST ----

 [('Retrieved', 'VBN'), ('April', 'NNP'), ('10', 'CD'), (',', ','), ('2017', 'CD'), (',', ','), ('from', 'IN'), ('https', 'NN'), (':', ':'), ('//www.indiegogo.com/projects/meet-the-pilot-smart-', 'JJ'), ('earpiece-language-translator-headphones-travel', 'JJ'), ('View', 'NNP'), ('publication', 'NN'), ('statsView', 'NN'), ('publication', 'NN'), ('stats', 'NNS'), ('https', 'VBP'), (':', ':'), ('//www.ncbi.nlm.nih.gov/pubmed/28269895', 'NN'), ('?', '.'), ('dopt=Abstract', 'JJ'), ('https', 'NN'), (':', ':'), ('//www.researchgate.net/publication/319164243', 'NN')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Retrieved', 'April', '10', ',', '2017', ',', 'https', ':', '//www.indiegogo.com/projects/meet-the-pilot-smart-', 'earpiece-language-translator-headphones-travel', 'View', 'publication', 'statsView', 'publication', 'stats', 'https', ':', '//www.ncbi.nlm.nih.gov/pubmed/28269895', '?', 'dopt=Abstract', 'https', ':', '//www.researchgate.net/publication/319164243']

 TOTAL FILTERED TOKENS ==>  23

 ---- POST FOR FILTERED TOKENS ----

 [('Retrieved', 'VBN'), ('April', 'NNP'), ('10', 'CD'), (',', ','), ('2017', 'CD'), (',', ','), ('https', 'NN'), (':', ':'), ('//www.indiegogo.com/projects/meet-the-pilot-smart-', 'JJ'), ('earpiece-language-translator-headphones-travel', 'JJ'), ('View', 'NNP'), ('publication', 'NN'), ('statsView', 'NN'), ('publication', 'NN'), ('stats', 'NNS'), ('https', 'VBP'), (':', ':'), ('//www.ncbi.nlm.nih.gov/pubmed/28269895', 'NN'), ('?', '.'), ('dopt=Abstract', 'JJ'), ('https', 'NN'), (':', ':'), ('//www.researchgate.net/publication/319164243', 'NN')] 



 ---- BI-GRAMS ---- 

 ['Retrieved April', 'April 10', '10 ,', ', 2017', '2017 ,', ', https', 'https :', ': //www.indiegogo.com/projects/meet-the-pilot-smart-', '//www.indiegogo.com/projects/meet-the-pilot-smart- earpiece-language-translator-headphones-travel', 'earpiece-language-translator-headphones-travel View', 'View publication', 'publication statsView', 'statsView publication', 'publication stats', 'stats https', 'https :', ': //www.ncbi.nlm.nih.gov/pubmed/28269895', '//www.ncbi.nlm.nih.gov/pubmed/28269895 ?', '? dopt=Abstract', 'dopt=Abstract https', 'https :', ': //www.researchgate.net/publication/319164243'] 

 TOTAL BIGRAMS --> 22 



 ---- TRI-GRAMS ---- 

 ['Retrieved April 10', 'April 10 ,', '10 , 2017', ', 2017 ,', '2017 , https', ', https :', 'https : //www.indiegogo.com/projects/meet-the-pilot-smart-', ': //www.indiegogo.com/projects/meet-the-pilot-smart- earpiece-language-translator-headphones-travel', '//www.indiegogo.com/projects/meet-the-pilot-smart- earpiece-language-translator-headphones-travel View', 'earpiece-language-translator-headphones-travel View publication', 'View publication statsView', 'publication statsView publication', 'statsView publication stats', 'publication stats https', 'stats https :', 'https : //www.ncbi.nlm.nih.gov/pubmed/28269895', ': //www.ncbi.nlm.nih.gov/pubmed/28269895 ?', '//www.ncbi.nlm.nih.gov/pubmed/28269895 ? dopt=Abstract', '? dopt=Abstract https', 'dopt=Abstract https :', 'https : //www.researchgate.net/publication/319164243'] 

 TOTAL TRIGRAMS --> 21 



 ---- NOUN PHRASES ---- 

 ['https', 'publication', 'statsView', 'publication', '', 'dopt=Abstract https', ''] 

 TOTAL NOUN PHRASES --> 7 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['retriev', 'april', '10', ',', '2017', ',', 'http', ':', '//www.indiegogo.com/projects/meet-the-pilot-smart-', 'earpiece-language-translator-headphones-travel', 'view', 'public', 'statsview', 'public', 'stat', 'http', ':', '//www.ncbi.nlm.nih.gov/pubmed/28269895', '?', 'dopt=abstract', 'http', ':', '//www.researchgate.net/publication/319164243']

 TOTAL PORTER STEM WORDS ==> 23



 ---- SNOWBALL STEMMING ----

['retriev', 'april', '10', ',', '2017', ',', 'https', ':', '//www.indiegogo.com/projects/meet-the-pilot-smart-', 'earpiece-language-translator-headphones-travel', 'view', 'public', 'statsview', 'public', 'stat', 'https', ':', '//www.ncbi.nlm.nih.gov/pubmed/28269895', '?', 'dopt=abstract', 'https', ':', '//www.researchgate.net/publication/319164243']

 TOTAL SNOWBALL STEM WORDS ==> 23



 ---- LEMMATIZATION ----

['Retrieved', 'April', '10', ',', '2017', ',', 'http', ':', '//www.indiegogo.com/projects/meet-the-pilot-smart-', 'earpiece-language-translator-headphones-travel', 'View', 'publication', 'statsView', 'publication', 'stats', 'http', ':', '//www.ncbi.nlm.nih.gov/pubmed/28269895', '?', 'dopt=Abstract', 'http', ':', '//www.researchgate.net/publication/319164243']

 TOTAL LEMMATIZE WORDS ==> 23

************************************************************************************************************************

