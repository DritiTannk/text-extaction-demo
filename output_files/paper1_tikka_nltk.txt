1 --> W H I T E  P A P E R sentiment recall precision part of speech machine learning data ratio NLP syntax tuning themes named entity extraction accuracy training AI Lexalytics, Inc., 48 North Pleasant St. Unit 301, Amherst MA 01002 USA | 1-800-377-8036 | www.lexalytics.com  Machine Learning for   Natural Language Processing   and Text Analytics https://www.lexalytics.com/ https://www.lexalytics.com/  W H I T E  P A P E R T A B L E  O F  C O N T E N T S 2|       |   Lexalytics, Inc., 48 North Pleasant St. Unit 301, Amherst MA 01002 USA   |   1-800-377-8036 |   www.lexalytics.com Introduction Machine learning is everywhere, from helping us make better toast to  researching drug discovery and designs. 


 ---- TOKENS ----

 ['W', 'H', 'I', 'T', 'E', 'P', 'A', 'P', 'E', 'R', 'sentiment', 'recall', 'precision', 'part', 'of', 'speech', 'machine', 'learning', 'data', 'ratio', 'NLP', 'syntax', 'tuning', 'themes', 'named', 'entity', 'extraction', 'accuracy', 'training', 'AI', 'Lexalytics', ',', 'Inc.', ',', '48', 'North', 'Pleasant', 'St.', 'Unit', '301', ',', 'Amherst', 'MA', '01002', 'USA', '|', '1-800-377-8036', '|', 'www.lexalytics.com', 'Machine', 'Learning', 'for', 'Natural', 'Language', 'Processing', 'and', 'Text', 'Analytics', 'https', ':', '//www.lexalytics.com/', 'https', ':', '//www.lexalytics.com/', 'W', 'H', 'I', 'T', 'E', 'P', 'A', 'P', 'E', 'R', 'T', 'A', 'B', 'L', 'E', 'O', 'F', 'C', 'O', 'N', 'T', 'E', 'N', 'T', 'S', '2|', '|', 'Lexalytics', ',', 'Inc.', ',', '48', 'North', 'Pleasant', 'St.', 'Unit', '301', ',', 'Amherst', 'MA', '01002', 'USA', '|', '1-800-377-8036', '|', 'www.lexalytics.com', 'Introduction', 'Machine', 'learning', 'is', 'everywhere', ',', 'from', 'helping', 'us', 'make', 'better', 'toast', 'to', 'researching', 'drug', 'discovery', 'and', 'designs', '.'] 

 TOTAL TOKENS ==> 129

 ---- POST ----

 [('W', 'NNP'), ('H', 'NNP'), ('I', 'PRP'), ('T', 'NNP'), ('E', 'NNP'), ('P', 'NNP'), ('A', 'NNP'), ('P', 'NNP'), ('E', 'NNP'), ('R', 'NNP'), ('sentiment', 'NN'), ('recall', 'NN'), ('precision', 'NN'), ('part', 'NN'), ('of', 'IN'), ('speech', 'NN'), ('machine', 'NN'), ('learning', 'VBG'), ('data', 'NNS'), ('ratio', 'NN'), ('NLP', 'NNP'), ('syntax', 'NN'), ('tuning', 'VBG'), ('themes', 'NNS'), ('named', 'VBN'), ('entity', 'NN'), ('extraction', 'NN'), ('accuracy', 'NN'), ('training', 'NN'), ('AI', 'NNP'), ('Lexalytics', 'NNP'), (',', ','), ('Inc.', 'NNP'), (',', ','), ('48', 'CD'), ('North', 'NNP'), ('Pleasant', 'NNP'), ('St.', 'NNP'), ('Unit', 'NNP'), ('301', 'CD'), (',', ','), ('Amherst', 'NNP'), ('MA', 'NNP'), ('01002', 'CD'), ('USA', 'NNP'), ('|', 'NNP'), ('1-800-377-8036', 'JJ'), ('|', 'NNP'), ('www.lexalytics.com', 'NN'), ('Machine', 'NNP'), ('Learning', 'NNP'), ('for', 'IN'), ('Natural', 'NNP'), ('Language', 'NNP'), ('Processing', 'NNP'), ('and', 'CC'), ('Text', 'NNP'), ('Analytics', 'NNP'), ('https', 'NN'), (':', ':'), ('//www.lexalytics.com/', 'JJ'), ('https', 'NN'), (':', ':'), ('//www.lexalytics.com/', 'JJ'), ('W', 'NNP'), ('H', 'NNP'), ('I', 'PRP'), ('T', 'NNP'), ('E', 'NNP'), ('P', 'NNP'), ('A', 'NNP'), ('P', 'NNP'), ('E', 'NNP'), ('R', 'NNP'), ('T', 'NNP'), ('A', 'NNP'), ('B', 'NNP'), ('L', 'NNP'), ('E', 'NNP'), ('O', 'NNP'), ('F', 'NNP'), ('C', 'NNP'), ('O', 'NNP'), ('N', 'NNP'), ('T', 'NNP'), ('E', 'NNP'), ('N', 'NNP'), ('T', 'NNP'), ('S', 'NNP'), ('2|', 'CD'), ('|', 'NNP'), ('Lexalytics', 'NNP'), (',', ','), ('Inc.', 'NNP'), (',', ','), ('48', 'CD'), ('North', 'NNP'), ('Pleasant', 'NNP'), ('St.', 'NNP'), ('Unit', 'NNP'), ('301', 'CD'), (',', ','), ('Amherst', 'NNP'), ('MA', 'NNP'), ('01002', 'CD'), ('USA', 'NNP'), ('|', 'NNP'), ('1-800-377-8036', 'JJ'), ('|', 'NNP'), ('www.lexalytics.com', 'NN'), ('Introduction', 'NNP'), ('Machine', 'NNP'), ('learning', 'NN'), ('is', 'VBZ'), ('everywhere', 'RB'), (',', ','), ('from', 'IN'), ('helping', 'VBG'), ('us', 'PRP'), ('make', 'VBP'), ('better', 'JJR'), ('toast', 'NN'), ('to', 'TO'), ('researching', 'VBG'), ('drug', 'NN'), ('discovery', 'NN'), ('and', 'CC'), ('designs', 'NNS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['W', 'H', 'E', 'P', 'P', 'E', 'R', 'sentiment', 'recall', 'precision', 'part', 'speech', 'machine', 'learning', 'data', 'ratio', 'NLP', 'syntax', 'tuning', 'themes', 'named', 'entity', 'extraction', 'accuracy', 'training', 'AI', 'Lexalytics', ',', 'Inc.', ',', '48', 'North', 'Pleasant', 'St.', 'Unit', '301', ',', 'Amherst', '01002', 'USA', '|', '1-800-377-8036', '|', 'www.lexalytics.com', 'Machine', 'Learning', 'Natural', 'Language', 'Processing', 'Text', 'Analytics', 'https', ':', '//www.lexalytics.com/', 'https', ':', '//www.lexalytics.com/', 'W', 'H', 'E', 'P', 'P', 'E', 'R', 'B', 'L', 'E', 'F', 'C', 'N', 'E', 'N', '2|', '|', 'Lexalytics', ',', 'Inc.', ',', '48', 'North', 'Pleasant', 'St.', 'Unit', '301', ',', 'Amherst', '01002', 'USA', '|', '1-800-377-8036', '|', 'www.lexalytics.com', 'Introduction', 'Machine', 'learning', 'everywhere', ',', 'helping', 'us', 'make', 'better', 'toast', 'researching', 'drug', 'discovery', 'designs', '.']

 TOTAL FILTERED TOKENS ==>  107

 ---- POST FOR FILTERED TOKENS ----

 [('W', 'NNP'), ('H', 'NNP'), ('E', 'NNP'), ('P', 'NNP'), ('P', 'NNP'), ('E', 'NNP'), ('R', 'NNP'), ('sentiment', 'NN'), ('recall', 'NN'), ('precision', 'NN'), ('part', 'NN'), ('speech', 'NN'), ('machine', 'NN'), ('learning', 'VBG'), ('data', 'NNS'), ('ratio', 'NN'), ('NLP', 'NNP'), ('syntax', 'NN'), ('tuning', 'VBG'), ('themes', 'NNS'), ('named', 'VBN'), ('entity', 'NN'), ('extraction', 'NN'), ('accuracy', 'NN'), ('training', 'NN'), ('AI', 'NNP'), ('Lexalytics', 'NNP'), (',', ','), ('Inc.', 'NNP'), (',', ','), ('48', 'CD'), ('North', 'NNP'), ('Pleasant', 'NNP'), ('St.', 'NNP'), ('Unit', 'NNP'), ('301', 'CD'), (',', ','), ('Amherst', 'NNP'), ('01002', 'CD'), ('USA', 'NNP'), ('|', 'NNP'), ('1-800-377-8036', 'JJ'), ('|', 'NNP'), ('www.lexalytics.com', 'NN'), ('Machine', 'NNP'), ('Learning', 'NNP'), ('Natural', 'NNP'), ('Language', 'NNP'), ('Processing', 'NNP'), ('Text', 'NNP'), ('Analytics', 'NNP'), ('https', 'NN'), (':', ':'), ('//www.lexalytics.com/', 'JJ'), ('https', 'NN'), (':', ':'), ('//www.lexalytics.com/', 'JJ'), ('W', 'NNP'), ('H', 'NNP'), ('E', 'NNP'), ('P', 'NNP'), ('P', 'NNP'), ('E', 'NNP'), ('R', 'NNP'), ('B', 'NNP'), ('L', 'NNP'), ('E', 'NNP'), ('F', 'NNP'), ('C', 'NNP'), ('N', 'NNP'), ('E', 'NNP'), ('N', 'NNP'), ('2|', 'CD'), ('|', 'NNP'), ('Lexalytics', 'NNP'), (',', ','), ('Inc.', 'NNP'), (',', ','), ('48', 'CD'), ('North', 'NNP'), ('Pleasant', 'NNP'), ('St.', 'NNP'), ('Unit', 'NNP'), ('301', 'CD'), (',', ','), ('Amherst', 'NNP'), ('01002', 'CD'), ('USA', 'NNP'), ('|', 'NNP'), ('1-800-377-8036', 'JJ'), ('|', 'NNP'), ('www.lexalytics.com', 'NN'), ('Introduction', 'NNP'), ('Machine', 'NNP'), ('learning', 'VBG'), ('everywhere', 'RB'), (',', ','), ('helping', 'VBG'), ('us', 'PRP'), ('make', 'VBP'), ('better', 'JJR'), ('toast', 'NN'), ('researching', 'VBG'), ('drug', 'NN'), ('discovery', 'NN'), ('designs', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['W H', 'H E', 'E P', 'P P', 'P E', 'E R', 'R sentiment', 'sentiment recall', 'recall precision', 'precision part', 'part speech', 'speech machine', 'machine learning', 'learning data', 'data ratio', 'ratio NLP', 'NLP syntax', 'syntax tuning', 'tuning themes', 'themes named', 'named entity', 'entity extraction', 'extraction accuracy', 'accuracy training', 'training AI', 'AI Lexalytics', 'Lexalytics ,', ', Inc.', 'Inc. ,', ', 48', '48 North', 'North Pleasant', 'Pleasant St.', 'St. Unit', 'Unit 301', '301 ,', ', Amherst', 'Amherst 01002', '01002 USA', 'USA |', '| 1-800-377-8036', '1-800-377-8036 |', '| www.lexalytics.com', 'www.lexalytics.com Machine', 'Machine Learning', 'Learning Natural', 'Natural Language', 'Language Processing', 'Processing Text', 'Text Analytics', 'Analytics https', 'https :', ': //www.lexalytics.com/', '//www.lexalytics.com/ https', 'https :', ': //www.lexalytics.com/', '//www.lexalytics.com/ W', 'W H', 'H E', 'E P', 'P P', 'P E', 'E R', 'R B', 'B L', 'L E', 'E F', 'F C', 'C N', 'N E', 'E N', 'N 2|', '2| |', '| Lexalytics', 'Lexalytics ,', ', Inc.', 'Inc. ,', ', 48', '48 North', 'North Pleasant', 'Pleasant St.', 'St. Unit', 'Unit 301', '301 ,', ', Amherst', 'Amherst 01002', '01002 USA', 'USA |', '| 1-800-377-8036', '1-800-377-8036 |', '| www.lexalytics.com', 'www.lexalytics.com Introduction', 'Introduction Machine', 'Machine learning', 'learning everywhere', 'everywhere ,', ', helping', 'helping us', 'us make', 'make better', 'better toast', 'toast researching', 'researching drug', 'drug discovery', 'discovery designs', 'designs .'] 

 TOTAL BIGRAMS --> 106 



 ---- TRI-GRAMS ---- 

 ['W H E', 'H E P', 'E P P', 'P P E', 'P E R', 'E R sentiment', 'R sentiment recall', 'sentiment recall precision', 'recall precision part', 'precision part speech', 'part speech machine', 'speech machine learning', 'machine learning data', 'learning data ratio', 'data ratio NLP', 'ratio NLP syntax', 'NLP syntax tuning', 'syntax tuning themes', 'tuning themes named', 'themes named entity', 'named entity extraction', 'entity extraction accuracy', 'extraction accuracy training', 'accuracy training AI', 'training AI Lexalytics', 'AI Lexalytics ,', 'Lexalytics , Inc.', ', Inc. ,', 'Inc. , 48', ', 48 North', '48 North Pleasant', 'North Pleasant St.', 'Pleasant St. Unit', 'St. Unit 301', 'Unit 301 ,', '301 , Amherst', ', Amherst 01002', 'Amherst 01002 USA', '01002 USA |', 'USA | 1-800-377-8036', '| 1-800-377-8036 |', '1-800-377-8036 | www.lexalytics.com', '| www.lexalytics.com Machine', 'www.lexalytics.com Machine Learning', 'Machine Learning Natural', 'Learning Natural Language', 'Natural Language Processing', 'Language Processing Text', 'Processing Text Analytics', 'Text Analytics https', 'Analytics https :', 'https : //www.lexalytics.com/', ': //www.lexalytics.com/ https', '//www.lexalytics.com/ https :', 'https : //www.lexalytics.com/', ': //www.lexalytics.com/ W', '//www.lexalytics.com/ W H', 'W H E', 'H E P', 'E P P', 'P P E', 'P E R', 'E R B', 'R B L', 'B L E', 'L E F', 'E F C', 'F C N', 'C N E', 'N E N', 'E N 2|', 'N 2| |', '2| | Lexalytics', '| Lexalytics ,', 'Lexalytics , Inc.', ', Inc. ,', 'Inc. , 48', ', 48 North', '48 North Pleasant', 'North Pleasant St.', 'Pleasant St. Unit', 'St. Unit 301', 'Unit 301 ,', '301 , Amherst', ', Amherst 01002', 'Amherst 01002 USA', '01002 USA |', 'USA | 1-800-377-8036', '| 1-800-377-8036 |', '1-800-377-8036 | www.lexalytics.com', '| www.lexalytics.com Introduction', 'www.lexalytics.com Introduction Machine', 'Introduction Machine learning', 'Machine learning everywhere', 'learning everywhere ,', 'everywhere , helping', ', helping us', 'helping us make', 'us make better', 'make better toast', 'better toast researching', 'toast researching drug', 'researching drug discovery', 'drug discovery designs', 'discovery designs .'] 

 TOTAL TRIGRAMS --> 105 



 ---- NOUN PHRASES ---- 

 ['sentiment', 'recall', 'precision', 'part', 'speech', 'machine', 'ratio', 'syntax', 'entity', 'extraction', 'accuracy', 'training', 'www.lexalytics.com', 'https', ' https', 'www.lexalytics.com', 'toast', 'drug', 'discovery', 'designs'] 

 TOTAL NOUN PHRASES --> 20 



 ---- NER ----

 
 ORGANIZATION ---> ['NLP', 'USA', 'USA', 'Introduction Machine']
 TOTAL ORGANIZATION ENTITY --> 4 


 PERSON ---> ['Machine Learning Natural Language']
 TOTAL PERSON ENTITY --> 1 


 GPE ---> ['North', 'North']
 TOTAL GPE ENTITY --> 2 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['w', 'h', 'e', 'p', 'p', 'e', 'r', 'sentiment', 'recal', 'precis', 'part', 'speech', 'machin', 'learn', 'data', 'ratio', 'nlp', 'syntax', 'tune', 'theme', 'name', 'entiti', 'extract', 'accuraci', 'train', 'ai', 'lexalyt', ',', 'inc.', ',', '48', 'north', 'pleasant', 'st.', 'unit', '301', ',', 'amherst', '01002', 'usa', '|', '1-800-377-8036', '|', 'www.lexalytics.com', 'machin', 'learn', 'natur', 'languag', 'process', 'text', 'analyt', 'http', ':', '//www.lexalytics.com/', 'http', ':', '//www.lexalytics.com/', 'w', 'h', 'e', 'p', 'p', 'e', 'r', 'b', 'l', 'e', 'f', 'c', 'n', 'e', 'n', '2|', '|', 'lexalyt', ',', 'inc.', ',', '48', 'north', 'pleasant', 'st.', 'unit', '301', ',', 'amherst', '01002', 'usa', '|', '1-800-377-8036', '|', 'www.lexalytics.com', 'introduct', 'machin', 'learn', 'everywher', ',', 'help', 'us', 'make', 'better', 'toast', 'research', 'drug', 'discoveri', 'design', '.']

 TOTAL PORTER STEM WORDS ==> 107



 ---- SNOWBALL STEMMING ----

['w', 'h', 'e', 'p', 'p', 'e', 'r', 'sentiment', 'recal', 'precis', 'part', 'speech', 'machin', 'learn', 'data', 'ratio', 'nlp', 'syntax', 'tune', 'theme', 'name', 'entiti', 'extract', 'accuraci', 'train', 'ai', 'lexalyt', ',', 'inc.', ',', '48', 'north', 'pleasant', 'st.', 'unit', '301', ',', 'amherst', '01002', 'usa', '|', '1-800-377-8036', '|', 'www.lexalytics.com', 'machin', 'learn', 'natur', 'languag', 'process', 'text', 'analyt', 'https', ':', '//www.lexalytics.com/', 'https', ':', '//www.lexalytics.com/', 'w', 'h', 'e', 'p', 'p', 'e', 'r', 'b', 'l', 'e', 'f', 'c', 'n', 'e', 'n', '2|', '|', 'lexalyt', ',', 'inc.', ',', '48', 'north', 'pleasant', 'st.', 'unit', '301', ',', 'amherst', '01002', 'usa', '|', '1-800-377-8036', '|', 'www.lexalytics.com', 'introduct', 'machin', 'learn', 'everywher', ',', 'help', 'us', 'make', 'better', 'toast', 'research', 'drug', 'discoveri', 'design', '.']

 TOTAL SNOWBALL STEM WORDS ==> 107



 ---- LEMMATIZATION ----

['W', 'H', 'E', 'P', 'P', 'E', 'R', 'sentiment', 'recall', 'precision', 'part', 'speech', 'machine', 'learning', 'data', 'ratio', 'NLP', 'syntax', 'tuning', 'theme', 'named', 'entity', 'extraction', 'accuracy', 'training', 'AI', 'Lexalytics', ',', 'Inc.', ',', '48', 'North', 'Pleasant', 'St.', 'Unit', '301', ',', 'Amherst', '01002', 'USA', '|', '1-800-377-8036', '|', 'www.lexalytics.com', 'Machine', 'Learning', 'Natural', 'Language', 'Processing', 'Text', 'Analytics', 'http', ':', '//www.lexalytics.com/', 'http', ':', '//www.lexalytics.com/', 'W', 'H', 'E', 'P', 'P', 'E', 'R', 'B', 'L', 'E', 'F', 'C', 'N', 'E', 'N', '2|', '|', 'Lexalytics', ',', 'Inc.', ',', '48', 'North', 'Pleasant', 'St.', 'Unit', '301', ',', 'Amherst', '01002', 'USA', '|', '1-800-377-8036', '|', 'www.lexalytics.com', 'Introduction', 'Machine', 'learning', 'everywhere', ',', 'helping', 'u', 'make', 'better', 'toast', 'researching', 'drug', 'discovery', 'design', '.']

 TOTAL LEMMATIZE WORDS ==> 107

************************************************************************************************************************

2 --> Sometimes the term is used  interchangeably with artificial intelligence (AI), but they’re not the same  thing. 


 ---- TOKENS ----

 ['Sometimes', 'the', 'term', 'is', 'used', 'interchangeably', 'with', 'artificial', 'intelligence', '(', 'AI', ')', ',', 'but', 'they', '’', 're', 'not', 'the', 'same', 'thing', '.'] 

 TOTAL TOKENS ==> 22

 ---- POST ----

 [('Sometimes', 'RB'), ('the', 'DT'), ('term', 'NN'), ('is', 'VBZ'), ('used', 'VBN'), ('interchangeably', 'RB'), ('with', 'IN'), ('artificial', 'JJ'), ('intelligence', 'NN'), ('(', '('), ('AI', 'NNP'), (')', ')'), (',', ','), ('but', 'CC'), ('they', 'PRP'), ('’', 'VBP'), ('re', 'VBZ'), ('not', 'RB'), ('the', 'DT'), ('same', 'JJ'), ('thing', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Sometimes', 'term', 'used', 'interchangeably', 'artificial', 'intelligence', '(', 'AI', ')', ',', '’', 'thing', '.']

 TOTAL FILTERED TOKENS ==>  13

 ---- POST FOR FILTERED TOKENS ----

 [('Sometimes', 'RB'), ('term', 'NN'), ('used', 'VBN'), ('interchangeably', 'RB'), ('artificial', 'JJ'), ('intelligence', 'NN'), ('(', '('), ('AI', 'NNP'), (')', ')'), (',', ','), ('’', 'JJ'), ('thing', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Sometimes term', 'term used', 'used interchangeably', 'interchangeably artificial', 'artificial intelligence', 'intelligence (', '( AI', 'AI )', ') ,', ', ’', '’ thing', 'thing .'] 

 TOTAL BIGRAMS --> 12 



 ---- TRI-GRAMS ---- 

 ['Sometimes term used', 'term used interchangeably', 'used interchangeably artificial', 'interchangeably artificial intelligence', 'artificial intelligence (', 'intelligence ( AI', '( AI )', 'AI ) ,', ') , ’', ', ’ thing', '’ thing .'] 

 TOTAL TRIGRAMS --> 11 



 ---- NOUN PHRASES ---- 

 ['term', 'artificial intelligence', '’ thing'] 

 TOTAL NOUN PHRASES --> 3 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['sometim', 'term', 'use', 'interchang', 'artifici', 'intellig', '(', 'ai', ')', ',', '’', 'thing', '.']

 TOTAL PORTER STEM WORDS ==> 13



 ---- SNOWBALL STEMMING ----

['sometim', 'term', 'use', 'interchang', 'artifici', 'intellig', '(', 'ai', ')', ',', '’', 'thing', '.']

 TOTAL SNOWBALL STEM WORDS ==> 13



 ---- LEMMATIZATION ----

['Sometimes', 'term', 'used', 'interchangeably', 'artificial', 'intelligence', '(', 'AI', ')', ',', '’', 'thing', '.']

 TOTAL LEMMATIZE WORDS ==> 13

************************************************************************************************************************

3 --> While all AI involves machine learning, not all machine learning is AI. 


 ---- TOKENS ----

 ['While', 'all', 'AI', 'involves', 'machine', 'learning', ',', 'not', 'all', 'machine', 'learning', 'is', 'AI', '.'] 

 TOTAL TOKENS ==> 14

 ---- POST ----

 [('While', 'IN'), ('all', 'DT'), ('AI', 'NNP'), ('involves', 'VBZ'), ('machine', 'NN'), ('learning', 'NN'), (',', ','), ('not', 'RB'), ('all', 'DT'), ('machine', 'NN'), ('learning', 'NN'), ('is', 'VBZ'), ('AI', 'NNP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['AI', 'involves', 'machine', 'learning', ',', 'machine', 'learning', 'AI', '.']

 TOTAL FILTERED TOKENS ==>  9

 ---- POST FOR FILTERED TOKENS ----

 [('AI', 'NNP'), ('involves', 'VBZ'), ('machine', 'NN'), ('learning', 'NN'), (',', ','), ('machine', 'NN'), ('learning', 'NN'), ('AI', 'NNP'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['AI involves', 'involves machine', 'machine learning', 'learning ,', ', machine', 'machine learning', 'learning AI', 'AI .'] 

 TOTAL BIGRAMS --> 8 



 ---- TRI-GRAMS ---- 

 ['AI involves machine', 'involves machine learning', 'machine learning ,', 'learning , machine', ', machine learning', 'machine learning AI', 'learning AI .'] 

 TOTAL TRIGRAMS --> 7 



 ---- NOUN PHRASES ---- 

 ['machine', 'learning', 'machine', 'learning'] 

 TOTAL NOUN PHRASES --> 4 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['ai', 'involv', 'machin', 'learn', ',', 'machin', 'learn', 'ai', '.']

 TOTAL PORTER STEM WORDS ==> 9



 ---- SNOWBALL STEMMING ----

['ai', 'involv', 'machin', 'learn', ',', 'machin', 'learn', 'ai', '.']

 TOTAL SNOWBALL STEM WORDS ==> 9



 ---- LEMMATIZATION ----

['AI', 'involves', 'machine', 'learning', ',', 'machine', 'learning', 'AI', '.']

 TOTAL LEMMATIZE WORDS ==> 9

************************************************************************************************************************

4 --> Lexalytics’ core text analytics engine, Salience, can be considered a  “narrow” AI: It uses many different types of machine learning to solve  the task of understanding and analyzing text, but is focused exclusively  on text. 


 ---- TOKENS ----

 ['Lexalytics', '’', 'core', 'text', 'analytics', 'engine', ',', 'Salience', ',', 'can', 'be', 'considered', 'a', '“', 'narrow', '”', 'AI', ':', 'It', 'uses', 'many', 'different', 'types', 'of', 'machine', 'learning', 'to', 'solve', 'the', 'task', 'of', 'understanding', 'and', 'analyzing', 'text', ',', 'but', 'is', 'focused', 'exclusively', 'on', 'text', '.'] 

 TOTAL TOKENS ==> 43

 ---- POST ----

 [('Lexalytics', 'NNS'), ('’', 'VBP'), ('core', 'NN'), ('text', 'NN'), ('analytics', 'NNS'), ('engine', 'NN'), (',', ','), ('Salience', 'NNP'), (',', ','), ('can', 'MD'), ('be', 'VB'), ('considered', 'VBN'), ('a', 'DT'), ('“', 'JJ'), ('narrow', 'JJ'), ('”', 'NN'), ('AI', 'NNP'), (':', ':'), ('It', 'PRP'), ('uses', 'VBZ'), ('many', 'JJ'), ('different', 'JJ'), ('types', 'NNS'), ('of', 'IN'), ('machine', 'NN'), ('learning', 'VBG'), ('to', 'TO'), ('solve', 'VB'), ('the', 'DT'), ('task', 'NN'), ('of', 'IN'), ('understanding', 'VBG'), ('and', 'CC'), ('analyzing', 'VBG'), ('text', 'NN'), (',', ','), ('but', 'CC'), ('is', 'VBZ'), ('focused', 'VBN'), ('exclusively', 'RB'), ('on', 'IN'), ('text', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Lexalytics', '’', 'core', 'text', 'analytics', 'engine', ',', 'Salience', ',', 'considered', '“', 'narrow', '”', 'AI', ':', 'uses', 'many', 'different', 'types', 'machine', 'learning', 'solve', 'task', 'understanding', 'analyzing', 'text', ',', 'focused', 'exclusively', 'text', '.']

 TOTAL FILTERED TOKENS ==>  31

 ---- POST FOR FILTERED TOKENS ----

 [('Lexalytics', 'NNS'), ('’', 'VBP'), ('core', 'NN'), ('text', 'NN'), ('analytics', 'NNS'), ('engine', 'NN'), (',', ','), ('Salience', 'NNP'), (',', ','), ('considered', 'VBN'), ('“', 'JJ'), ('narrow', 'JJ'), ('”', 'NN'), ('AI', 'NNP'), (':', ':'), ('uses', 'VBZ'), ('many', 'JJ'), ('different', 'JJ'), ('types', 'NNS'), ('machine', 'NN'), ('learning', 'VBG'), ('solve', 'JJ'), ('task', 'NN'), ('understanding', 'VBG'), ('analyzing', 'VBG'), ('text', 'NN'), (',', ','), ('focused', 'VBD'), ('exclusively', 'RB'), ('text', 'JJ'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Lexalytics ’', '’ core', 'core text', 'text analytics', 'analytics engine', 'engine ,', ', Salience', 'Salience ,', ', considered', 'considered “', '“ narrow', 'narrow ”', '” AI', 'AI :', ': uses', 'uses many', 'many different', 'different types', 'types machine', 'machine learning', 'learning solve', 'solve task', 'task understanding', 'understanding analyzing', 'analyzing text', 'text ,', ', focused', 'focused exclusively', 'exclusively text', 'text .'] 

 TOTAL BIGRAMS --> 30 



 ---- TRI-GRAMS ---- 

 ['Lexalytics ’ core', '’ core text', 'core text analytics', 'text analytics engine', 'analytics engine ,', 'engine , Salience', ', Salience ,', 'Salience , considered', ', considered “', 'considered “ narrow', '“ narrow ”', 'narrow ” AI', '” AI :', 'AI : uses', ': uses many', 'uses many different', 'many different types', 'different types machine', 'types machine learning', 'machine learning solve', 'learning solve task', 'solve task understanding', 'task understanding analyzing', 'understanding analyzing text', 'analyzing text ,', 'text , focused', ', focused exclusively', 'focused exclusively text', 'exclusively text .'] 

 TOTAL TRIGRAMS --> 29 



 ---- NOUN PHRASES ---- 

 ['core', 'text', 'engine', '“ narrow ”', 'machine', 'solve task', 'text'] 

 TOTAL NOUN PHRASES --> 7 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> ['Salience']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['lexalyt', '’', 'core', 'text', 'analyt', 'engin', ',', 'salienc', ',', 'consid', '“', 'narrow', '”', 'ai', ':', 'use', 'mani', 'differ', 'type', 'machin', 'learn', 'solv', 'task', 'understand', 'analyz', 'text', ',', 'focus', 'exclus', 'text', '.']

 TOTAL PORTER STEM WORDS ==> 31



 ---- SNOWBALL STEMMING ----

['lexalyt', '’', 'core', 'text', 'analyt', 'engin', ',', 'salienc', ',', 'consid', '“', 'narrow', '”', 'ai', ':', 'use', 'mani', 'differ', 'type', 'machin', 'learn', 'solv', 'task', 'understand', 'analyz', 'text', ',', 'focus', 'exclus', 'text', '.']

 TOTAL SNOWBALL STEM WORDS ==> 31



 ---- LEMMATIZATION ----

['Lexalytics', '’', 'core', 'text', 'analytics', 'engine', ',', 'Salience', ',', 'considered', '“', 'narrow', '”', 'AI', ':', 'us', 'many', 'different', 'type', 'machine', 'learning', 'solve', 'task', 'understanding', 'analyzing', 'text', ',', 'focused', 'exclusively', 'text', '.']

 TOTAL LEMMATIZE WORDS ==> 31

************************************************************************************************************************

5 --> We’ll be looking at the machine learning and natural language  processing (NLP) elements that Salience is built upon. 


 ---- TOKENS ----

 ['We', '’', 'll', 'be', 'looking', 'at', 'the', 'machine', 'learning', 'and', 'natural', 'language', 'processing', '(', 'NLP', ')', 'elements', 'that', 'Salience', 'is', 'built', 'upon', '.'] 

 TOTAL TOKENS ==> 23

 ---- POST ----

 [('We', 'PRP'), ('’', 'VBP'), ('ll', 'JJ'), ('be', 'VB'), ('looking', 'VBG'), ('at', 'IN'), ('the', 'DT'), ('machine', 'NN'), ('learning', 'NN'), ('and', 'CC'), ('natural', 'JJ'), ('language', 'NN'), ('processing', 'NN'), ('(', '('), ('NLP', 'NNP'), (')', ')'), ('elements', 'VBZ'), ('that', 'IN'), ('Salience', 'NNP'), ('is', 'VBZ'), ('built', 'VBN'), ('upon', 'IN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['’', 'looking', 'machine', 'learning', 'natural', 'language', 'processing', '(', 'NLP', ')', 'elements', 'Salience', 'built', 'upon', '.']

 TOTAL FILTERED TOKENS ==>  15

 ---- POST FOR FILTERED TOKENS ----

 [('’', 'JJ'), ('looking', 'VBG'), ('machine', 'NN'), ('learning', 'VBG'), ('natural', 'JJ'), ('language', 'NN'), ('processing', 'NN'), ('(', '('), ('NLP', 'NNP'), (')', ')'), ('elements', 'VBZ'), ('Salience', 'NNP'), ('built', 'VBN'), ('upon', 'IN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['’ looking', 'looking machine', 'machine learning', 'learning natural', 'natural language', 'language processing', 'processing (', '( NLP', 'NLP )', ') elements', 'elements Salience', 'Salience built', 'built upon', 'upon .'] 

 TOTAL BIGRAMS --> 14 



 ---- TRI-GRAMS ---- 

 ['’ looking machine', 'looking machine learning', 'machine learning natural', 'learning natural language', 'natural language processing', 'language processing (', 'processing ( NLP', '( NLP )', 'NLP ) elements', ') elements Salience', 'elements Salience built', 'Salience built upon', 'built upon .'] 

 TOTAL TRIGRAMS --> 13 



 ---- NOUN PHRASES ---- 

 ['machine', 'natural language', 'processing'] 

 TOTAL NOUN PHRASES --> 3 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> ['Salience']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['’', 'look', 'machin', 'learn', 'natur', 'languag', 'process', '(', 'nlp', ')', 'element', 'salienc', 'built', 'upon', '.']

 TOTAL PORTER STEM WORDS ==> 15



 ---- SNOWBALL STEMMING ----

['’', 'look', 'machin', 'learn', 'natur', 'languag', 'process', '(', 'nlp', ')', 'element', 'salienc', 'built', 'upon', '.']

 TOTAL SNOWBALL STEM WORDS ==> 15



 ---- LEMMATIZATION ----

['’', 'looking', 'machine', 'learning', 'natural', 'language', 'processing', '(', 'NLP', ')', 'element', 'Salience', 'built', 'upon', '.']

 TOTAL LEMMATIZE WORDS ==> 15

************************************************************************************************************************

6 --> We’ll discuss the different aspects of text analytics and how Lexalytics,  a company with more than a decade of experience in machine learning,  applies machine learning to solve problems in natural language processing. 


 ---- TOKENS ----

 ['We', '’', 'll', 'discuss', 'the', 'different', 'aspects', 'of', 'text', 'analytics', 'and', 'how', 'Lexalytics', ',', 'a', 'company', 'with', 'more', 'than', 'a', 'decade', 'of', 'experience', 'in', 'machine', 'learning', ',', 'applies', 'machine', 'learning', 'to', 'solve', 'problems', 'in', 'natural', 'language', 'processing', '.'] 

 TOTAL TOKENS ==> 38

 ---- POST ----

 [('We', 'PRP'), ('’', 'VBP'), ('ll', 'JJ'), ('discuss', 'VBP'), ('the', 'DT'), ('different', 'JJ'), ('aspects', 'NNS'), ('of', 'IN'), ('text', 'NN'), ('analytics', 'NNS'), ('and', 'CC'), ('how', 'WRB'), ('Lexalytics', 'NNP'), (',', ','), ('a', 'DT'), ('company', 'NN'), ('with', 'IN'), ('more', 'JJR'), ('than', 'IN'), ('a', 'DT'), ('decade', 'NN'), ('of', 'IN'), ('experience', 'NN'), ('in', 'IN'), ('machine', 'NN'), ('learning', 'NN'), (',', ','), ('applies', 'NNS'), ('machine', 'NN'), ('learning', 'VBG'), ('to', 'TO'), ('solve', 'VB'), ('problems', 'NNS'), ('in', 'IN'), ('natural', 'JJ'), ('language', 'NN'), ('processing', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['’', 'discuss', 'different', 'aspects', 'text', 'analytics', 'Lexalytics', ',', 'company', 'decade', 'experience', 'machine', 'learning', ',', 'applies', 'machine', 'learning', 'solve', 'problems', 'natural', 'language', 'processing', '.']

 TOTAL FILTERED TOKENS ==>  23

 ---- POST FOR FILTERED TOKENS ----

 [('’', 'JJ'), ('discuss', 'NN'), ('different', 'JJ'), ('aspects', 'NNS'), ('text', 'JJ'), ('analytics', 'NNS'), ('Lexalytics', 'NNS'), (',', ','), ('company', 'NN'), ('decade', 'NN'), ('experience', 'NN'), ('machine', 'NN'), ('learning', 'NN'), (',', ','), ('applies', 'NNS'), ('machine', 'NN'), ('learning', 'VBG'), ('solve', 'VB'), ('problems', 'NNS'), ('natural', 'JJ'), ('language', 'NN'), ('processing', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['’ discuss', 'discuss different', 'different aspects', 'aspects text', 'text analytics', 'analytics Lexalytics', 'Lexalytics ,', ', company', 'company decade', 'decade experience', 'experience machine', 'machine learning', 'learning ,', ', applies', 'applies machine', 'machine learning', 'learning solve', 'solve problems', 'problems natural', 'natural language', 'language processing', 'processing .'] 

 TOTAL BIGRAMS --> 22 



 ---- TRI-GRAMS ---- 

 ['’ discuss different', 'discuss different aspects', 'different aspects text', 'aspects text analytics', 'text analytics Lexalytics', 'analytics Lexalytics ,', 'Lexalytics , company', ', company decade', 'company decade experience', 'decade experience machine', 'experience machine learning', 'machine learning ,', 'learning , applies', ', applies machine', 'applies machine learning', 'machine learning solve', 'learning solve problems', 'solve problems natural', 'problems natural language', 'natural language processing', 'language processing .'] 

 TOTAL TRIGRAMS --> 21 



 ---- NOUN PHRASES ---- 

 ['’ discuss', 'company', 'decade', 'experience', 'machine', 'learning', 'machine', 'natural language', 'processing'] 

 TOTAL NOUN PHRASES --> 9 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['’', 'discuss', 'differ', 'aspect', 'text', 'analyt', 'lexalyt', ',', 'compani', 'decad', 'experi', 'machin', 'learn', ',', 'appli', 'machin', 'learn', 'solv', 'problem', 'natur', 'languag', 'process', '.']

 TOTAL PORTER STEM WORDS ==> 23



 ---- SNOWBALL STEMMING ----

['’', 'discuss', 'differ', 'aspect', 'text', 'analyt', 'lexalyt', ',', 'compani', 'decad', 'experi', 'machin', 'learn', ',', 'appli', 'machin', 'learn', 'solv', 'problem', 'natur', 'languag', 'process', '.']

 TOTAL SNOWBALL STEM WORDS ==> 23



 ---- LEMMATIZATION ----

['’', 'discus', 'different', 'aspect', 'text', 'analytics', 'Lexalytics', ',', 'company', 'decade', 'experience', 'machine', 'learning', ',', 'applies', 'machine', 'learning', 'solve', 'problem', 'natural', 'language', 'processing', '.']

 TOTAL LEMMATIZE WORDS ==> 23

************************************************************************************************************************

7 --> 3 KINDS OF TEXT ANALYTICS SYSTEMS   Rules-based (pure NLP)    Machine learning-based (pure ML)   Hybrid (a combination of ML and NLP) For further reading, you can consult our white papers “Build vs. Buy,”  which talks about the economics of machine learning in a text analytics  context, and “Tune First, Then Train,” which discusses our philosophy   of customization for better accuracy and more-relevant results. 


 ---- TOKENS ----

 ['3', 'KINDS', 'OF', 'TEXT', 'ANALYTICS', 'SYSTEMS', 'Rules-based', '(', 'pure', 'NLP', ')', 'Machine', 'learning-based', '(', 'pure', 'ML', ')', 'Hybrid', '(', 'a', 'combination', 'of', 'ML', 'and', 'NLP', ')', 'For', 'further', 'reading', ',', 'you', 'can', 'consult', 'our', 'white', 'papers', '“', 'Build', 'vs.', 'Buy', ',', '”', 'which', 'talks', 'about', 'the', 'economics', 'of', 'machine', 'learning', 'in', 'a', 'text', 'analytics', 'context', ',', 'and', '“', 'Tune', 'First', ',', 'Then', 'Train', ',', '”', 'which', 'discusses', 'our', 'philosophy', 'of', 'customization', 'for', 'better', 'accuracy', 'and', 'more-relevant', 'results', '.'] 

 TOTAL TOKENS ==> 78

 ---- POST ----

 [('3', 'CD'), ('KINDS', 'NNP'), ('OF', 'NNP'), ('TEXT', 'NNP'), ('ANALYTICS', 'NNP'), ('SYSTEMS', 'NNP'), ('Rules-based', 'JJ'), ('(', '('), ('pure', 'JJ'), ('NLP', 'NNP'), (')', ')'), ('Machine', 'NNP'), ('learning-based', 'JJ'), ('(', '('), ('pure', 'JJ'), ('ML', 'NNP'), (')', ')'), ('Hybrid', 'NNP'), ('(', '('), ('a', 'DT'), ('combination', 'NN'), ('of', 'IN'), ('ML', 'NNP'), ('and', 'CC'), ('NLP', 'NNP'), (')', ')'), ('For', 'IN'), ('further', 'JJ'), ('reading', 'NN'), (',', ','), ('you', 'PRP'), ('can', 'MD'), ('consult', 'VB'), ('our', 'PRP$'), ('white', 'JJ'), ('papers', 'NNS'), ('“', 'VBP'), ('Build', 'NNP'), ('vs.', 'FW'), ('Buy', 'NNP'), (',', ','), ('”', 'NNP'), ('which', 'WDT'), ('talks', 'VBZ'), ('about', 'IN'), ('the', 'DT'), ('economics', 'NNS'), ('of', 'IN'), ('machine', 'NN'), ('learning', 'NN'), ('in', 'IN'), ('a', 'DT'), ('text', 'NN'), ('analytics', 'NN'), ('context', 'NN'), (',', ','), ('and', 'CC'), ('“', 'NNP'), ('Tune', 'NNP'), ('First', 'NNP'), (',', ','), ('Then', 'RB'), ('Train', 'NNP'), (',', ','), ('”', 'NNP'), ('which', 'WDT'), ('discusses', 'VBZ'), ('our', 'PRP$'), ('philosophy', 'NN'), ('of', 'IN'), ('customization', 'NN'), ('for', 'IN'), ('better', 'JJR'), ('accuracy', 'NN'), ('and', 'CC'), ('more-relevant', 'JJ'), ('results', 'NNS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['3', 'KINDS', 'TEXT', 'ANALYTICS', 'SYSTEMS', 'Rules-based', '(', 'pure', 'NLP', ')', 'Machine', 'learning-based', '(', 'pure', 'ML', ')', 'Hybrid', '(', 'combination', 'ML', 'NLP', ')', 'reading', ',', 'consult', 'white', 'papers', '“', 'Build', 'vs.', 'Buy', ',', '”', 'talks', 'economics', 'machine', 'learning', 'text', 'analytics', 'context', ',', '“', 'Tune', 'First', ',', 'Train', ',', '”', 'discusses', 'philosophy', 'customization', 'better', 'accuracy', 'more-relevant', 'results', '.']

 TOTAL FILTERED TOKENS ==>  56

 ---- POST FOR FILTERED TOKENS ----

 [('3', 'CD'), ('KINDS', 'NNP'), ('TEXT', 'NNP'), ('ANALYTICS', 'NNP'), ('SYSTEMS', 'NNP'), ('Rules-based', 'JJ'), ('(', '('), ('pure', 'JJ'), ('NLP', 'NNP'), (')', ')'), ('Machine', 'NNP'), ('learning-based', 'JJ'), ('(', '('), ('pure', 'JJ'), ('ML', 'NNP'), (')', ')'), ('Hybrid', 'NNP'), ('(', '('), ('combination', 'NN'), ('ML', 'NNP'), ('NLP', 'NNP'), (')', ')'), ('reading', 'NN'), (',', ','), ('consult', 'NN'), ('white', 'JJ'), ('papers', 'NNS'), ('“', 'VBP'), ('Build', 'NNP'), ('vs.', 'FW'), ('Buy', 'NNP'), (',', ','), ('”', 'JJ'), ('talks', 'NNS'), ('economics', 'VBP'), ('machine', 'NN'), ('learning', 'VBG'), ('text', 'JJ'), ('analytics', 'NNS'), ('context', 'NN'), (',', ','), ('“', 'NNP'), ('Tune', 'NNP'), ('First', 'NNP'), (',', ','), ('Train', 'NNP'), (',', ','), ('”', 'NNP'), ('discusses', 'VBZ'), ('philosophy', 'NN'), ('customization', 'NN'), ('better', 'RBR'), ('accuracy', 'NN'), ('more-relevant', 'JJ'), ('results', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['3 KINDS', 'KINDS TEXT', 'TEXT ANALYTICS', 'ANALYTICS SYSTEMS', 'SYSTEMS Rules-based', 'Rules-based (', '( pure', 'pure NLP', 'NLP )', ') Machine', 'Machine learning-based', 'learning-based (', '( pure', 'pure ML', 'ML )', ') Hybrid', 'Hybrid (', '( combination', 'combination ML', 'ML NLP', 'NLP )', ') reading', 'reading ,', ', consult', 'consult white', 'white papers', 'papers “', '“ Build', 'Build vs.', 'vs. Buy', 'Buy ,', ', ”', '” talks', 'talks economics', 'economics machine', 'machine learning', 'learning text', 'text analytics', 'analytics context', 'context ,', ', “', '“ Tune', 'Tune First', 'First ,', ', Train', 'Train ,', ', ”', '” discusses', 'discusses philosophy', 'philosophy customization', 'customization better', 'better accuracy', 'accuracy more-relevant', 'more-relevant results', 'results .'] 

 TOTAL BIGRAMS --> 55 



 ---- TRI-GRAMS ---- 

 ['3 KINDS TEXT', 'KINDS TEXT ANALYTICS', 'TEXT ANALYTICS SYSTEMS', 'ANALYTICS SYSTEMS Rules-based', 'SYSTEMS Rules-based (', 'Rules-based ( pure', '( pure NLP', 'pure NLP )', 'NLP ) Machine', ') Machine learning-based', 'Machine learning-based (', 'learning-based ( pure', '( pure ML', 'pure ML )', 'ML ) Hybrid', ') Hybrid (', 'Hybrid ( combination', '( combination ML', 'combination ML NLP', 'ML NLP )', 'NLP ) reading', ') reading ,', 'reading , consult', ', consult white', 'consult white papers', 'white papers “', 'papers “ Build', '“ Build vs.', 'Build vs. Buy', 'vs. Buy ,', 'Buy , ”', ', ” talks', '” talks economics', 'talks economics machine', 'economics machine learning', 'machine learning text', 'learning text analytics', 'text analytics context', 'analytics context ,', 'context , “', ', “ Tune', '“ Tune First', 'Tune First ,', 'First , Train', ', Train ,', 'Train , ”', ', ” discusses', '” discusses philosophy', 'discusses philosophy customization', 'philosophy customization better', 'customization better accuracy', 'better accuracy more-relevant', 'accuracy more-relevant results', 'more-relevant results .'] 

 TOTAL TRIGRAMS --> 54 



 ---- NOUN PHRASES ---- 

 ['reading', 'consult', 'machine', 'context', 'philosophy', 'customization', 'accuracy'] 

 TOTAL NOUN PHRASES --> 7 



 ---- NER ----

 
 ORGANIZATION ---> ['KINDS']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> ['Machine', 'Build']
 TOTAL PERSON ENTITY --> 2 


 GPE ---> ['Hybrid', 'Train']
 TOTAL GPE ENTITY --> 2 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['3', 'kind', 'text', 'analyt', 'system', 'rules-bas', '(', 'pure', 'nlp', ')', 'machin', 'learning-bas', '(', 'pure', 'ml', ')', 'hybrid', '(', 'combin', 'ml', 'nlp', ')', 'read', ',', 'consult', 'white', 'paper', '“', 'build', 'vs.', 'buy', ',', '”', 'talk', 'econom', 'machin', 'learn', 'text', 'analyt', 'context', ',', '“', 'tune', 'first', ',', 'train', ',', '”', 'discuss', 'philosophi', 'custom', 'better', 'accuraci', 'more-relev', 'result', '.']

 TOTAL PORTER STEM WORDS ==> 56



 ---- SNOWBALL STEMMING ----

['3', 'kind', 'text', 'analyt', 'system', 'rules-bas', '(', 'pure', 'nlp', ')', 'machin', 'learning-bas', '(', 'pure', 'ml', ')', 'hybrid', '(', 'combin', 'ml', 'nlp', ')', 'read', ',', 'consult', 'white', 'paper', '“', 'build', 'vs.', 'buy', ',', '”', 'talk', 'econom', 'machin', 'learn', 'text', 'analyt', 'context', ',', '“', 'tune', 'first', ',', 'train', ',', '”', 'discuss', 'philosophi', 'custom', 'better', 'accuraci', 'more-relev', 'result', '.']

 TOTAL SNOWBALL STEM WORDS ==> 56



 ---- LEMMATIZATION ----

['3', 'KINDS', 'TEXT', 'ANALYTICS', 'SYSTEMS', 'Rules-based', '(', 'pure', 'NLP', ')', 'Machine', 'learning-based', '(', 'pure', 'ML', ')', 'Hybrid', '(', 'combination', 'ML', 'NLP', ')', 'reading', ',', 'consult', 'white', 'paper', '“', 'Build', 'vs.', 'Buy', ',', '”', 'talk', 'economics', 'machine', 'learning', 'text', 'analytics', 'context', ',', '“', 'Tune', 'First', ',', 'Train', ',', '”', 'discus', 'philosophy', 'customization', 'better', 'accuracy', 'more-relevant', 'result', '.']

 TOTAL LEMMATIZE WORDS ==> 56

************************************************************************************************************************

8 --> When  taken together with this paper, these resources offer a more complete  view of text analytics solutions. 


 ---- TOKENS ----

 ['When', 'taken', 'together', 'with', 'this', 'paper', ',', 'these', 'resources', 'offer', 'a', 'more', 'complete', 'view', 'of', 'text', 'analytics', 'solutions', '.'] 

 TOTAL TOKENS ==> 19

 ---- POST ----

 [('When', 'WRB'), ('taken', 'VBN'), ('together', 'RB'), ('with', 'IN'), ('this', 'DT'), ('paper', 'NN'), (',', ','), ('these', 'DT'), ('resources', 'NNS'), ('offer', 'VBP'), ('a', 'DT'), ('more', 'RBR'), ('complete', 'JJ'), ('view', 'NN'), ('of', 'IN'), ('text', 'NN'), ('analytics', 'NNS'), ('solutions', 'NNS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['taken', 'together', 'paper', ',', 'resources', 'offer', 'complete', 'view', 'text', 'analytics', 'solutions', '.']

 TOTAL FILTERED TOKENS ==>  12

 ---- POST FOR FILTERED TOKENS ----

 [('taken', 'VBN'), ('together', 'RB'), ('paper', 'NN'), (',', ','), ('resources', 'NNS'), ('offer', 'VBP'), ('complete', 'JJ'), ('view', 'NN'), ('text', 'IN'), ('analytics', 'NNS'), ('solutions', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['taken together', 'together paper', 'paper ,', ', resources', 'resources offer', 'offer complete', 'complete view', 'view text', 'text analytics', 'analytics solutions', 'solutions .'] 

 TOTAL BIGRAMS --> 11 



 ---- TRI-GRAMS ---- 

 ['taken together paper', 'together paper ,', 'paper , resources', ', resources offer', 'resources offer complete', 'offer complete view', 'complete view text', 'view text analytics', 'text analytics solutions', 'analytics solutions .'] 

 TOTAL TRIGRAMS --> 10 



 ---- NOUN PHRASES ---- 

 ['paper', 'complete view'] 

 TOTAL NOUN PHRASES --> 2 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['taken', 'togeth', 'paper', ',', 'resourc', 'offer', 'complet', 'view', 'text', 'analyt', 'solut', '.']

 TOTAL PORTER STEM WORDS ==> 12



 ---- SNOWBALL STEMMING ----

['taken', 'togeth', 'paper', ',', 'resourc', 'offer', 'complet', 'view', 'text', 'analyt', 'solut', '.']

 TOTAL SNOWBALL STEM WORDS ==> 12



 ---- LEMMATIZATION ----

['taken', 'together', 'paper', ',', 'resource', 'offer', 'complete', 'view', 'text', 'analytics', 'solution', '.']

 TOTAL LEMMATIZE WORDS ==> 12

************************************************************************************************************************

9 --> Machine Learning   is Really Machine Teaching  .........................3   Supervised, Semi-Supervised and  Unsupervised Machine Learning   Supervised Learning ..............................5  Semi-Supervised Learning ................6  Unsupervised Learning ........................6 Happier by the Dozen:   The More Models, the Merrier .................... 7 Coding vs. Learning:   Making the Case for Each ............................9 Black Box/Clear Box:   Looking Inside the Data ............................... 10 Tune First, Then Train:   Efficiency before Complexity ....................12 Summary/Conclusion .................................. 14 https://www.lexalytics.com/ https://www.lexalytics.com/ https://www.lexalytics.com/resources/wp-content/uploads/sites/3/2019/02/Lexalytics_Tune_First_Then_Train_Whitepaper.pdf https://www.lexalytics.com/lexablog/build-or-buy-text-analytics-natural-language-processing  W H I T E  P A P E R 3|       |   Lexalytics, Inc., 48 North Pleasant St. Unit 301, Amherst MA 01002 USA   |   1-800-377-8036 |   www.lexalytics.com M A C H I N E  L E A R N I N G   I S  R E A L L Y  M A C H I N E  T E A C H I N G  Before we start delving into the different aspects of text analytics, let’s clarify  some basic machine learning concepts. 


 ---- TOKENS ----

 ['Machine', 'Learning', 'is', 'Really', 'Machine', 'Teaching', '.........................', '3', 'Supervised', ',', 'Semi-Supervised', 'and', 'Unsupervised', 'Machine', 'Learning', 'Supervised', 'Learning', '..............................', '5', 'Semi-Supervised', 'Learning', '................', '6', 'Unsupervised', 'Learning', '........................', '6', 'Happier', 'by', 'the', 'Dozen', ':', 'The', 'More', 'Models', ',', 'the', 'Merrier', '....................', '7', 'Coding', 'vs.', 'Learning', ':', 'Making', 'the', 'Case', 'for', 'Each', '............................', '9', 'Black', 'Box/Clear', 'Box', ':', 'Looking', 'Inside', 'the', 'Data', '...............................', '10', 'Tune', 'First', ',', 'Then', 'Train', ':', 'Efficiency', 'before', 'Complexity', '....................', '12', 'Summary/Conclusion', '..................................', '14', 'https', ':', '//www.lexalytics.com/', 'https', ':', '//www.lexalytics.com/', 'https', ':', '//www.lexalytics.com/resources/wp-content/uploads/sites/3/2019/02/Lexalytics_Tune_First_Then_Train_Whitepaper.pdf', 'https', ':', '//www.lexalytics.com/lexablog/build-or-buy-text-analytics-natural-language-processing', 'W', 'H', 'I', 'T', 'E', 'P', 'A', 'P', 'E', 'R', '3|', '|', 'Lexalytics', ',', 'Inc.', ',', '48', 'North', 'Pleasant', 'St.', 'Unit', '301', ',', 'Amherst', 'MA', '01002', 'USA', '|', '1-800-377-8036', '|', 'www.lexalytics.com', 'M', 'A', 'C', 'H', 'I', 'N', 'E', 'L', 'E', 'A', 'R', 'N', 'I', 'N', 'G', 'I', 'S', 'R', 'E', 'A', 'L', 'L', 'Y', 'M', 'A', 'C', 'H', 'I', 'N', 'E', 'T', 'E', 'A', 'C', 'H', 'I', 'N', 'G', 'Before', 'we', 'start', 'delving', 'into', 'the', 'different', 'aspects', 'of', 'text', 'analytics', ',', 'let', '’', 's', 'clarify', 'some', 'basic', 'machine', 'learning', 'concepts', '.'] 

 TOTAL TOKENS ==> 178

 ---- POST ----

 [('Machine', 'NN'), ('Learning', 'NNP'), ('is', 'VBZ'), ('Really', 'RB'), ('Machine', 'JJ'), ('Teaching', 'NNP'), ('.........................', 'NN'), ('3', 'CD'), ('Supervised', 'VBN'), (',', ','), ('Semi-Supervised', 'JJ'), ('and', 'CC'), ('Unsupervised', 'JJ'), ('Machine', 'NNP'), ('Learning', 'NNP'), ('Supervised', 'VBD'), ('Learning', 'NNP'), ('..............................', 'JJ'), ('5', 'CD'), ('Semi-Supervised', 'JJ'), ('Learning', 'NNP'), ('................', 'NNP'), ('6', 'CD'), ('Unsupervised', 'VBD'), ('Learning', 'NNP'), ('........................', 'NNP'), ('6', 'CD'), ('Happier', 'NNP'), ('by', 'IN'), ('the', 'DT'), ('Dozen', 'NNP'), (':', ':'), ('The', 'DT'), ('More', 'JJR'), ('Models', 'NNS'), (',', ','), ('the', 'DT'), ('Merrier', 'NNP'), ('....................', 'NNP'), ('7', 'CD'), ('Coding', 'NNP'), ('vs.', 'FW'), ('Learning', 'NNP'), (':', ':'), ('Making', 'VBG'), ('the', 'DT'), ('Case', 'NNP'), ('for', 'IN'), ('Each', 'DT'), ('............................', 'CD'), ('9', 'CD'), ('Black', 'NNP'), ('Box/Clear', 'NNP'), ('Box', 'NNP'), (':', ':'), ('Looking', 'VBG'), ('Inside', 'IN'), ('the', 'DT'), ('Data', 'NNP'), ('...............................', 'NNP'), ('10', 'CD'), ('Tune', 'NNP'), ('First', 'NNP'), (',', ','), ('Then', 'RB'), ('Train', 'NN'), (':', ':'), ('Efficiency', 'NN'), ('before', 'IN'), ('Complexity', 'NNP'), ('....................', 'VBP'), ('12', 'CD'), ('Summary/Conclusion', 'NNP'), ('..................................', 'VBD'), ('14', 'CD'), ('https', 'NN'), (':', ':'), ('//www.lexalytics.com/', 'JJ'), ('https', 'NN'), (':', ':'), ('//www.lexalytics.com/', 'JJ'), ('https', 'NN'), (':', ':'), ('//www.lexalytics.com/resources/wp-content/uploads/sites/3/2019/02/Lexalytics_Tune_First_Then_Train_Whitepaper.pdf', 'JJ'), ('https', 'NN'), (':', ':'), ('//www.lexalytics.com/lexablog/build-or-buy-text-analytics-natural-language-processing', 'JJ'), ('W', 'NNP'), ('H', 'NNP'), ('I', 'PRP'), ('T', 'NNP'), ('E', 'NNP'), ('P', 'NNP'), ('A', 'NNP'), ('P', 'NNP'), ('E', 'NNP'), ('R', 'NNP'), ('3|', 'CD'), ('|', 'NNP'), ('Lexalytics', 'NNP'), (',', ','), ('Inc.', 'NNP'), (',', ','), ('48', 'CD'), ('North', 'NNP'), ('Pleasant', 'NNP'), ('St.', 'NNP'), ('Unit', 'NNP'), ('301', 'CD'), (',', ','), ('Amherst', 'NNP'), ('MA', 'NNP'), ('01002', 'CD'), ('USA', 'NNP'), ('|', 'NNP'), ('1-800-377-8036', 'JJ'), ('|', 'NNP'), ('www.lexalytics.com', 'NN'), ('M', 'NNP'), ('A', 'NNP'), ('C', 'NNP'), ('H', 'NNP'), ('I', 'PRP'), ('N', 'NNP'), ('E', 'NNP'), ('L', 'NNP'), ('E', 'NNP'), ('A', 'NNP'), ('R', 'NNP'), ('N', 'NNP'), ('I', 'PRP'), ('N', 'NNP'), ('G', 'NNP'), ('I', 'PRP'), ('S', 'NNP'), ('R', 'NNP'), ('E', 'NNP'), ('A', 'NNP'), ('L', 'NNP'), ('L', 'NNP'), ('Y', 'NNP'), ('M', 'NNP'), ('A', 'NNP'), ('C', 'NNP'), ('H', 'NNP'), ('I', 'PRP'), ('N', 'NNP'), ('E', 'NNP'), ('T', 'NNP'), ('E', 'NNP'), ('A', 'NNP'), ('C', 'NNP'), ('H', 'NNP'), ('I', 'PRP'), ('N', 'NNP'), ('G', 'NNP'), ('Before', 'IN'), ('we', 'PRP'), ('start', 'VBP'), ('delving', 'VBG'), ('into', 'IN'), ('the', 'DT'), ('different', 'JJ'), ('aspects', 'NNS'), ('of', 'IN'), ('text', 'NN'), ('analytics', 'NNS'), (',', ','), ('let', 'VB'), ('’', 'NNP'), ('s', 'VB'), ('clarify', 'VB'), ('some', 'DT'), ('basic', 'JJ'), ('machine', 'NN'), ('learning', 'VBG'), ('concepts', 'NNS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Machine', 'Learning', 'Really', 'Machine', 'Teaching', '.........................', '3', 'Supervised', ',', 'Semi-Supervised', 'Unsupervised', 'Machine', 'Learning', 'Supervised', 'Learning', '..............................', '5', 'Semi-Supervised', 'Learning', '................', '6', 'Unsupervised', 'Learning', '........................', '6', 'Happier', 'Dozen', ':', 'Models', ',', 'Merrier', '....................', '7', 'Coding', 'vs.', 'Learning', ':', 'Making', 'Case', '............................', '9', 'Black', 'Box/Clear', 'Box', ':', 'Looking', 'Inside', 'Data', '...............................', '10', 'Tune', 'First', ',', 'Train', ':', 'Efficiency', 'Complexity', '....................', '12', 'Summary/Conclusion', '..................................', '14', 'https', ':', '//www.lexalytics.com/', 'https', ':', '//www.lexalytics.com/', 'https', ':', '//www.lexalytics.com/resources/wp-content/uploads/sites/3/2019/02/Lexalytics_Tune_First_Then_Train_Whitepaper.pdf', 'https', ':', '//www.lexalytics.com/lexablog/build-or-buy-text-analytics-natural-language-processing', 'W', 'H', 'E', 'P', 'P', 'E', 'R', '3|', '|', 'Lexalytics', ',', 'Inc.', ',', '48', 'North', 'Pleasant', 'St.', 'Unit', '301', ',', 'Amherst', '01002', 'USA', '|', '1-800-377-8036', '|', 'www.lexalytics.com', 'C', 'H', 'N', 'E', 'L', 'E', 'R', 'N', 'N', 'G', 'R', 'E', 'L', 'L', 'C', 'H', 'N', 'E', 'E', 'C', 'H', 'N', 'G', 'start', 'delving', 'different', 'aspects', 'text', 'analytics', ',', 'let', '’', 'clarify', 'basic', 'machine', 'learning', 'concepts', '.']

 TOTAL FILTERED TOKENS ==>  139

 ---- POST FOR FILTERED TOKENS ----

 [('Machine', 'NN'), ('Learning', 'NNP'), ('Really', 'NNP'), ('Machine', 'NNP'), ('Teaching', 'NNP'), ('.........................', 'VBD'), ('3', 'CD'), ('Supervised', 'JJ'), (',', ','), ('Semi-Supervised', 'JJ'), ('Unsupervised', 'JJ'), ('Machine', 'NNP'), ('Learning', 'NNP'), ('Supervised', 'VBD'), ('Learning', 'NNP'), ('..............................', 'JJ'), ('5', 'CD'), ('Semi-Supervised', 'JJ'), ('Learning', 'NNP'), ('................', 'NNP'), ('6', 'CD'), ('Unsupervised', 'VBD'), ('Learning', 'NNP'), ('........................', 'NNP'), ('6', 'CD'), ('Happier', 'NNP'), ('Dozen', 'NNP'), (':', ':'), ('Models', 'NNS'), (',', ','), ('Merrier', 'NNP'), ('....................', 'VBZ'), ('7', 'CD'), ('Coding', 'NNP'), ('vs.', 'FW'), ('Learning', 'NNP'), (':', ':'), ('Making', 'VBG'), ('Case', 'NNP'), ('............................', 'NNP'), ('9', 'CD'), ('Black', 'NNP'), ('Box/Clear', 'NNP'), ('Box', 'NNP'), (':', ':'), ('Looking', 'VBG'), ('Inside', 'NNP'), ('Data', 'NNP'), ('...............................', 'NNP'), ('10', 'CD'), ('Tune', 'NNP'), ('First', 'NNP'), (',', ','), ('Train', 'NN'), (':', ':'), ('Efficiency', 'NN'), ('Complexity', 'NNP'), ('....................', 'NNP'), ('12', 'CD'), ('Summary/Conclusion', 'NNP'), ('..................................', 'VBD'), ('14', 'CD'), ('https', 'NN'), (':', ':'), ('//www.lexalytics.com/', 'JJ'), ('https', 'NN'), (':', ':'), ('//www.lexalytics.com/', 'JJ'), ('https', 'NN'), (':', ':'), ('//www.lexalytics.com/resources/wp-content/uploads/sites/3/2019/02/Lexalytics_Tune_First_Then_Train_Whitepaper.pdf', 'JJ'), ('https', 'NN'), (':', ':'), ('//www.lexalytics.com/lexablog/build-or-buy-text-analytics-natural-language-processing', 'JJ'), ('W', 'NNP'), ('H', 'NNP'), ('E', 'NNP'), ('P', 'NNP'), ('P', 'NNP'), ('E', 'NNP'), ('R', 'NNP'), ('3|', 'CD'), ('|', 'NNP'), ('Lexalytics', 'NNP'), (',', ','), ('Inc.', 'NNP'), (',', ','), ('48', 'CD'), ('North', 'NNP'), ('Pleasant', 'NNP'), ('St.', 'NNP'), ('Unit', 'NNP'), ('301', 'CD'), (',', ','), ('Amherst', 'NNP'), ('01002', 'CD'), ('USA', 'NNP'), ('|', 'NNP'), ('1-800-377-8036', 'JJ'), ('|', 'NNP'), ('www.lexalytics.com', 'NN'), ('C', 'NNP'), ('H', 'NNP'), ('N', 'NNP'), ('E', 'NNP'), ('L', 'NNP'), ('E', 'NNP'), ('R', 'NNP'), ('N', 'NNP'), ('N', 'NNP'), ('G', 'NNP'), ('R', 'NNP'), ('E', 'NNP'), ('L', 'NNP'), ('L', 'NNP'), ('C', 'NNP'), ('H', 'NNP'), ('N', 'NNP'), ('E', 'NNP'), ('E', 'NNP'), ('C', 'NNP'), ('H', 'NNP'), ('N', 'NNP'), ('G', 'NNP'), ('start', 'VBP'), ('delving', 'VBG'), ('different', 'JJ'), ('aspects', 'NNS'), ('text', 'JJ'), ('analytics', 'NNS'), (',', ','), ('let', 'VB'), ('’', 'NNP'), ('clarify', 'VB'), ('basic', 'JJ'), ('machine', 'NN'), ('learning', 'VBG'), ('concepts', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Machine Learning', 'Learning Really', 'Really Machine', 'Machine Teaching', 'Teaching .........................', '......................... 3', '3 Supervised', 'Supervised ,', ', Semi-Supervised', 'Semi-Supervised Unsupervised', 'Unsupervised Machine', 'Machine Learning', 'Learning Supervised', 'Supervised Learning', 'Learning ..............................', '.............................. 5', '5 Semi-Supervised', 'Semi-Supervised Learning', 'Learning ................', '................ 6', '6 Unsupervised', 'Unsupervised Learning', 'Learning ........................', '........................ 6', '6 Happier', 'Happier Dozen', 'Dozen :', ': Models', 'Models ,', ', Merrier', 'Merrier ....................', '.................... 7', '7 Coding', 'Coding vs.', 'vs. Learning', 'Learning :', ': Making', 'Making Case', 'Case ............................', '............................ 9', '9 Black', 'Black Box/Clear', 'Box/Clear Box', 'Box :', ': Looking', 'Looking Inside', 'Inside Data', 'Data ...............................', '............................... 10', '10 Tune', 'Tune First', 'First ,', ', Train', 'Train :', ': Efficiency', 'Efficiency Complexity', 'Complexity ....................', '.................... 12', '12 Summary/Conclusion', 'Summary/Conclusion ..................................', '.................................. 14', '14 https', 'https :', ': //www.lexalytics.com/', '//www.lexalytics.com/ https', 'https :', ': //www.lexalytics.com/', '//www.lexalytics.com/ https', 'https :', ': //www.lexalytics.com/resources/wp-content/uploads/sites/3/2019/02/Lexalytics_Tune_First_Then_Train_Whitepaper.pdf', '//www.lexalytics.com/resources/wp-content/uploads/sites/3/2019/02/Lexalytics_Tune_First_Then_Train_Whitepaper.pdf https', 'https :', ': //www.lexalytics.com/lexablog/build-or-buy-text-analytics-natural-language-processing', '//www.lexalytics.com/lexablog/build-or-buy-text-analytics-natural-language-processing W', 'W H', 'H E', 'E P', 'P P', 'P E', 'E R', 'R 3|', '3| |', '| Lexalytics', 'Lexalytics ,', ', Inc.', 'Inc. ,', ', 48', '48 North', 'North Pleasant', 'Pleasant St.', 'St. Unit', 'Unit 301', '301 ,', ', Amherst', 'Amherst 01002', '01002 USA', 'USA |', '| 1-800-377-8036', '1-800-377-8036 |', '| www.lexalytics.com', 'www.lexalytics.com C', 'C H', 'H N', 'N E', 'E L', 'L E', 'E R', 'R N', 'N N', 'N G', 'G R', 'R E', 'E L', 'L L', 'L C', 'C H', 'H N', 'N E', 'E E', 'E C', 'C H', 'H N', 'N G', 'G start', 'start delving', 'delving different', 'different aspects', 'aspects text', 'text analytics', 'analytics ,', ', let', 'let ’', '’ clarify', 'clarify basic', 'basic machine', 'machine learning', 'learning concepts', 'concepts .'] 

 TOTAL BIGRAMS --> 138 



 ---- TRI-GRAMS ---- 

 ['Machine Learning Really', 'Learning Really Machine', 'Really Machine Teaching', 'Machine Teaching .........................', 'Teaching ......................... 3', '......................... 3 Supervised', '3 Supervised ,', 'Supervised , Semi-Supervised', ', Semi-Supervised Unsupervised', 'Semi-Supervised Unsupervised Machine', 'Unsupervised Machine Learning', 'Machine Learning Supervised', 'Learning Supervised Learning', 'Supervised Learning ..............................', 'Learning .............................. 5', '.............................. 5 Semi-Supervised', '5 Semi-Supervised Learning', 'Semi-Supervised Learning ................', 'Learning ................ 6', '................ 6 Unsupervised', '6 Unsupervised Learning', 'Unsupervised Learning ........................', 'Learning ........................ 6', '........................ 6 Happier', '6 Happier Dozen', 'Happier Dozen :', 'Dozen : Models', ': Models ,', 'Models , Merrier', ', Merrier ....................', 'Merrier .................... 7', '.................... 7 Coding', '7 Coding vs.', 'Coding vs. Learning', 'vs. Learning :', 'Learning : Making', ': Making Case', 'Making Case ............................', 'Case ............................ 9', '............................ 9 Black', '9 Black Box/Clear', 'Black Box/Clear Box', 'Box/Clear Box :', 'Box : Looking', ': Looking Inside', 'Looking Inside Data', 'Inside Data ...............................', 'Data ............................... 10', '............................... 10 Tune', '10 Tune First', 'Tune First ,', 'First , Train', ', Train :', 'Train : Efficiency', ': Efficiency Complexity', 'Efficiency Complexity ....................', 'Complexity .................... 12', '.................... 12 Summary/Conclusion', '12 Summary/Conclusion ..................................', 'Summary/Conclusion .................................. 14', '.................................. 14 https', '14 https :', 'https : //www.lexalytics.com/', ': //www.lexalytics.com/ https', '//www.lexalytics.com/ https :', 'https : //www.lexalytics.com/', ': //www.lexalytics.com/ https', '//www.lexalytics.com/ https :', 'https : //www.lexalytics.com/resources/wp-content/uploads/sites/3/2019/02/Lexalytics_Tune_First_Then_Train_Whitepaper.pdf', ': //www.lexalytics.com/resources/wp-content/uploads/sites/3/2019/02/Lexalytics_Tune_First_Then_Train_Whitepaper.pdf https', '//www.lexalytics.com/resources/wp-content/uploads/sites/3/2019/02/Lexalytics_Tune_First_Then_Train_Whitepaper.pdf https :', 'https : //www.lexalytics.com/lexablog/build-or-buy-text-analytics-natural-language-processing', ': //www.lexalytics.com/lexablog/build-or-buy-text-analytics-natural-language-processing W', '//www.lexalytics.com/lexablog/build-or-buy-text-analytics-natural-language-processing W H', 'W H E', 'H E P', 'E P P', 'P P E', 'P E R', 'E R 3|', 'R 3| |', '3| | Lexalytics', '| Lexalytics ,', 'Lexalytics , Inc.', ', Inc. ,', 'Inc. , 48', ', 48 North', '48 North Pleasant', 'North Pleasant St.', 'Pleasant St. Unit', 'St. Unit 301', 'Unit 301 ,', '301 , Amherst', ', Amherst 01002', 'Amherst 01002 USA', '01002 USA |', 'USA | 1-800-377-8036', '| 1-800-377-8036 |', '1-800-377-8036 | www.lexalytics.com', '| www.lexalytics.com C', 'www.lexalytics.com C H', 'C H N', 'H N E', 'N E L', 'E L E', 'L E R', 'E R N', 'R N N', 'N N G', 'N G R', 'G R E', 'R E L', 'E L L', 'L L C', 'L C H', 'C H N', 'H N E', 'N E E', 'E E C', 'E C H', 'C H N', 'H N G', 'N G start', 'G start delving', 'start delving different', 'delving different aspects', 'different aspects text', 'aspects text analytics', 'text analytics ,', 'analytics , let', ', let ’', 'let ’ clarify', '’ clarify basic', 'clarify basic machine', 'basic machine learning', 'machine learning concepts', 'learning concepts .'] 

 TOTAL TRIGRAMS --> 137 



 ---- NOUN PHRASES ---- 

 ['Machine', 'Train', 'Efficiency', 'https', ' https', ' https', ' https', 'www.lexalytics.com', 'basic machine'] 

 TOTAL NOUN PHRASES --> 9 



 ---- NER ----

 
 ORGANIZATION ---> ['Merrier', 'USA']
 TOTAL ORGANIZATION ENTITY --> 2 


 PERSON ---> ['Machine Learning Really Machine Teaching', 'Machine Learning', 'Case', 'Inside Data', 'Efficiency Complexity']
 TOTAL PERSON ENTITY --> 5 


 GPE ---> ['Train', 'North']
 TOTAL GPE ENTITY --> 2 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['machin', 'learn', 'realli', 'machin', 'teach', '.........................', '3', 'supervis', ',', 'semi-supervis', 'unsupervis', 'machin', 'learn', 'supervis', 'learn', '..............................', '5', 'semi-supervis', 'learn', '................', '6', 'unsupervis', 'learn', '........................', '6', 'happier', 'dozen', ':', 'model', ',', 'merrier', '....................', '7', 'code', 'vs.', 'learn', ':', 'make', 'case', '............................', '9', 'black', 'box/clear', 'box', ':', 'look', 'insid', 'data', '...............................', '10', 'tune', 'first', ',', 'train', ':', 'effici', 'complex', '....................', '12', 'summary/conclus', '..................................', '14', 'http', ':', '//www.lexalytics.com/', 'http', ':', '//www.lexalytics.com/', 'http', ':', '//www.lexalytics.com/resources/wp-content/uploads/sites/3/2019/02/lexalytics_tune_first_then_train_whitepaper.pdf', 'http', ':', '//www.lexalytics.com/lexablog/build-or-buy-text-analytics-natural-language-process', 'w', 'h', 'e', 'p', 'p', 'e', 'r', '3|', '|', 'lexalyt', ',', 'inc.', ',', '48', 'north', 'pleasant', 'st.', 'unit', '301', ',', 'amherst', '01002', 'usa', '|', '1-800-377-8036', '|', 'www.lexalytics.com', 'c', 'h', 'n', 'e', 'l', 'e', 'r', 'n', 'n', 'g', 'r', 'e', 'l', 'l', 'c', 'h', 'n', 'e', 'e', 'c', 'h', 'n', 'g', 'start', 'delv', 'differ', 'aspect', 'text', 'analyt', ',', 'let', '’', 'clarifi', 'basic', 'machin', 'learn', 'concept', '.']

 TOTAL PORTER STEM WORDS ==> 139



 ---- SNOWBALL STEMMING ----

['machin', 'learn', 'realli', 'machin', 'teach', '.........................', '3', 'supervis', ',', 'semi-supervis', 'unsupervis', 'machin', 'learn', 'supervis', 'learn', '..............................', '5', 'semi-supervis', 'learn', '................', '6', 'unsupervis', 'learn', '........................', '6', 'happier', 'dozen', ':', 'model', ',', 'merrier', '....................', '7', 'code', 'vs.', 'learn', ':', 'make', 'case', '............................', '9', 'black', 'box/clear', 'box', ':', 'look', 'insid', 'data', '...............................', '10', 'tune', 'first', ',', 'train', ':', 'effici', 'complex', '....................', '12', 'summary/conclus', '..................................', '14', 'https', ':', '//www.lexalytics.com/', 'https', ':', '//www.lexalytics.com/', 'https', ':', '//www.lexalytics.com/resources/wp-content/uploads/sites/3/2019/02/lexalytics_tune_first_then_train_whitepaper.pdf', 'https', ':', '//www.lexalytics.com/lexablog/build-or-buy-text-analytics-natural-language-process', 'w', 'h', 'e', 'p', 'p', 'e', 'r', '3|', '|', 'lexalyt', ',', 'inc.', ',', '48', 'north', 'pleasant', 'st.', 'unit', '301', ',', 'amherst', '01002', 'usa', '|', '1-800-377-8036', '|', 'www.lexalytics.com', 'c', 'h', 'n', 'e', 'l', 'e', 'r', 'n', 'n', 'g', 'r', 'e', 'l', 'l', 'c', 'h', 'n', 'e', 'e', 'c', 'h', 'n', 'g', 'start', 'delv', 'differ', 'aspect', 'text', 'analyt', ',', 'let', '’', 'clarifi', 'basic', 'machin', 'learn', 'concept', '.']

 TOTAL SNOWBALL STEM WORDS ==> 139



 ---- LEMMATIZATION ----

['Machine', 'Learning', 'Really', 'Machine', 'Teaching', '.........................', '3', 'Supervised', ',', 'Semi-Supervised', 'Unsupervised', 'Machine', 'Learning', 'Supervised', 'Learning', '..............................', '5', 'Semi-Supervised', 'Learning', '................', '6', 'Unsupervised', 'Learning', '........................', '6', 'Happier', 'Dozen', ':', 'Models', ',', 'Merrier', '....................', '7', 'Coding', 'vs.', 'Learning', ':', 'Making', 'Case', '............................', '9', 'Black', 'Box/Clear', 'Box', ':', 'Looking', 'Inside', 'Data', '...............................', '10', 'Tune', 'First', ',', 'Train', ':', 'Efficiency', 'Complexity', '....................', '12', 'Summary/Conclusion', '..................................', '14', 'http', ':', '//www.lexalytics.com/', 'http', ':', '//www.lexalytics.com/', 'http', ':', '//www.lexalytics.com/resources/wp-content/uploads/sites/3/2019/02/Lexalytics_Tune_First_Then_Train_Whitepaper.pdf', 'http', ':', '//www.lexalytics.com/lexablog/build-or-buy-text-analytics-natural-language-processing', 'W', 'H', 'E', 'P', 'P', 'E', 'R', '3|', '|', 'Lexalytics', ',', 'Inc.', ',', '48', 'North', 'Pleasant', 'St.', 'Unit', '301', ',', 'Amherst', '01002', 'USA', '|', '1-800-377-8036', '|', 'www.lexalytics.com', 'C', 'H', 'N', 'E', 'L', 'E', 'R', 'N', 'N', 'G', 'R', 'E', 'L', 'L', 'C', 'H', 'N', 'E', 'E', 'C', 'H', 'N', 'G', 'start', 'delving', 'different', 'aspect', 'text', 'analytics', ',', 'let', '’', 'clarify', 'basic', 'machine', 'learning', 'concept', '.']

 TOTAL LEMMATIZE WORDS ==> 139

************************************************************************************************************************

10 --> Most importantly, “machine learning” really means “machine teaching.” We  know what the machine needs to learn, so our task is to create a learning  framework and provide properly-formatted, relevant, clean data that the  machine can learn from. 


 ---- TOKENS ----

 ['Most', 'importantly', ',', '“', 'machine', 'learning', '”', 'really', 'means', '“', 'machine', 'teaching.', '”', 'We', 'know', 'what', 'the', 'machine', 'needs', 'to', 'learn', ',', 'so', 'our', 'task', 'is', 'to', 'create', 'a', 'learning', 'framework', 'and', 'provide', 'properly-formatted', ',', 'relevant', ',', 'clean', 'data', 'that', 'the', 'machine', 'can', 'learn', 'from', '.'] 

 TOTAL TOKENS ==> 46

 ---- POST ----

 [('Most', 'JJS'), ('importantly', 'RB'), (',', ','), ('“', 'FW'), ('machine', 'NN'), ('learning', 'VBG'), ('”', 'NNP'), ('really', 'RB'), ('means', 'VBZ'), ('“', 'JJ'), ('machine', 'NN'), ('teaching.', 'NN'), ('”', 'IN'), ('We', 'PRP'), ('know', 'VBP'), ('what', 'WP'), ('the', 'DT'), ('machine', 'NN'), ('needs', 'VBZ'), ('to', 'TO'), ('learn', 'VB'), (',', ','), ('so', 'IN'), ('our', 'PRP$'), ('task', 'NN'), ('is', 'VBZ'), ('to', 'TO'), ('create', 'VB'), ('a', 'DT'), ('learning', 'JJ'), ('framework', 'NN'), ('and', 'CC'), ('provide', 'VB'), ('properly-formatted', 'JJ'), (',', ','), ('relevant', 'JJ'), (',', ','), ('clean', 'JJ'), ('data', 'NNS'), ('that', 'IN'), ('the', 'DT'), ('machine', 'NN'), ('can', 'MD'), ('learn', 'VB'), ('from', 'IN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['importantly', ',', '“', 'machine', 'learning', '”', 'really', 'means', '“', 'machine', 'teaching.', '”', 'know', 'machine', 'needs', 'learn', ',', 'task', 'create', 'learning', 'framework', 'provide', 'properly-formatted', ',', 'relevant', ',', 'clean', 'data', 'machine', 'learn', '.']

 TOTAL FILTERED TOKENS ==>  31

 ---- POST FOR FILTERED TOKENS ----

 [('importantly', 'RB'), (',', ','), ('“', 'FW'), ('machine', 'NN'), ('learning', 'VBG'), ('”', 'NNP'), ('really', 'RB'), ('means', 'VBZ'), ('“', 'JJ'), ('machine', 'NN'), ('teaching.', 'NN'), ('”', 'NN'), ('know', 'VBP'), ('machine', 'NN'), ('needs', 'NNS'), ('learn', 'VBP'), (',', ','), ('task', 'JJ'), ('create', 'NN'), ('learning', 'VBG'), ('framework', 'JJ'), ('provide', 'RB'), ('properly-formatted', 'JJ'), (',', ','), ('relevant', 'JJ'), (',', ','), ('clean', 'JJ'), ('data', 'NNS'), ('machine', 'NN'), ('learn', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['importantly ,', ', “', '“ machine', 'machine learning', 'learning ”', '” really', 'really means', 'means “', '“ machine', 'machine teaching.', 'teaching. ”', '” know', 'know machine', 'machine needs', 'needs learn', 'learn ,', ', task', 'task create', 'create learning', 'learning framework', 'framework provide', 'provide properly-formatted', 'properly-formatted ,', ', relevant', 'relevant ,', ', clean', 'clean data', 'data machine', 'machine learn', 'learn .'] 

 TOTAL BIGRAMS --> 30 



 ---- TRI-GRAMS ---- 

 ['importantly , “', ', “ machine', '“ machine learning', 'machine learning ”', 'learning ” really', '” really means', 'really means “', 'means “ machine', '“ machine teaching.', 'machine teaching. ”', 'teaching. ” know', '” know machine', 'know machine needs', 'machine needs learn', 'needs learn ,', 'learn , task', ', task create', 'task create learning', 'create learning framework', 'learning framework provide', 'framework provide properly-formatted', 'provide properly-formatted ,', 'properly-formatted , relevant', ', relevant ,', 'relevant , clean', ', clean data', 'clean data machine', 'data machine learn', 'machine learn .'] 

 TOTAL TRIGRAMS --> 29 



 ---- NOUN PHRASES ---- 

 ['machine', '“ machine', 'teaching.', '”', 'machine', 'task create', 'machine', 'learn'] 

 TOTAL NOUN PHRASES --> 8 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['importantli', ',', '“', 'machin', 'learn', '”', 'realli', 'mean', '“', 'machin', 'teaching.', '”', 'know', 'machin', 'need', 'learn', ',', 'task', 'creat', 'learn', 'framework', 'provid', 'properly-format', ',', 'relev', ',', 'clean', 'data', 'machin', 'learn', '.']

 TOTAL PORTER STEM WORDS ==> 31



 ---- SNOWBALL STEMMING ----

['import', ',', '“', 'machin', 'learn', '”', 'realli', 'mean', '“', 'machin', 'teaching.', '”', 'know', 'machin', 'need', 'learn', ',', 'task', 'creat', 'learn', 'framework', 'provid', 'properly-format', ',', 'relev', ',', 'clean', 'data', 'machin', 'learn', '.']

 TOTAL SNOWBALL STEM WORDS ==> 31



 ---- LEMMATIZATION ----

['importantly', ',', '“', 'machine', 'learning', '”', 'really', 'mean', '“', 'machine', 'teaching.', '”', 'know', 'machine', 'need', 'learn', ',', 'task', 'create', 'learning', 'framework', 'provide', 'properly-formatted', ',', 'relevant', ',', 'clean', 'data', 'machine', 'learn', '.']

 TOTAL LEMMATIZE WORDS ==> 31

************************************************************************************************************************

11 --> The goal is to create a system where the model continuously improves  at the task you’ve set it. 


 ---- TOKENS ----

 ['The', 'goal', 'is', 'to', 'create', 'a', 'system', 'where', 'the', 'model', 'continuously', 'improves', 'at', 'the', 'task', 'you', '’', 've', 'set', 'it', '.'] 

 TOTAL TOKENS ==> 21

 ---- POST ----

 [('The', 'DT'), ('goal', 'NN'), ('is', 'VBZ'), ('to', 'TO'), ('create', 'VB'), ('a', 'DT'), ('system', 'NN'), ('where', 'WRB'), ('the', 'DT'), ('model', 'NN'), ('continuously', 'RB'), ('improves', 'VBZ'), ('at', 'IN'), ('the', 'DT'), ('task', 'NN'), ('you', 'PRP'), ('’', 'VBP'), ('ve', 'JJ'), ('set', 'NN'), ('it', 'PRP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['goal', 'create', 'system', 'model', 'continuously', 'improves', 'task', '’', 'set', '.']

 TOTAL FILTERED TOKENS ==>  10

 ---- POST FOR FILTERED TOKENS ----

 [('goal', 'NN'), ('create', 'NN'), ('system', 'NN'), ('model', 'NN'), ('continuously', 'RB'), ('improves', 'VBZ'), ('task', 'NN'), ('’', 'NN'), ('set', 'VBN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['goal create', 'create system', 'system model', 'model continuously', 'continuously improves', 'improves task', 'task ’', '’ set', 'set .'] 

 TOTAL BIGRAMS --> 9 



 ---- TRI-GRAMS ---- 

 ['goal create system', 'create system model', 'system model continuously', 'model continuously improves', 'continuously improves task', 'improves task ’', 'task ’ set', '’ set .'] 

 TOTAL TRIGRAMS --> 8 



 ---- NOUN PHRASES ---- 

 ['goal', 'create', 'system', 'model', 'task', '’'] 

 TOTAL NOUN PHRASES --> 6 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['goal', 'creat', 'system', 'model', 'continu', 'improv', 'task', '’', 'set', '.']

 TOTAL PORTER STEM WORDS ==> 10



 ---- SNOWBALL STEMMING ----

['goal', 'creat', 'system', 'model', 'continu', 'improv', 'task', '’', 'set', '.']

 TOTAL SNOWBALL STEM WORDS ==> 10



 ---- LEMMATIZATION ----

['goal', 'create', 'system', 'model', 'continuously', 'improves', 'task', '’', 'set', '.']

 TOTAL LEMMATIZE WORDS ==> 10

************************************************************************************************************************

12 --> Input is key. 


 ---- TOKENS ----

 ['Input', 'is', 'key', '.'] 

 TOTAL TOKENS ==> 4

 ---- POST ----

 [('Input', 'NNP'), ('is', 'VBZ'), ('key', 'JJ'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Input', 'key', '.']

 TOTAL FILTERED TOKENS ==>  3

 ---- POST FOR FILTERED TOKENS ----

 [('Input', 'NNP'), ('key', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Input key', 'key .'] 

 TOTAL BIGRAMS --> 2 



 ---- TRI-GRAMS ---- 

 ['Input key .'] 

 TOTAL TRIGRAMS --> 1 



 ---- NOUN PHRASES ---- 

 ['key'] 

 TOTAL NOUN PHRASES --> 1 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> ['Input']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['input', 'key', '.']

 TOTAL PORTER STEM WORDS ==> 3



 ---- SNOWBALL STEMMING ----

['input', 'key', '.']

 TOTAL SNOWBALL STEM WORDS ==> 3



 ---- LEMMATIZATION ----

['Input', 'key', '.']

 TOTAL LEMMATIZE WORDS ==> 3

************************************************************************************************************************

13 --> Unlike algorithmic programming, a  machine learning model is able to generalize and deal with novel cases. 


 ---- TOKENS ----

 ['Unlike', 'algorithmic', 'programming', ',', 'a', 'machine', 'learning', 'model', 'is', 'able', 'to', 'generalize', 'and', 'deal', 'with', 'novel', 'cases', '.'] 

 TOTAL TOKENS ==> 18

 ---- POST ----

 [('Unlike', 'IN'), ('algorithmic', 'JJ'), ('programming', 'NN'), (',', ','), ('a', 'DT'), ('machine', 'NN'), ('learning', 'VBG'), ('model', 'NN'), ('is', 'VBZ'), ('able', 'JJ'), ('to', 'TO'), ('generalize', 'VB'), ('and', 'CC'), ('deal', 'VB'), ('with', 'IN'), ('novel', 'JJ'), ('cases', 'NNS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Unlike', 'algorithmic', 'programming', ',', 'machine', 'learning', 'model', 'able', 'generalize', 'deal', 'novel', 'cases', '.']

 TOTAL FILTERED TOKENS ==>  13

 ---- POST FOR FILTERED TOKENS ----

 [('Unlike', 'IN'), ('algorithmic', 'JJ'), ('programming', 'NN'), (',', ','), ('machine', 'NN'), ('learning', 'VBG'), ('model', 'NN'), ('able', 'JJ'), ('generalize', 'JJ'), ('deal', 'NN'), ('novel', 'JJ'), ('cases', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Unlike algorithmic', 'algorithmic programming', 'programming ,', ', machine', 'machine learning', 'learning model', 'model able', 'able generalize', 'generalize deal', 'deal novel', 'novel cases', 'cases .'] 

 TOTAL BIGRAMS --> 12 



 ---- TRI-GRAMS ---- 

 ['Unlike algorithmic programming', 'algorithmic programming ,', 'programming , machine', ', machine learning', 'machine learning model', 'learning model able', 'model able generalize', 'able generalize deal', 'generalize deal novel', 'deal novel cases', 'novel cases .'] 

 TOTAL TRIGRAMS --> 11 



 ---- NOUN PHRASES ---- 

 ['algorithmic programming', 'machine', 'model', 'able generalize deal'] 

 TOTAL NOUN PHRASES --> 4 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['unlik', 'algorithm', 'program', ',', 'machin', 'learn', 'model', 'abl', 'gener', 'deal', 'novel', 'case', '.']

 TOTAL PORTER STEM WORDS ==> 13



 ---- SNOWBALL STEMMING ----

['unlik', 'algorithm', 'program', ',', 'machin', 'learn', 'model', 'abl', 'general', 'deal', 'novel', 'case', '.']

 TOTAL SNOWBALL STEM WORDS ==> 13



 ---- LEMMATIZATION ----

['Unlike', 'algorithmic', 'programming', ',', 'machine', 'learning', 'model', 'able', 'generalize', 'deal', 'novel', 'case', '.']

 TOTAL LEMMATIZE WORDS ==> 13

************************************************************************************************************************

14 --> If a  case resembles something the model has seen before, the model can use  this prior “learning” to evaluate the case. 


 ---- TOKENS ----

 ['If', 'a', 'case', 'resembles', 'something', 'the', 'model', 'has', 'seen', 'before', ',', 'the', 'model', 'can', 'use', 'this', 'prior', '“', 'learning', '”', 'to', 'evaluate', 'the', 'case', '.'] 

 TOTAL TOKENS ==> 25

 ---- POST ----

 [('If', 'IN'), ('a', 'DT'), ('case', 'NN'), ('resembles', 'VBZ'), ('something', 'NN'), ('the', 'DT'), ('model', 'NN'), ('has', 'VBZ'), ('seen', 'VBN'), ('before', 'IN'), (',', ','), ('the', 'DT'), ('model', 'NN'), ('can', 'MD'), ('use', 'VB'), ('this', 'DT'), ('prior', 'JJ'), ('“', 'NN'), ('learning', 'VBG'), ('”', 'NNS'), ('to', 'TO'), ('evaluate', 'VB'), ('the', 'DT'), ('case', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['case', 'resembles', 'something', 'model', 'seen', ',', 'model', 'use', 'prior', '“', 'learning', '”', 'evaluate', 'case', '.']

 TOTAL FILTERED TOKENS ==>  15

 ---- POST FOR FILTERED TOKENS ----

 [('case', 'NN'), ('resembles', 'VBZ'), ('something', 'NN'), ('model', 'NN'), ('seen', 'VBN'), (',', ','), ('model', 'NN'), ('use', 'NN'), ('prior', 'JJ'), ('“', 'NN'), ('learning', 'VBG'), ('”', 'JJ'), ('evaluate', 'JJ'), ('case', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['case resembles', 'resembles something', 'something model', 'model seen', 'seen ,', ', model', 'model use', 'use prior', 'prior “', '“ learning', 'learning ”', '” evaluate', 'evaluate case', 'case .'] 

 TOTAL BIGRAMS --> 14 



 ---- TRI-GRAMS ---- 

 ['case resembles something', 'resembles something model', 'something model seen', 'model seen ,', 'seen , model', ', model use', 'model use prior', 'use prior “', 'prior “ learning', '“ learning ”', 'learning ” evaluate', '” evaluate case', 'evaluate case .'] 

 TOTAL TRIGRAMS --> 13 



 ---- NOUN PHRASES ---- 

 ['case', 'something', 'model', 'model', 'use', 'prior “', '” evaluate case'] 

 TOTAL NOUN PHRASES --> 7 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['case', 'resembl', 'someth', 'model', 'seen', ',', 'model', 'use', 'prior', '“', 'learn', '”', 'evalu', 'case', '.']

 TOTAL PORTER STEM WORDS ==> 15



 ---- SNOWBALL STEMMING ----

['case', 'resembl', 'someth', 'model', 'seen', ',', 'model', 'use', 'prior', '“', 'learn', '”', 'evalu', 'case', '.']

 TOTAL SNOWBALL STEM WORDS ==> 15



 ---- LEMMATIZATION ----

['case', 'resembles', 'something', 'model', 'seen', ',', 'model', 'use', 'prior', '“', 'learning', '”', 'evaluate', 'case', '.']

 TOTAL LEMMATIZE WORDS ==> 15

************************************************************************************************************************

15 --> When we talk about a “model,” we’re talking about a mathematical  representation. 


 ---- TOKENS ----

 ['When', 'we', 'talk', 'about', 'a', '“', 'model', ',', '”', 'we', '’', 're', 'talking', 'about', 'a', 'mathematical', 'representation', '.'] 

 TOTAL TOKENS ==> 18

 ---- POST ----

 [('When', 'WRB'), ('we', 'PRP'), ('talk', 'VBP'), ('about', 'IN'), ('a', 'DT'), ('“', 'JJ'), ('model', 'NN'), (',', ','), ('”', 'IN'), ('we', 'PRP'), ('’', 'VBP'), ('re', 'JJ'), ('talking', 'VBG'), ('about', 'IN'), ('a', 'DT'), ('mathematical', 'JJ'), ('representation', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['talk', '“', 'model', ',', '”', '’', 'talking', 'mathematical', 'representation', '.']

 TOTAL FILTERED TOKENS ==>  10

 ---- POST FOR FILTERED TOKENS ----

 [('talk', 'NN'), ('“', 'NNP'), ('model', 'NN'), (',', ','), ('”', 'NNP'), ('’', 'NNP'), ('talking', 'VBG'), ('mathematical', 'JJ'), ('representation', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['talk “', '“ model', 'model ,', ', ”', '” ’', '’ talking', 'talking mathematical', 'mathematical representation', 'representation .'] 

 TOTAL BIGRAMS --> 9 



 ---- TRI-GRAMS ---- 

 ['talk “ model', '“ model ,', 'model , ”', ', ” ’', '” ’ talking', '’ talking mathematical', 'talking mathematical representation', 'mathematical representation .'] 

 TOTAL TRIGRAMS --> 8 



 ---- NOUN PHRASES ---- 

 ['talk', 'model', 'mathematical representation'] 

 TOTAL NOUN PHRASES --> 3 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['talk', '“', 'model', ',', '”', '’', 'talk', 'mathemat', 'represent', '.']

 TOTAL PORTER STEM WORDS ==> 10



 ---- SNOWBALL STEMMING ----

['talk', '“', 'model', ',', '”', '’', 'talk', 'mathemat', 'represent', '.']

 TOTAL SNOWBALL STEM WORDS ==> 10



 ---- LEMMATIZATION ----

['talk', '“', 'model', ',', '”', '’', 'talking', 'mathematical', 'representation', '.']

 TOTAL LEMMATIZE WORDS ==> 10

************************************************************************************************************************

16 --> A machine learning model is the sum of the learning  that has been acquired from the training data. 


 ---- TOKENS ----

 ['A', 'machine', 'learning', 'model', 'is', 'the', 'sum', 'of', 'the', 'learning', 'that', 'has', 'been', 'acquired', 'from', 'the', 'training', 'data', '.'] 

 TOTAL TOKENS ==> 19

 ---- POST ----

 [('A', 'DT'), ('machine', 'NN'), ('learning', 'VBG'), ('model', 'NN'), ('is', 'VBZ'), ('the', 'DT'), ('sum', 'NN'), ('of', 'IN'), ('the', 'DT'), ('learning', 'NN'), ('that', 'WDT'), ('has', 'VBZ'), ('been', 'VBN'), ('acquired', 'VBN'), ('from', 'IN'), ('the', 'DT'), ('training', 'NN'), ('data', 'NNS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['machine', 'learning', 'model', 'sum', 'learning', 'acquired', 'training', 'data', '.']

 TOTAL FILTERED TOKENS ==>  9

 ---- POST FOR FILTERED TOKENS ----

 [('machine', 'NN'), ('learning', 'VBG'), ('model', 'NN'), ('sum', 'NN'), ('learning', 'VBG'), ('acquired', 'VBD'), ('training', 'NN'), ('data', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['machine learning', 'learning model', 'model sum', 'sum learning', 'learning acquired', 'acquired training', 'training data', 'data .'] 

 TOTAL BIGRAMS --> 8 



 ---- TRI-GRAMS ---- 

 ['machine learning model', 'learning model sum', 'model sum learning', 'sum learning acquired', 'learning acquired training', 'acquired training data', 'training data .'] 

 TOTAL TRIGRAMS --> 7 



 ---- NOUN PHRASES ---- 

 ['machine', 'model', 'sum', 'training'] 

 TOTAL NOUN PHRASES --> 4 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['machin', 'learn', 'model', 'sum', 'learn', 'acquir', 'train', 'data', '.']

 TOTAL PORTER STEM WORDS ==> 9



 ---- SNOWBALL STEMMING ----

['machin', 'learn', 'model', 'sum', 'learn', 'acquir', 'train', 'data', '.']

 TOTAL SNOWBALL STEM WORDS ==> 9



 ---- LEMMATIZATION ----

['machine', 'learning', 'model', 'sum', 'learning', 'acquired', 'training', 'data', '.']

 TOTAL LEMMATIZE WORDS ==> 9

************************************************************************************************************************

17 --> The model changes as  more learning is acquired. 


 ---- TOKENS ----

 ['The', 'model', 'changes', 'as', 'more', 'learning', 'is', 'acquired', '.'] 

 TOTAL TOKENS ==> 9

 ---- POST ----

 [('The', 'DT'), ('model', 'NN'), ('changes', 'NNS'), ('as', 'IN'), ('more', 'JJR'), ('learning', 'NN'), ('is', 'VBZ'), ('acquired', 'VBN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['model', 'changes', 'learning', 'acquired', '.']

 TOTAL FILTERED TOKENS ==>  5

 ---- POST FOR FILTERED TOKENS ----

 [('model', 'NN'), ('changes', 'NNS'), ('learning', 'VBG'), ('acquired', 'VBD'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['model changes', 'changes learning', 'learning acquired', 'acquired .'] 

 TOTAL BIGRAMS --> 4 



 ---- TRI-GRAMS ---- 

 ['model changes learning', 'changes learning acquired', 'learning acquired .'] 

 TOTAL TRIGRAMS --> 3 



 ---- NOUN PHRASES ---- 

 ['model'] 

 TOTAL NOUN PHRASES --> 1 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['model', 'chang', 'learn', 'acquir', '.']

 TOTAL PORTER STEM WORDS ==> 5



 ---- SNOWBALL STEMMING ----

['model', 'chang', 'learn', 'acquir', '.']

 TOTAL SNOWBALL STEM WORDS ==> 5



 ---- LEMMATIZATION ----

['model', 'change', 'learning', 'acquired', '.']

 TOTAL LEMMATIZE WORDS ==> 5

************************************************************************************************************************

18 --> 3 MAJOR PARTS TO MACHINE LEARNING  Training data   Model algorithm   Hyper-parameters creates a learning framework   and provides data that the  machine can learn from. 


 ---- TOKENS ----

 ['3', 'MAJOR', 'PARTS', 'TO', 'MACHINE', 'LEARNING', 'Training', 'data', 'Model', 'algorithm', 'Hyper-parameters', 'creates', 'a', 'learning', 'framework', 'and', 'provides', 'data', 'that', 'the', 'machine', 'can', 'learn', 'from', '.'] 

 TOTAL TOKENS ==> 25

 ---- POST ----

 [('3', 'CD'), ('MAJOR', 'JJ'), ('PARTS', 'NNS'), ('TO', 'NNP'), ('MACHINE', 'NNP'), ('LEARNING', 'NNP'), ('Training', 'NNP'), ('data', 'NNS'), ('Model', 'NNP'), ('algorithm', 'IN'), ('Hyper-parameters', 'NNP'), ('creates', 'VBZ'), ('a', 'DT'), ('learning', 'NN'), ('framework', 'NN'), ('and', 'CC'), ('provides', 'VBZ'), ('data', 'NNS'), ('that', 'IN'), ('the', 'DT'), ('machine', 'NN'), ('can', 'MD'), ('learn', 'VB'), ('from', 'IN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['3', 'MAJOR', 'PARTS', 'MACHINE', 'LEARNING', 'Training', 'data', 'Model', 'algorithm', 'Hyper-parameters', 'creates', 'learning', 'framework', 'provides', 'data', 'machine', 'learn', '.']

 TOTAL FILTERED TOKENS ==>  18

 ---- POST FOR FILTERED TOKENS ----

 [('3', 'CD'), ('MAJOR', 'JJ'), ('PARTS', 'NNP'), ('MACHINE', 'NNP'), ('LEARNING', 'NNP'), ('Training', 'NNP'), ('data', 'NNS'), ('Model', 'NNP'), ('algorithm', 'IN'), ('Hyper-parameters', 'NNP'), ('creates', 'NNS'), ('learning', 'VBG'), ('framework', 'NN'), ('provides', 'VBZ'), ('data', 'NNS'), ('machine', 'NN'), ('learn', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['3 MAJOR', 'MAJOR PARTS', 'PARTS MACHINE', 'MACHINE LEARNING', 'LEARNING Training', 'Training data', 'data Model', 'Model algorithm', 'algorithm Hyper-parameters', 'Hyper-parameters creates', 'creates learning', 'learning framework', 'framework provides', 'provides data', 'data machine', 'machine learn', 'learn .'] 

 TOTAL BIGRAMS --> 17 



 ---- TRI-GRAMS ---- 

 ['3 MAJOR PARTS', 'MAJOR PARTS MACHINE', 'PARTS MACHINE LEARNING', 'MACHINE LEARNING Training', 'LEARNING Training data', 'Training data Model', 'data Model algorithm', 'Model algorithm Hyper-parameters', 'algorithm Hyper-parameters creates', 'Hyper-parameters creates learning', 'creates learning framework', 'learning framework provides', 'framework provides data', 'provides data machine', 'data machine learn', 'machine learn .'] 

 TOTAL TRIGRAMS --> 16 



 ---- NOUN PHRASES ---- 

 ['framework', 'machine', 'learn'] 

 TOTAL NOUN PHRASES --> 3 



 ---- NER ----

 
 ORGANIZATION ---> ['PARTS', 'MACHINE']
 TOTAL ORGANIZATION ENTITY --> 2 


 PERSON ---> ['Model']
 TOTAL PERSON ENTITY --> 1 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['3', 'major', 'part', 'machin', 'learn', 'train', 'data', 'model', 'algorithm', 'hyper-paramet', 'creat', 'learn', 'framework', 'provid', 'data', 'machin', 'learn', '.']

 TOTAL PORTER STEM WORDS ==> 18



 ---- SNOWBALL STEMMING ----

['3', 'major', 'part', 'machin', 'learn', 'train', 'data', 'model', 'algorithm', 'hyper-paramet', 'creat', 'learn', 'framework', 'provid', 'data', 'machin', 'learn', '.']

 TOTAL SNOWBALL STEM WORDS ==> 18



 ---- LEMMATIZATION ----

['3', 'MAJOR', 'PARTS', 'MACHINE', 'LEARNING', 'Training', 'data', 'Model', 'algorithm', 'Hyper-parameters', 'creates', 'learning', 'framework', 'provides', 'data', 'machine', 'learn', '.']

 TOTAL LEMMATIZE WORDS ==> 18

************************************************************************************************************************

19 --> Machine  teaching   (aka learning) https://www.lexalytics.com/ https://www.lexalytics.com/  W H I T E  P A P E R 4|       |   Lexalytics, Inc., 48 North Pleasant St. Unit 301, Amherst MA 01002 USA   |   1-800-377-8036 |   www.lexalytics.com The output of this system is a machine learning model. 


 ---- TOKENS ----

 ['Machine', 'teaching', '(', 'aka', 'learning', ')', 'https', ':', '//www.lexalytics.com/', 'https', ':', '//www.lexalytics.com/', 'W', 'H', 'I', 'T', 'E', 'P', 'A', 'P', 'E', 'R', '4|', '|', 'Lexalytics', ',', 'Inc.', ',', '48', 'North', 'Pleasant', 'St.', 'Unit', '301', ',', 'Amherst', 'MA', '01002', 'USA', '|', '1-800-377-8036', '|', 'www.lexalytics.com', 'The', 'output', 'of', 'this', 'system', 'is', 'a', 'machine', 'learning', 'model', '.'] 

 TOTAL TOKENS ==> 54

 ---- POST ----

 [('Machine', 'NN'), ('teaching', 'NN'), ('(', '('), ('aka', 'IN'), ('learning', 'VBG'), (')', ')'), ('https', 'NN'), (':', ':'), ('//www.lexalytics.com/', 'JJ'), ('https', 'NN'), (':', ':'), ('//www.lexalytics.com/', 'JJ'), ('W', 'NNP'), ('H', 'NNP'), ('I', 'PRP'), ('T', 'NNP'), ('E', 'NNP'), ('P', 'NNP'), ('A', 'NNP'), ('P', 'NNP'), ('E', 'NNP'), ('R', 'NNP'), ('4|', 'CD'), ('|', 'NNP'), ('Lexalytics', 'NNP'), (',', ','), ('Inc.', 'NNP'), (',', ','), ('48', 'CD'), ('North', 'NNP'), ('Pleasant', 'NNP'), ('St.', 'NNP'), ('Unit', 'NNP'), ('301', 'CD'), (',', ','), ('Amherst', 'NNP'), ('MA', 'NNP'), ('01002', 'CD'), ('USA', 'NNP'), ('|', 'NNP'), ('1-800-377-8036', 'JJ'), ('|', 'NNP'), ('www.lexalytics.com', 'VBD'), ('The', 'DT'), ('output', 'NN'), ('of', 'IN'), ('this', 'DT'), ('system', 'NN'), ('is', 'VBZ'), ('a', 'DT'), ('machine', 'NN'), ('learning', 'VBG'), ('model', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Machine', 'teaching', '(', 'aka', 'learning', ')', 'https', ':', '//www.lexalytics.com/', 'https', ':', '//www.lexalytics.com/', 'W', 'H', 'E', 'P', 'P', 'E', 'R', '4|', '|', 'Lexalytics', ',', 'Inc.', ',', '48', 'North', 'Pleasant', 'St.', 'Unit', '301', ',', 'Amherst', '01002', 'USA', '|', '1-800-377-8036', '|', 'www.lexalytics.com', 'output', 'system', 'machine', 'learning', 'model', '.']

 TOTAL FILTERED TOKENS ==>  45

 ---- POST FOR FILTERED TOKENS ----

 [('Machine', 'NN'), ('teaching', 'NN'), ('(', '('), ('aka', 'IN'), ('learning', 'VBG'), (')', ')'), ('https', 'NN'), (':', ':'), ('//www.lexalytics.com/', 'JJ'), ('https', 'NN'), (':', ':'), ('//www.lexalytics.com/', 'JJ'), ('W', 'NNP'), ('H', 'NNP'), ('E', 'NNP'), ('P', 'NNP'), ('P', 'NNP'), ('E', 'NNP'), ('R', 'NNP'), ('4|', 'CD'), ('|', 'NNP'), ('Lexalytics', 'NNP'), (',', ','), ('Inc.', 'NNP'), (',', ','), ('48', 'CD'), ('North', 'NNP'), ('Pleasant', 'NNP'), ('St.', 'NNP'), ('Unit', 'NNP'), ('301', 'CD'), (',', ','), ('Amherst', 'NNP'), ('01002', 'CD'), ('USA', 'NNP'), ('|', 'NNP'), ('1-800-377-8036', 'JJ'), ('|', 'NNP'), ('www.lexalytics.com', 'NN'), ('output', 'NN'), ('system', 'NN'), ('machine', 'NN'), ('learning', 'VBG'), ('model', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Machine teaching', 'teaching (', '( aka', 'aka learning', 'learning )', ') https', 'https :', ': //www.lexalytics.com/', '//www.lexalytics.com/ https', 'https :', ': //www.lexalytics.com/', '//www.lexalytics.com/ W', 'W H', 'H E', 'E P', 'P P', 'P E', 'E R', 'R 4|', '4| |', '| Lexalytics', 'Lexalytics ,', ', Inc.', 'Inc. ,', ', 48', '48 North', 'North Pleasant', 'Pleasant St.', 'St. Unit', 'Unit 301', '301 ,', ', Amherst', 'Amherst 01002', '01002 USA', 'USA |', '| 1-800-377-8036', '1-800-377-8036 |', '| www.lexalytics.com', 'www.lexalytics.com output', 'output system', 'system machine', 'machine learning', 'learning model', 'model .'] 

 TOTAL BIGRAMS --> 44 



 ---- TRI-GRAMS ---- 

 ['Machine teaching (', 'teaching ( aka', '( aka learning', 'aka learning )', 'learning ) https', ') https :', 'https : //www.lexalytics.com/', ': //www.lexalytics.com/ https', '//www.lexalytics.com/ https :', 'https : //www.lexalytics.com/', ': //www.lexalytics.com/ W', '//www.lexalytics.com/ W H', 'W H E', 'H E P', 'E P P', 'P P E', 'P E R', 'E R 4|', 'R 4| |', '4| | Lexalytics', '| Lexalytics ,', 'Lexalytics , Inc.', ', Inc. ,', 'Inc. , 48', ', 48 North', '48 North Pleasant', 'North Pleasant St.', 'Pleasant St. Unit', 'St. Unit 301', 'Unit 301 ,', '301 , Amherst', ', Amherst 01002', 'Amherst 01002 USA', '01002 USA |', 'USA | 1-800-377-8036', '| 1-800-377-8036 |', '1-800-377-8036 | www.lexalytics.com', '| www.lexalytics.com output', 'www.lexalytics.com output system', 'output system machine', 'system machine learning', 'machine learning model', 'learning model .'] 

 TOTAL TRIGRAMS --> 43 



 ---- NOUN PHRASES ---- 

 ['Machine', 'teaching', 'https', ' https', 'www.lexalytics.com', 'output', 'system', 'machine', 'model'] 

 TOTAL NOUN PHRASES --> 9 



 ---- NER ----

 
 ORGANIZATION ---> ['USA']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> ['Machine', 'North']
 TOTAL GPE ENTITY --> 2 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['machin', 'teach', '(', 'aka', 'learn', ')', 'http', ':', '//www.lexalytics.com/', 'http', ':', '//www.lexalytics.com/', 'w', 'h', 'e', 'p', 'p', 'e', 'r', '4|', '|', 'lexalyt', ',', 'inc.', ',', '48', 'north', 'pleasant', 'st.', 'unit', '301', ',', 'amherst', '01002', 'usa', '|', '1-800-377-8036', '|', 'www.lexalytics.com', 'output', 'system', 'machin', 'learn', 'model', '.']

 TOTAL PORTER STEM WORDS ==> 45



 ---- SNOWBALL STEMMING ----

['machin', 'teach', '(', 'aka', 'learn', ')', 'https', ':', '//www.lexalytics.com/', 'https', ':', '//www.lexalytics.com/', 'w', 'h', 'e', 'p', 'p', 'e', 'r', '4|', '|', 'lexalyt', ',', 'inc.', ',', '48', 'north', 'pleasant', 'st.', 'unit', '301', ',', 'amherst', '01002', 'usa', '|', '1-800-377-8036', '|', 'www.lexalytics.com', 'output', 'system', 'machin', 'learn', 'model', '.']

 TOTAL SNOWBALL STEM WORDS ==> 45



 ---- LEMMATIZATION ----

['Machine', 'teaching', '(', 'aka', 'learning', ')', 'http', ':', '//www.lexalytics.com/', 'http', ':', '//www.lexalytics.com/', 'W', 'H', 'E', 'P', 'P', 'E', 'R', '4|', '|', 'Lexalytics', ',', 'Inc.', ',', '48', 'North', 'Pleasant', 'St.', 'Unit', '301', ',', 'Amherst', '01002', 'USA', '|', '1-800-377-8036', '|', 'www.lexalytics.com', 'output', 'system', 'machine', 'learning', 'model', '.']

 TOTAL LEMMATIZE WORDS ==> 45

************************************************************************************************************************

20 --> If you were baking a cake: • the training data would be the ingredients  • the time and temperature would be the hyper-parameters  • the cake would be the model  Lexalytics Hyper-Parameter Optimization Video  |  3:35 Once the model is created (baked), we can run it against new data  to evaluate what it’s learned, and whether further adjustments   are needed. 


 ---- TOKENS ----

 ['If', 'you', 'were', 'baking', 'a', 'cake', ':', '•', 'the', 'training', 'data', 'would', 'be', 'the', 'ingredients', '•', 'the', 'time', 'and', 'temperature', 'would', 'be', 'the', 'hyper-parameters', '•', 'the', 'cake', 'would', 'be', 'the', 'model', 'Lexalytics', 'Hyper-Parameter', 'Optimization', 'Video', '|', '3:35', 'Once', 'the', 'model', 'is', 'created', '(', 'baked', ')', ',', 'we', 'can', 'run', 'it', 'against', 'new', 'data', 'to', 'evaluate', 'what', 'it', '’', 's', 'learned', ',', 'and', 'whether', 'further', 'adjustments', 'are', 'needed', '.'] 

 TOTAL TOKENS ==> 68

 ---- POST ----

 [('If', 'IN'), ('you', 'PRP'), ('were', 'VBD'), ('baking', 'VBG'), ('a', 'DT'), ('cake', 'NN'), (':', ':'), ('•', 'VB'), ('the', 'DT'), ('training', 'NN'), ('data', 'NNS'), ('would', 'MD'), ('be', 'VB'), ('the', 'DT'), ('ingredients', 'NNS'), ('•', 'IN'), ('the', 'DT'), ('time', 'NN'), ('and', 'CC'), ('temperature', 'NN'), ('would', 'MD'), ('be', 'VB'), ('the', 'DT'), ('hyper-parameters', 'NNS'), ('•', 'VBP'), ('the', 'DT'), ('cake', 'NN'), ('would', 'MD'), ('be', 'VB'), ('the', 'DT'), ('model', 'NN'), ('Lexalytics', 'NNP'), ('Hyper-Parameter', 'NNP'), ('Optimization', 'NNP'), ('Video', 'NNP'), ('|', 'NNP'), ('3:35', 'CD'), ('Once', 'IN'), ('the', 'DT'), ('model', 'NN'), ('is', 'VBZ'), ('created', 'VBN'), ('(', '('), ('baked', 'VBN'), (')', ')'), (',', ','), ('we', 'PRP'), ('can', 'MD'), ('run', 'VB'), ('it', 'PRP'), ('against', 'IN'), ('new', 'JJ'), ('data', 'NNS'), ('to', 'TO'), ('evaluate', 'VB'), ('what', 'WP'), ('it', 'PRP'), ('’', 'VBD'), ('s', 'NN'), ('learned', 'VBN'), (',', ','), ('and', 'CC'), ('whether', 'IN'), ('further', 'JJ'), ('adjustments', 'NNS'), ('are', 'VBP'), ('needed', 'VBN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['baking', 'cake', ':', '•', 'training', 'data', 'would', 'ingredients', '•', 'time', 'temperature', 'would', 'hyper-parameters', '•', 'cake', 'would', 'model', 'Lexalytics', 'Hyper-Parameter', 'Optimization', 'Video', '|', '3:35', 'model', 'created', '(', 'baked', ')', ',', 'run', 'new', 'data', 'evaluate', '’', 'learned', ',', 'whether', 'adjustments', 'needed', '.']

 TOTAL FILTERED TOKENS ==>  40

 ---- POST FOR FILTERED TOKENS ----

 [('baking', 'VBG'), ('cake', 'NN'), (':', ':'), ('•', 'JJ'), ('training', 'NN'), ('data', 'NNS'), ('would', 'MD'), ('ingredients', 'VB'), ('•', 'JJ'), ('time', 'NN'), ('temperature', 'NN'), ('would', 'MD'), ('hyper-parameters', 'NNS'), ('•', 'JJ'), ('cake', 'NN'), ('would', 'MD'), ('model', 'VB'), ('Lexalytics', 'NNP'), ('Hyper-Parameter', 'NNP'), ('Optimization', 'NNP'), ('Video', 'NNP'), ('|', 'NNP'), ('3:35', 'CD'), ('model', 'NN'), ('created', 'VBD'), ('(', '('), ('baked', 'VBN'), (')', ')'), (',', ','), ('run', 'VBP'), ('new', 'JJ'), ('data', 'NNS'), ('evaluate', 'VBP'), ('’', 'NN'), ('learned', 'VBN'), (',', ','), ('whether', 'IN'), ('adjustments', 'NNS'), ('needed', 'VBN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['baking cake', 'cake :', ': •', '• training', 'training data', 'data would', 'would ingredients', 'ingredients •', '• time', 'time temperature', 'temperature would', 'would hyper-parameters', 'hyper-parameters •', '• cake', 'cake would', 'would model', 'model Lexalytics', 'Lexalytics Hyper-Parameter', 'Hyper-Parameter Optimization', 'Optimization Video', 'Video |', '| 3:35', '3:35 model', 'model created', 'created (', '( baked', 'baked )', ') ,', ', run', 'run new', 'new data', 'data evaluate', 'evaluate ’', '’ learned', 'learned ,', ', whether', 'whether adjustments', 'adjustments needed', 'needed .'] 

 TOTAL BIGRAMS --> 39 



 ---- TRI-GRAMS ---- 

 ['baking cake :', 'cake : •', ': • training', '• training data', 'training data would', 'data would ingredients', 'would ingredients •', 'ingredients • time', '• time temperature', 'time temperature would', 'temperature would hyper-parameters', 'would hyper-parameters •', 'hyper-parameters • cake', '• cake would', 'cake would model', 'would model Lexalytics', 'model Lexalytics Hyper-Parameter', 'Lexalytics Hyper-Parameter Optimization', 'Hyper-Parameter Optimization Video', 'Optimization Video |', 'Video | 3:35', '| 3:35 model', '3:35 model created', 'model created (', 'created ( baked', '( baked )', 'baked ) ,', ') , run', ', run new', 'run new data', 'new data evaluate', 'data evaluate ’', 'evaluate ’ learned', '’ learned ,', 'learned , whether', ', whether adjustments', 'whether adjustments needed', 'adjustments needed .'] 

 TOTAL TRIGRAMS --> 38 



 ---- NOUN PHRASES ---- 

 ['cake', '• training', '• time', 'temperature', '• cake', 'model', '’'] 

 TOTAL NOUN PHRASES --> 7 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> ['Lexalytics']
 TOTAL PERSON ENTITY --> 1 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['bake', 'cake', ':', '•', 'train', 'data', 'would', 'ingredi', '•', 'time', 'temperatur', 'would', 'hyper-paramet', '•', 'cake', 'would', 'model', 'lexalyt', 'hyper-paramet', 'optim', 'video', '|', '3:35', 'model', 'creat', '(', 'bake', ')', ',', 'run', 'new', 'data', 'evalu', '’', 'learn', ',', 'whether', 'adjust', 'need', '.']

 TOTAL PORTER STEM WORDS ==> 40



 ---- SNOWBALL STEMMING ----

['bake', 'cake', ':', '•', 'train', 'data', 'would', 'ingredi', '•', 'time', 'temperatur', 'would', 'hyper-paramet', '•', 'cake', 'would', 'model', 'lexalyt', 'hyper-paramet', 'optim', 'video', '|', '3:35', 'model', 'creat', '(', 'bake', ')', ',', 'run', 'new', 'data', 'evalu', '’', 'learn', ',', 'whether', 'adjust', 'need', '.']

 TOTAL SNOWBALL STEM WORDS ==> 40



 ---- LEMMATIZATION ----

['baking', 'cake', ':', '•', 'training', 'data', 'would', 'ingredient', '•', 'time', 'temperature', 'would', 'hyper-parameters', '•', 'cake', 'would', 'model', 'Lexalytics', 'Hyper-Parameter', 'Optimization', 'Video', '|', '3:35', 'model', 'created', '(', 'baked', ')', ',', 'run', 'new', 'data', 'evaluate', '’', 'learned', ',', 'whether', 'adjustment', 'needed', '.']

 TOTAL LEMMATIZE WORDS ==> 40

************************************************************************************************************************

21 --> However, making adjustments isn’t just a matter of writing a line   of code that tells the model what to do. 


 ---- TOKENS ----

 ['However', ',', 'making', 'adjustments', 'isn', '’', 't', 'just', 'a', 'matter', 'of', 'writing', 'a', 'line', 'of', 'code', 'that', 'tells', 'the', 'model', 'what', 'to', 'do', '.'] 

 TOTAL TOKENS ==> 24

 ---- POST ----

 [('However', 'RB'), (',', ','), ('making', 'VBG'), ('adjustments', 'NNS'), ('isn', 'JJ'), ('’', 'NNP'), ('t', 'NN'), ('just', 'RB'), ('a', 'DT'), ('matter', 'NN'), ('of', 'IN'), ('writing', 'VBG'), ('a', 'DT'), ('line', 'NN'), ('of', 'IN'), ('code', 'NN'), ('that', 'WDT'), ('tells', 'VBZ'), ('the', 'DT'), ('model', 'NN'), ('what', 'WP'), ('to', 'TO'), ('do', 'VB'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['However', ',', 'making', 'adjustments', '’', 'matter', 'writing', 'line', 'code', 'tells', 'model', '.']

 TOTAL FILTERED TOKENS ==>  12

 ---- POST FOR FILTERED TOKENS ----

 [('However', 'RB'), (',', ','), ('making', 'VBG'), ('adjustments', 'NNS'), ('’', 'JJ'), ('matter', 'NN'), ('writing', 'VBG'), ('line', 'NN'), ('code', 'NN'), ('tells', 'NNS'), ('model', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['However ,', ', making', 'making adjustments', 'adjustments ’', '’ matter', 'matter writing', 'writing line', 'line code', 'code tells', 'tells model', 'model .'] 

 TOTAL BIGRAMS --> 11 



 ---- TRI-GRAMS ---- 

 ['However , making', ', making adjustments', 'making adjustments ’', 'adjustments ’ matter', '’ matter writing', 'matter writing line', 'writing line code', 'line code tells', 'code tells model', 'tells model .'] 

 TOTAL TRIGRAMS --> 10 



 ---- NOUN PHRASES ---- 

 ['’ matter', 'line', 'code', 'model'] 

 TOTAL NOUN PHRASES --> 4 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['howev', ',', 'make', 'adjust', '’', 'matter', 'write', 'line', 'code', 'tell', 'model', '.']

 TOTAL PORTER STEM WORDS ==> 12



 ---- SNOWBALL STEMMING ----

['howev', ',', 'make', 'adjust', '’', 'matter', 'write', 'line', 'code', 'tell', 'model', '.']

 TOTAL SNOWBALL STEM WORDS ==> 12



 ---- LEMMATIZATION ----

['However', ',', 'making', 'adjustment', '’', 'matter', 'writing', 'line', 'code', 'tell', 'model', '.']

 TOTAL LEMMATIZE WORDS ==> 12

************************************************************************************************************************

22 --> That kind of direct approach is  known as “algorithmic programming” – what most people call “coding.”   With machine learning, we need to convince the model that it wants to do   what we want it to do. 


 ---- TOKENS ----

 ['That', 'kind', 'of', 'direct', 'approach', 'is', 'known', 'as', '“', 'algorithmic', 'programming', '”', '–', 'what', 'most', 'people', 'call', '“', 'coding.', '”', 'With', 'machine', 'learning', ',', 'we', 'need', 'to', 'convince', 'the', 'model', 'that', 'it', 'wants', 'to', 'do', 'what', 'we', 'want', 'it', 'to', 'do', '.'] 

 TOTAL TOKENS ==> 42

 ---- POST ----

 [('That', 'DT'), ('kind', 'NN'), ('of', 'IN'), ('direct', 'JJ'), ('approach', 'NN'), ('is', 'VBZ'), ('known', 'VBN'), ('as', 'IN'), ('“', 'JJ'), ('algorithmic', 'JJ'), ('programming', 'NN'), ('”', 'NN'), ('–', 'VBP'), ('what', 'WP'), ('most', 'JJS'), ('people', 'NNS'), ('call', 'VBP'), ('“', 'JJ'), ('coding.', 'NN'), ('”', 'NN'), ('With', 'IN'), ('machine', 'NN'), ('learning', 'NN'), (',', ','), ('we', 'PRP'), ('need', 'VBP'), ('to', 'TO'), ('convince', 'VB'), ('the', 'DT'), ('model', 'NN'), ('that', 'IN'), ('it', 'PRP'), ('wants', 'VBZ'), ('to', 'TO'), ('do', 'VB'), ('what', 'WP'), ('we', 'PRP'), ('want', 'VBP'), ('it', 'PRP'), ('to', 'TO'), ('do', 'VB'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['kind', 'direct', 'approach', 'known', '“', 'algorithmic', 'programming', '”', '–', 'people', 'call', '“', 'coding.', '”', 'machine', 'learning', ',', 'need', 'convince', 'model', 'wants', 'want', '.']

 TOTAL FILTERED TOKENS ==>  23

 ---- POST FOR FILTERED TOKENS ----

 [('kind', 'NN'), ('direct', 'JJ'), ('approach', 'NN'), ('known', 'VBN'), ('“', 'JJ'), ('algorithmic', 'JJ'), ('programming', 'NN'), ('”', 'JJ'), ('–', 'JJ'), ('people', 'NNS'), ('call', 'VBP'), ('“', 'JJ'), ('coding.', 'NN'), ('”', 'NNP'), ('machine', 'NN'), ('learning', 'NN'), (',', ','), ('need', 'VBP'), ('convince', 'NN'), ('model', 'NN'), ('wants', 'VBZ'), ('want', 'VBP'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['kind direct', 'direct approach', 'approach known', 'known “', '“ algorithmic', 'algorithmic programming', 'programming ”', '” –', '– people', 'people call', 'call “', '“ coding.', 'coding. ”', '” machine', 'machine learning', 'learning ,', ', need', 'need convince', 'convince model', 'model wants', 'wants want', 'want .'] 

 TOTAL BIGRAMS --> 22 



 ---- TRI-GRAMS ---- 

 ['kind direct approach', 'direct approach known', 'approach known “', 'known “ algorithmic', '“ algorithmic programming', 'algorithmic programming ”', 'programming ” –', '” – people', '– people call', 'people call “', 'call “ coding.', '“ coding. ”', 'coding. ” machine', '” machine learning', 'machine learning ,', 'learning , need', ', need convince', 'need convince model', 'convince model wants', 'model wants want', 'wants want .'] 

 TOTAL TRIGRAMS --> 21 



 ---- NOUN PHRASES ---- 

 ['kind', 'direct approach', '“ algorithmic programming', '“ coding.', 'machine', 'learning', 'convince', 'model'] 

 TOTAL NOUN PHRASES --> 8 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['kind', 'direct', 'approach', 'known', '“', 'algorithm', 'program', '”', '–', 'peopl', 'call', '“', 'coding.', '”', 'machin', 'learn', ',', 'need', 'convinc', 'model', 'want', 'want', '.']

 TOTAL PORTER STEM WORDS ==> 23



 ---- SNOWBALL STEMMING ----

['kind', 'direct', 'approach', 'known', '“', 'algorithm', 'program', '”', '–', 'peopl', 'call', '“', 'coding.', '”', 'machin', 'learn', ',', 'need', 'convinc', 'model', 'want', 'want', '.']

 TOTAL SNOWBALL STEM WORDS ==> 23



 ---- LEMMATIZATION ----

['kind', 'direct', 'approach', 'known', '“', 'algorithmic', 'programming', '”', '–', 'people', 'call', '“', 'coding.', '”', 'machine', 'learning', ',', 'need', 'convince', 'model', 'want', 'want', '.']

 TOTAL LEMMATIZE WORDS ==> 23

************************************************************************************************************************

23 --> Writing a line of code is clearly the more precise, concise approach –   and one that’s going to almost certainly be less work than machine   learning. 


 ---- TOKENS ----

 ['Writing', 'a', 'line', 'of', 'code', 'is', 'clearly', 'the', 'more', 'precise', ',', 'concise', 'approach', '–', 'and', 'one', 'that', '’', 's', 'going', 'to', 'almost', 'certainly', 'be', 'less', 'work', 'than', 'machine', 'learning', '.'] 

 TOTAL TOKENS ==> 30

 ---- POST ----

 [('Writing', 'VBG'), ('a', 'DT'), ('line', 'NN'), ('of', 'IN'), ('code', 'NN'), ('is', 'VBZ'), ('clearly', 'RB'), ('the', 'DT'), ('more', 'RBR'), ('precise', 'JJ'), (',', ','), ('concise', 'VB'), ('approach', 'NN'), ('–', 'NN'), ('and', 'CC'), ('one', 'CD'), ('that', 'WDT'), ('’', 'VBZ'), ('s', 'RB'), ('going', 'VBG'), ('to', 'TO'), ('almost', 'RB'), ('certainly', 'RB'), ('be', 'VB'), ('less', 'JJR'), ('work', 'NN'), ('than', 'IN'), ('machine', 'NN'), ('learning', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Writing', 'line', 'code', 'clearly', 'precise', ',', 'concise', 'approach', '–', 'one', '’', 'going', 'almost', 'certainly', 'less', 'work', 'machine', 'learning', '.']

 TOTAL FILTERED TOKENS ==>  19

 ---- POST FOR FILTERED TOKENS ----

 [('Writing', 'VBG'), ('line', 'NN'), ('code', 'NN'), ('clearly', 'RB'), ('precise', 'RB'), (',', ','), ('concise', 'VB'), ('approach', 'NN'), ('–', 'IN'), ('one', 'CD'), ('’', 'NN'), ('going', 'VBG'), ('almost', 'RB'), ('certainly', 'RB'), ('less', 'RBR'), ('work', 'JJ'), ('machine', 'NN'), ('learning', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Writing line', 'line code', 'code clearly', 'clearly precise', 'precise ,', ', concise', 'concise approach', 'approach –', '– one', 'one ’', '’ going', 'going almost', 'almost certainly', 'certainly less', 'less work', 'work machine', 'machine learning', 'learning .'] 

 TOTAL BIGRAMS --> 18 



 ---- TRI-GRAMS ---- 

 ['Writing line code', 'line code clearly', 'code clearly precise', 'clearly precise ,', 'precise , concise', ', concise approach', 'concise approach –', 'approach – one', '– one ’', 'one ’ going', '’ going almost', 'going almost certainly', 'almost certainly less', 'certainly less work', 'less work machine', 'work machine learning', 'machine learning .'] 

 TOTAL TRIGRAMS --> 17 



 ---- NOUN PHRASES ---- 

 ['line', 'code', 'approach', '’', 'work machine', 'learning'] 

 TOTAL NOUN PHRASES --> 6 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['write', 'line', 'code', 'clearli', 'precis', ',', 'concis', 'approach', '–', 'one', '’', 'go', 'almost', 'certainli', 'less', 'work', 'machin', 'learn', '.']

 TOTAL PORTER STEM WORDS ==> 19



 ---- SNOWBALL STEMMING ----

['write', 'line', 'code', 'clear', 'precis', ',', 'concis', 'approach', '–', 'one', '’', 'go', 'almost', 'certain', 'less', 'work', 'machin', 'learn', '.']

 TOTAL SNOWBALL STEM WORDS ==> 19



 ---- LEMMATIZATION ----

['Writing', 'line', 'code', 'clearly', 'precise', ',', 'concise', 'approach', '–', 'one', '’', 'going', 'almost', 'certainly', 'le', 'work', 'machine', 'learning', '.']

 TOTAL LEMMATIZE WORDS ==> 19

************************************************************************************************************************

24 --> We talk about this in the white paper “Tune First, Then Train.” However, coding isn’t always the right solution. 


 ---- TOKENS ----

 ['We', 'talk', 'about', 'this', 'in', 'the', 'white', 'paper', '“', 'Tune', 'First', ',', 'Then', 'Train.', '”', 'However', ',', 'coding', 'isn', '’', 't', 'always', 'the', 'right', 'solution', '.'] 

 TOTAL TOKENS ==> 26

 ---- POST ----

 [('We', 'PRP'), ('talk', 'VBP'), ('about', 'IN'), ('this', 'DT'), ('in', 'IN'), ('the', 'DT'), ('white', 'JJ'), ('paper', 'NN'), ('“', 'NN'), ('Tune', 'NNP'), ('First', 'NNP'), (',', ','), ('Then', 'RB'), ('Train.', 'NNP'), ('”', 'NNP'), ('However', 'RB'), (',', ','), ('coding', 'VBG'), ('isn', 'JJ'), ('’', 'NNP'), ('t', 'NN'), ('always', 'RB'), ('the', 'DT'), ('right', 'JJ'), ('solution', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['talk', 'white', 'paper', '“', 'Tune', 'First', ',', 'Train.', '”', 'However', ',', 'coding', '’', 'always', 'right', 'solution', '.']

 TOTAL FILTERED TOKENS ==>  17

 ---- POST FOR FILTERED TOKENS ----

 [('talk', 'NN'), ('white', 'JJ'), ('paper', 'NN'), ('“', 'NN'), ('Tune', 'NNP'), ('First', 'NNP'), (',', ','), ('Train.', 'NNP'), ('”', 'NNP'), ('However', 'RB'), (',', ','), ('coding', 'VBG'), ('’', 'NN'), ('always', 'RB'), ('right', 'JJ'), ('solution', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['talk white', 'white paper', 'paper “', '“ Tune', 'Tune First', 'First ,', ', Train.', 'Train. ”', '” However', 'However ,', ', coding', 'coding ’', '’ always', 'always right', 'right solution', 'solution .'] 

 TOTAL BIGRAMS --> 16 



 ---- TRI-GRAMS ---- 

 ['talk white paper', 'white paper “', 'paper “ Tune', '“ Tune First', 'Tune First ,', 'First , Train.', ', Train. ”', 'Train. ” However', '” However ,', 'However , coding', ', coding ’', 'coding ’ always', '’ always right', 'always right solution', 'right solution .'] 

 TOTAL TRIGRAMS --> 15 



 ---- NOUN PHRASES ---- 

 ['talk', 'white paper', '“', '’', 'right solution'] 

 TOTAL NOUN PHRASES --> 5 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> ['Tune First']
 TOTAL PERSON ENTITY --> 1 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['talk', 'white', 'paper', '“', 'tune', 'first', ',', 'train.', '”', 'howev', ',', 'code', '’', 'alway', 'right', 'solut', '.']

 TOTAL PORTER STEM WORDS ==> 17



 ---- SNOWBALL STEMMING ----

['talk', 'white', 'paper', '“', 'tune', 'first', ',', 'train.', '”', 'howev', ',', 'code', '’', 'alway', 'right', 'solut', '.']

 TOTAL SNOWBALL STEM WORDS ==> 17



 ---- LEMMATIZATION ----

['talk', 'white', 'paper', '“', 'Tune', 'First', ',', 'Train.', '”', 'However', ',', 'coding', '’', 'always', 'right', 'solution', '.']

 TOTAL LEMMATIZE WORDS ==> 17

************************************************************************************************************************

25 --> Machine learning is   much better than coding at dealing with novel cases and learning   from the experience. 


 ---- TOKENS ----

 ['Machine', 'learning', 'is', 'much', 'better', 'than', 'coding', 'at', 'dealing', 'with', 'novel', 'cases', 'and', 'learning', 'from', 'the', 'experience', '.'] 

 TOTAL TOKENS ==> 18

 ---- POST ----

 [('Machine', 'NN'), ('learning', 'NN'), ('is', 'VBZ'), ('much', 'RB'), ('better', 'JJR'), ('than', 'IN'), ('coding', 'VBG'), ('at', 'IN'), ('dealing', 'VBG'), ('with', 'IN'), ('novel', 'JJ'), ('cases', 'NNS'), ('and', 'CC'), ('learning', 'VBG'), ('from', 'IN'), ('the', 'DT'), ('experience', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Machine', 'learning', 'much', 'better', 'coding', 'dealing', 'novel', 'cases', 'learning', 'experience', '.']

 TOTAL FILTERED TOKENS ==>  11

 ---- POST FOR FILTERED TOKENS ----

 [('Machine', 'NN'), ('learning', 'VBG'), ('much', 'JJ'), ('better', 'RBR'), ('coding', 'VBG'), ('dealing', 'VBG'), ('novel', 'JJ'), ('cases', 'NNS'), ('learning', 'VBG'), ('experience', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Machine learning', 'learning much', 'much better', 'better coding', 'coding dealing', 'dealing novel', 'novel cases', 'cases learning', 'learning experience', 'experience .'] 

 TOTAL BIGRAMS --> 10 



 ---- TRI-GRAMS ---- 

 ['Machine learning much', 'learning much better', 'much better coding', 'better coding dealing', 'coding dealing novel', 'dealing novel cases', 'novel cases learning', 'cases learning experience', 'learning experience .'] 

 TOTAL TRIGRAMS --> 9 



 ---- NOUN PHRASES ---- 

 ['Machine', 'experience'] 

 TOTAL NOUN PHRASES --> 2 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> ['Machine']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['machin', 'learn', 'much', 'better', 'code', 'deal', 'novel', 'case', 'learn', 'experi', '.']

 TOTAL PORTER STEM WORDS ==> 11



 ---- SNOWBALL STEMMING ----

['machin', 'learn', 'much', 'better', 'code', 'deal', 'novel', 'case', 'learn', 'experi', '.']

 TOTAL SNOWBALL STEM WORDS ==> 11



 ---- LEMMATIZATION ----

['Machine', 'learning', 'much', 'better', 'coding', 'dealing', 'novel', 'case', 'learning', 'experience', '.']

 TOTAL LEMMATIZE WORDS ==> 11

************************************************************************************************************************

26 --> In the next section we’ll review the main classes of machine learning. 


 ---- TOKENS ----

 ['In', 'the', 'next', 'section', 'we', '’', 'll', 'review', 'the', 'main', 'classes', 'of', 'machine', 'learning', '.'] 

 TOTAL TOKENS ==> 15

 ---- POST ----

 [('In', 'IN'), ('the', 'DT'), ('next', 'JJ'), ('section', 'NN'), ('we', 'PRP'), ('’', 'VBP'), ('ll', 'JJ'), ('review', 'VBP'), ('the', 'DT'), ('main', 'JJ'), ('classes', 'NNS'), ('of', 'IN'), ('machine', 'NN'), ('learning', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['next', 'section', '’', 'review', 'main', 'classes', 'machine', 'learning', '.']

 TOTAL FILTERED TOKENS ==>  9

 ---- POST FOR FILTERED TOKENS ----

 [('next', 'JJ'), ('section', 'NN'), ('’', 'NNP'), ('review', 'NN'), ('main', 'JJ'), ('classes', 'NNS'), ('machine', 'NN'), ('learning', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['next section', 'section ’', '’ review', 'review main', 'main classes', 'classes machine', 'machine learning', 'learning .'] 

 TOTAL BIGRAMS --> 8 



 ---- TRI-GRAMS ---- 

 ['next section ’', 'section ’ review', '’ review main', 'review main classes', 'main classes machine', 'classes machine learning', 'machine learning .'] 

 TOTAL TRIGRAMS --> 7 



 ---- NOUN PHRASES ---- 

 ['next section', 'review', 'machine', 'learning'] 

 TOTAL NOUN PHRASES --> 4 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['next', 'section', '’', 'review', 'main', 'class', 'machin', 'learn', '.']

 TOTAL PORTER STEM WORDS ==> 9



 ---- SNOWBALL STEMMING ----

['next', 'section', '’', 'review', 'main', 'class', 'machin', 'learn', '.']

 TOTAL SNOWBALL STEM WORDS ==> 9



 ---- LEMMATIZATION ----

['next', 'section', '’', 'review', 'main', 'class', 'machine', 'learning', '.']

 TOTAL LEMMATIZE WORDS ==> 9

************************************************************************************************************************

27 --> is simply a matter of writing   a line of code that tells the   model what to do. 


 ---- TOKENS ----

 ['is', 'simply', 'a', 'matter', 'of', 'writing', 'a', 'line', 'of', 'code', 'that', 'tells', 'the', 'model', 'what', 'to', 'do', '.'] 

 TOTAL TOKENS ==> 18

 ---- POST ----

 [('is', 'VBZ'), ('simply', 'RB'), ('a', 'DT'), ('matter', 'NN'), ('of', 'IN'), ('writing', 'VBG'), ('a', 'DT'), ('line', 'NN'), ('of', 'IN'), ('code', 'NN'), ('that', 'WDT'), ('tells', 'VBZ'), ('the', 'DT'), ('model', 'NN'), ('what', 'WP'), ('to', 'TO'), ('do', 'VB'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['simply', 'matter', 'writing', 'line', 'code', 'tells', 'model', '.']

 TOTAL FILTERED TOKENS ==>  8

 ---- POST FOR FILTERED TOKENS ----

 [('simply', 'RB'), ('matter', 'NN'), ('writing', 'VBG'), ('line', 'NN'), ('code', 'NN'), ('tells', 'NNS'), ('model', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['simply matter', 'matter writing', 'writing line', 'line code', 'code tells', 'tells model', 'model .'] 

 TOTAL BIGRAMS --> 7 



 ---- TRI-GRAMS ---- 

 ['simply matter writing', 'matter writing line', 'writing line code', 'line code tells', 'code tells model', 'tells model .'] 

 TOTAL TRIGRAMS --> 6 



 ---- NOUN PHRASES ---- 

 ['matter', 'line', 'code', 'model'] 

 TOTAL NOUN PHRASES --> 4 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['simpli', 'matter', 'write', 'line', 'code', 'tell', 'model', '.']

 TOTAL PORTER STEM WORDS ==> 8



 ---- SNOWBALL STEMMING ----

['simpli', 'matter', 'write', 'line', 'code', 'tell', 'model', '.']

 TOTAL SNOWBALL STEM WORDS ==> 8



 ---- LEMMATIZATION ----

['simply', 'matter', 'writing', 'line', 'code', 'tell', 'model', '.']

 TOTAL LEMMATIZE WORDS ==> 8

************************************************************************************************************************

28 --> Algorithmic  programming https://www.lexalytics.com/ https://www.lexalytics.com/ https://www.lexalytics.com/resources/videos?id=EeTkMc1o1uQ https://www.lexalytics.com/resources/wp-content/uploads/sites/3/2019/02/Lexalytics_Tune_First_Then_Train_Whitepaper.pdf  W H I T E  P A P E R 5|       |   Lexalytics, Inc., 48 North Pleasant St. Unit 301, Amherst MA 01002 USA   |   1-800-377-8036 |   www.lexalytics.com means feeding a   machine learning model   an annotated dataset. 


 ---- TOKENS ----

 ['Algorithmic', 'programming', 'https', ':', '//www.lexalytics.com/', 'https', ':', '//www.lexalytics.com/', 'https', ':', '//www.lexalytics.com/resources/videos', '?', 'id=EeTkMc1o1uQ', 'https', ':', '//www.lexalytics.com/resources/wp-content/uploads/sites/3/2019/02/Lexalytics_Tune_First_Then_Train_Whitepaper.pdf', 'W', 'H', 'I', 'T', 'E', 'P', 'A', 'P', 'E', 'R', '5|', '|', 'Lexalytics', ',', 'Inc.', ',', '48', 'North', 'Pleasant', 'St.', 'Unit', '301', ',', 'Amherst', 'MA', '01002', 'USA', '|', '1-800-377-8036', '|', 'www.lexalytics.com', 'means', 'feeding', 'a', 'machine', 'learning', 'model', 'an', 'annotated', 'dataset', '.'] 

 TOTAL TOKENS ==> 57

 ---- POST ----

 [('Algorithmic', 'NNP'), ('programming', 'VBG'), ('https', 'NN'), (':', ':'), ('//www.lexalytics.com/', 'JJ'), ('https', 'NN'), (':', ':'), ('//www.lexalytics.com/', 'JJ'), ('https', 'NN'), (':', ':'), ('//www.lexalytics.com/resources/videos', 'NN'), ('?', '.'), ('id=EeTkMc1o1uQ', 'JJ'), ('https', 'NN'), (':', ':'), ('//www.lexalytics.com/resources/wp-content/uploads/sites/3/2019/02/Lexalytics_Tune_First_Then_Train_Whitepaper.pdf', 'JJ'), ('W', 'NNP'), ('H', 'NNP'), ('I', 'PRP'), ('T', 'NNP'), ('E', 'NNP'), ('P', 'NNP'), ('A', 'NNP'), ('P', 'NNP'), ('E', 'NNP'), ('R', 'NNP'), ('5|', 'CD'), ('|', 'NNP'), ('Lexalytics', 'NNP'), (',', ','), ('Inc.', 'NNP'), (',', ','), ('48', 'CD'), ('North', 'NNP'), ('Pleasant', 'NNP'), ('St.', 'NNP'), ('Unit', 'NNP'), ('301', 'CD'), (',', ','), ('Amherst', 'NNP'), ('MA', 'NNP'), ('01002', 'CD'), ('USA', 'NNP'), ('|', 'NNP'), ('1-800-377-8036', 'JJ'), ('|', 'NNP'), ('www.lexalytics.com', 'NN'), ('means', 'VBZ'), ('feeding', 'VBG'), ('a', 'DT'), ('machine', 'NN'), ('learning', 'VBG'), ('model', 'NN'), ('an', 'DT'), ('annotated', 'JJ'), ('dataset', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Algorithmic', 'programming', 'https', ':', '//www.lexalytics.com/', 'https', ':', '//www.lexalytics.com/', 'https', ':', '//www.lexalytics.com/resources/videos', '?', 'id=EeTkMc1o1uQ', 'https', ':', '//www.lexalytics.com/resources/wp-content/uploads/sites/3/2019/02/Lexalytics_Tune_First_Then_Train_Whitepaper.pdf', 'W', 'H', 'E', 'P', 'P', 'E', 'R', '5|', '|', 'Lexalytics', ',', 'Inc.', ',', '48', 'North', 'Pleasant', 'St.', 'Unit', '301', ',', 'Amherst', '01002', 'USA', '|', '1-800-377-8036', '|', 'www.lexalytics.com', 'means', 'feeding', 'machine', 'learning', 'model', 'annotated', 'dataset', '.']

 TOTAL FILTERED TOKENS ==>  51

 ---- POST FOR FILTERED TOKENS ----

 [('Algorithmic', 'NNP'), ('programming', 'VBG'), ('https', 'NN'), (':', ':'), ('//www.lexalytics.com/', 'JJ'), ('https', 'NN'), (':', ':'), ('//www.lexalytics.com/', 'JJ'), ('https', 'NN'), (':', ':'), ('//www.lexalytics.com/resources/videos', 'NN'), ('?', '.'), ('id=EeTkMc1o1uQ', 'JJ'), ('https', 'NN'), (':', ':'), ('//www.lexalytics.com/resources/wp-content/uploads/sites/3/2019/02/Lexalytics_Tune_First_Then_Train_Whitepaper.pdf', 'JJ'), ('W', 'NNP'), ('H', 'NNP'), ('E', 'NNP'), ('P', 'NNP'), ('P', 'NNP'), ('E', 'NNP'), ('R', 'NNP'), ('5|', 'CD'), ('|', 'NNP'), ('Lexalytics', 'NNP'), (',', ','), ('Inc.', 'NNP'), (',', ','), ('48', 'CD'), ('North', 'NNP'), ('Pleasant', 'NNP'), ('St.', 'NNP'), ('Unit', 'NNP'), ('301', 'CD'), (',', ','), ('Amherst', 'NNP'), ('01002', 'CD'), ('USA', 'NNP'), ('|', 'NNP'), ('1-800-377-8036', 'JJ'), ('|', 'NNP'), ('www.lexalytics.com', 'NN'), ('means', 'VBZ'), ('feeding', 'JJ'), ('machine', 'NN'), ('learning', 'VBG'), ('model', 'NN'), ('annotated', 'VBD'), ('dataset', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Algorithmic programming', 'programming https', 'https :', ': //www.lexalytics.com/', '//www.lexalytics.com/ https', 'https :', ': //www.lexalytics.com/', '//www.lexalytics.com/ https', 'https :', ': //www.lexalytics.com/resources/videos', '//www.lexalytics.com/resources/videos ?', '? id=EeTkMc1o1uQ', 'id=EeTkMc1o1uQ https', 'https :', ': //www.lexalytics.com/resources/wp-content/uploads/sites/3/2019/02/Lexalytics_Tune_First_Then_Train_Whitepaper.pdf', '//www.lexalytics.com/resources/wp-content/uploads/sites/3/2019/02/Lexalytics_Tune_First_Then_Train_Whitepaper.pdf W', 'W H', 'H E', 'E P', 'P P', 'P E', 'E R', 'R 5|', '5| |', '| Lexalytics', 'Lexalytics ,', ', Inc.', 'Inc. ,', ', 48', '48 North', 'North Pleasant', 'Pleasant St.', 'St. Unit', 'Unit 301', '301 ,', ', Amherst', 'Amherst 01002', '01002 USA', 'USA |', '| 1-800-377-8036', '1-800-377-8036 |', '| www.lexalytics.com', 'www.lexalytics.com means', 'means feeding', 'feeding machine', 'machine learning', 'learning model', 'model annotated', 'annotated dataset', 'dataset .'] 

 TOTAL BIGRAMS --> 50 



 ---- TRI-GRAMS ---- 

 ['Algorithmic programming https', 'programming https :', 'https : //www.lexalytics.com/', ': //www.lexalytics.com/ https', '//www.lexalytics.com/ https :', 'https : //www.lexalytics.com/', ': //www.lexalytics.com/ https', '//www.lexalytics.com/ https :', 'https : //www.lexalytics.com/resources/videos', ': //www.lexalytics.com/resources/videos ?', '//www.lexalytics.com/resources/videos ? id=EeTkMc1o1uQ', '? id=EeTkMc1o1uQ https', 'id=EeTkMc1o1uQ https :', 'https : //www.lexalytics.com/resources/wp-content/uploads/sites/3/2019/02/Lexalytics_Tune_First_Then_Train_Whitepaper.pdf', ': //www.lexalytics.com/resources/wp-content/uploads/sites/3/2019/02/Lexalytics_Tune_First_Then_Train_Whitepaper.pdf W', '//www.lexalytics.com/resources/wp-content/uploads/sites/3/2019/02/Lexalytics_Tune_First_Then_Train_Whitepaper.pdf W H', 'W H E', 'H E P', 'E P P', 'P P E', 'P E R', 'E R 5|', 'R 5| |', '5| | Lexalytics', '| Lexalytics ,', 'Lexalytics , Inc.', ', Inc. ,', 'Inc. , 48', ', 48 North', '48 North Pleasant', 'North Pleasant St.', 'Pleasant St. Unit', 'St. Unit 301', 'Unit 301 ,', '301 , Amherst', ', Amherst 01002', 'Amherst 01002 USA', '01002 USA |', 'USA | 1-800-377-8036', '| 1-800-377-8036 |', '1-800-377-8036 | www.lexalytics.com', '| www.lexalytics.com means', 'www.lexalytics.com means feeding', 'means feeding machine', 'feeding machine learning', 'machine learning model', 'learning model annotated', 'model annotated dataset', 'annotated dataset .'] 

 TOTAL TRIGRAMS --> 49 



 ---- NOUN PHRASES ---- 

 ['https', ' https', ' https', '', 'id=EeTkMc1o1uQ https', 'www.lexalytics.com', 'feeding machine', 'model', 'dataset'] 

 TOTAL NOUN PHRASES --> 9 



 ---- NER ----

 
 ORGANIZATION ---> ['USA']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> ['Algorithmic', 'North']
 TOTAL GPE ENTITY --> 2 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['algorithm', 'program', 'http', ':', '//www.lexalytics.com/', 'http', ':', '//www.lexalytics.com/', 'http', ':', '//www.lexalytics.com/resources/video', '?', 'id=eetkmc1o1uq', 'http', ':', '//www.lexalytics.com/resources/wp-content/uploads/sites/3/2019/02/lexalytics_tune_first_then_train_whitepaper.pdf', 'w', 'h', 'e', 'p', 'p', 'e', 'r', '5|', '|', 'lexalyt', ',', 'inc.', ',', '48', 'north', 'pleasant', 'st.', 'unit', '301', ',', 'amherst', '01002', 'usa', '|', '1-800-377-8036', '|', 'www.lexalytics.com', 'mean', 'feed', 'machin', 'learn', 'model', 'annot', 'dataset', '.']

 TOTAL PORTER STEM WORDS ==> 51



 ---- SNOWBALL STEMMING ----

['algorithm', 'program', 'https', ':', '//www.lexalytics.com/', 'https', ':', '//www.lexalytics.com/', 'https', ':', '//www.lexalytics.com/resources/video', '?', 'id=eetkmc1o1uq', 'https', ':', '//www.lexalytics.com/resources/wp-content/uploads/sites/3/2019/02/lexalytics_tune_first_then_train_whitepaper.pdf', 'w', 'h', 'e', 'p', 'p', 'e', 'r', '5|', '|', 'lexalyt', ',', 'inc.', ',', '48', 'north', 'pleasant', 'st.', 'unit', '301', ',', 'amherst', '01002', 'usa', '|', '1-800-377-8036', '|', 'www.lexalytics.com', 'mean', 'feed', 'machin', 'learn', 'model', 'annot', 'dataset', '.']

 TOTAL SNOWBALL STEM WORDS ==> 51



 ---- LEMMATIZATION ----

['Algorithmic', 'programming', 'http', ':', '//www.lexalytics.com/', 'http', ':', '//www.lexalytics.com/', 'http', ':', '//www.lexalytics.com/resources/videos', '?', 'id=EeTkMc1o1uQ', 'http', ':', '//www.lexalytics.com/resources/wp-content/uploads/sites/3/2019/02/Lexalytics_Tune_First_Then_Train_Whitepaper.pdf', 'W', 'H', 'E', 'P', 'P', 'E', 'R', '5|', '|', 'Lexalytics', ',', 'Inc.', ',', '48', 'North', 'Pleasant', 'St.', 'Unit', '301', ',', 'Amherst', '01002', 'USA', '|', '1-800-377-8036', '|', 'www.lexalytics.com', 'mean', 'feeding', 'machine', 'learning', 'model', 'annotated', 'dataset', '.']

 TOTAL LEMMATIZE WORDS ==> 51

************************************************************************************************************************

29 --> Supervised  learning S U P E R V I S E D ,  U N S U P E R V I S E D ,   A N D  S E M I - S U P E R V I S E D   M A C H I N E  L E A R N I N G  There are three relevant classes of machine learning: supervised learning,  unsupervised learning, and semi-supervised learning. 


 ---- TOKENS ----

 ['Supervised', 'learning', 'S', 'U', 'P', 'E', 'R', 'V', 'I', 'S', 'E', 'D', ',', 'U', 'N', 'S', 'U', 'P', 'E', 'R', 'V', 'I', 'S', 'E', 'D', ',', 'A', 'N', 'D', 'S', 'E', 'M', 'I', '-', 'S', 'U', 'P', 'E', 'R', 'V', 'I', 'S', 'E', 'D', 'M', 'A', 'C', 'H', 'I', 'N', 'E', 'L', 'E', 'A', 'R', 'N', 'I', 'N', 'G', 'There', 'are', 'three', 'relevant', 'classes', 'of', 'machine', 'learning', ':', 'supervised', 'learning', ',', 'unsupervised', 'learning', ',', 'and', 'semi-supervised', 'learning', '.'] 

 TOTAL TOKENS ==> 78

 ---- POST ----

 [('Supervised', 'VBN'), ('learning', 'VBG'), ('S', 'NNP'), ('U', 'NNP'), ('P', 'NNP'), ('E', 'NNP'), ('R', 'NNP'), ('V', 'NNP'), ('I', 'PRP'), ('S', 'NNP'), ('E', 'NNP'), ('D', 'NNP'), (',', ','), ('U', 'NNP'), ('N', 'NNP'), ('S', 'NNP'), ('U', 'NNP'), ('P', 'NNP'), ('E', 'NNP'), ('R', 'NNP'), ('V', 'NNP'), ('I', 'PRP'), ('S', 'NNP'), ('E', 'NNP'), ('D', 'NNP'), (',', ','), ('A', 'NNP'), ('N', 'NNP'), ('D', 'NNP'), ('S', 'NNP'), ('E', 'NNP'), ('M', 'NNP'), ('I', 'PRP'), ('-', ':'), ('S', 'NNP'), ('U', 'NNP'), ('P', 'NNP'), ('E', 'NNP'), ('R', 'NNP'), ('V', 'NNP'), ('I', 'PRP'), ('S', 'NNP'), ('E', 'NNP'), ('D', 'NNP'), ('M', 'NNP'), ('A', 'NNP'), ('C', 'NNP'), ('H', 'NNP'), ('I', 'PRP'), ('N', 'NNP'), ('E', 'NNP'), ('L', 'NNP'), ('E', 'NNP'), ('A', 'NNP'), ('R', 'NNP'), ('N', 'NNP'), ('I', 'PRP'), ('N', 'NNP'), ('G', 'NNP'), ('There', 'EX'), ('are', 'VBP'), ('three', 'CD'), ('relevant', 'JJ'), ('classes', 'NNS'), ('of', 'IN'), ('machine', 'NN'), ('learning', 'NN'), (':', ':'), ('supervised', 'VBN'), ('learning', 'NN'), (',', ','), ('unsupervised', 'JJ'), ('learning', 'NN'), (',', ','), ('and', 'CC'), ('semi-supervised', 'JJ'), ('learning', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Supervised', 'learning', 'U', 'P', 'E', 'R', 'V', 'E', ',', 'U', 'N', 'U', 'P', 'E', 'R', 'V', 'E', ',', 'N', 'E', '-', 'U', 'P', 'E', 'R', 'V', 'E', 'C', 'H', 'N', 'E', 'L', 'E', 'R', 'N', 'N', 'G', 'three', 'relevant', 'classes', 'machine', 'learning', ':', 'supervised', 'learning', ',', 'unsupervised', 'learning', ',', 'semi-supervised', 'learning', '.']

 TOTAL FILTERED TOKENS ==>  52

 ---- POST FOR FILTERED TOKENS ----

 [('Supervised', 'VBN'), ('learning', 'VBG'), ('U', 'NNP'), ('P', 'NNP'), ('E', 'NNP'), ('R', 'NNP'), ('V', 'NNP'), ('E', 'NNP'), (',', ','), ('U', 'NNP'), ('N', 'NNP'), ('U', 'NNP'), ('P', 'NNP'), ('E', 'NNP'), ('R', 'NNP'), ('V', 'NNP'), ('E', 'NNP'), (',', ','), ('N', 'NNP'), ('E', 'NNP'), ('-', ':'), ('U', 'NNP'), ('P', 'NNP'), ('E', 'NNP'), ('R', 'NNP'), ('V', 'NNP'), ('E', 'NNP'), ('C', 'NNP'), ('H', 'NNP'), ('N', 'NNP'), ('E', 'NNP'), ('L', 'NNP'), ('E', 'NNP'), ('R', 'NNP'), ('N', 'NNP'), ('N', 'NNP'), ('G', 'NNP'), ('three', 'CD'), ('relevant', 'JJ'), ('classes', 'NNS'), ('machine', 'NN'), ('learning', 'NN'), (':', ':'), ('supervised', 'VBN'), ('learning', 'NN'), (',', ','), ('unsupervised', 'JJ'), ('learning', 'NN'), (',', ','), ('semi-supervised', 'JJ'), ('learning', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Supervised learning', 'learning U', 'U P', 'P E', 'E R', 'R V', 'V E', 'E ,', ', U', 'U N', 'N U', 'U P', 'P E', 'E R', 'R V', 'V E', 'E ,', ', N', 'N E', 'E -', '- U', 'U P', 'P E', 'E R', 'R V', 'V E', 'E C', 'C H', 'H N', 'N E', 'E L', 'L E', 'E R', 'R N', 'N N', 'N G', 'G three', 'three relevant', 'relevant classes', 'classes machine', 'machine learning', 'learning :', ': supervised', 'supervised learning', 'learning ,', ', unsupervised', 'unsupervised learning', 'learning ,', ', semi-supervised', 'semi-supervised learning', 'learning .'] 

 TOTAL BIGRAMS --> 51 



 ---- TRI-GRAMS ---- 

 ['Supervised learning U', 'learning U P', 'U P E', 'P E R', 'E R V', 'R V E', 'V E ,', 'E , U', ', U N', 'U N U', 'N U P', 'U P E', 'P E R', 'E R V', 'R V E', 'V E ,', 'E , N', ', N E', 'N E -', 'E - U', '- U P', 'U P E', 'P E R', 'E R V', 'R V E', 'V E C', 'E C H', 'C H N', 'H N E', 'N E L', 'E L E', 'L E R', 'E R N', 'R N N', 'N N G', 'N G three', 'G three relevant', 'three relevant classes', 'relevant classes machine', 'classes machine learning', 'machine learning :', 'learning : supervised', ': supervised learning', 'supervised learning ,', 'learning , unsupervised', ', unsupervised learning', 'unsupervised learning ,', 'learning , semi-supervised', ', semi-supervised learning', 'semi-supervised learning .'] 

 TOTAL TRIGRAMS --> 50 



 ---- NOUN PHRASES ---- 

 ['machine', 'learning', 'learning', 'unsupervised learning', 'semi-supervised learning'] 

 TOTAL NOUN PHRASES --> 5 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> ['U P', 'U N', 'N E', 'U P']
 TOTAL PERSON ENTITY --> 4 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['supervis', 'learn', 'u', 'p', 'e', 'r', 'v', 'e', ',', 'u', 'n', 'u', 'p', 'e', 'r', 'v', 'e', ',', 'n', 'e', '-', 'u', 'p', 'e', 'r', 'v', 'e', 'c', 'h', 'n', 'e', 'l', 'e', 'r', 'n', 'n', 'g', 'three', 'relev', 'class', 'machin', 'learn', ':', 'supervis', 'learn', ',', 'unsupervis', 'learn', ',', 'semi-supervis', 'learn', '.']

 TOTAL PORTER STEM WORDS ==> 52



 ---- SNOWBALL STEMMING ----

['supervis', 'learn', 'u', 'p', 'e', 'r', 'v', 'e', ',', 'u', 'n', 'u', 'p', 'e', 'r', 'v', 'e', ',', 'n', 'e', '-', 'u', 'p', 'e', 'r', 'v', 'e', 'c', 'h', 'n', 'e', 'l', 'e', 'r', 'n', 'n', 'g', 'three', 'relev', 'class', 'machin', 'learn', ':', 'supervis', 'learn', ',', 'unsupervis', 'learn', ',', 'semi-supervis', 'learn', '.']

 TOTAL SNOWBALL STEM WORDS ==> 52



 ---- LEMMATIZATION ----

['Supervised', 'learning', 'U', 'P', 'E', 'R', 'V', 'E', ',', 'U', 'N', 'U', 'P', 'E', 'R', 'V', 'E', ',', 'N', 'E', '-', 'U', 'P', 'E', 'R', 'V', 'E', 'C', 'H', 'N', 'E', 'L', 'E', 'R', 'N', 'N', 'G', 'three', 'relevant', 'class', 'machine', 'learning', ':', 'supervised', 'learning', ',', 'unsupervised', 'learning', ',', 'semi-supervised', 'learning', '.']

 TOTAL LEMMATIZE WORDS ==> 52

************************************************************************************************************************

30 --> Lexalytics uses all  three depending on the problem we’re trying to solve. 


 ---- TOKENS ----

 ['Lexalytics', 'uses', 'all', 'three', 'depending', 'on', 'the', 'problem', 'we', '’', 're', 'trying', 'to', 'solve', '.'] 

 TOTAL TOKENS ==> 15

 ---- POST ----

 [('Lexalytics', 'NNS'), ('uses', 'VBZ'), ('all', 'DT'), ('three', 'CD'), ('depending', 'VBG'), ('on', 'IN'), ('the', 'DT'), ('problem', 'NN'), ('we', 'PRP'), ('’', 'VBP'), ('re', 'VB'), ('trying', 'VBG'), ('to', 'TO'), ('solve', 'VB'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Lexalytics', 'uses', 'three', 'depending', 'problem', '’', 'trying', 'solve', '.']

 TOTAL FILTERED TOKENS ==>  9

 ---- POST FOR FILTERED TOKENS ----

 [('Lexalytics', 'NNS'), ('uses', 'NNS'), ('three', 'CD'), ('depending', 'VBG'), ('problem', 'NN'), ('’', 'NNP'), ('trying', 'VBG'), ('solve', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Lexalytics uses', 'uses three', 'three depending', 'depending problem', 'problem ’', '’ trying', 'trying solve', 'solve .'] 

 TOTAL BIGRAMS --> 8 



 ---- TRI-GRAMS ---- 

 ['Lexalytics uses three', 'uses three depending', 'three depending problem', 'depending problem ’', 'problem ’ trying', '’ trying solve', 'trying solve .'] 

 TOTAL TRIGRAMS --> 7 



 ---- NOUN PHRASES ---- 

 ['problem', 'solve'] 

 TOTAL NOUN PHRASES --> 2 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['lexalyt', 'use', 'three', 'depend', 'problem', '’', 'tri', 'solv', '.']

 TOTAL PORTER STEM WORDS ==> 9



 ---- SNOWBALL STEMMING ----

['lexalyt', 'use', 'three', 'depend', 'problem', '’', 'tri', 'solv', '.']

 TOTAL SNOWBALL STEM WORDS ==> 9



 ---- LEMMATIZATION ----

['Lexalytics', 'us', 'three', 'depending', 'problem', '’', 'trying', 'solve', '.']

 TOTAL LEMMATIZE WORDS ==> 9

************************************************************************************************************************

31 --> Supervised learning  Supervised learning means feeding a machine learning model a dataset   that has been annotated in some way. 


 ---- TOKENS ----

 ['Supervised', 'learning', 'Supervised', 'learning', 'means', 'feeding', 'a', 'machine', 'learning', 'model', 'a', 'dataset', 'that', 'has', 'been', 'annotated', 'in', 'some', 'way', '.'] 

 TOTAL TOKENS ==> 20

 ---- POST ----

 [('Supervised', 'VBN'), ('learning', 'NN'), ('Supervised', 'VBD'), ('learning', 'NN'), ('means', 'NNS'), ('feeding', 'VBG'), ('a', 'DT'), ('machine', 'NN'), ('learning', 'VBG'), ('model', 'NN'), ('a', 'DT'), ('dataset', 'NN'), ('that', 'WDT'), ('has', 'VBZ'), ('been', 'VBN'), ('annotated', 'VBN'), ('in', 'IN'), ('some', 'DT'), ('way', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Supervised', 'learning', 'Supervised', 'learning', 'means', 'feeding', 'machine', 'learning', 'model', 'dataset', 'annotated', 'way', '.']

 TOTAL FILTERED TOKENS ==>  13

 ---- POST FOR FILTERED TOKENS ----

 [('Supervised', 'VBN'), ('learning', 'NN'), ('Supervised', 'VBD'), ('learning', 'NN'), ('means', 'NNS'), ('feeding', 'VBG'), ('machine', 'NN'), ('learning', 'VBG'), ('model', 'NN'), ('dataset', 'NN'), ('annotated', 'VBD'), ('way', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Supervised learning', 'learning Supervised', 'Supervised learning', 'learning means', 'means feeding', 'feeding machine', 'machine learning', 'learning model', 'model dataset', 'dataset annotated', 'annotated way', 'way .'] 

 TOTAL BIGRAMS --> 12 



 ---- TRI-GRAMS ---- 

 ['Supervised learning Supervised', 'learning Supervised learning', 'Supervised learning means', 'learning means feeding', 'means feeding machine', 'feeding machine learning', 'machine learning model', 'learning model dataset', 'model dataset annotated', 'dataset annotated way', 'annotated way .'] 

 TOTAL TRIGRAMS --> 11 



 ---- NOUN PHRASES ---- 

 ['learning', 'learning', 'machine', 'model', 'dataset', 'way'] 

 TOTAL NOUN PHRASES --> 6 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['supervis', 'learn', 'supervis', 'learn', 'mean', 'feed', 'machin', 'learn', 'model', 'dataset', 'annot', 'way', '.']

 TOTAL PORTER STEM WORDS ==> 13



 ---- SNOWBALL STEMMING ----

['supervis', 'learn', 'supervis', 'learn', 'mean', 'feed', 'machin', 'learn', 'model', 'dataset', 'annot', 'way', '.']

 TOTAL SNOWBALL STEM WORDS ==> 13



 ---- LEMMATIZATION ----

['Supervised', 'learning', 'Supervised', 'learning', 'mean', 'feeding', 'machine', 'learning', 'model', 'dataset', 'annotated', 'way', '.']

 TOTAL LEMMATIZE WORDS ==> 13

************************************************************************************************************************

32 --> For example, we might collect 10,000  customer support comments and mark them up based on which are  related to software and which are related to hardware. 


 ---- TOKENS ----

 ['For', 'example', ',', 'we', 'might', 'collect', '10,000', 'customer', 'support', 'comments', 'and', 'mark', 'them', 'up', 'based', 'on', 'which', 'are', 'related', 'to', 'software', 'and', 'which', 'are', 'related', 'to', 'hardware', '.'] 

 TOTAL TOKENS ==> 28

 ---- POST ----

 [('For', 'IN'), ('example', 'NN'), (',', ','), ('we', 'PRP'), ('might', 'MD'), ('collect', 'VB'), ('10,000', 'CD'), ('customer', 'NN'), ('support', 'NN'), ('comments', 'NNS'), ('and', 'CC'), ('mark', 'VB'), ('them', 'PRP'), ('up', 'RP'), ('based', 'VBN'), ('on', 'IN'), ('which', 'WDT'), ('are', 'VBP'), ('related', 'VBN'), ('to', 'TO'), ('software', 'NN'), ('and', 'CC'), ('which', 'WDT'), ('are', 'VBP'), ('related', 'VBN'), ('to', 'TO'), ('hardware', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['example', ',', 'might', 'collect', '10,000', 'customer', 'support', 'comments', 'mark', 'based', 'related', 'software', 'related', 'hardware', '.']

 TOTAL FILTERED TOKENS ==>  15

 ---- POST FOR FILTERED TOKENS ----

 [('example', 'NN'), (',', ','), ('might', 'MD'), ('collect', 'VB'), ('10,000', 'CD'), ('customer', 'NN'), ('support', 'NN'), ('comments', 'NNS'), ('mark', 'NN'), ('based', 'VBN'), ('related', 'JJ'), ('software', 'NN'), ('related', 'VBN'), ('hardware', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['example ,', ', might', 'might collect', 'collect 10,000', '10,000 customer', 'customer support', 'support comments', 'comments mark', 'mark based', 'based related', 'related software', 'software related', 'related hardware', 'hardware .'] 

 TOTAL BIGRAMS --> 14 



 ---- TRI-GRAMS ---- 

 ['example , might', ', might collect', 'might collect 10,000', 'collect 10,000 customer', '10,000 customer support', 'customer support comments', 'support comments mark', 'comments mark based', 'mark based related', 'based related software', 'related software related', 'software related hardware', 'related hardware .'] 

 TOTAL TRIGRAMS --> 13 



 ---- NOUN PHRASES ---- 

 ['example', 'customer', 'support', 'mark', 'related software', 'hardware'] 

 TOTAL NOUN PHRASES --> 6 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['exampl', ',', 'might', 'collect', '10,000', 'custom', 'support', 'comment', 'mark', 'base', 'relat', 'softwar', 'relat', 'hardwar', '.']

 TOTAL PORTER STEM WORDS ==> 15



 ---- SNOWBALL STEMMING ----

['exampl', ',', 'might', 'collect', '10,000', 'custom', 'support', 'comment', 'mark', 'base', 'relat', 'softwar', 'relat', 'hardwar', '.']

 TOTAL SNOWBALL STEM WORDS ==> 15



 ---- LEMMATIZATION ----

['example', ',', 'might', 'collect', '10,000', 'customer', 'support', 'comment', 'mark', 'based', 'related', 'software', 'related', 'hardware', '.']

 TOTAL LEMMATIZE WORDS ==> 15

************************************************************************************************************************

33 --> In doing so, we’re  showing the machine what information it needs to evaluate each comment. 


 ---- TOKENS ----

 ['In', 'doing', 'so', ',', 'we', '’', 're', 'showing', 'the', 'machine', 'what', 'information', 'it', 'needs', 'to', 'evaluate', 'each', 'comment', '.'] 

 TOTAL TOKENS ==> 19

 ---- POST ----

 [('In', 'IN'), ('doing', 'VBG'), ('so', 'RB'), (',', ','), ('we', 'PRP'), ('’', 'VBP'), ('re', 'JJ'), ('showing', 'VBG'), ('the', 'DT'), ('machine', 'NN'), ('what', 'WP'), ('information', 'NN'), ('it', 'PRP'), ('needs', 'VBZ'), ('to', 'TO'), ('evaluate', 'VB'), ('each', 'DT'), ('comment', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 [',', '’', 'showing', 'machine', 'information', 'needs', 'evaluate', 'comment', '.']

 TOTAL FILTERED TOKENS ==>  9

 ---- POST FOR FILTERED TOKENS ----

 [(',', ','), ('’', 'JJ'), ('showing', 'VBG'), ('machine', 'NN'), ('information', 'NN'), ('needs', 'VBZ'), ('evaluate', 'JJ'), ('comment', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 [', ’', '’ showing', 'showing machine', 'machine information', 'information needs', 'needs evaluate', 'evaluate comment', 'comment .'] 

 TOTAL BIGRAMS --> 8 



 ---- TRI-GRAMS ---- 

 [', ’ showing', '’ showing machine', 'showing machine information', 'machine information needs', 'information needs evaluate', 'needs evaluate comment', 'evaluate comment .'] 

 TOTAL TRIGRAMS --> 7 



 ---- NOUN PHRASES ---- 

 ['machine', 'information', 'evaluate comment'] 

 TOTAL NOUN PHRASES --> 3 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

[',', '’', 'show', 'machin', 'inform', 'need', 'evalu', 'comment', '.']

 TOTAL PORTER STEM WORDS ==> 9



 ---- SNOWBALL STEMMING ----

[',', '’', 'show', 'machin', 'inform', 'need', 'evalu', 'comment', '.']

 TOTAL SNOWBALL STEM WORDS ==> 9



 ---- LEMMATIZATION ----

[',', '’', 'showing', 'machine', 'information', 'need', 'evaluate', 'comment', '.']

 TOTAL LEMMATIZE WORDS ==> 9

************************************************************************************************************************

34 --> This is the most direct way of teaching a model what you want it to do. 


 ---- TOKENS ----

 ['This', 'is', 'the', 'most', 'direct', 'way', 'of', 'teaching', 'a', 'model', 'what', 'you', 'want', 'it', 'to', 'do', '.'] 

 TOTAL TOKENS ==> 17

 ---- POST ----

 [('This', 'DT'), ('is', 'VBZ'), ('the', 'DT'), ('most', 'RBS'), ('direct', 'JJ'), ('way', 'NN'), ('of', 'IN'), ('teaching', 'VBG'), ('a', 'DT'), ('model', 'NN'), ('what', 'WP'), ('you', 'PRP'), ('want', 'VBP'), ('it', 'PRP'), ('to', 'TO'), ('do', 'VB'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['direct', 'way', 'teaching', 'model', 'want', '.']

 TOTAL FILTERED TOKENS ==>  6

 ---- POST FOR FILTERED TOKENS ----

 [('direct', 'JJ'), ('way', 'NN'), ('teaching', 'VBG'), ('model', 'NN'), ('want', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['direct way', 'way teaching', 'teaching model', 'model want', 'want .'] 

 TOTAL BIGRAMS --> 5 



 ---- TRI-GRAMS ---- 

 ['direct way teaching', 'way teaching model', 'teaching model want', 'model want .'] 

 TOTAL TRIGRAMS --> 4 



 ---- NOUN PHRASES ---- 

 ['direct way', 'model', 'want'] 

 TOTAL NOUN PHRASES --> 3 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['direct', 'way', 'teach', 'model', 'want', '.']

 TOTAL PORTER STEM WORDS ==> 6



 ---- SNOWBALL STEMMING ----

['direct', 'way', 'teach', 'model', 'want', '.']

 TOTAL SNOWBALL STEM WORDS ==> 6



 ---- LEMMATIZATION ----

['direct', 'way', 'teaching', 'model', 'want', '.']

 TOTAL LEMMATIZE WORDS ==> 6

************************************************************************************************************************

35 --> It’s  also the most work. 


 ---- TOKENS ----

 ['It', '’', 's', 'also', 'the', 'most', 'work', '.'] 

 TOTAL TOKENS ==> 8

 ---- POST ----

 [('It', 'PRP'), ('’', 'VBZ'), ('s', 'NN'), ('also', 'RB'), ('the', 'DT'), ('most', 'RBS'), ('work', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['’', 'also', 'work', '.']

 TOTAL FILTERED TOKENS ==>  4

 ---- POST FOR FILTERED TOKENS ----

 [('’', 'NN'), ('also', 'RB'), ('work', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['’ also', 'also work', 'work .'] 

 TOTAL BIGRAMS --> 3 



 ---- TRI-GRAMS ---- 

 ['’ also work', 'also work .'] 

 TOTAL TRIGRAMS --> 2 



 ---- NOUN PHRASES ---- 

 ['’', 'work'] 

 TOTAL NOUN PHRASES --> 2 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['’', 'also', 'work', '.']

 TOTAL PORTER STEM WORDS ==> 4



 ---- SNOWBALL STEMMING ----

['’', 'also', 'work', '.']

 TOTAL SNOWBALL STEM WORDS ==> 4



 ---- LEMMATIZATION ----

['’', 'also', 'work', '.']

 TOTAL LEMMATIZE WORDS ==> 4

************************************************************************************************************************

36 --> At Lexalytics, we use supervised learning for NLP tasks  like sentiment analysis and for certain methods of categorization. 


 ---- TOKENS ----

 ['At', 'Lexalytics', ',', 'we', 'use', 'supervised', 'learning', 'for', 'NLP', 'tasks', 'like', 'sentiment', 'analysis', 'and', 'for', 'certain', 'methods', 'of', 'categorization', '.'] 

 TOTAL TOKENS ==> 20

 ---- POST ----

 [('At', 'IN'), ('Lexalytics', 'NNP'), (',', ','), ('we', 'PRP'), ('use', 'VBP'), ('supervised', 'JJ'), ('learning', 'NN'), ('for', 'IN'), ('NLP', 'NNP'), ('tasks', 'NNS'), ('like', 'IN'), ('sentiment', 'NN'), ('analysis', 'NN'), ('and', 'CC'), ('for', 'IN'), ('certain', 'JJ'), ('methods', 'NNS'), ('of', 'IN'), ('categorization', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Lexalytics', ',', 'use', 'supervised', 'learning', 'NLP', 'tasks', 'like', 'sentiment', 'analysis', 'certain', 'methods', 'categorization', '.']

 TOTAL FILTERED TOKENS ==>  14

 ---- POST FOR FILTERED TOKENS ----

 [('Lexalytics', 'NNS'), (',', ','), ('use', 'NN'), ('supervised', 'VBD'), ('learning', 'VBG'), ('NLP', 'NNP'), ('tasks', 'NNS'), ('like', 'IN'), ('sentiment', 'NN'), ('analysis', 'NN'), ('certain', 'JJ'), ('methods', 'NNS'), ('categorization', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Lexalytics ,', ', use', 'use supervised', 'supervised learning', 'learning NLP', 'NLP tasks', 'tasks like', 'like sentiment', 'sentiment analysis', 'analysis certain', 'certain methods', 'methods categorization', 'categorization .'] 

 TOTAL BIGRAMS --> 13 



 ---- TRI-GRAMS ---- 

 ['Lexalytics , use', ', use supervised', 'use supervised learning', 'supervised learning NLP', 'learning NLP tasks', 'NLP tasks like', 'tasks like sentiment', 'like sentiment analysis', 'sentiment analysis certain', 'analysis certain methods', 'certain methods categorization', 'methods categorization .'] 

 TOTAL TRIGRAMS --> 12 



 ---- NOUN PHRASES ---- 

 ['use', 'sentiment', 'analysis', 'categorization'] 

 TOTAL NOUN PHRASES --> 4 



 ---- NER ----

 
 ORGANIZATION ---> ['NLP']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['lexalyt', ',', 'use', 'supervis', 'learn', 'nlp', 'task', 'like', 'sentiment', 'analysi', 'certain', 'method', 'categor', '.']

 TOTAL PORTER STEM WORDS ==> 14



 ---- SNOWBALL STEMMING ----

['lexalyt', ',', 'use', 'supervis', 'learn', 'nlp', 'task', 'like', 'sentiment', 'analysi', 'certain', 'method', 'categor', '.']

 TOTAL SNOWBALL STEM WORDS ==> 14



 ---- LEMMATIZATION ----

['Lexalytics', ',', 'use', 'supervised', 'learning', 'NLP', 'task', 'like', 'sentiment', 'analysis', 'certain', 'method', 'categorization', '.']

 TOTAL LEMMATIZE WORDS ==> 14

************************************************************************************************************************

37 --> For example, we train sentiment analysis models on hand-scored examples  because the perspective of the sentiment analysis can change based on  context. 


 ---- TOKENS ----

 ['For', 'example', ',', 'we', 'train', 'sentiment', 'analysis', 'models', 'on', 'hand-scored', 'examples', 'because', 'the', 'perspective', 'of', 'the', 'sentiment', 'analysis', 'can', 'change', 'based', 'on', 'context', '.'] 

 TOTAL TOKENS ==> 24

 ---- POST ----

 [('For', 'IN'), ('example', 'NN'), (',', ','), ('we', 'PRP'), ('train', 'VBP'), ('sentiment', 'JJ'), ('analysis', 'NN'), ('models', 'NNS'), ('on', 'IN'), ('hand-scored', 'JJ'), ('examples', 'NNS'), ('because', 'IN'), ('the', 'DT'), ('perspective', 'NN'), ('of', 'IN'), ('the', 'DT'), ('sentiment', 'NN'), ('analysis', 'NN'), ('can', 'MD'), ('change', 'VB'), ('based', 'VBN'), ('on', 'IN'), ('context', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['example', ',', 'train', 'sentiment', 'analysis', 'models', 'hand-scored', 'examples', 'perspective', 'sentiment', 'analysis', 'change', 'based', 'context', '.']

 TOTAL FILTERED TOKENS ==>  15

 ---- POST FOR FILTERED TOKENS ----

 [('example', 'NN'), (',', ','), ('train', 'VB'), ('sentiment', 'JJ'), ('analysis', 'NN'), ('models', 'NNS'), ('hand-scored', 'JJ'), ('examples', 'NNS'), ('perspective', 'JJ'), ('sentiment', 'NN'), ('analysis', 'NN'), ('change', 'NN'), ('based', 'VBN'), ('context', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['example ,', ', train', 'train sentiment', 'sentiment analysis', 'analysis models', 'models hand-scored', 'hand-scored examples', 'examples perspective', 'perspective sentiment', 'sentiment analysis', 'analysis change', 'change based', 'based context', 'context .'] 

 TOTAL BIGRAMS --> 14 



 ---- TRI-GRAMS ---- 

 ['example , train', ', train sentiment', 'train sentiment analysis', 'sentiment analysis models', 'analysis models hand-scored', 'models hand-scored examples', 'hand-scored examples perspective', 'examples perspective sentiment', 'perspective sentiment analysis', 'sentiment analysis change', 'analysis change based', 'change based context', 'based context .'] 

 TOTAL TRIGRAMS --> 13 



 ---- NOUN PHRASES ---- 

 ['example', 'sentiment analysis', 'perspective sentiment', 'analysis', 'change', 'context'] 

 TOTAL NOUN PHRASES --> 6 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['exampl', ',', 'train', 'sentiment', 'analysi', 'model', 'hand-scor', 'exampl', 'perspect', 'sentiment', 'analysi', 'chang', 'base', 'context', '.']

 TOTAL PORTER STEM WORDS ==> 15



 ---- SNOWBALL STEMMING ----

['exampl', ',', 'train', 'sentiment', 'analysi', 'model', 'hand-scor', 'exampl', 'perspect', 'sentiment', 'analysi', 'chang', 'base', 'context', '.']

 TOTAL SNOWBALL STEM WORDS ==> 15



 ---- LEMMATIZATION ----

['example', ',', 'train', 'sentiment', 'analysis', 'model', 'hand-scored', 'example', 'perspective', 'sentiment', 'analysis', 'change', 'based', 'context', '.']

 TOTAL LEMMATIZE WORDS ==> 15

************************************************************************************************************************

38 --> Consider the following: “SuperBank lost US$100,000,000 last month.” Well, were they expected to lose US$200,000,000? 


 ---- TOKENS ----

 ['Consider', 'the', 'following', ':', '“', 'SuperBank', 'lost', 'US', '$', '100,000,000', 'last', 'month.', '”', 'Well', ',', 'were', 'they', 'expected', 'to', 'lose', 'US', '$', '200,000,000', '?'] 

 TOTAL TOKENS ==> 24

 ---- POST ----

 [('Consider', 'VB'), ('the', 'DT'), ('following', 'NN'), (':', ':'), ('“', 'JJ'), ('SuperBank', 'NNP'), ('lost', 'VBD'), ('US', 'NNP'), ('$', '$'), ('100,000,000', 'CD'), ('last', 'JJ'), ('month.', 'NN'), ('”', 'NNP'), ('Well', 'NNP'), (',', ','), ('were', 'VBD'), ('they', 'PRP'), ('expected', 'VBD'), ('to', 'TO'), ('lose', 'VB'), ('US', 'NNP'), ('$', '$'), ('200,000,000', 'CD'), ('?', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Consider', 'following', ':', '“', 'SuperBank', 'lost', 'US', '$', '100,000,000', 'last', 'month.', '”', 'Well', ',', 'expected', 'lose', 'US', '$', '200,000,000', '?']

 TOTAL FILTERED TOKENS ==>  20

 ---- POST FOR FILTERED TOKENS ----

 [('Consider', 'VB'), ('following', 'VBG'), (':', ':'), ('“', 'JJ'), ('SuperBank', 'NNP'), ('lost', 'VBD'), ('US', 'NNP'), ('$', '$'), ('100,000,000', 'CD'), ('last', 'JJ'), ('month.', 'NN'), ('”', 'NNP'), ('Well', 'NNP'), (',', ','), ('expected', 'VBN'), ('lose', 'VBP'), ('US', 'NNP'), ('$', '$'), ('200,000,000', 'CD'), ('?', '.')] 



 ---- BI-GRAMS ---- 

 ['Consider following', 'following :', ': “', '“ SuperBank', 'SuperBank lost', 'lost US', 'US $', '$ 100,000,000', '100,000,000 last', 'last month.', 'month. ”', '” Well', 'Well ,', ', expected', 'expected lose', 'lose US', 'US $', '$ 200,000,000', '200,000,000 ?'] 

 TOTAL BIGRAMS --> 19 



 ---- TRI-GRAMS ---- 

 ['Consider following :', 'following : “', ': “ SuperBank', '“ SuperBank lost', 'SuperBank lost US', 'lost US $', 'US $ 100,000,000', '$ 100,000,000 last', '100,000,000 last month.', 'last month. ”', 'month. ” Well', '” Well ,', 'Well , expected', ', expected lose', 'expected lose US', 'lose US $', 'US $ 200,000,000', '$ 200,000,000 ?'] 

 TOTAL TRIGRAMS --> 18 



 ---- NOUN PHRASES ---- 

 ['last month.'] 

 TOTAL NOUN PHRASES --> 1 



 ---- NER ----

 
 ORGANIZATION ---> ['SuperBank', 'US', 'US']
 TOTAL ORGANIZATION ENTITY --> 3 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['consid', 'follow', ':', '“', 'superbank', 'lost', 'us', '$', '100,000,000', 'last', 'month.', '”', 'well', ',', 'expect', 'lose', 'us', '$', '200,000,000', '?']

 TOTAL PORTER STEM WORDS ==> 20



 ---- SNOWBALL STEMMING ----

['consid', 'follow', ':', '“', 'superbank', 'lost', 'us', '$', '100,000,000', 'last', 'month.', '”', 'well', ',', 'expect', 'lose', 'us', '$', '200,000,000', '?']

 TOTAL SNOWBALL STEM WORDS ==> 20



 ---- LEMMATIZATION ----

['Consider', 'following', ':', '“', 'SuperBank', 'lost', 'US', '$', '100,000,000', 'last', 'month.', '”', 'Well', ',', 'expected', 'lose', 'US', '$', '200,000,000', '?']

 TOTAL LEMMATIZE WORDS ==> 20

************************************************************************************************************************

39 --> US$50,000,000? 


 ---- TOKENS ----

 ['US', '$', '50,000,000', '?'] 

 TOTAL TOKENS ==> 4

 ---- POST ----

 [('US', 'NNP'), ('$', '$'), ('50,000,000', 'CD'), ('?', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['US', '$', '50,000,000', '?']

 TOTAL FILTERED TOKENS ==>  4

 ---- POST FOR FILTERED TOKENS ----

 [('US', 'NNP'), ('$', '$'), ('50,000,000', 'CD'), ('?', '.')] 



 ---- BI-GRAMS ---- 

 ['US $', '$ 50,000,000', '50,000,000 ?'] 

 TOTAL BIGRAMS --> 3 



 ---- TRI-GRAMS ---- 

 ['US $ 50,000,000', '$ 50,000,000 ?'] 

 TOTAL TRIGRAMS --> 2 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['us', '$', '50,000,000', '?']

 TOTAL PORTER STEM WORDS ==> 4



 ---- SNOWBALL STEMMING ----

['us', '$', '50,000,000', '?']

 TOTAL SNOWBALL STEM WORDS ==> 4



 ---- LEMMATIZATION ----

['US', '$', '50,000,000', '?']

 TOTAL LEMMATIZE WORDS ==> 4

************************************************************************************************************************

40 --> The  sentiment of this statement very much depends on who is looking at it. 


 ---- TOKENS ----

 ['The', 'sentiment', 'of', 'this', 'statement', 'very', 'much', 'depends', 'on', 'who', 'is', 'looking', 'at', 'it', '.'] 

 TOTAL TOKENS ==> 15

 ---- POST ----

 [('The', 'DT'), ('sentiment', 'NN'), ('of', 'IN'), ('this', 'DT'), ('statement', 'NN'), ('very', 'RB'), ('much', 'JJ'), ('depends', 'NNS'), ('on', 'IN'), ('who', 'WP'), ('is', 'VBZ'), ('looking', 'VBG'), ('at', 'IN'), ('it', 'PRP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['sentiment', 'statement', 'much', 'depends', 'looking', '.']

 TOTAL FILTERED TOKENS ==>  6

 ---- POST FOR FILTERED TOKENS ----

 [('sentiment', 'NN'), ('statement', 'NN'), ('much', 'JJ'), ('depends', 'VBZ'), ('looking', 'VBG'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['sentiment statement', 'statement much', 'much depends', 'depends looking', 'looking .'] 

 TOTAL BIGRAMS --> 5 



 ---- TRI-GRAMS ---- 

 ['sentiment statement much', 'statement much depends', 'much depends looking', 'depends looking .'] 

 TOTAL TRIGRAMS --> 4 



 ---- NOUN PHRASES ---- 

 ['sentiment', 'statement'] 

 TOTAL NOUN PHRASES --> 2 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['sentiment', 'statement', 'much', 'depend', 'look', '.']

 TOTAL PORTER STEM WORDS ==> 6



 ---- SNOWBALL STEMMING ----

['sentiment', 'statement', 'much', 'depend', 'look', '.']

 TOTAL SNOWBALL STEM WORDS ==> 6



 ---- LEMMATIZATION ----

['sentiment', 'statement', 'much', 'depends', 'looking', '.']

 TOTAL LEMMATIZE WORDS ==> 6

************************************************************************************************************************

41 --> Another example would be “This perfume smells like my grandmother.”   Do you love your grandmother? 


 ---- TOKENS ----

 ['Another', 'example', 'would', 'be', '“', 'This', 'perfume', 'smells', 'like', 'my', 'grandmother.', '”', 'Do', 'you', 'love', 'your', 'grandmother', '?'] 

 TOTAL TOKENS ==> 18

 ---- POST ----

 [('Another', 'DT'), ('example', 'NN'), ('would', 'MD'), ('be', 'VB'), ('“', 'VBN'), ('This', 'DT'), ('perfume', 'NN'), ('smells', 'VBZ'), ('like', 'IN'), ('my', 'PRP$'), ('grandmother.', 'NN'), ('”', 'NN'), ('Do', 'NNP'), ('you', 'PRP'), ('love', 'VB'), ('your', 'PRP$'), ('grandmother', 'NN'), ('?', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Another', 'example', 'would', '“', 'perfume', 'smells', 'like', 'grandmother.', '”', 'love', 'grandmother', '?']

 TOTAL FILTERED TOKENS ==>  12

 ---- POST FOR FILTERED TOKENS ----

 [('Another', 'DT'), ('example', 'NN'), ('would', 'MD'), ('“', 'VB'), ('perfume', 'VB'), ('smells', 'NNS'), ('like', 'IN'), ('grandmother.', 'NN'), ('”', 'NNP'), ('love', 'VB'), ('grandmother', 'NN'), ('?', '.')] 



 ---- BI-GRAMS ---- 

 ['Another example', 'example would', 'would “', '“ perfume', 'perfume smells', 'smells like', 'like grandmother.', 'grandmother. ”', '” love', 'love grandmother', 'grandmother ?'] 

 TOTAL BIGRAMS --> 11 



 ---- TRI-GRAMS ---- 

 ['Another example would', 'example would “', 'would “ perfume', '“ perfume smells', 'perfume smells like', 'smells like grandmother.', 'like grandmother. ”', 'grandmother. ” love', '” love grandmother', 'love grandmother ?'] 

 TOTAL TRIGRAMS --> 10 



 ---- NOUN PHRASES ---- 

 ['Another example', 'grandmother.', 'grandmother'] 

 TOTAL NOUN PHRASES --> 3 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['anoth', 'exampl', 'would', '“', 'perfum', 'smell', 'like', 'grandmother.', '”', 'love', 'grandmoth', '?']

 TOTAL PORTER STEM WORDS ==> 12



 ---- SNOWBALL STEMMING ----

['anoth', 'exampl', 'would', '“', 'perfum', 'smell', 'like', 'grandmother.', '”', 'love', 'grandmoth', '?']

 TOTAL SNOWBALL STEM WORDS ==> 12



 ---- LEMMATIZATION ----

['Another', 'example', 'would', '“', 'perfume', 'smell', 'like', 'grandmother.', '”', 'love', 'grandmother', '?']

 TOTAL LEMMATIZE WORDS ==> 12

************************************************************************************************************************

42 --> Ultimately, any extraction that requires that the machine understand   your perspective needs to be supervised somehow, and this requires   lots of work. 


 ---- TOKENS ----

 ['Ultimately', ',', 'any', 'extraction', 'that', 'requires', 'that', 'the', 'machine', 'understand', 'your', 'perspective', 'needs', 'to', 'be', 'supervised', 'somehow', ',', 'and', 'this', 'requires', 'lots', 'of', 'work', '.'] 

 TOTAL TOKENS ==> 25

 ---- POST ----

 [('Ultimately', 'RB'), (',', ','), ('any', 'DT'), ('extraction', 'NN'), ('that', 'WDT'), ('requires', 'VBZ'), ('that', 'IN'), ('the', 'DT'), ('machine', 'NN'), ('understand', 'VB'), ('your', 'PRP$'), ('perspective', 'NN'), ('needs', 'NNS'), ('to', 'TO'), ('be', 'VB'), ('supervised', 'VBN'), ('somehow', 'RB'), (',', ','), ('and', 'CC'), ('this', 'DT'), ('requires', 'VBZ'), ('lots', 'NNS'), ('of', 'IN'), ('work', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Ultimately', ',', 'extraction', 'requires', 'machine', 'understand', 'perspective', 'needs', 'supervised', 'somehow', ',', 'requires', 'lots', 'work', '.']

 TOTAL FILTERED TOKENS ==>  15

 ---- POST FOR FILTERED TOKENS ----

 [('Ultimately', 'RB'), (',', ','), ('extraction', 'NN'), ('requires', 'VBZ'), ('machine', 'NN'), ('understand', 'JJ'), ('perspective', 'NN'), ('needs', 'NNS'), ('supervised', 'VBD'), ('somehow', 'RB'), (',', ','), ('requires', 'VBZ'), ('lots', 'JJ'), ('work', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Ultimately ,', ', extraction', 'extraction requires', 'requires machine', 'machine understand', 'understand perspective', 'perspective needs', 'needs supervised', 'supervised somehow', 'somehow ,', ', requires', 'requires lots', 'lots work', 'work .'] 

 TOTAL BIGRAMS --> 14 



 ---- TRI-GRAMS ---- 

 ['Ultimately , extraction', ', extraction requires', 'extraction requires machine', 'requires machine understand', 'machine understand perspective', 'understand perspective needs', 'perspective needs supervised', 'needs supervised somehow', 'supervised somehow ,', 'somehow , requires', ', requires lots', 'requires lots work', 'lots work .'] 

 TOTAL TRIGRAMS --> 13 



 ---- NOUN PHRASES ---- 

 ['extraction', 'machine', 'understand perspective', 'lots work'] 

 TOTAL NOUN PHRASES --> 4 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['ultim', ',', 'extract', 'requir', 'machin', 'understand', 'perspect', 'need', 'supervis', 'somehow', ',', 'requir', 'lot', 'work', '.']

 TOTAL PORTER STEM WORDS ==> 15



 ---- SNOWBALL STEMMING ----

['ultim', ',', 'extract', 'requir', 'machin', 'understand', 'perspect', 'need', 'supervis', 'somehow', ',', 'requir', 'lot', 'work', '.']

 TOTAL SNOWBALL STEM WORDS ==> 15



 ---- LEMMATIZATION ----

['Ultimately', ',', 'extraction', 'requires', 'machine', 'understand', 'perspective', 'need', 'supervised', 'somehow', ',', 'requires', 'lot', 'work', '.']

 TOTAL LEMMATIZE WORDS ==> 15

************************************************************************************************************************

43 --> https://www.lexalytics.com/ https://www.lexalytics.com/  W H I T E  P A P E R 6|       |   Lexalytics, Inc., 48 North Pleasant St. Unit 301, Amherst MA 01002 USA   |   1-800-377-8036 |   www.lexalytics.com Unsupervised learning Unsupervised learning is where we hand the machine a whole bunch  of content and tell it to find the patterns. 


 ---- TOKENS ----

 ['https', ':', '//www.lexalytics.com/', 'https', ':', '//www.lexalytics.com/', 'W', 'H', 'I', 'T', 'E', 'P', 'A', 'P', 'E', 'R', '6|', '|', 'Lexalytics', ',', 'Inc.', ',', '48', 'North', 'Pleasant', 'St.', 'Unit', '301', ',', 'Amherst', 'MA', '01002', 'USA', '|', '1-800-377-8036', '|', 'www.lexalytics.com', 'Unsupervised', 'learning', 'Unsupervised', 'learning', 'is', 'where', 'we', 'hand', 'the', 'machine', 'a', 'whole', 'bunch', 'of', 'content', 'and', 'tell', 'it', 'to', 'find', 'the', 'patterns', '.'] 

 TOTAL TOKENS ==> 60

 ---- POST ----

 [('https', 'NN'), (':', ':'), ('//www.lexalytics.com/', 'JJ'), ('https', 'NN'), (':', ':'), ('//www.lexalytics.com/', 'JJ'), ('W', 'NNP'), ('H', 'NNP'), ('I', 'PRP'), ('T', 'NNP'), ('E', 'NNP'), ('P', 'NNP'), ('A', 'NNP'), ('P', 'NNP'), ('E', 'NNP'), ('R', 'NNP'), ('6|', 'CD'), ('|', 'NNP'), ('Lexalytics', 'NNP'), (',', ','), ('Inc.', 'NNP'), (',', ','), ('48', 'CD'), ('North', 'NNP'), ('Pleasant', 'NNP'), ('St.', 'NNP'), ('Unit', 'NNP'), ('301', 'CD'), (',', ','), ('Amherst', 'NNP'), ('MA', 'NNP'), ('01002', 'CD'), ('USA', 'NNP'), ('|', 'NNP'), ('1-800-377-8036', 'JJ'), ('|', 'NNP'), ('www.lexalytics.com', 'NN'), ('Unsupervised', 'VBD'), ('learning', 'VBG'), ('Unsupervised', 'VBN'), ('learning', 'NN'), ('is', 'VBZ'), ('where', 'WRB'), ('we', 'PRP'), ('hand', 'NN'), ('the', 'DT'), ('machine', 'NN'), ('a', 'DT'), ('whole', 'JJ'), ('bunch', 'NN'), ('of', 'IN'), ('content', 'NN'), ('and', 'CC'), ('tell', 'VB'), ('it', 'PRP'), ('to', 'TO'), ('find', 'VB'), ('the', 'DT'), ('patterns', 'NNS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['https', ':', '//www.lexalytics.com/', 'https', ':', '//www.lexalytics.com/', 'W', 'H', 'E', 'P', 'P', 'E', 'R', '6|', '|', 'Lexalytics', ',', 'Inc.', ',', '48', 'North', 'Pleasant', 'St.', 'Unit', '301', ',', 'Amherst', '01002', 'USA', '|', '1-800-377-8036', '|', 'www.lexalytics.com', 'Unsupervised', 'learning', 'Unsupervised', 'learning', 'hand', 'machine', 'whole', 'bunch', 'content', 'tell', 'find', 'patterns', '.']

 TOTAL FILTERED TOKENS ==>  46

 ---- POST FOR FILTERED TOKENS ----

 [('https', 'NN'), (':', ':'), ('//www.lexalytics.com/', 'JJ'), ('https', 'NN'), (':', ':'), ('//www.lexalytics.com/', 'JJ'), ('W', 'NNP'), ('H', 'NNP'), ('E', 'NNP'), ('P', 'NNP'), ('P', 'NNP'), ('E', 'NNP'), ('R', 'NNP'), ('6|', 'CD'), ('|', 'NNP'), ('Lexalytics', 'NNP'), (',', ','), ('Inc.', 'NNP'), (',', ','), ('48', 'CD'), ('North', 'NNP'), ('Pleasant', 'NNP'), ('St.', 'NNP'), ('Unit', 'NNP'), ('301', 'CD'), (',', ','), ('Amherst', 'NNP'), ('01002', 'CD'), ('USA', 'NNP'), ('|', 'NNP'), ('1-800-377-8036', 'JJ'), ('|', 'NNP'), ('www.lexalytics.com', 'NN'), ('Unsupervised', 'VBD'), ('learning', 'VBG'), ('Unsupervised', 'VBN'), ('learning', 'JJ'), ('hand', 'NN'), ('machine', 'NN'), ('whole', 'JJ'), ('bunch', 'NN'), ('content', 'NN'), ('tell', 'NN'), ('find', 'VBP'), ('patterns', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['https :', ': //www.lexalytics.com/', '//www.lexalytics.com/ https', 'https :', ': //www.lexalytics.com/', '//www.lexalytics.com/ W', 'W H', 'H E', 'E P', 'P P', 'P E', 'E R', 'R 6|', '6| |', '| Lexalytics', 'Lexalytics ,', ', Inc.', 'Inc. ,', ', 48', '48 North', 'North Pleasant', 'Pleasant St.', 'St. Unit', 'Unit 301', '301 ,', ', Amherst', 'Amherst 01002', '01002 USA', 'USA |', '| 1-800-377-8036', '1-800-377-8036 |', '| www.lexalytics.com', 'www.lexalytics.com Unsupervised', 'Unsupervised learning', 'learning Unsupervised', 'Unsupervised learning', 'learning hand', 'hand machine', 'machine whole', 'whole bunch', 'bunch content', 'content tell', 'tell find', 'find patterns', 'patterns .'] 

 TOTAL BIGRAMS --> 45 



 ---- TRI-GRAMS ---- 

 ['https : //www.lexalytics.com/', ': //www.lexalytics.com/ https', '//www.lexalytics.com/ https :', 'https : //www.lexalytics.com/', ': //www.lexalytics.com/ W', '//www.lexalytics.com/ W H', 'W H E', 'H E P', 'E P P', 'P P E', 'P E R', 'E R 6|', 'R 6| |', '6| | Lexalytics', '| Lexalytics ,', 'Lexalytics , Inc.', ', Inc. ,', 'Inc. , 48', ', 48 North', '48 North Pleasant', 'North Pleasant St.', 'Pleasant St. Unit', 'St. Unit 301', 'Unit 301 ,', '301 , Amherst', ', Amherst 01002', 'Amherst 01002 USA', '01002 USA |', 'USA | 1-800-377-8036', '| 1-800-377-8036 |', '1-800-377-8036 | www.lexalytics.com', '| www.lexalytics.com Unsupervised', 'www.lexalytics.com Unsupervised learning', 'Unsupervised learning Unsupervised', 'learning Unsupervised learning', 'Unsupervised learning hand', 'learning hand machine', 'hand machine whole', 'machine whole bunch', 'whole bunch content', 'bunch content tell', 'content tell find', 'tell find patterns', 'find patterns .'] 

 TOTAL TRIGRAMS --> 44 



 ---- NOUN PHRASES ---- 

 ['https', ' https', 'www.lexalytics.com', 'learning hand', 'machine', 'whole bunch', 'content', 'tell'] 

 TOTAL NOUN PHRASES --> 8 



 ---- NER ----

 
 ORGANIZATION ---> ['USA']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> ['North']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['http', ':', '//www.lexalytics.com/', 'http', ':', '//www.lexalytics.com/', 'w', 'h', 'e', 'p', 'p', 'e', 'r', '6|', '|', 'lexalyt', ',', 'inc.', ',', '48', 'north', 'pleasant', 'st.', 'unit', '301', ',', 'amherst', '01002', 'usa', '|', '1-800-377-8036', '|', 'www.lexalytics.com', 'unsupervis', 'learn', 'unsupervis', 'learn', 'hand', 'machin', 'whole', 'bunch', 'content', 'tell', 'find', 'pattern', '.']

 TOTAL PORTER STEM WORDS ==> 46



 ---- SNOWBALL STEMMING ----

['https', ':', '//www.lexalytics.com/', 'https', ':', '//www.lexalytics.com/', 'w', 'h', 'e', 'p', 'p', 'e', 'r', '6|', '|', 'lexalyt', ',', 'inc.', ',', '48', 'north', 'pleasant', 'st.', 'unit', '301', ',', 'amherst', '01002', 'usa', '|', '1-800-377-8036', '|', 'www.lexalytics.com', 'unsupervis', 'learn', 'unsupervis', 'learn', 'hand', 'machin', 'whole', 'bunch', 'content', 'tell', 'find', 'pattern', '.']

 TOTAL SNOWBALL STEM WORDS ==> 46



 ---- LEMMATIZATION ----

['http', ':', '//www.lexalytics.com/', 'http', ':', '//www.lexalytics.com/', 'W', 'H', 'E', 'P', 'P', 'E', 'R', '6|', '|', 'Lexalytics', ',', 'Inc.', ',', '48', 'North', 'Pleasant', 'St.', 'Unit', '301', ',', 'Amherst', '01002', 'USA', '|', '1-800-377-8036', '|', 'www.lexalytics.com', 'Unsupervised', 'learning', 'Unsupervised', 'learning', 'hand', 'machine', 'whole', 'bunch', 'content', 'tell', 'find', 'pattern', '.']

 TOTAL LEMMATIZE WORDS ==> 46

************************************************************************************************************************

44 --> This is how we built the syntax parser in Salience: We took 40GB of text and had the parser analyze every  sentence to understand how subjects and verbs fit together. 


 ---- TOKENS ----

 ['This', 'is', 'how', 'we', 'built', 'the', 'syntax', 'parser', 'in', 'Salience', ':', 'We', 'took', '40GB', 'of', 'text', 'and', 'had', 'the', 'parser', 'analyze', 'every', 'sentence', 'to', 'understand', 'how', 'subjects', 'and', 'verbs', 'fit', 'together', '.'] 

 TOTAL TOKENS ==> 32

 ---- POST ----

 [('This', 'DT'), ('is', 'VBZ'), ('how', 'WRB'), ('we', 'PRP'), ('built', 'VBD'), ('the', 'DT'), ('syntax', 'NN'), ('parser', 'NN'), ('in', 'IN'), ('Salience', 'NN'), (':', ':'), ('We', 'PRP'), ('took', 'VBD'), ('40GB', 'CD'), ('of', 'IN'), ('text', 'NN'), ('and', 'CC'), ('had', 'VBD'), ('the', 'DT'), ('parser', 'NN'), ('analyze', 'NN'), ('every', 'DT'), ('sentence', 'NN'), ('to', 'TO'), ('understand', 'VB'), ('how', 'WRB'), ('subjects', 'NNS'), ('and', 'CC'), ('verbs', 'JJ'), ('fit', 'NN'), ('together', 'RB'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['built', 'syntax', 'parser', 'Salience', ':', 'took', '40GB', 'text', 'parser', 'analyze', 'every', 'sentence', 'understand', 'subjects', 'verbs', 'fit', 'together', '.']

 TOTAL FILTERED TOKENS ==>  18

 ---- POST FOR FILTERED TOKENS ----

 [('built', 'VBN'), ('syntax', 'JJ'), ('parser', 'NN'), ('Salience', 'NN'), (':', ':'), ('took', 'VBD'), ('40GB', 'CD'), ('text', 'NN'), ('parser', 'NN'), ('analyze', 'NN'), ('every', 'DT'), ('sentence', 'NN'), ('understand', 'JJ'), ('subjects', 'NNS'), ('verbs', 'VBP'), ('fit', 'JJ'), ('together', 'RB'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['built syntax', 'syntax parser', 'parser Salience', 'Salience :', ': took', 'took 40GB', '40GB text', 'text parser', 'parser analyze', 'analyze every', 'every sentence', 'sentence understand', 'understand subjects', 'subjects verbs', 'verbs fit', 'fit together', 'together .'] 

 TOTAL BIGRAMS --> 17 



 ---- TRI-GRAMS ---- 

 ['built syntax parser', 'syntax parser Salience', 'parser Salience :', 'Salience : took', ': took 40GB', 'took 40GB text', '40GB text parser', 'text parser analyze', 'parser analyze every', 'analyze every sentence', 'every sentence understand', 'sentence understand subjects', 'understand subjects verbs', 'subjects verbs fit', 'verbs fit together', 'fit together .'] 

 TOTAL TRIGRAMS --> 16 



 ---- NOUN PHRASES ---- 

 ['syntax parser', 'Salience', 'text', 'parser', 'analyze', 'every sentence'] 

 TOTAL NOUN PHRASES --> 6 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['built', 'syntax', 'parser', 'salienc', ':', 'took', '40gb', 'text', 'parser', 'analyz', 'everi', 'sentenc', 'understand', 'subject', 'verb', 'fit', 'togeth', '.']

 TOTAL PORTER STEM WORDS ==> 18



 ---- SNOWBALL STEMMING ----

['built', 'syntax', 'parser', 'salienc', ':', 'took', '40gb', 'text', 'parser', 'analyz', 'everi', 'sentenc', 'understand', 'subject', 'verb', 'fit', 'togeth', '.']

 TOTAL SNOWBALL STEM WORDS ==> 18



 ---- LEMMATIZATION ----

['built', 'syntax', 'parser', 'Salience', ':', 'took', '40GB', 'text', 'parser', 'analyze', 'every', 'sentence', 'understand', 'subject', 'verb', 'fit', 'together', '.']

 TOTAL LEMMATIZE WORDS ==> 18

************************************************************************************************************************

45 --> Consider   the following:  “I threw the ball over the mountain.” One way to understand syntax is to parse the entire sentence, like   you’re doing a sentence diagram from 6th grade. 


 ---- TOKENS ----

 ['Consider', 'the', 'following', ':', '“', 'I', 'threw', 'the', 'ball', 'over', 'the', 'mountain.', '”', 'One', 'way', 'to', 'understand', 'syntax', 'is', 'to', 'parse', 'the', 'entire', 'sentence', ',', 'like', 'you', '’', 're', 'doing', 'a', 'sentence', 'diagram', 'from', '6th', 'grade', '.'] 

 TOTAL TOKENS ==> 37

 ---- POST ----

 [('Consider', 'VB'), ('the', 'DT'), ('following', 'NN'), (':', ':'), ('“', 'NN'), ('I', 'PRP'), ('threw', 'VBD'), ('the', 'DT'), ('ball', 'NN'), ('over', 'IN'), ('the', 'DT'), ('mountain.', 'NN'), ('”', 'VBD'), ('One', 'CD'), ('way', 'NN'), ('to', 'TO'), ('understand', 'VB'), ('syntax', 'NN'), ('is', 'VBZ'), ('to', 'TO'), ('parse', 'VB'), ('the', 'DT'), ('entire', 'JJ'), ('sentence', 'NN'), (',', ','), ('like', 'IN'), ('you', 'PRP'), ('’', 'VBP'), ('re', 'VB'), ('doing', 'VBG'), ('a', 'DT'), ('sentence', 'NN'), ('diagram', 'NN'), ('from', 'IN'), ('6th', 'CD'), ('grade', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Consider', 'following', ':', '“', 'threw', 'ball', 'mountain.', '”', 'One', 'way', 'understand', 'syntax', 'parse', 'entire', 'sentence', ',', 'like', '’', 'sentence', 'diagram', '6th', 'grade', '.']

 TOTAL FILTERED TOKENS ==>  23

 ---- POST FOR FILTERED TOKENS ----

 [('Consider', 'VB'), ('following', 'VBG'), (':', ':'), ('“', 'JJ'), ('threw', 'VBD'), ('ball', 'JJ'), ('mountain.', 'NN'), ('”', 'VBD'), ('One', 'CD'), ('way', 'NN'), ('understand', 'JJ'), ('syntax', 'NN'), ('parse', 'NN'), ('entire', 'JJ'), ('sentence', 'NN'), (',', ','), ('like', 'IN'), ('’', 'JJ'), ('sentence', 'NN'), ('diagram', 'VBD'), ('6th', 'CD'), ('grade', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Consider following', 'following :', ': “', '“ threw', 'threw ball', 'ball mountain.', 'mountain. ”', '” One', 'One way', 'way understand', 'understand syntax', 'syntax parse', 'parse entire', 'entire sentence', 'sentence ,', ', like', 'like ’', '’ sentence', 'sentence diagram', 'diagram 6th', '6th grade', 'grade .'] 

 TOTAL BIGRAMS --> 22 



 ---- TRI-GRAMS ---- 

 ['Consider following :', 'following : “', ': “ threw', '“ threw ball', 'threw ball mountain.', 'ball mountain. ”', 'mountain. ” One', '” One way', 'One way understand', 'way understand syntax', 'understand syntax parse', 'syntax parse entire', 'parse entire sentence', 'entire sentence ,', 'sentence , like', ', like ’', 'like ’ sentence', '’ sentence diagram', 'sentence diagram 6th', 'diagram 6th grade', '6th grade .'] 

 TOTAL TRIGRAMS --> 21 



 ---- NOUN PHRASES ---- 

 ['ball mountain.', 'way', 'understand syntax', 'parse', 'entire sentence', '’ sentence', 'grade'] 

 TOTAL NOUN PHRASES --> 7 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['consid', 'follow', ':', '“', 'threw', 'ball', 'mountain.', '”', 'one', 'way', 'understand', 'syntax', 'pars', 'entir', 'sentenc', ',', 'like', '’', 'sentenc', 'diagram', '6th', 'grade', '.']

 TOTAL PORTER STEM WORDS ==> 23



 ---- SNOWBALL STEMMING ----

['consid', 'follow', ':', '“', 'threw', 'ball', 'mountain.', '”', 'one', 'way', 'understand', 'syntax', 'pars', 'entir', 'sentenc', ',', 'like', '’', 'sentenc', 'diagram', '6th', 'grade', '.']

 TOTAL SNOWBALL STEM WORDS ==> 23



 ---- LEMMATIZATION ----

['Consider', 'following', ':', '“', 'threw', 'ball', 'mountain.', '”', 'One', 'way', 'understand', 'syntax', 'parse', 'entire', 'sentence', ',', 'like', '’', 'sentence', 'diagram', '6th', 'grade', '.']

 TOTAL LEMMATIZE WORDS ==> 23

************************************************************************************************************************

46 --> Those are quite  computationally intensive (along with being irritating for 6th graders),   and so you can’t do that for high-volume content – it just takes too long  for each document to process. 


 ---- TOKENS ----

 ['Those', 'are', 'quite', 'computationally', 'intensive', '(', 'along', 'with', 'being', 'irritating', 'for', '6th', 'graders', ')', ',', 'and', 'so', 'you', 'can', '’', 't', 'do', 'that', 'for', 'high-volume', 'content', '–', 'it', 'just', 'takes', 'too', 'long', 'for', 'each', 'document', 'to', 'process', '.'] 

 TOTAL TOKENS ==> 38

 ---- POST ----

 [('Those', 'DT'), ('are', 'VBP'), ('quite', 'RB'), ('computationally', 'RB'), ('intensive', 'JJ'), ('(', '('), ('along', 'IN'), ('with', 'IN'), ('being', 'VBG'), ('irritating', 'VBG'), ('for', 'IN'), ('6th', 'CD'), ('graders', 'NNS'), (')', ')'), (',', ','), ('and', 'CC'), ('so', 'RB'), ('you', 'PRP'), ('can', 'MD'), ('’', 'VB'), ('t', 'VB'), ('do', 'VBP'), ('that', 'DT'), ('for', 'IN'), ('high-volume', 'JJ'), ('content', 'NN'), ('–', 'VBZ'), ('it', 'PRP'), ('just', 'RB'), ('takes', 'VBZ'), ('too', 'RB'), ('long', 'RB'), ('for', 'IN'), ('each', 'DT'), ('document', 'NN'), ('to', 'TO'), ('process', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['quite', 'computationally', 'intensive', '(', 'along', 'irritating', '6th', 'graders', ')', ',', '’', 'high-volume', 'content', '–', 'takes', 'long', 'document', 'process', '.']

 TOTAL FILTERED TOKENS ==>  19

 ---- POST FOR FILTERED TOKENS ----

 [('quite', 'RB'), ('computationally', 'RB'), ('intensive', 'JJ'), ('(', '('), ('along', 'IN'), ('irritating', 'VBG'), ('6th', 'CD'), ('graders', 'NNS'), (')', ')'), (',', ','), ('’', 'JJ'), ('high-volume', 'JJ'), ('content', 'NN'), ('–', 'NNP'), ('takes', 'VBZ'), ('long', 'JJ'), ('document', 'NN'), ('process', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['quite computationally', 'computationally intensive', 'intensive (', '( along', 'along irritating', 'irritating 6th', '6th graders', 'graders )', ') ,', ', ’', '’ high-volume', 'high-volume content', 'content –', '– takes', 'takes long', 'long document', 'document process', 'process .'] 

 TOTAL BIGRAMS --> 18 



 ---- TRI-GRAMS ---- 

 ['quite computationally intensive', 'computationally intensive (', 'intensive ( along', '( along irritating', 'along irritating 6th', 'irritating 6th graders', '6th graders )', 'graders ) ,', ') , ’', ', ’ high-volume', '’ high-volume content', 'high-volume content –', 'content – takes', '– takes long', 'takes long document', 'long document process', 'document process .'] 

 TOTAL TRIGRAMS --> 17 



 ---- NOUN PHRASES ---- 

 ['’ high-volume content', 'long document', 'process'] 

 TOTAL NOUN PHRASES --> 3 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['quit', 'comput', 'intens', '(', 'along', 'irrit', '6th', 'grader', ')', ',', '’', 'high-volum', 'content', '–', 'take', 'long', 'document', 'process', '.']

 TOTAL PORTER STEM WORDS ==> 19



 ---- SNOWBALL STEMMING ----

['quit', 'comput', 'intens', '(', 'along', 'irrit', '6th', 'grader', ')', ',', '’', 'high-volum', 'content', '–', 'take', 'long', 'document', 'process', '.']

 TOTAL SNOWBALL STEM WORDS ==> 19



 ---- LEMMATIZATION ----

['quite', 'computationally', 'intensive', '(', 'along', 'irritating', '6th', 'grader', ')', ',', '’', 'high-volume', 'content', '–', 'take', 'long', 'document', 'process', '.']

 TOTAL LEMMATIZE WORDS ==> 19

************************************************************************************************************************

47 --> But what if you were to process a bunch of content ahead of time to  come up with a set of relationships that shows how words like “ball,”  “threw” and “mountain” were typically related across millions and billions of  sentences. 


 ---- TOKENS ----

 ['But', 'what', 'if', 'you', 'were', 'to', 'process', 'a', 'bunch', 'of', 'content', 'ahead', 'of', 'time', 'to', 'come', 'up', 'with', 'a', 'set', 'of', 'relationships', 'that', 'shows', 'how', 'words', 'like', '“', 'ball', ',', '”', '“', 'threw', '”', 'and', '“', 'mountain', '”', 'were', 'typically', 'related', 'across', 'millions', 'and', 'billions', 'of', 'sentences', '.'] 

 TOTAL TOKENS ==> 48

 ---- POST ----

 [('But', 'CC'), ('what', 'WP'), ('if', 'IN'), ('you', 'PRP'), ('were', 'VBD'), ('to', 'TO'), ('process', 'VB'), ('a', 'DT'), ('bunch', 'NN'), ('of', 'IN'), ('content', 'JJ'), ('ahead', 'RB'), ('of', 'IN'), ('time', 'NN'), ('to', 'TO'), ('come', 'VB'), ('up', 'RP'), ('with', 'IN'), ('a', 'DT'), ('set', 'NN'), ('of', 'IN'), ('relationships', 'NNS'), ('that', 'WDT'), ('shows', 'VBZ'), ('how', 'WRB'), ('words', 'NNS'), ('like', 'IN'), ('“', 'NNP'), ('ball', 'NN'), (',', ','), ('”', 'NNP'), ('“', 'NNP'), ('threw', 'VBD'), ('”', 'NNP'), ('and', 'CC'), ('“', 'NNP'), ('mountain', 'NN'), ('”', 'NNS'), ('were', 'VBD'), ('typically', 'RB'), ('related', 'VBN'), ('across', 'IN'), ('millions', 'NNS'), ('and', 'CC'), ('billions', 'NNS'), ('of', 'IN'), ('sentences', 'NNS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['process', 'bunch', 'content', 'ahead', 'time', 'come', 'set', 'relationships', 'shows', 'words', 'like', '“', 'ball', ',', '”', '“', 'threw', '”', '“', 'mountain', '”', 'typically', 'related', 'across', 'millions', 'billions', 'sentences', '.']

 TOTAL FILTERED TOKENS ==>  28

 ---- POST FOR FILTERED TOKENS ----

 [('process', 'NN'), ('bunch', 'NN'), ('content', 'NN'), ('ahead', 'RB'), ('time', 'NN'), ('come', 'JJ'), ('set', 'VBN'), ('relationships', 'NNS'), ('shows', 'VBZ'), ('words', 'NNS'), ('like', 'IN'), ('“', 'NNP'), ('ball', 'NN'), (',', ','), ('”', 'NNP'), ('“', 'NNP'), ('threw', 'VBD'), ('”', 'NNP'), ('“', 'NNP'), ('mountain', 'NN'), ('”', 'NNP'), ('typically', 'RB'), ('related', 'JJ'), ('across', 'IN'), ('millions', 'NNS'), ('billions', 'NNS'), ('sentences', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['process bunch', 'bunch content', 'content ahead', 'ahead time', 'time come', 'come set', 'set relationships', 'relationships shows', 'shows words', 'words like', 'like “', '“ ball', 'ball ,', ', ”', '” “', '“ threw', 'threw ”', '” “', '“ mountain', 'mountain ”', '” typically', 'typically related', 'related across', 'across millions', 'millions billions', 'billions sentences', 'sentences .'] 

 TOTAL BIGRAMS --> 27 



 ---- TRI-GRAMS ---- 

 ['process bunch content', 'bunch content ahead', 'content ahead time', 'ahead time come', 'time come set', 'come set relationships', 'set relationships shows', 'relationships shows words', 'shows words like', 'words like “', 'like “ ball', '“ ball ,', 'ball , ”', ', ” “', '” “ threw', '“ threw ”', 'threw ” “', '” “ mountain', '“ mountain ”', 'mountain ” typically', '” typically related', 'typically related across', 'related across millions', 'across millions billions', 'millions billions sentences', 'billions sentences .'] 

 TOTAL TRIGRAMS --> 26 



 ---- NOUN PHRASES ---- 

 ['process', 'bunch', 'content', 'time', 'ball', 'mountain'] 

 TOTAL NOUN PHRASES --> 6 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['process', 'bunch', 'content', 'ahead', 'time', 'come', 'set', 'relationship', 'show', 'word', 'like', '“', 'ball', ',', '”', '“', 'threw', '”', '“', 'mountain', '”', 'typic', 'relat', 'across', 'million', 'billion', 'sentenc', '.']

 TOTAL PORTER STEM WORDS ==> 28



 ---- SNOWBALL STEMMING ----

['process', 'bunch', 'content', 'ahead', 'time', 'come', 'set', 'relationship', 'show', 'word', 'like', '“', 'ball', ',', '”', '“', 'threw', '”', '“', 'mountain', '”', 'typic', 'relat', 'across', 'million', 'billion', 'sentenc', '.']

 TOTAL SNOWBALL STEM WORDS ==> 28



 ---- LEMMATIZATION ----

['process', 'bunch', 'content', 'ahead', 'time', 'come', 'set', 'relationship', 'show', 'word', 'like', '“', 'ball', ',', '”', '“', 'threw', '”', '“', 'mountain', '”', 'typically', 'related', 'across', 'million', 'billion', 'sentence', '.']

 TOTAL LEMMATIZE WORDS ==> 28

************************************************************************************************************************

48 --> As a human, you naturally know that it is far more likely that “threw”   is acting on “ball,” than it is likely that “threw” is acting on “mountain.”  You don’t throw mountains, you throw balls. 


 ---- TOKENS ----

 ['As', 'a', 'human', ',', 'you', 'naturally', 'know', 'that', 'it', 'is', 'far', 'more', 'likely', 'that', '“', 'threw', '”', 'is', 'acting', 'on', '“', 'ball', ',', '”', 'than', 'it', 'is', 'likely', 'that', '“', 'threw', '”', 'is', 'acting', 'on', '“', 'mountain.', '”', 'You', 'don', '’', 't', 'throw', 'mountains', ',', 'you', 'throw', 'balls', '.'] 

 TOTAL TOKENS ==> 49

 ---- POST ----

 [('As', 'IN'), ('a', 'DT'), ('human', 'JJ'), (',', ','), ('you', 'PRP'), ('naturally', 'RB'), ('know', 'VBP'), ('that', 'IN'), ('it', 'PRP'), ('is', 'VBZ'), ('far', 'RB'), ('more', 'RBR'), ('likely', 'JJ'), ('that', 'IN'), ('“', 'NNP'), ('threw', 'VBD'), ('”', 'NNP'), ('is', 'VBZ'), ('acting', 'VBG'), ('on', 'IN'), ('“', 'NNP'), ('ball', 'NN'), (',', ','), ('”', 'JJR'), ('than', 'IN'), ('it', 'PRP'), ('is', 'VBZ'), ('likely', 'JJ'), ('that', 'IN'), ('“', 'NNP'), ('threw', 'VBD'), ('”', 'NNP'), ('is', 'VBZ'), ('acting', 'VBG'), ('on', 'IN'), ('“', 'NNP'), ('mountain.', 'NN'), ('”', 'NNP'), ('You', 'PRP'), ('don', 'VBP'), ('’', 'JJ'), ('t', 'NN'), ('throw', 'NN'), ('mountains', 'NNS'), (',', ','), ('you', 'PRP'), ('throw', 'VBP'), ('balls', 'NNS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['human', ',', 'naturally', 'know', 'far', 'likely', '“', 'threw', '”', 'acting', '“', 'ball', ',', '”', 'likely', '“', 'threw', '”', 'acting', '“', 'mountain.', '”', '’', 'throw', 'mountains', ',', 'throw', 'balls', '.']

 TOTAL FILTERED TOKENS ==>  29

 ---- POST FOR FILTERED TOKENS ----

 [('human', 'NN'), (',', ','), ('naturally', 'RB'), ('know', 'VBP'), ('far', 'RB'), ('likely', 'JJ'), ('“', 'JJ'), ('threw', 'VBD'), ('”', 'JJ'), ('acting', 'VBG'), ('“', 'NN'), ('ball', 'NN'), (',', ','), ('”', 'NNP'), ('likely', 'RB'), ('“', 'VBD'), ('threw', 'JJ'), ('”', 'NNP'), ('acting', 'VBG'), ('“', 'NNP'), ('mountain.', 'NN'), ('”', 'NNP'), ('’', 'NNP'), ('throw', 'NN'), ('mountains', 'NNS'), (',', ','), ('throw', 'NN'), ('balls', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['human ,', ', naturally', 'naturally know', 'know far', 'far likely', 'likely “', '“ threw', 'threw ”', '” acting', 'acting “', '“ ball', 'ball ,', ', ”', '” likely', 'likely “', '“ threw', 'threw ”', '” acting', 'acting “', '“ mountain.', 'mountain. ”', '” ’', '’ throw', 'throw mountains', 'mountains ,', ', throw', 'throw balls', 'balls .'] 

 TOTAL BIGRAMS --> 28 



 ---- TRI-GRAMS ---- 

 ['human , naturally', ', naturally know', 'naturally know far', 'know far likely', 'far likely “', 'likely “ threw', '“ threw ”', 'threw ” acting', '” acting “', 'acting “ ball', '“ ball ,', 'ball , ”', ', ” likely', '” likely “', 'likely “ threw', '“ threw ”', 'threw ” acting', '” acting “', 'acting “ mountain.', '“ mountain. ”', 'mountain. ” ’', '” ’ throw', '’ throw mountains', 'throw mountains ,', 'mountains , throw', ', throw balls', 'throw balls .'] 

 TOTAL TRIGRAMS --> 27 



 ---- NOUN PHRASES ---- 

 ['human', '“', 'ball', 'mountain.', 'throw', 'throw'] 

 TOTAL NOUN PHRASES --> 6 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['human', ',', 'natur', 'know', 'far', 'like', '“', 'threw', '”', 'act', '“', 'ball', ',', '”', 'like', '“', 'threw', '”', 'act', '“', 'mountain.', '”', '’', 'throw', 'mountain', ',', 'throw', 'ball', '.']

 TOTAL PORTER STEM WORDS ==> 29



 ---- SNOWBALL STEMMING ----

['human', ',', 'natur', 'know', 'far', 'like', '“', 'threw', '”', 'act', '“', 'ball', ',', '”', 'like', '“', 'threw', '”', 'act', '“', 'mountain.', '”', '’', 'throw', 'mountain', ',', 'throw', 'ball', '.']

 TOTAL SNOWBALL STEM WORDS ==> 29



 ---- LEMMATIZATION ----

['human', ',', 'naturally', 'know', 'far', 'likely', '“', 'threw', '”', 'acting', '“', 'ball', ',', '”', 'likely', '“', 'threw', '”', 'acting', '“', 'mountain.', '”', '’', 'throw', 'mountain', ',', 'throw', 'ball', '.']

 TOTAL LEMMATIZE WORDS ==> 29

************************************************************************************************************************

49 --> That sort of probabilistic relationship can be extracted using unsupervised  learning. 


 ---- TOKENS ----

 ['That', 'sort', 'of', 'probabilistic', 'relationship', 'can', 'be', 'extracted', 'using', 'unsupervised', 'learning', '.'] 

 TOTAL TOKENS ==> 12

 ---- POST ----

 [('That', 'DT'), ('sort', 'NN'), ('of', 'IN'), ('probabilistic', 'JJ'), ('relationship', 'NN'), ('can', 'MD'), ('be', 'VB'), ('extracted', 'VBN'), ('using', 'VBG'), ('unsupervised', 'JJ'), ('learning', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['sort', 'probabilistic', 'relationship', 'extracted', 'using', 'unsupervised', 'learning', '.']

 TOTAL FILTERED TOKENS ==>  8

 ---- POST FOR FILTERED TOKENS ----

 [('sort', 'NN'), ('probabilistic', 'JJ'), ('relationship', 'NN'), ('extracted', 'VBD'), ('using', 'VBG'), ('unsupervised', 'JJ'), ('learning', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['sort probabilistic', 'probabilistic relationship', 'relationship extracted', 'extracted using', 'using unsupervised', 'unsupervised learning', 'learning .'] 

 TOTAL BIGRAMS --> 7 



 ---- TRI-GRAMS ---- 

 ['sort probabilistic relationship', 'probabilistic relationship extracted', 'relationship extracted using', 'extracted using unsupervised', 'using unsupervised learning', 'unsupervised learning .'] 

 TOTAL TRIGRAMS --> 6 



 ---- NOUN PHRASES ---- 

 ['sort', 'probabilistic relationship', 'unsupervised learning'] 

 TOTAL NOUN PHRASES --> 3 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['sort', 'probabilist', 'relationship', 'extract', 'use', 'unsupervis', 'learn', '.']

 TOTAL PORTER STEM WORDS ==> 8



 ---- SNOWBALL STEMMING ----

['sort', 'probabilist', 'relationship', 'extract', 'use', 'unsupervis', 'learn', '.']

 TOTAL SNOWBALL STEM WORDS ==> 8



 ---- LEMMATIZATION ----

['sort', 'probabilistic', 'relationship', 'extracted', 'using', 'unsupervised', 'learning', '.']

 TOTAL LEMMATIZE WORDS ==> 8

************************************************************************************************************************

50 --> The syntax matrix was an excellent candidate for unsupervised  learning, as it involved discovering generally applicable patterns from a very  large corpus of content. 


 ---- TOKENS ----

 ['The', 'syntax', 'matrix', 'was', 'an', 'excellent', 'candidate', 'for', 'unsupervised', 'learning', ',', 'as', 'it', 'involved', 'discovering', 'generally', 'applicable', 'patterns', 'from', 'a', 'very', 'large', 'corpus', 'of', 'content', '.'] 

 TOTAL TOKENS ==> 26

 ---- POST ----

 [('The', 'DT'), ('syntax', 'NN'), ('matrix', 'NN'), ('was', 'VBD'), ('an', 'DT'), ('excellent', 'JJ'), ('candidate', 'NN'), ('for', 'IN'), ('unsupervised', 'JJ'), ('learning', 'NN'), (',', ','), ('as', 'IN'), ('it', 'PRP'), ('involved', 'VBD'), ('discovering', 'VBG'), ('generally', 'RB'), ('applicable', 'JJ'), ('patterns', 'NNS'), ('from', 'IN'), ('a', 'DT'), ('very', 'RB'), ('large', 'JJ'), ('corpus', 'NN'), ('of', 'IN'), ('content', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['syntax', 'matrix', 'excellent', 'candidate', 'unsupervised', 'learning', ',', 'involved', 'discovering', 'generally', 'applicable', 'patterns', 'large', 'corpus', 'content', '.']

 TOTAL FILTERED TOKENS ==>  16

 ---- POST FOR FILTERED TOKENS ----

 [('syntax', 'NN'), ('matrix', 'NN'), ('excellent', 'JJ'), ('candidate', 'NN'), ('unsupervised', 'VBD'), ('learning', 'NN'), (',', ','), ('involved', 'VBN'), ('discovering', 'VBG'), ('generally', 'RB'), ('applicable', 'JJ'), ('patterns', 'NNS'), ('large', 'JJ'), ('corpus', 'NN'), ('content', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['syntax matrix', 'matrix excellent', 'excellent candidate', 'candidate unsupervised', 'unsupervised learning', 'learning ,', ', involved', 'involved discovering', 'discovering generally', 'generally applicable', 'applicable patterns', 'patterns large', 'large corpus', 'corpus content', 'content .'] 

 TOTAL BIGRAMS --> 15 



 ---- TRI-GRAMS ---- 

 ['syntax matrix excellent', 'matrix excellent candidate', 'excellent candidate unsupervised', 'candidate unsupervised learning', 'unsupervised learning ,', 'learning , involved', ', involved discovering', 'involved discovering generally', 'discovering generally applicable', 'generally applicable patterns', 'applicable patterns large', 'patterns large corpus', 'large corpus content', 'corpus content .'] 

 TOTAL TRIGRAMS --> 14 



 ---- NOUN PHRASES ---- 

 ['syntax', 'matrix', 'excellent candidate', 'learning', 'large corpus', 'content'] 

 TOTAL NOUN PHRASES --> 6 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['syntax', 'matrix', 'excel', 'candid', 'unsupervis', 'learn', ',', 'involv', 'discov', 'gener', 'applic', 'pattern', 'larg', 'corpu', 'content', '.']

 TOTAL PORTER STEM WORDS ==> 16



 ---- SNOWBALL STEMMING ----

['syntax', 'matrix', 'excel', 'candid', 'unsupervis', 'learn', ',', 'involv', 'discov', 'general', 'applic', 'pattern', 'larg', 'corpus', 'content', '.']

 TOTAL SNOWBALL STEM WORDS ==> 16



 ---- LEMMATIZATION ----

['syntax', 'matrix', 'excellent', 'candidate', 'unsupervised', 'learning', ',', 'involved', 'discovering', 'generally', 'applicable', 'pattern', 'large', 'corpus', 'content', '.']

 TOTAL LEMMATIZE WORDS ==> 16

************************************************************************************************************************

51 --> Because it is a matrix, it can be evaluated really fast  for each sentence, unlike a full parser. 


 ---- TOKENS ----

 ['Because', 'it', 'is', 'a', 'matrix', ',', 'it', 'can', 'be', 'evaluated', 'really', 'fast', 'for', 'each', 'sentence', ',', 'unlike', 'a', 'full', 'parser', '.'] 

 TOTAL TOKENS ==> 21

 ---- POST ----

 [('Because', 'IN'), ('it', 'PRP'), ('is', 'VBZ'), ('a', 'DT'), ('matrix', 'NN'), (',', ','), ('it', 'PRP'), ('can', 'MD'), ('be', 'VB'), ('evaluated', 'VBN'), ('really', 'RB'), ('fast', 'JJ'), ('for', 'IN'), ('each', 'DT'), ('sentence', 'NN'), (',', ','), ('unlike', 'IN'), ('a', 'DT'), ('full', 'JJ'), ('parser', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['matrix', ',', 'evaluated', 'really', 'fast', 'sentence', ',', 'unlike', 'full', 'parser', '.']

 TOTAL FILTERED TOKENS ==>  11

 ---- POST FOR FILTERED TOKENS ----

 [('matrix', 'NN'), (',', ','), ('evaluated', 'VBN'), ('really', 'RB'), ('fast', 'JJ'), ('sentence', 'NN'), (',', ','), ('unlike', 'IN'), ('full', 'JJ'), ('parser', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['matrix ,', ', evaluated', 'evaluated really', 'really fast', 'fast sentence', 'sentence ,', ', unlike', 'unlike full', 'full parser', 'parser .'] 

 TOTAL BIGRAMS --> 10 



 ---- TRI-GRAMS ---- 

 ['matrix , evaluated', ', evaluated really', 'evaluated really fast', 'really fast sentence', 'fast sentence ,', 'sentence , unlike', ', unlike full', 'unlike full parser', 'full parser .'] 

 TOTAL TRIGRAMS --> 9 



 ---- NOUN PHRASES ---- 

 ['matrix', 'fast sentence', 'full parser'] 

 TOTAL NOUN PHRASES --> 3 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['matrix', ',', 'evalu', 'realli', 'fast', 'sentenc', ',', 'unlik', 'full', 'parser', '.']

 TOTAL PORTER STEM WORDS ==> 11



 ---- SNOWBALL STEMMING ----

['matrix', ',', 'evalu', 'realli', 'fast', 'sentenc', ',', 'unlik', 'full', 'parser', '.']

 TOTAL SNOWBALL STEM WORDS ==> 11



 ---- LEMMATIZATION ----

['matrix', ',', 'evaluated', 'really', 'fast', 'sentence', ',', 'unlike', 'full', 'parser', '.']

 TOTAL LEMMATIZE WORDS ==> 11

************************************************************************************************************************

52 --> As the amount of content created every day grows exponentially,  unsupervised techniques become more and more valuable. 


 ---- TOKENS ----

 ['As', 'the', 'amount', 'of', 'content', 'created', 'every', 'day', 'grows', 'exponentially', ',', 'unsupervised', 'techniques', 'become', 'more', 'and', 'more', 'valuable', '.'] 

 TOTAL TOKENS ==> 19

 ---- POST ----

 [('As', 'IN'), ('the', 'DT'), ('amount', 'NN'), ('of', 'IN'), ('content', 'NN'), ('created', 'VBN'), ('every', 'DT'), ('day', 'NN'), ('grows', 'VBZ'), ('exponentially', 'RB'), (',', ','), ('unsupervised', 'JJ'), ('techniques', 'NNS'), ('become', 'VB'), ('more', 'RBR'), ('and', 'CC'), ('more', 'RBR'), ('valuable', 'JJ'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['amount', 'content', 'created', 'every', 'day', 'grows', 'exponentially', ',', 'unsupervised', 'techniques', 'become', 'valuable', '.']

 TOTAL FILTERED TOKENS ==>  13

 ---- POST FOR FILTERED TOKENS ----

 [('amount', 'NN'), ('content', 'NN'), ('created', 'VBD'), ('every', 'DT'), ('day', 'NN'), ('grows', 'VBZ'), ('exponentially', 'RB'), (',', ','), ('unsupervised', 'JJ'), ('techniques', 'NNS'), ('become', 'VBP'), ('valuable', 'JJ'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['amount content', 'content created', 'created every', 'every day', 'day grows', 'grows exponentially', 'exponentially ,', ', unsupervised', 'unsupervised techniques', 'techniques become', 'become valuable', 'valuable .'] 

 TOTAL BIGRAMS --> 12 



 ---- TRI-GRAMS ---- 

 ['amount content created', 'content created every', 'created every day', 'every day grows', 'day grows exponentially', 'grows exponentially ,', 'exponentially , unsupervised', ', unsupervised techniques', 'unsupervised techniques become', 'techniques become valuable', 'become valuable .'] 

 TOTAL TRIGRAMS --> 11 



 ---- NOUN PHRASES ---- 

 ['amount', 'content', 'every day'] 

 TOTAL NOUN PHRASES --> 3 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['amount', 'content', 'creat', 'everi', 'day', 'grow', 'exponenti', ',', 'unsupervis', 'techniqu', 'becom', 'valuabl', '.']

 TOTAL PORTER STEM WORDS ==> 13



 ---- SNOWBALL STEMMING ----

['amount', 'content', 'creat', 'everi', 'day', 'grow', 'exponenti', ',', 'unsupervis', 'techniqu', 'becom', 'valuabl', '.']

 TOTAL SNOWBALL STEM WORDS ==> 13



 ---- LEMMATIZATION ----

['amount', 'content', 'created', 'every', 'day', 'grows', 'exponentially', ',', 'unsupervised', 'technique', 'become', 'valuable', '.']

 TOTAL LEMMATIZE WORDS ==> 13

************************************************************************************************************************

53 --> Semi-supervised learning Semi-supervised learning is a combination of unsupervised and supervised  learning techniques. 


 ---- TOKENS ----

 ['Semi-supervised', 'learning', 'Semi-supervised', 'learning', 'is', 'a', 'combination', 'of', 'unsupervised', 'and', 'supervised', 'learning', 'techniques', '.'] 

 TOTAL TOKENS ==> 14

 ---- POST ----

 [('Semi-supervised', 'JJ'), ('learning', 'VBG'), ('Semi-supervised', 'JJ'), ('learning', 'NN'), ('is', 'VBZ'), ('a', 'DT'), ('combination', 'NN'), ('of', 'IN'), ('unsupervised', 'JJ'), ('and', 'CC'), ('supervised', 'VBD'), ('learning', 'VBG'), ('techniques', 'NNS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Semi-supervised', 'learning', 'Semi-supervised', 'learning', 'combination', 'unsupervised', 'supervised', 'learning', 'techniques', '.']

 TOTAL FILTERED TOKENS ==>  10

 ---- POST FOR FILTERED TOKENS ----

 [('Semi-supervised', 'JJ'), ('learning', 'VBG'), ('Semi-supervised', 'JJ'), ('learning', 'JJ'), ('combination', 'NN'), ('unsupervised', 'VBD'), ('supervised', 'JJ'), ('learning', 'NN'), ('techniques', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Semi-supervised learning', 'learning Semi-supervised', 'Semi-supervised learning', 'learning combination', 'combination unsupervised', 'unsupervised supervised', 'supervised learning', 'learning techniques', 'techniques .'] 

 TOTAL BIGRAMS --> 9 



 ---- TRI-GRAMS ---- 

 ['Semi-supervised learning Semi-supervised', 'learning Semi-supervised learning', 'Semi-supervised learning combination', 'learning combination unsupervised', 'combination unsupervised supervised', 'unsupervised supervised learning', 'supervised learning techniques', 'learning techniques .'] 

 TOTAL TRIGRAMS --> 8 



 ---- NOUN PHRASES ---- 

 ['Semi-supervised learning combination', 'supervised learning'] 

 TOTAL NOUN PHRASES --> 2 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['semi-supervis', 'learn', 'semi-supervis', 'learn', 'combin', 'unsupervis', 'supervis', 'learn', 'techniqu', '.']

 TOTAL PORTER STEM WORDS ==> 10



 ---- SNOWBALL STEMMING ----

['semi-supervis', 'learn', 'semi-supervis', 'learn', 'combin', 'unsupervis', 'supervis', 'learn', 'techniqu', '.']

 TOTAL SNOWBALL STEM WORDS ==> 10



 ---- LEMMATIZATION ----

['Semi-supervised', 'learning', 'Semi-supervised', 'learning', 'combination', 'unsupervised', 'supervised', 'learning', 'technique', '.']

 TOTAL LEMMATIZE WORDS ==> 10

************************************************************************************************************************

54 --> With this approach we’ll have both marked-up  supervised content and un-marked data. 


 ---- TOKENS ----

 ['With', 'this', 'approach', 'we', '’', 'll', 'have', 'both', 'marked-up', 'supervised', 'content', 'and', 'un-marked', 'data', '.'] 

 TOTAL TOKENS ==> 15

 ---- POST ----

 [('With', 'IN'), ('this', 'DT'), ('approach', 'NN'), ('we', 'PRP'), ('’', 'VBP'), ('ll', 'NNS'), ('have', 'VBP'), ('both', 'DT'), ('marked-up', 'NNS'), ('supervised', 'VBN'), ('content', 'JJ'), ('and', 'CC'), ('un-marked', 'JJ'), ('data', 'NNS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['approach', '’', 'marked-up', 'supervised', 'content', 'un-marked', 'data', '.']

 TOTAL FILTERED TOKENS ==>  8

 ---- POST FOR FILTERED TOKENS ----

 [('approach', 'NN'), ('’', 'VBD'), ('marked-up', 'NNS'), ('supervised', 'JJ'), ('content', 'JJ'), ('un-marked', 'JJ'), ('data', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['approach ’', '’ marked-up', 'marked-up supervised', 'supervised content', 'content un-marked', 'un-marked data', 'data .'] 

 TOTAL BIGRAMS --> 7 



 ---- TRI-GRAMS ---- 

 ['approach ’ marked-up', '’ marked-up supervised', 'marked-up supervised content', 'supervised content un-marked', 'content un-marked data', 'un-marked data .'] 

 TOTAL TRIGRAMS --> 6 



 ---- NOUN PHRASES ---- 

 ['approach'] 

 TOTAL NOUN PHRASES --> 1 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['approach', '’', 'marked-up', 'supervis', 'content', 'un-mark', 'data', '.']

 TOTAL PORTER STEM WORDS ==> 8



 ---- SNOWBALL STEMMING ----

['approach', '’', 'marked-up', 'supervis', 'content', 'un-mark', 'data', '.']

 TOTAL SNOWBALL STEM WORDS ==> 8



 ---- LEMMATIZATION ----

['approach', '’', 'marked-up', 'supervised', 'content', 'un-marked', 'data', '.']

 TOTAL LEMMATIZE WORDS ==> 8

************************************************************************************************************************

55 --> The machine learning model   uses the marked-up content to generalize and make assertions about   the rest of the data. 


 ---- TOKENS ----

 ['The', 'machine', 'learning', 'model', 'uses', 'the', 'marked-up', 'content', 'to', 'generalize', 'and', 'make', 'assertions', 'about', 'the', 'rest', 'of', 'the', 'data', '.'] 

 TOTAL TOKENS ==> 20

 ---- POST ----

 [('The', 'DT'), ('machine', 'NN'), ('learning', 'VBG'), ('model', 'NN'), ('uses', 'VBZ'), ('the', 'DT'), ('marked-up', 'JJ'), ('content', 'NN'), ('to', 'TO'), ('generalize', 'VB'), ('and', 'CC'), ('make', 'VB'), ('assertions', 'NNS'), ('about', 'IN'), ('the', 'DT'), ('rest', 'NN'), ('of', 'IN'), ('the', 'DT'), ('data', 'NNS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['machine', 'learning', 'model', 'uses', 'marked-up', 'content', 'generalize', 'make', 'assertions', 'rest', 'data', '.']

 TOTAL FILTERED TOKENS ==>  12

 ---- POST FOR FILTERED TOKENS ----

 [('machine', 'NN'), ('learning', 'VBG'), ('model', 'JJ'), ('uses', 'NNS'), ('marked-up', 'JJ'), ('content', 'JJ'), ('generalize', 'NNS'), ('make', 'VBP'), ('assertions', 'NNS'), ('rest', 'VB'), ('data', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['machine learning', 'learning model', 'model uses', 'uses marked-up', 'marked-up content', 'content generalize', 'generalize make', 'make assertions', 'assertions rest', 'rest data', 'data .'] 

 TOTAL BIGRAMS --> 11 



 ---- TRI-GRAMS ---- 

 ['machine learning model', 'learning model uses', 'model uses marked-up', 'uses marked-up content', 'marked-up content generalize', 'content generalize make', 'generalize make assertions', 'make assertions rest', 'assertions rest data', 'rest data .'] 

 TOTAL TRIGRAMS --> 10 



 ---- NOUN PHRASES ---- 

 ['machine'] 

 TOTAL NOUN PHRASES --> 1 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['machin', 'learn', 'model', 'use', 'marked-up', 'content', 'gener', 'make', 'assert', 'rest', 'data', '.']

 TOTAL PORTER STEM WORDS ==> 12



 ---- SNOWBALL STEMMING ----

['machin', 'learn', 'model', 'use', 'marked-up', 'content', 'general', 'make', 'assert', 'rest', 'data', '.']

 TOTAL SNOWBALL STEM WORDS ==> 12



 ---- LEMMATIZATION ----

['machine', 'learning', 'model', 'us', 'marked-up', 'content', 'generalize', 'make', 'assertion', 'rest', 'data', '.']

 TOTAL LEMMATIZE WORDS ==> 12

************************************************************************************************************************

56 --> Now that we’ve reviewed the machine learning essentials, let’s look at  how to combine machine learning and algorithmic natural language  processing to build a high-performing text analytics AI. 


 ---- TOKENS ----

 ['Now', 'that', 'we', '’', 've', 'reviewed', 'the', 'machine', 'learning', 'essentials', ',', 'let', '’', 's', 'look', 'at', 'how', 'to', 'combine', 'machine', 'learning', 'and', 'algorithmic', 'natural', 'language', 'processing', 'to', 'build', 'a', 'high-performing', 'text', 'analytics', 'AI', '.'] 

 TOTAL TOKENS ==> 34

 ---- POST ----

 [('Now', 'RB'), ('that', 'IN'), ('we', 'PRP'), ('’', 'VBP'), ('ve', 'JJ'), ('reviewed', 'VBD'), ('the', 'DT'), ('machine', 'NN'), ('learning', 'NN'), ('essentials', 'NNS'), (',', ','), ('let', 'VB'), ('’', 'NNP'), ('s', 'VB'), ('look', 'NN'), ('at', 'IN'), ('how', 'WRB'), ('to', 'TO'), ('combine', 'VB'), ('machine', 'NN'), ('learning', 'NN'), ('and', 'CC'), ('algorithmic', 'JJ'), ('natural', 'JJ'), ('language', 'NN'), ('processing', 'NN'), ('to', 'TO'), ('build', 'VB'), ('a', 'DT'), ('high-performing', 'JJ'), ('text', 'NN'), ('analytics', 'NNS'), ('AI', 'NNP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['’', 'reviewed', 'machine', 'learning', 'essentials', ',', 'let', '’', 'look', 'combine', 'machine', 'learning', 'algorithmic', 'natural', 'language', 'processing', 'build', 'high-performing', 'text', 'analytics', 'AI', '.']

 TOTAL FILTERED TOKENS ==>  22

 ---- POST FOR FILTERED TOKENS ----

 [('’', 'NN'), ('reviewed', 'VBD'), ('machine', 'NN'), ('learning', 'NN'), ('essentials', 'NNS'), (',', ','), ('let', 'VB'), ('’', 'NNP'), ('look', 'VB'), ('combine', 'JJ'), ('machine', 'NN'), ('learning', 'VBG'), ('algorithmic', 'JJ'), ('natural', 'JJ'), ('language', 'NN'), ('processing', 'NN'), ('build', 'VB'), ('high-performing', 'JJ'), ('text', 'NN'), ('analytics', 'NNS'), ('AI', 'NNP'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['’ reviewed', 'reviewed machine', 'machine learning', 'learning essentials', 'essentials ,', ', let', 'let ’', '’ look', 'look combine', 'combine machine', 'machine learning', 'learning algorithmic', 'algorithmic natural', 'natural language', 'language processing', 'processing build', 'build high-performing', 'high-performing text', 'text analytics', 'analytics AI', 'AI .'] 

 TOTAL BIGRAMS --> 21 



 ---- TRI-GRAMS ---- 

 ['’ reviewed machine', 'reviewed machine learning', 'machine learning essentials', 'learning essentials ,', 'essentials , let', ', let ’', 'let ’ look', '’ look combine', 'look combine machine', 'combine machine learning', 'machine learning algorithmic', 'learning algorithmic natural', 'algorithmic natural language', 'natural language processing', 'language processing build', 'processing build high-performing', 'build high-performing text', 'high-performing text analytics', 'text analytics AI', 'analytics AI .'] 

 TOTAL TRIGRAMS --> 20 



 ---- NOUN PHRASES ---- 

 ['’', 'machine', 'learning', 'combine machine', 'algorithmic natural language', 'processing', 'high-performing text'] 

 TOTAL NOUN PHRASES --> 7 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['’', 'review', 'machin', 'learn', 'essenti', ',', 'let', '’', 'look', 'combin', 'machin', 'learn', 'algorithm', 'natur', 'languag', 'process', 'build', 'high-perform', 'text', 'analyt', 'ai', '.']

 TOTAL PORTER STEM WORDS ==> 22



 ---- SNOWBALL STEMMING ----

['’', 'review', 'machin', 'learn', 'essenti', ',', 'let', '’', 'look', 'combin', 'machin', 'learn', 'algorithm', 'natur', 'languag', 'process', 'build', 'high-perform', 'text', 'analyt', 'ai', '.']

 TOTAL SNOWBALL STEM WORDS ==> 22



 ---- LEMMATIZATION ----

['’', 'reviewed', 'machine', 'learning', 'essential', ',', 'let', '’', 'look', 'combine', 'machine', 'learning', 'algorithmic', 'natural', 'language', 'processing', 'build', 'high-performing', 'text', 'analytics', 'AI', '.']

 TOTAL LEMMATIZE WORDS ==> 22

************************************************************************************************************************

57 --> is the combination   of unsupervised and  supervised learning. 


 ---- TOKENS ----

 ['is', 'the', 'combination', 'of', 'unsupervised', 'and', 'supervised', 'learning', '.'] 

 TOTAL TOKENS ==> 9

 ---- POST ----

 [('is', 'VBZ'), ('the', 'DT'), ('combination', 'NN'), ('of', 'IN'), ('unsupervised', 'JJ'), ('and', 'CC'), ('supervised', 'VBD'), ('learning', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['combination', 'unsupervised', 'supervised', 'learning', '.']

 TOTAL FILTERED TOKENS ==>  5

 ---- POST FOR FILTERED TOKENS ----

 [('combination', 'NN'), ('unsupervised', 'VBD'), ('supervised', 'JJ'), ('learning', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['combination unsupervised', 'unsupervised supervised', 'supervised learning', 'learning .'] 

 TOTAL BIGRAMS --> 4 



 ---- TRI-GRAMS ---- 

 ['combination unsupervised supervised', 'unsupervised supervised learning', 'supervised learning .'] 

 TOTAL TRIGRAMS --> 3 



 ---- NOUN PHRASES ---- 

 ['combination', 'supervised learning'] 

 TOTAL NOUN PHRASES --> 2 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['combin', 'unsupervis', 'supervis', 'learn', '.']

 TOTAL PORTER STEM WORDS ==> 5



 ---- SNOWBALL STEMMING ----

['combin', 'unsupervis', 'supervis', 'learn', '.']

 TOTAL SNOWBALL STEM WORDS ==> 5



 ---- LEMMATIZATION ----

['combination', 'unsupervised', 'supervised', 'learning', '.']

 TOTAL LEMMATIZE WORDS ==> 5

************************************************************************************************************************

58 --> Semi-supervised  learning is where the machine   takes content and is told to  find patterns within it. 


 ---- TOKENS ----

 ['Semi-supervised', 'learning', 'is', 'where', 'the', 'machine', 'takes', 'content', 'and', 'is', 'told', 'to', 'find', 'patterns', 'within', 'it', '.'] 

 TOTAL TOKENS ==> 17

 ---- POST ----

 [('Semi-supervised', 'JJ'), ('learning', 'NN'), ('is', 'VBZ'), ('where', 'WRB'), ('the', 'DT'), ('machine', 'NN'), ('takes', 'VBZ'), ('content', 'NN'), ('and', 'CC'), ('is', 'VBZ'), ('told', 'VBN'), ('to', 'TO'), ('find', 'VB'), ('patterns', 'NNS'), ('within', 'IN'), ('it', 'PRP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Semi-supervised', 'learning', 'machine', 'takes', 'content', 'told', 'find', 'patterns', 'within', '.']

 TOTAL FILTERED TOKENS ==>  10

 ---- POST FOR FILTERED TOKENS ----

 [('Semi-supervised', 'JJ'), ('learning', 'NN'), ('machine', 'NN'), ('takes', 'VBZ'), ('content', 'JJ'), ('told', 'VBD'), ('find', 'NN'), ('patterns', 'NNS'), ('within', 'IN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Semi-supervised learning', 'learning machine', 'machine takes', 'takes content', 'content told', 'told find', 'find patterns', 'patterns within', 'within .'] 

 TOTAL BIGRAMS --> 9 



 ---- TRI-GRAMS ---- 

 ['Semi-supervised learning machine', 'learning machine takes', 'machine takes content', 'takes content told', 'content told find', 'told find patterns', 'find patterns within', 'patterns within .'] 

 TOTAL TRIGRAMS --> 8 



 ---- NOUN PHRASES ---- 

 ['Semi-supervised learning', 'machine', 'find'] 

 TOTAL NOUN PHRASES --> 3 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['semi-supervis', 'learn', 'machin', 'take', 'content', 'told', 'find', 'pattern', 'within', '.']

 TOTAL PORTER STEM WORDS ==> 10



 ---- SNOWBALL STEMMING ----

['semi-supervis', 'learn', 'machin', 'take', 'content', 'told', 'find', 'pattern', 'within', '.']

 TOTAL SNOWBALL STEM WORDS ==> 10



 ---- LEMMATIZATION ----

['Semi-supervised', 'learning', 'machine', 'take', 'content', 'told', 'find', 'pattern', 'within', '.']

 TOTAL LEMMATIZE WORDS ==> 10

************************************************************************************************************************

59 --> Unsupervised  learning https://www.lexalytics.com/ https://www.lexalytics.com/  W H I T E  P A P E R 7|       |   Lexalytics, Inc., 48 North Pleasant St. Unit 301, Amherst MA 01002 USA   |   1-800-377-8036 |   www.lexalytics.com H A P P I E R  B Y  T H E  D O Z E N :   T H E  M O R E  M O D E L S ,  T H E  M E R R I E R  Machine learning models are very good at performing single tasks, such   as determining the sentiment polarity of a document or the part-of-speech  for a given word. 


 ---- TOKENS ----

 ['Unsupervised', 'learning', 'https', ':', '//www.lexalytics.com/', 'https', ':', '//www.lexalytics.com/', 'W', 'H', 'I', 'T', 'E', 'P', 'A', 'P', 'E', 'R', '7|', '|', 'Lexalytics', ',', 'Inc.', ',', '48', 'North', 'Pleasant', 'St.', 'Unit', '301', ',', 'Amherst', 'MA', '01002', 'USA', '|', '1-800-377-8036', '|', 'www.lexalytics.com', 'H', 'A', 'P', 'P', 'I', 'E', 'R', 'B', 'Y', 'T', 'H', 'E', 'D', 'O', 'Z', 'E', 'N', ':', 'T', 'H', 'E', 'M', 'O', 'R', 'E', 'M', 'O', 'D', 'E', 'L', 'S', ',', 'T', 'H', 'E', 'M', 'E', 'R', 'R', 'I', 'E', 'R', 'Machine', 'learning', 'models', 'are', 'very', 'good', 'at', 'performing', 'single', 'tasks', ',', 'such', 'as', 'determining', 'the', 'sentiment', 'polarity', 'of', 'a', 'document', 'or', 'the', 'part-of-speech', 'for', 'a', 'given', 'word', '.'] 

 TOTAL TOKENS ==> 109

 ---- POST ----

 [('Unsupervised', 'VBN'), ('learning', 'VBG'), ('https', 'NN'), (':', ':'), ('//www.lexalytics.com/', 'JJ'), ('https', 'NN'), (':', ':'), ('//www.lexalytics.com/', 'JJ'), ('W', 'NNP'), ('H', 'NNP'), ('I', 'PRP'), ('T', 'NNP'), ('E', 'NNP'), ('P', 'NNP'), ('A', 'NNP'), ('P', 'NNP'), ('E', 'NNP'), ('R', 'NNP'), ('7|', 'CD'), ('|', 'NNP'), ('Lexalytics', 'NNP'), (',', ','), ('Inc.', 'NNP'), (',', ','), ('48', 'CD'), ('North', 'NNP'), ('Pleasant', 'NNP'), ('St.', 'NNP'), ('Unit', 'NNP'), ('301', 'CD'), (',', ','), ('Amherst', 'NNP'), ('MA', 'NNP'), ('01002', 'CD'), ('USA', 'NNP'), ('|', 'NNP'), ('1-800-377-8036', 'JJ'), ('|', 'NNP'), ('www.lexalytics.com', 'NN'), ('H', 'NNP'), ('A', 'NNP'), ('P', 'NNP'), ('P', 'NNP'), ('I', 'PRP'), ('E', 'NNP'), ('R', 'NNP'), ('B', 'NNP'), ('Y', 'NNP'), ('T', 'NNP'), ('H', 'NNP'), ('E', 'NNP'), ('D', 'NNP'), ('O', 'NNP'), ('Z', 'NNP'), ('E', 'NNP'), ('N', 'NNP'), (':', ':'), ('T', 'NNP'), ('H', 'NNP'), ('E', 'NNP'), ('M', 'NNP'), ('O', 'NNP'), ('R', 'NNP'), ('E', 'NNP'), ('M', 'NNP'), ('O', 'NNP'), ('D', 'NNP'), ('E', 'NNP'), ('L', 'NNP'), ('S', 'NNP'), (',', ','), ('T', 'NNP'), ('H', 'NNP'), ('E', 'NNP'), ('M', 'NNP'), ('E', 'NNP'), ('R', 'NNP'), ('R', 'NNP'), ('I', 'PRP'), ('E', 'VBP'), ('R', 'JJ'), ('Machine', 'NNP'), ('learning', 'NN'), ('models', 'NNS'), ('are', 'VBP'), ('very', 'RB'), ('good', 'JJ'), ('at', 'IN'), ('performing', 'VBG'), ('single', 'JJ'), ('tasks', 'NNS'), (',', ','), ('such', 'JJ'), ('as', 'IN'), ('determining', 'VBG'), ('the', 'DT'), ('sentiment', 'NN'), ('polarity', 'NN'), ('of', 'IN'), ('a', 'DT'), ('document', 'NN'), ('or', 'CC'), ('the', 'DT'), ('part-of-speech', 'NN'), ('for', 'IN'), ('a', 'DT'), ('given', 'VBN'), ('word', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Unsupervised', 'learning', 'https', ':', '//www.lexalytics.com/', 'https', ':', '//www.lexalytics.com/', 'W', 'H', 'E', 'P', 'P', 'E', 'R', '7|', '|', 'Lexalytics', ',', 'Inc.', ',', '48', 'North', 'Pleasant', 'St.', 'Unit', '301', ',', 'Amherst', '01002', 'USA', '|', '1-800-377-8036', '|', 'www.lexalytics.com', 'H', 'P', 'P', 'E', 'R', 'B', 'H', 'E', 'Z', 'E', 'N', ':', 'H', 'E', 'R', 'E', 'E', 'L', ',', 'H', 'E', 'E', 'R', 'R', 'E', 'R', 'Machine', 'learning', 'models', 'good', 'performing', 'single', 'tasks', ',', 'determining', 'sentiment', 'polarity', 'document', 'part-of-speech', 'given', 'word', '.']

 TOTAL FILTERED TOKENS ==>  77

 ---- POST FOR FILTERED TOKENS ----

 [('Unsupervised', 'VBN'), ('learning', 'VBG'), ('https', 'NN'), (':', ':'), ('//www.lexalytics.com/', 'JJ'), ('https', 'NN'), (':', ':'), ('//www.lexalytics.com/', 'JJ'), ('W', 'NNP'), ('H', 'NNP'), ('E', 'NNP'), ('P', 'NNP'), ('P', 'NNP'), ('E', 'NNP'), ('R', 'NNP'), ('7|', 'CD'), ('|', 'NNP'), ('Lexalytics', 'NNP'), (',', ','), ('Inc.', 'NNP'), (',', ','), ('48', 'CD'), ('North', 'NNP'), ('Pleasant', 'NNP'), ('St.', 'NNP'), ('Unit', 'NNP'), ('301', 'CD'), (',', ','), ('Amherst', 'NNP'), ('01002', 'CD'), ('USA', 'NNP'), ('|', 'NNP'), ('1-800-377-8036', 'JJ'), ('|', 'NNP'), ('www.lexalytics.com', 'NN'), ('H', 'NNP'), ('P', 'NNP'), ('P', 'NNP'), ('E', 'NNP'), ('R', 'NNP'), ('B', 'NNP'), ('H', 'NNP'), ('E', 'NNP'), ('Z', 'NNP'), ('E', 'NNP'), ('N', 'NNP'), (':', ':'), ('H', 'NNP'), ('E', 'NNP'), ('R', 'NNP'), ('E', 'NNP'), ('E', 'NNP'), ('L', 'NNP'), (',', ','), ('H', 'NNP'), ('E', 'NNP'), ('E', 'NNP'), ('R', 'NNP'), ('R', 'NNP'), ('E', 'NNP'), ('R', 'NNP'), ('Machine', 'NNP'), ('learning', 'NN'), ('models', 'NNS'), ('good', 'JJ'), ('performing', 'VBG'), ('single', 'JJ'), ('tasks', 'NNS'), (',', ','), ('determining', 'VBG'), ('sentiment', 'NN'), ('polarity', 'NN'), ('document', 'NN'), ('part-of-speech', 'JJ'), ('given', 'VBN'), ('word', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Unsupervised learning', 'learning https', 'https :', ': //www.lexalytics.com/', '//www.lexalytics.com/ https', 'https :', ': //www.lexalytics.com/', '//www.lexalytics.com/ W', 'W H', 'H E', 'E P', 'P P', 'P E', 'E R', 'R 7|', '7| |', '| Lexalytics', 'Lexalytics ,', ', Inc.', 'Inc. ,', ', 48', '48 North', 'North Pleasant', 'Pleasant St.', 'St. Unit', 'Unit 301', '301 ,', ', Amherst', 'Amherst 01002', '01002 USA', 'USA |', '| 1-800-377-8036', '1-800-377-8036 |', '| www.lexalytics.com', 'www.lexalytics.com H', 'H P', 'P P', 'P E', 'E R', 'R B', 'B H', 'H E', 'E Z', 'Z E', 'E N', 'N :', ': H', 'H E', 'E R', 'R E', 'E E', 'E L', 'L ,', ', H', 'H E', 'E E', 'E R', 'R R', 'R E', 'E R', 'R Machine', 'Machine learning', 'learning models', 'models good', 'good performing', 'performing single', 'single tasks', 'tasks ,', ', determining', 'determining sentiment', 'sentiment polarity', 'polarity document', 'document part-of-speech', 'part-of-speech given', 'given word', 'word .'] 

 TOTAL BIGRAMS --> 76 



 ---- TRI-GRAMS ---- 

 ['Unsupervised learning https', 'learning https :', 'https : //www.lexalytics.com/', ': //www.lexalytics.com/ https', '//www.lexalytics.com/ https :', 'https : //www.lexalytics.com/', ': //www.lexalytics.com/ W', '//www.lexalytics.com/ W H', 'W H E', 'H E P', 'E P P', 'P P E', 'P E R', 'E R 7|', 'R 7| |', '7| | Lexalytics', '| Lexalytics ,', 'Lexalytics , Inc.', ', Inc. ,', 'Inc. , 48', ', 48 North', '48 North Pleasant', 'North Pleasant St.', 'Pleasant St. Unit', 'St. Unit 301', 'Unit 301 ,', '301 , Amherst', ', Amherst 01002', 'Amherst 01002 USA', '01002 USA |', 'USA | 1-800-377-8036', '| 1-800-377-8036 |', '1-800-377-8036 | www.lexalytics.com', '| www.lexalytics.com H', 'www.lexalytics.com H P', 'H P P', 'P P E', 'P E R', 'E R B', 'R B H', 'B H E', 'H E Z', 'E Z E', 'Z E N', 'E N :', 'N : H', ': H E', 'H E R', 'E R E', 'R E E', 'E E L', 'E L ,', 'L , H', ', H E', 'H E E', 'E E R', 'E R R', 'R R E', 'R E R', 'E R Machine', 'R Machine learning', 'Machine learning models', 'learning models good', 'models good performing', 'good performing single', 'performing single tasks', 'single tasks ,', 'tasks , determining', ', determining sentiment', 'determining sentiment polarity', 'sentiment polarity document', 'polarity document part-of-speech', 'document part-of-speech given', 'part-of-speech given word', 'given word .'] 

 TOTAL TRIGRAMS --> 75 



 ---- NOUN PHRASES ---- 

 ['https', ' https', 'www.lexalytics.com', 'learning', 'sentiment', 'polarity', 'document', 'word'] 

 TOTAL NOUN PHRASES --> 8 



 ---- NER ----

 
 ORGANIZATION ---> ['USA']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> ['H E']
 TOTAL PERSON ENTITY --> 1 


 GPE ---> ['North']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['unsupervis', 'learn', 'http', ':', '//www.lexalytics.com/', 'http', ':', '//www.lexalytics.com/', 'w', 'h', 'e', 'p', 'p', 'e', 'r', '7|', '|', 'lexalyt', ',', 'inc.', ',', '48', 'north', 'pleasant', 'st.', 'unit', '301', ',', 'amherst', '01002', 'usa', '|', '1-800-377-8036', '|', 'www.lexalytics.com', 'h', 'p', 'p', 'e', 'r', 'b', 'h', 'e', 'z', 'e', 'n', ':', 'h', 'e', 'r', 'e', 'e', 'l', ',', 'h', 'e', 'e', 'r', 'r', 'e', 'r', 'machin', 'learn', 'model', 'good', 'perform', 'singl', 'task', ',', 'determin', 'sentiment', 'polar', 'document', 'part-of-speech', 'given', 'word', '.']

 TOTAL PORTER STEM WORDS ==> 77



 ---- SNOWBALL STEMMING ----

['unsupervis', 'learn', 'https', ':', '//www.lexalytics.com/', 'https', ':', '//www.lexalytics.com/', 'w', 'h', 'e', 'p', 'p', 'e', 'r', '7|', '|', 'lexalyt', ',', 'inc.', ',', '48', 'north', 'pleasant', 'st.', 'unit', '301', ',', 'amherst', '01002', 'usa', '|', '1-800-377-8036', '|', 'www.lexalytics.com', 'h', 'p', 'p', 'e', 'r', 'b', 'h', 'e', 'z', 'e', 'n', ':', 'h', 'e', 'r', 'e', 'e', 'l', ',', 'h', 'e', 'e', 'r', 'r', 'e', 'r', 'machin', 'learn', 'model', 'good', 'perform', 'singl', 'task', ',', 'determin', 'sentiment', 'polar', 'document', 'part-of-speech', 'given', 'word', '.']

 TOTAL SNOWBALL STEM WORDS ==> 77



 ---- LEMMATIZATION ----

['Unsupervised', 'learning', 'http', ':', '//www.lexalytics.com/', 'http', ':', '//www.lexalytics.com/', 'W', 'H', 'E', 'P', 'P', 'E', 'R', '7|', '|', 'Lexalytics', ',', 'Inc.', ',', '48', 'North', 'Pleasant', 'St.', 'Unit', '301', ',', 'Amherst', '01002', 'USA', '|', '1-800-377-8036', '|', 'www.lexalytics.com', 'H', 'P', 'P', 'E', 'R', 'B', 'H', 'E', 'Z', 'E', 'N', ':', 'H', 'E', 'R', 'E', 'E', 'L', ',', 'H', 'E', 'E', 'R', 'R', 'E', 'R', 'Machine', 'learning', 'model', 'good', 'performing', 'single', 'task', ',', 'determining', 'sentiment', 'polarity', 'document', 'part-of-speech', 'given', 'word', '.']

 TOTAL LEMMATIZE WORDS ==> 77

************************************************************************************************************************

60 --> However, models are not good at tasks that require layers  of interpretation. 


 ---- TOKENS ----

 ['However', ',', 'models', 'are', 'not', 'good', 'at', 'tasks', 'that', 'require', 'layers', 'of', 'interpretation', '.'] 

 TOTAL TOKENS ==> 14

 ---- POST ----

 [('However', 'RB'), (',', ','), ('models', 'NNS'), ('are', 'VBP'), ('not', 'RB'), ('good', 'JJ'), ('at', 'IN'), ('tasks', 'NNS'), ('that', 'WDT'), ('require', 'VBP'), ('layers', 'NNS'), ('of', 'IN'), ('interpretation', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['However', ',', 'models', 'good', 'tasks', 'require', 'layers', 'interpretation', '.']

 TOTAL FILTERED TOKENS ==>  9

 ---- POST FOR FILTERED TOKENS ----

 [('However', 'RB'), (',', ','), ('models', 'NNS'), ('good', 'JJ'), ('tasks', 'NNS'), ('require', 'VBP'), ('layers', 'NNS'), ('interpretation', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['However ,', ', models', 'models good', 'good tasks', 'tasks require', 'require layers', 'layers interpretation', 'interpretation .'] 

 TOTAL BIGRAMS --> 8 



 ---- TRI-GRAMS ---- 

 ['However , models', ', models good', 'models good tasks', 'good tasks require', 'tasks require layers', 'require layers interpretation', 'layers interpretation .'] 

 TOTAL TRIGRAMS --> 7 



 ---- NOUN PHRASES ---- 

 ['interpretation'] 

 TOTAL NOUN PHRASES --> 1 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['howev', ',', 'model', 'good', 'task', 'requir', 'layer', 'interpret', '.']

 TOTAL PORTER STEM WORDS ==> 9



 ---- SNOWBALL STEMMING ----

['howev', ',', 'model', 'good', 'task', 'requir', 'layer', 'interpret', '.']

 TOTAL SNOWBALL STEM WORDS ==> 9



 ---- LEMMATIZATION ----

['However', ',', 'model', 'good', 'task', 'require', 'layer', 'interpretation', '.']

 TOTAL LEMMATIZE WORDS ==> 9

************************************************************************************************************************

61 --> Take the following sentence: “Lexalytics is the best text analytics company ever.”  Besides agreeing with its obvious truth, what might we want to know   about this sentence? 


 ---- TOKENS ----

 ['Take', 'the', 'following', 'sentence', ':', '“', 'Lexalytics', 'is', 'the', 'best', 'text', 'analytics', 'company', 'ever.', '”', 'Besides', 'agreeing', 'with', 'its', 'obvious', 'truth', ',', 'what', 'might', 'we', 'want', 'to', 'know', 'about', 'this', 'sentence', '?'] 

 TOTAL TOKENS ==> 32

 ---- POST ----

 [('Take', 'VB'), ('the', 'DT'), ('following', 'JJ'), ('sentence', 'NN'), (':', ':'), ('“', 'NN'), ('Lexalytics', 'NNP'), ('is', 'VBZ'), ('the', 'DT'), ('best', 'JJS'), ('text', 'NN'), ('analytics', 'NNS'), ('company', 'NN'), ('ever.', 'VBZ'), ('”', 'NNP'), ('Besides', 'NNP'), ('agreeing', 'VBG'), ('with', 'IN'), ('its', 'PRP$'), ('obvious', 'JJ'), ('truth', 'NN'), (',', ','), ('what', 'WP'), ('might', 'MD'), ('we', 'PRP'), ('want', 'VB'), ('to', 'TO'), ('know', 'VB'), ('about', 'IN'), ('this', 'DT'), ('sentence', 'NN'), ('?', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Take', 'following', 'sentence', ':', '“', 'Lexalytics', 'best', 'text', 'analytics', 'company', 'ever.', '”', 'Besides', 'agreeing', 'obvious', 'truth', ',', 'might', 'want', 'know', 'sentence', '?']

 TOTAL FILTERED TOKENS ==>  22

 ---- POST FOR FILTERED TOKENS ----

 [('Take', 'VB'), ('following', 'VBG'), ('sentence', 'NN'), (':', ':'), ('“', 'NN'), ('Lexalytics', 'NNP'), ('best', 'JJS'), ('text', 'NN'), ('analytics', 'NNS'), ('company', 'NN'), ('ever.', 'VBZ'), ('”', 'NNP'), ('Besides', 'NNP'), ('agreeing', 'VBG'), ('obvious', 'JJ'), ('truth', 'NN'), (',', ','), ('might', 'MD'), ('want', 'VB'), ('know', 'JJ'), ('sentence', 'NN'), ('?', '.')] 



 ---- BI-GRAMS ---- 

 ['Take following', 'following sentence', 'sentence :', ': “', '“ Lexalytics', 'Lexalytics best', 'best text', 'text analytics', 'analytics company', 'company ever.', 'ever. ”', '” Besides', 'Besides agreeing', 'agreeing obvious', 'obvious truth', 'truth ,', ', might', 'might want', 'want know', 'know sentence', 'sentence ?'] 

 TOTAL BIGRAMS --> 21 



 ---- TRI-GRAMS ---- 

 ['Take following sentence', 'following sentence :', 'sentence : “', ': “ Lexalytics', '“ Lexalytics best', 'Lexalytics best text', 'best text analytics', 'text analytics company', 'analytics company ever.', 'company ever. ”', 'ever. ” Besides', '” Besides agreeing', 'Besides agreeing obvious', 'agreeing obvious truth', 'obvious truth ,', 'truth , might', ', might want', 'might want know', 'want know sentence', 'know sentence ?'] 

 TOTAL TRIGRAMS --> 20 



 ---- NOUN PHRASES ---- 

 ['sentence', '“', 'text', 'company', 'obvious truth', 'know sentence'] 

 TOTAL NOUN PHRASES --> 6 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['take', 'follow', 'sentenc', ':', '“', 'lexalyt', 'best', 'text', 'analyt', 'compani', 'ever.', '”', 'besid', 'agre', 'obviou', 'truth', ',', 'might', 'want', 'know', 'sentenc', '?']

 TOTAL PORTER STEM WORDS ==> 22



 ---- SNOWBALL STEMMING ----

['take', 'follow', 'sentenc', ':', '“', 'lexalyt', 'best', 'text', 'analyt', 'compani', 'ever.', '”', 'besid', 'agre', 'obvious', 'truth', ',', 'might', 'want', 'know', 'sentenc', '?']

 TOTAL SNOWBALL STEM WORDS ==> 22



 ---- LEMMATIZATION ----

['Take', 'following', 'sentence', ':', '“', 'Lexalytics', 'best', 'text', 'analytics', 'company', 'ever.', '”', 'Besides', 'agreeing', 'obvious', 'truth', ',', 'might', 'want', 'know', 'sentence', '?']

 TOTAL LEMMATIZE WORDS ==> 22

************************************************************************************************************************

62 --> First, we want to know whether it contains any   entities (companies, people, products and so on). 


 ---- TOKENS ----

 ['First', ',', 'we', 'want', 'to', 'know', 'whether', 'it', 'contains', 'any', 'entities', '(', 'companies', ',', 'people', ',', 'products', 'and', 'so', 'on', ')', '.'] 

 TOTAL TOKENS ==> 22

 ---- POST ----

 [('First', 'RB'), (',', ','), ('we', 'PRP'), ('want', 'VBP'), ('to', 'TO'), ('know', 'VB'), ('whether', 'IN'), ('it', 'PRP'), ('contains', 'VBZ'), ('any', 'DT'), ('entities', 'NNS'), ('(', '('), ('companies', 'NNS'), (',', ','), ('people', 'NNS'), (',', ','), ('products', 'NNS'), ('and', 'CC'), ('so', 'RB'), ('on', 'IN'), (')', ')'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['First', ',', 'want', 'know', 'whether', 'contains', 'entities', '(', 'companies', ',', 'people', ',', 'products', ')', '.']

 TOTAL FILTERED TOKENS ==>  15

 ---- POST FOR FILTERED TOKENS ----

 [('First', 'RB'), (',', ','), ('want', 'VBP'), ('know', 'VBP'), ('whether', 'IN'), ('contains', 'NNS'), ('entities', 'NNS'), ('(', '('), ('companies', 'NNS'), (',', ','), ('people', 'NNS'), (',', ','), ('products', 'NNS'), (')', ')'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['First ,', ', want', 'want know', 'know whether', 'whether contains', 'contains entities', 'entities (', '( companies', 'companies ,', ', people', 'people ,', ', products', 'products )', ') .'] 

 TOTAL BIGRAMS --> 14 



 ---- TRI-GRAMS ---- 

 ['First , want', ', want know', 'want know whether', 'know whether contains', 'whether contains entities', 'contains entities (', 'entities ( companies', '( companies ,', 'companies , people', ', people ,', 'people , products', ', products )', 'products ) .'] 

 TOTAL TRIGRAMS --> 13 



 ---- NOUN PHRASES ---- 

 [] 

 TOTAL NOUN PHRASES --> 0 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['first', ',', 'want', 'know', 'whether', 'contain', 'entiti', '(', 'compani', ',', 'peopl', ',', 'product', ')', '.']

 TOTAL PORTER STEM WORDS ==> 15



 ---- SNOWBALL STEMMING ----

['first', ',', 'want', 'know', 'whether', 'contain', 'entiti', '(', 'compani', ',', 'peopl', ',', 'product', ')', '.']

 TOTAL SNOWBALL STEM WORDS ==> 15



 ---- LEMMATIZATION ----

['First', ',', 'want', 'know', 'whether', 'contains', 'entity', '(', 'company', ',', 'people', ',', 'product', ')', '.']

 TOTAL LEMMATIZE WORDS ==> 15

************************************************************************************************************************

63 --> Second, we want to   know whether there’s any sentiment associated with those entities. 


 ---- TOKENS ----

 ['Second', ',', 'we', 'want', 'to', 'know', 'whether', 'there', '’', 's', 'any', 'sentiment', 'associated', 'with', 'those', 'entities', '.'] 

 TOTAL TOKENS ==> 17

 ---- POST ----

 [('Second', 'JJ'), (',', ','), ('we', 'PRP'), ('want', 'VBP'), ('to', 'TO'), ('know', 'VB'), ('whether', 'IN'), ('there', 'EX'), ('’', 'NNP'), ('s', 'VBD'), ('any', 'DT'), ('sentiment', 'NN'), ('associated', 'VBN'), ('with', 'IN'), ('those', 'DT'), ('entities', 'NNS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Second', ',', 'want', 'know', 'whether', '’', 'sentiment', 'associated', 'entities', '.']

 TOTAL FILTERED TOKENS ==>  10

 ---- POST FOR FILTERED TOKENS ----

 [('Second', 'JJ'), (',', ','), ('want', 'VBP'), ('know', 'VBP'), ('whether', 'IN'), ('’', 'JJ'), ('sentiment', 'NN'), ('associated', 'VBN'), ('entities', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Second ,', ', want', 'want know', 'know whether', 'whether ’', '’ sentiment', 'sentiment associated', 'associated entities', 'entities .'] 

 TOTAL BIGRAMS --> 9 



 ---- TRI-GRAMS ---- 

 ['Second , want', ', want know', 'want know whether', 'know whether ’', 'whether ’ sentiment', '’ sentiment associated', 'sentiment associated entities', 'associated entities .'] 

 TOTAL TRIGRAMS --> 8 



 ---- NOUN PHRASES ---- 

 ['’ sentiment'] 

 TOTAL NOUN PHRASES --> 1 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> ['Second']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['second', ',', 'want', 'know', 'whether', '’', 'sentiment', 'associ', 'entiti', '.']

 TOTAL PORTER STEM WORDS ==> 10



 ---- SNOWBALL STEMMING ----

['second', ',', 'want', 'know', 'whether', '’', 'sentiment', 'associ', 'entiti', '.']

 TOTAL SNOWBALL STEM WORDS ==> 10



 ---- LEMMATIZATION ----

['Second', ',', 'want', 'know', 'whether', '’', 'sentiment', 'associated', 'entity', '.']

 TOTAL LEMMATIZE WORDS ==> 10

************************************************************************************************************************

64 --> Third, we want to know whether a particular industry is being discussed. 


 ---- TOKENS ----

 ['Third', ',', 'we', 'want', 'to', 'know', 'whether', 'a', 'particular', 'industry', 'is', 'being', 'discussed', '.'] 

 TOTAL TOKENS ==> 14

 ---- POST ----

 [('Third', 'NNP'), (',', ','), ('we', 'PRP'), ('want', 'VBP'), ('to', 'TO'), ('know', 'VB'), ('whether', 'IN'), ('a', 'DT'), ('particular', 'JJ'), ('industry', 'NN'), ('is', 'VBZ'), ('being', 'VBG'), ('discussed', 'VBN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Third', ',', 'want', 'know', 'whether', 'particular', 'industry', 'discussed', '.']

 TOTAL FILTERED TOKENS ==>  9

 ---- POST FOR FILTERED TOKENS ----

 [('Third', 'NNP'), (',', ','), ('want', 'VBP'), ('know', 'VBP'), ('whether', 'IN'), ('particular', 'JJ'), ('industry', 'NN'), ('discussed', 'VBD'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Third ,', ', want', 'want know', 'know whether', 'whether particular', 'particular industry', 'industry discussed', 'discussed .'] 

 TOTAL BIGRAMS --> 8 



 ---- TRI-GRAMS ---- 

 ['Third , want', ', want know', 'want know whether', 'know whether particular', 'whether particular industry', 'particular industry discussed', 'industry discussed .'] 

 TOTAL TRIGRAMS --> 7 



 ---- NOUN PHRASES ---- 

 ['particular industry'] 

 TOTAL NOUN PHRASES --> 1 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> ['Third']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['third', ',', 'want', 'know', 'whether', 'particular', 'industri', 'discuss', '.']

 TOTAL PORTER STEM WORDS ==> 9



 ---- SNOWBALL STEMMING ----

['third', ',', 'want', 'know', 'whether', 'particular', 'industri', 'discuss', '.']

 TOTAL SNOWBALL STEM WORDS ==> 9



 ---- LEMMATIZATION ----

['Third', ',', 'want', 'know', 'whether', 'particular', 'industry', 'discussed', '.']

 TOTAL LEMMATIZE WORDS ==> 9

************************************************************************************************************************

65 --> Finally, we might ask whether any sentiment is being expressed   towards that industry. 


 ---- TOKENS ----

 ['Finally', ',', 'we', 'might', 'ask', 'whether', 'any', 'sentiment', 'is', 'being', 'expressed', 'towards', 'that', 'industry', '.'] 

 TOTAL TOKENS ==> 15

 ---- POST ----

 [('Finally', 'RB'), (',', ','), ('we', 'PRP'), ('might', 'MD'), ('ask', 'VB'), ('whether', 'IN'), ('any', 'DT'), ('sentiment', 'NN'), ('is', 'VBZ'), ('being', 'VBG'), ('expressed', 'VBN'), ('towards', 'NNS'), ('that', 'IN'), ('industry', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Finally', ',', 'might', 'ask', 'whether', 'sentiment', 'expressed', 'towards', 'industry', '.']

 TOTAL FILTERED TOKENS ==>  10

 ---- POST FOR FILTERED TOKENS ----

 [('Finally', 'RB'), (',', ','), ('might', 'MD'), ('ask', 'VB'), ('whether', 'IN'), ('sentiment', 'NN'), ('expressed', 'VBN'), ('towards', 'NNS'), ('industry', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Finally ,', ', might', 'might ask', 'ask whether', 'whether sentiment', 'sentiment expressed', 'expressed towards', 'towards industry', 'industry .'] 

 TOTAL BIGRAMS --> 9 



 ---- TRI-GRAMS ---- 

 ['Finally , might', ', might ask', 'might ask whether', 'ask whether sentiment', 'whether sentiment expressed', 'sentiment expressed towards', 'expressed towards industry', 'towards industry .'] 

 TOTAL TRIGRAMS --> 8 



 ---- NOUN PHRASES ---- 

 ['sentiment', 'industry'] 

 TOTAL NOUN PHRASES --> 2 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['final', ',', 'might', 'ask', 'whether', 'sentiment', 'express', 'toward', 'industri', '.']

 TOTAL PORTER STEM WORDS ==> 10



 ---- SNOWBALL STEMMING ----

['final', ',', 'might', 'ask', 'whether', 'sentiment', 'express', 'toward', 'industri', '.']

 TOTAL SNOWBALL STEM WORDS ==> 10



 ---- LEMMATIZATION ----

['Finally', ',', 'might', 'ask', 'whether', 'sentiment', 'expressed', 'towards', 'industry', '.']

 TOTAL LEMMATIZE WORDS ==> 10

************************************************************************************************************************

66 --> One single machine learning model can’t do all of that. 


 ---- TOKENS ----

 ['One', 'single', 'machine', 'learning', 'model', 'can', '’', 't', 'do', 'all', 'of', 'that', '.'] 

 TOTAL TOKENS ==> 13

 ---- POST ----

 [('One', 'CD'), ('single', 'JJ'), ('machine', 'NN'), ('learning', 'VBG'), ('model', 'NN'), ('can', 'MD'), ('’', 'VB'), ('t', 'VB'), ('do', 'VBP'), ('all', 'DT'), ('of', 'IN'), ('that', 'DT'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['One', 'single', 'machine', 'learning', 'model', '’', '.']

 TOTAL FILTERED TOKENS ==>  7

 ---- POST FOR FILTERED TOKENS ----

 [('One', 'CD'), ('single', 'JJ'), ('machine', 'NN'), ('learning', 'VBG'), ('model', 'NN'), ('’', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['One single', 'single machine', 'machine learning', 'learning model', 'model ’', '’ .'] 

 TOTAL BIGRAMS --> 6 



 ---- TRI-GRAMS ---- 

 ['One single machine', 'single machine learning', 'machine learning model', 'learning model ’', 'model ’ .'] 

 TOTAL TRIGRAMS --> 5 



 ---- NOUN PHRASES ---- 

 ['single machine', 'model', '’'] 

 TOTAL NOUN PHRASES --> 3 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['one', 'singl', 'machin', 'learn', 'model', '’', '.']

 TOTAL PORTER STEM WORDS ==> 7



 ---- SNOWBALL STEMMING ----

['one', 'singl', 'machin', 'learn', 'model', '’', '.']

 TOTAL SNOWBALL STEM WORDS ==> 7



 ---- LEMMATIZATION ----

['One', 'single', 'machine', 'learning', 'model', '’', '.']

 TOTAL LEMMATIZE WORDS ==> 7

************************************************************************************************************************

67 --> You’ll need at least  four separate models:  Identify and name any entities (Lexalytics)  Determine the sentiment associated with that entity (positive)  Industry classification (text analytics)  Industry sentiment (neutral) If you just train a single model, you can only solve #1 or #3. 


 ---- TOKENS ----

 ['You', '’', 'll', 'need', 'at', 'least', 'four', 'separate', 'models', ':', 'Identify', 'and', 'name', 'any', 'entities', '(', 'Lexalytics', ')', 'Determine', 'the', 'sentiment', 'associated', 'with', 'that', 'entity', '(', 'positive', ')', 'Industry', 'classification', '(', 'text', 'analytics', ')', 'Industry', 'sentiment', '(', 'neutral', ')', 'If', 'you', 'just', 'train', 'a', 'single', 'model', ',', 'you', 'can', 'only', 'solve', '#', '1', 'or', '#', '3', '.'] 

 TOTAL TOKENS ==> 57

 ---- POST ----

 [('You', 'PRP'), ('’', 'VBP'), ('ll', 'JJ'), ('need', 'VBP'), ('at', 'IN'), ('least', 'JJS'), ('four', 'CD'), ('separate', 'JJ'), ('models', 'NNS'), (':', ':'), ('Identify', 'NNP'), ('and', 'CC'), ('name', 'NN'), ('any', 'DT'), ('entities', 'NNS'), ('(', '('), ('Lexalytics', 'NNPS'), (')', ')'), ('Determine', 'VBP'), ('the', 'DT'), ('sentiment', 'NN'), ('associated', 'VBN'), ('with', 'IN'), ('that', 'DT'), ('entity', 'NN'), ('(', '('), ('positive', 'JJ'), (')', ')'), ('Industry', 'NN'), ('classification', 'NN'), ('(', '('), ('text', 'JJ'), ('analytics', 'NNS'), (')', ')'), ('Industry', 'NNP'), ('sentiment', 'NN'), ('(', '('), ('neutral', 'JJ'), (')', ')'), ('If', 'IN'), ('you', 'PRP'), ('just', 'RB'), ('train', 'VB'), ('a', 'DT'), ('single', 'JJ'), ('model', 'NN'), (',', ','), ('you', 'PRP'), ('can', 'MD'), ('only', 'RB'), ('solve', 'VB'), ('#', '#'), ('1', 'CD'), ('or', 'CC'), ('#', '#'), ('3', 'CD'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['’', 'need', 'least', 'four', 'separate', 'models', ':', 'Identify', 'name', 'entities', '(', 'Lexalytics', ')', 'Determine', 'sentiment', 'associated', 'entity', '(', 'positive', ')', 'Industry', 'classification', '(', 'text', 'analytics', ')', 'Industry', 'sentiment', '(', 'neutral', ')', 'train', 'single', 'model', ',', 'solve', '#', '1', '#', '3', '.']

 TOTAL FILTERED TOKENS ==>  41

 ---- POST FOR FILTERED TOKENS ----

 [('’', 'NNS'), ('need', 'VBP'), ('least', 'JJS'), ('four', 'CD'), ('separate', 'JJ'), ('models', 'NNS'), (':', ':'), ('Identify', 'NNP'), ('name', 'NN'), ('entities', 'NNS'), ('(', '('), ('Lexalytics', 'NNPS'), (')', ')'), ('Determine', 'NNP'), ('sentiment', 'NN'), ('associated', 'VBN'), ('entity', 'NN'), ('(', '('), ('positive', 'JJ'), (')', ')'), ('Industry', 'NN'), ('classification', 'NN'), ('(', '('), ('text', 'JJ'), ('analytics', 'NNS'), (')', ')'), ('Industry', 'NNP'), ('sentiment', 'NN'), ('(', '('), ('neutral', 'JJ'), (')', ')'), ('train', 'VBP'), ('single', 'JJ'), ('model', 'NN'), (',', ','), ('solve', 'VBP'), ('#', '#'), ('1', 'CD'), ('#', '#'), ('3', 'CD'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['’ need', 'need least', 'least four', 'four separate', 'separate models', 'models :', ': Identify', 'Identify name', 'name entities', 'entities (', '( Lexalytics', 'Lexalytics )', ') Determine', 'Determine sentiment', 'sentiment associated', 'associated entity', 'entity (', '( positive', 'positive )', ') Industry', 'Industry classification', 'classification (', '( text', 'text analytics', 'analytics )', ') Industry', 'Industry sentiment', 'sentiment (', '( neutral', 'neutral )', ') train', 'train single', 'single model', 'model ,', ', solve', 'solve #', '# 1', '1 #', '# 3', '3 .'] 

 TOTAL BIGRAMS --> 40 



 ---- TRI-GRAMS ---- 

 ['’ need least', 'need least four', 'least four separate', 'four separate models', 'separate models :', 'models : Identify', ': Identify name', 'Identify name entities', 'name entities (', 'entities ( Lexalytics', '( Lexalytics )', 'Lexalytics ) Determine', ') Determine sentiment', 'Determine sentiment associated', 'sentiment associated entity', 'associated entity (', 'entity ( positive', '( positive )', 'positive ) Industry', ') Industry classification', 'Industry classification (', 'classification ( text', '( text analytics', 'text analytics )', 'analytics ) Industry', ') Industry sentiment', 'Industry sentiment (', 'sentiment ( neutral', '( neutral )', 'neutral ) train', ') train single', 'train single model', 'single model ,', 'model , solve', ', solve #', 'solve # 1', '# 1 #', '1 # 3', '# 3 .'] 

 TOTAL TRIGRAMS --> 39 



 ---- NOUN PHRASES ---- 

 ['name', 'sentiment', 'entity', 'Industry', 'classification', 'sentiment', 'single model'] 

 TOTAL NOUN PHRASES --> 7 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> ['Determine']
 TOTAL PERSON ENTITY --> 1 


 GPE ---> ['Industry', 'Industry']
 TOTAL GPE ENTITY --> 2 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['’', 'need', 'least', 'four', 'separ', 'model', ':', 'identifi', 'name', 'entiti', '(', 'lexalyt', ')', 'determin', 'sentiment', 'associ', 'entiti', '(', 'posit', ')', 'industri', 'classif', '(', 'text', 'analyt', ')', 'industri', 'sentiment', '(', 'neutral', ')', 'train', 'singl', 'model', ',', 'solv', '#', '1', '#', '3', '.']

 TOTAL PORTER STEM WORDS ==> 41



 ---- SNOWBALL STEMMING ----

['’', 'need', 'least', 'four', 'separ', 'model', ':', 'identifi', 'name', 'entiti', '(', 'lexalyt', ')', 'determin', 'sentiment', 'associ', 'entiti', '(', 'posit', ')', 'industri', 'classif', '(', 'text', 'analyt', ')', 'industri', 'sentiment', '(', 'neutral', ')', 'train', 'singl', 'model', ',', 'solv', '#', '1', '#', '3', '.']

 TOTAL SNOWBALL STEM WORDS ==> 41



 ---- LEMMATIZATION ----

['’', 'need', 'least', 'four', 'separate', 'model', ':', 'Identify', 'name', 'entity', '(', 'Lexalytics', ')', 'Determine', 'sentiment', 'associated', 'entity', '(', 'positive', ')', 'Industry', 'classification', '(', 'text', 'analytics', ')', 'Industry', 'sentiment', '(', 'neutral', ')', 'train', 'single', 'model', ',', 'solve', '#', '1', '#', '3', '.']

 TOTAL LEMMATIZE WORDS ==> 41

************************************************************************************************************************

68 --> Calculating the  sentiment needed for #2 or #4 requires first knowing which entity you’re  trying to associate the sentiment with. 


 ---- TOKENS ----

 ['Calculating', 'the', 'sentiment', 'needed', 'for', '#', '2', 'or', '#', '4', 'requires', 'first', 'knowing', 'which', 'entity', 'you', '’', 're', 'trying', 'to', 'associate', 'the', 'sentiment', 'with', '.'] 

 TOTAL TOKENS ==> 25

 ---- POST ----

 [('Calculating', 'VBG'), ('the', 'DT'), ('sentiment', 'NN'), ('needed', 'VBN'), ('for', 'IN'), ('#', '#'), ('2', 'CD'), ('or', 'CC'), ('#', '#'), ('4', 'CD'), ('requires', 'VBZ'), ('first', 'JJ'), ('knowing', 'VBG'), ('which', 'WDT'), ('entity', 'NN'), ('you', 'PRP'), ('’', 'VBP'), ('re', 'VB'), ('trying', 'VBG'), ('to', 'TO'), ('associate', 'VB'), ('the', 'DT'), ('sentiment', 'NN'), ('with', 'IN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Calculating', 'sentiment', 'needed', '#', '2', '#', '4', 'requires', 'first', 'knowing', 'entity', '’', 'trying', 'associate', 'sentiment', '.']

 TOTAL FILTERED TOKENS ==>  16

 ---- POST FOR FILTERED TOKENS ----

 [('Calculating', 'VBG'), ('sentiment', 'NN'), ('needed', 'VBD'), ('#', '#'), ('2', 'CD'), ('#', '#'), ('4', 'CD'), ('requires', 'VBZ'), ('first', 'JJ'), ('knowing', 'VBG'), ('entity', 'NN'), ('’', 'NNP'), ('trying', 'VBG'), ('associate', 'JJ'), ('sentiment', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Calculating sentiment', 'sentiment needed', 'needed #', '# 2', '2 #', '# 4', '4 requires', 'requires first', 'first knowing', 'knowing entity', 'entity ’', '’ trying', 'trying associate', 'associate sentiment', 'sentiment .'] 

 TOTAL BIGRAMS --> 15 



 ---- TRI-GRAMS ---- 

 ['Calculating sentiment needed', 'sentiment needed #', 'needed # 2', '# 2 #', '2 # 4', '# 4 requires', '4 requires first', 'requires first knowing', 'first knowing entity', 'knowing entity ’', 'entity ’ trying', '’ trying associate', 'trying associate sentiment', 'associate sentiment .'] 

 TOTAL TRIGRAMS --> 14 



 ---- NOUN PHRASES ---- 

 ['sentiment', 'entity', 'associate sentiment'] 

 TOTAL NOUN PHRASES --> 3 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['calcul', 'sentiment', 'need', '#', '2', '#', '4', 'requir', 'first', 'know', 'entiti', '’', 'tri', 'associ', 'sentiment', '.']

 TOTAL PORTER STEM WORDS ==> 16



 ---- SNOWBALL STEMMING ----

['calcul', 'sentiment', 'need', '#', '2', '#', '4', 'requir', 'first', 'know', 'entiti', '’', 'tri', 'associ', 'sentiment', '.']

 TOTAL SNOWBALL STEM WORDS ==> 16



 ---- LEMMATIZATION ----

['Calculating', 'sentiment', 'needed', '#', '2', '#', '4', 'requires', 'first', 'knowing', 'entity', '’', 'trying', 'associate', 'sentiment', '.']

 TOTAL LEMMATIZE WORDS ==> 16

************************************************************************************************************************

69 --> If you only have a single model for  sentiment, you’ll end up rating the whole sentence as positive. 


 ---- TOKENS ----

 ['If', 'you', 'only', 'have', 'a', 'single', 'model', 'for', 'sentiment', ',', 'you', '’', 'll', 'end', 'up', 'rating', 'the', 'whole', 'sentence', 'as', 'positive', '.'] 

 TOTAL TOKENS ==> 22

 ---- POST ----

 [('If', 'IN'), ('you', 'PRP'), ('only', 'RB'), ('have', 'VBP'), ('a', 'DT'), ('single', 'JJ'), ('model', 'NN'), ('for', 'IN'), ('sentiment', 'NN'), (',', ','), ('you', 'PRP'), ('’', 'VBP'), ('ll', 'JJ'), ('end', 'VBP'), ('up', 'RP'), ('rating', 'NN'), ('the', 'DT'), ('whole', 'JJ'), ('sentence', 'NN'), ('as', 'IN'), ('positive', 'JJ'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['single', 'model', 'sentiment', ',', '’', 'end', 'rating', 'whole', 'sentence', 'positive', '.']

 TOTAL FILTERED TOKENS ==>  11

 ---- POST FOR FILTERED TOKENS ----

 [('single', 'JJ'), ('model', 'NN'), ('sentiment', 'NN'), (',', ','), ('’', 'JJ'), ('end', 'NN'), ('rating', 'NN'), ('whole', 'JJ'), ('sentence', 'NN'), ('positive', 'JJ'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['single model', 'model sentiment', 'sentiment ,', ', ’', '’ end', 'end rating', 'rating whole', 'whole sentence', 'sentence positive', 'positive .'] 

 TOTAL BIGRAMS --> 10 



 ---- TRI-GRAMS ---- 

 ['single model sentiment', 'model sentiment ,', 'sentiment , ’', ', ’ end', '’ end rating', 'end rating whole', 'rating whole sentence', 'whole sentence positive', 'sentence positive .'] 

 TOTAL TRIGRAMS --> 9 



 ---- NOUN PHRASES ---- 

 ['single model', 'sentiment', '’ end', 'rating', 'whole sentence'] 

 TOTAL NOUN PHRASES --> 5 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['singl', 'model', 'sentiment', ',', '’', 'end', 'rate', 'whole', 'sentenc', 'posit', '.']

 TOTAL PORTER STEM WORDS ==> 11



 ---- SNOWBALL STEMMING ----

['singl', 'model', 'sentiment', ',', '’', 'end', 'rate', 'whole', 'sentenc', 'posit', '.']

 TOTAL SNOWBALL STEM WORDS ==> 11



 ---- LEMMATIZATION ----

['single', 'model', 'sentiment', ',', '’', 'end', 'rating', 'whole', 'sentence', 'positive', '.']

 TOTAL LEMMATIZE WORDS ==> 11

************************************************************************************************************************

70 --> Additionally,  if you’re only using keywords to look for the term “text analytics,” you’ll rate  this sentence as positive for that phrase, which isn’t true. 


 ---- TOKENS ----

 ['Additionally', ',', 'if', 'you', '’', 're', 'only', 'using', 'keywords', 'to', 'look', 'for', 'the', 'term', '“', 'text', 'analytics', ',', '”', 'you', '’', 'll', 'rate', 'this', 'sentence', 'as', 'positive', 'for', 'that', 'phrase', ',', 'which', 'isn', '’', 't', 'true', '.'] 

 TOTAL TOKENS ==> 37

 ---- POST ----

 [('Additionally', 'RB'), (',', ','), ('if', 'IN'), ('you', 'PRP'), ('’', 'VBP'), ('re', 'VBG'), ('only', 'RB'), ('using', 'VBG'), ('keywords', 'NNS'), ('to', 'TO'), ('look', 'VB'), ('for', 'IN'), ('the', 'DT'), ('term', 'NN'), ('“', 'NNP'), ('text', 'NN'), ('analytics', 'NNS'), (',', ','), ('”', 'NNP'), ('you', 'PRP'), ('’', 'VBP'), ('ll', 'JJ'), ('rate', 'NN'), ('this', 'DT'), ('sentence', 'NN'), ('as', 'IN'), ('positive', 'JJ'), ('for', 'IN'), ('that', 'DT'), ('phrase', 'NN'), (',', ','), ('which', 'WDT'), ('isn', 'VBP'), ('’', 'JJ'), ('t', 'NN'), ('true', 'JJ'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Additionally', ',', '’', 'using', 'keywords', 'look', 'term', '“', 'text', 'analytics', ',', '”', '’', 'rate', 'sentence', 'positive', 'phrase', ',', '’', 'true', '.']

 TOTAL FILTERED TOKENS ==>  21

 ---- POST FOR FILTERED TOKENS ----

 [('Additionally', 'RB'), (',', ','), ('’', 'NNP'), ('using', 'VBG'), ('keywords', 'NNS'), ('look', 'VBP'), ('term', 'NN'), ('“', 'NNP'), ('text', 'NN'), ('analytics', 'NNS'), (',', ','), ('”', 'JJ'), ('’', 'NN'), ('rate', 'NN'), ('sentence', 'NN'), ('positive', 'JJ'), ('phrase', 'NN'), (',', ','), ('’', 'NNP'), ('true', 'JJ'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Additionally ,', ', ’', '’ using', 'using keywords', 'keywords look', 'look term', 'term “', '“ text', 'text analytics', 'analytics ,', ', ”', '” ’', '’ rate', 'rate sentence', 'sentence positive', 'positive phrase', 'phrase ,', ', ’', '’ true', 'true .'] 

 TOTAL BIGRAMS --> 20 



 ---- TRI-GRAMS ---- 

 ['Additionally , ’', ', ’ using', '’ using keywords', 'using keywords look', 'keywords look term', 'look term “', 'term “ text', '“ text analytics', 'text analytics ,', 'analytics , ”', ', ” ’', '” ’ rate', '’ rate sentence', 'rate sentence positive', 'sentence positive phrase', 'positive phrase ,', 'phrase , ’', ', ’ true', '’ true .'] 

 TOTAL TRIGRAMS --> 19 



 ---- NOUN PHRASES ---- 

 ['term', 'text', '” ’', 'rate', 'sentence', 'positive phrase'] 

 TOTAL NOUN PHRASES --> 6 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['addit', ',', '’', 'use', 'keyword', 'look', 'term', '“', 'text', 'analyt', ',', '”', '’', 'rate', 'sentenc', 'posit', 'phrase', ',', '’', 'true', '.']

 TOTAL PORTER STEM WORDS ==> 21



 ---- SNOWBALL STEMMING ----

['addit', ',', '’', 'use', 'keyword', 'look', 'term', '“', 'text', 'analyt', ',', '”', '’', 'rate', 'sentenc', 'posit', 'phrase', ',', '’', 'true', '.']

 TOTAL SNOWBALL STEM WORDS ==> 21



 ---- LEMMATIZATION ----

['Additionally', ',', '’', 'using', 'keywords', 'look', 'term', '“', 'text', 'analytics', ',', '”', '’', 'rate', 'sentence', 'positive', 'phrase', ',', '’', 'true', '.']

 TOTAL LEMMATIZE WORDS ==> 21

************************************************************************************************************************

71 --> Depending on what’s optimal for  the language, each of these steps is  machine learning or NLP code. 


 ---- TOKENS ----

 ['Depending', 'on', 'what', '’', 's', 'optimal', 'for', 'the', 'language', ',', 'each', 'of', 'these', 'steps', 'is', 'machine', 'learning', 'or', 'NLP', 'code', '.'] 

 TOTAL TOKENS ==> 21

 ---- POST ----

 [('Depending', 'VBG'), ('on', 'IN'), ('what', 'WP'), ('’', 'NNP'), ('s', 'VBD'), ('optimal', 'JJ'), ('for', 'IN'), ('the', 'DT'), ('language', 'NN'), (',', ','), ('each', 'DT'), ('of', 'IN'), ('these', 'DT'), ('steps', 'NNS'), ('is', 'VBZ'), ('machine', 'NN'), ('learning', 'VBG'), ('or', 'CC'), ('NLP', 'NNP'), ('code', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Depending', '’', 'optimal', 'language', ',', 'steps', 'machine', 'learning', 'NLP', 'code', '.']

 TOTAL FILTERED TOKENS ==>  11

 ---- POST FOR FILTERED TOKENS ----

 [('Depending', 'VBG'), ('’', 'NNP'), ('optimal', 'JJ'), ('language', 'NN'), (',', ','), ('steps', 'NNS'), ('machine', 'NN'), ('learning', 'VBG'), ('NLP', 'NNP'), ('code', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Depending ’', '’ optimal', 'optimal language', 'language ,', ', steps', 'steps machine', 'machine learning', 'learning NLP', 'NLP code', 'code .'] 

 TOTAL BIGRAMS --> 10 



 ---- TRI-GRAMS ---- 

 ['Depending ’ optimal', '’ optimal language', 'optimal language ,', 'language , steps', ', steps machine', 'steps machine learning', 'machine learning NLP', 'learning NLP code', 'NLP code .'] 

 TOTAL TRIGRAMS --> 9 



 ---- NOUN PHRASES ---- 

 ['optimal language', 'machine', 'code'] 

 TOTAL NOUN PHRASES --> 3 



 ---- NER ----

 
 ORGANIZATION ---> ['NLP']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['depend', '’', 'optim', 'languag', ',', 'step', 'machin', 'learn', 'nlp', 'code', '.']

 TOTAL PORTER STEM WORDS ==> 11



 ---- SNOWBALL STEMMING ----

['depend', '’', 'optim', 'languag', ',', 'step', 'machin', 'learn', 'nlp', 'code', '.']

 TOTAL SNOWBALL STEM WORDS ==> 11



 ---- LEMMATIZATION ----

['Depending', '’', 'optimal', 'language', ',', 'step', 'machine', 'learning', 'NLP', 'code', '.']

 TOTAL LEMMATIZE WORDS ==> 11

************************************************************************************************************************

72 --> TOKENS PHRASES SYNTAX  TREES SENTENCES text  parsing (NLP) PARTS  OF SPEECH SEMANTIC  RELATIONSHIPS https://www.lexalytics.com/ https://www.lexalytics.com/  W H I T E  P A P E R 8|       |   Lexalytics, Inc., 48 North Pleasant St. Unit 301, Amherst MA 01002 USA   |   1-800-377-8036 |   www.lexalytics.com Not only do you need at least four models to solve this task, but these  models are interdependent and have to interact with each other. 


 ---- TOKENS ----

 ['TOKENS', 'PHRASES', 'SYNTAX', 'TREES', 'SENTENCES', 'text', 'parsing', '(', 'NLP', ')', 'PARTS', 'OF', 'SPEECH', 'SEMANTIC', 'RELATIONSHIPS', 'https', ':', '//www.lexalytics.com/', 'https', ':', '//www.lexalytics.com/', 'W', 'H', 'I', 'T', 'E', 'P', 'A', 'P', 'E', 'R', '8|', '|', 'Lexalytics', ',', 'Inc.', ',', '48', 'North', 'Pleasant', 'St.', 'Unit', '301', ',', 'Amherst', 'MA', '01002', 'USA', '|', '1-800-377-8036', '|', 'www.lexalytics.com', 'Not', 'only', 'do', 'you', 'need', 'at', 'least', 'four', 'models', 'to', 'solve', 'this', 'task', ',', 'but', 'these', 'models', 'are', 'interdependent', 'and', 'have', 'to', 'interact', 'with', 'each', 'other', '.'] 

 TOTAL TOKENS ==> 79

 ---- POST ----

 [('TOKENS', 'NNP'), ('PHRASES', 'NNP'), ('SYNTAX', 'NNP'), ('TREES', 'NNP'), ('SENTENCES', 'NNP'), ('text', 'NN'), ('parsing', 'NN'), ('(', '('), ('NLP', 'NNP'), (')', ')'), ('PARTS', 'NNP'), ('OF', 'NNP'), ('SPEECH', 'NNP'), ('SEMANTIC', 'NNP'), ('RELATIONSHIPS', 'NNP'), ('https', 'NN'), (':', ':'), ('//www.lexalytics.com/', 'JJ'), ('https', 'NN'), (':', ':'), ('//www.lexalytics.com/', 'JJ'), ('W', 'NNP'), ('H', 'NNP'), ('I', 'PRP'), ('T', 'NNP'), ('E', 'NNP'), ('P', 'NNP'), ('A', 'NNP'), ('P', 'NNP'), ('E', 'NNP'), ('R', 'NNP'), ('8|', 'CD'), ('|', 'NNP'), ('Lexalytics', 'NNP'), (',', ','), ('Inc.', 'NNP'), (',', ','), ('48', 'CD'), ('North', 'NNP'), ('Pleasant', 'NNP'), ('St.', 'NNP'), ('Unit', 'NNP'), ('301', 'CD'), (',', ','), ('Amherst', 'NNP'), ('MA', 'NNP'), ('01002', 'CD'), ('USA', 'NNP'), ('|', 'NNP'), ('1-800-377-8036', 'JJ'), ('|', 'NNP'), ('www.lexalytics.com', 'NN'), ('Not', 'RB'), ('only', 'RB'), ('do', 'VBP'), ('you', 'PRP'), ('need', 'VB'), ('at', 'IN'), ('least', 'JJS'), ('four', 'CD'), ('models', 'NNS'), ('to', 'TO'), ('solve', 'VB'), ('this', 'DT'), ('task', 'NN'), (',', ','), ('but', 'CC'), ('these', 'DT'), ('models', 'NNS'), ('are', 'VBP'), ('interdependent', 'JJ'), ('and', 'CC'), ('have', 'VBP'), ('to', 'TO'), ('interact', 'VB'), ('with', 'IN'), ('each', 'DT'), ('other', 'JJ'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['TOKENS', 'PHRASES', 'SYNTAX', 'TREES', 'SENTENCES', 'text', 'parsing', '(', 'NLP', ')', 'PARTS', 'SPEECH', 'SEMANTIC', 'RELATIONSHIPS', 'https', ':', '//www.lexalytics.com/', 'https', ':', '//www.lexalytics.com/', 'W', 'H', 'E', 'P', 'P', 'E', 'R', '8|', '|', 'Lexalytics', ',', 'Inc.', ',', '48', 'North', 'Pleasant', 'St.', 'Unit', '301', ',', 'Amherst', '01002', 'USA', '|', '1-800-377-8036', '|', 'www.lexalytics.com', 'need', 'least', 'four', 'models', 'solve', 'task', ',', 'models', 'interdependent', 'interact', '.']

 TOTAL FILTERED TOKENS ==>  58

 ---- POST FOR FILTERED TOKENS ----

 [('TOKENS', 'NNP'), ('PHRASES', 'NNP'), ('SYNTAX', 'NNP'), ('TREES', 'NNP'), ('SENTENCES', 'NNP'), ('text', 'NN'), ('parsing', 'NN'), ('(', '('), ('NLP', 'NNP'), (')', ')'), ('PARTS', 'NNP'), ('SPEECH', 'NNP'), ('SEMANTIC', 'NNP'), ('RELATIONSHIPS', 'NNP'), ('https', 'NN'), (':', ':'), ('//www.lexalytics.com/', 'JJ'), ('https', 'NN'), (':', ':'), ('//www.lexalytics.com/', 'JJ'), ('W', 'NNP'), ('H', 'NNP'), ('E', 'NNP'), ('P', 'NNP'), ('P', 'NNP'), ('E', 'NNP'), ('R', 'NNP'), ('8|', 'CD'), ('|', 'NNP'), ('Lexalytics', 'NNP'), (',', ','), ('Inc.', 'NNP'), (',', ','), ('48', 'CD'), ('North', 'NNP'), ('Pleasant', 'NNP'), ('St.', 'NNP'), ('Unit', 'NNP'), ('301', 'CD'), (',', ','), ('Amherst', 'NNP'), ('01002', 'CD'), ('USA', 'NNP'), ('|', 'NNP'), ('1-800-377-8036', 'JJ'), ('|', 'NNP'), ('www.lexalytics.com', 'NN'), ('need', 'VBP'), ('least', 'JJS'), ('four', 'CD'), ('models', 'NNS'), ('solve', 'VBP'), ('task', 'NN'), (',', ','), ('models', 'NNS'), ('interdependent', 'VBP'), ('interact', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['TOKENS PHRASES', 'PHRASES SYNTAX', 'SYNTAX TREES', 'TREES SENTENCES', 'SENTENCES text', 'text parsing', 'parsing (', '( NLP', 'NLP )', ') PARTS', 'PARTS SPEECH', 'SPEECH SEMANTIC', 'SEMANTIC RELATIONSHIPS', 'RELATIONSHIPS https', 'https :', ': //www.lexalytics.com/', '//www.lexalytics.com/ https', 'https :', ': //www.lexalytics.com/', '//www.lexalytics.com/ W', 'W H', 'H E', 'E P', 'P P', 'P E', 'E R', 'R 8|', '8| |', '| Lexalytics', 'Lexalytics ,', ', Inc.', 'Inc. ,', ', 48', '48 North', 'North Pleasant', 'Pleasant St.', 'St. Unit', 'Unit 301', '301 ,', ', Amherst', 'Amherst 01002', '01002 USA', 'USA |', '| 1-800-377-8036', '1-800-377-8036 |', '| www.lexalytics.com', 'www.lexalytics.com need', 'need least', 'least four', 'four models', 'models solve', 'solve task', 'task ,', ', models', 'models interdependent', 'interdependent interact', 'interact .'] 

 TOTAL BIGRAMS --> 57 



 ---- TRI-GRAMS ---- 

 ['TOKENS PHRASES SYNTAX', 'PHRASES SYNTAX TREES', 'SYNTAX TREES SENTENCES', 'TREES SENTENCES text', 'SENTENCES text parsing', 'text parsing (', 'parsing ( NLP', '( NLP )', 'NLP ) PARTS', ') PARTS SPEECH', 'PARTS SPEECH SEMANTIC', 'SPEECH SEMANTIC RELATIONSHIPS', 'SEMANTIC RELATIONSHIPS https', 'RELATIONSHIPS https :', 'https : //www.lexalytics.com/', ': //www.lexalytics.com/ https', '//www.lexalytics.com/ https :', 'https : //www.lexalytics.com/', ': //www.lexalytics.com/ W', '//www.lexalytics.com/ W H', 'W H E', 'H E P', 'E P P', 'P P E', 'P E R', 'E R 8|', 'R 8| |', '8| | Lexalytics', '| Lexalytics ,', 'Lexalytics , Inc.', ', Inc. ,', 'Inc. , 48', ', 48 North', '48 North Pleasant', 'North Pleasant St.', 'Pleasant St. Unit', 'St. Unit 301', 'Unit 301 ,', '301 , Amherst', ', Amherst 01002', 'Amherst 01002 USA', '01002 USA |', 'USA | 1-800-377-8036', '| 1-800-377-8036 |', '1-800-377-8036 | www.lexalytics.com', '| www.lexalytics.com need', 'www.lexalytics.com need least', 'need least four', 'least four models', 'four models solve', 'models solve task', 'solve task ,', 'task , models', ', models interdependent', 'models interdependent interact', 'interdependent interact .'] 

 TOTAL TRIGRAMS --> 56 



 ---- NOUN PHRASES ---- 

 ['text', 'parsing', 'https', ' https', 'www.lexalytics.com', 'task', 'interact'] 

 TOTAL NOUN PHRASES --> 7 



 ---- NER ----

 
 ORGANIZATION ---> ['PHRASES', 'SYNTAX', 'TREES', 'PARTS', 'SEMANTIC', 'USA']
 TOTAL ORGANIZATION ENTITY --> 6 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> ['TOKENS', 'North']
 TOTAL GPE ENTITY --> 2 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['token', 'phrase', 'syntax', 'tree', 'sentenc', 'text', 'pars', '(', 'nlp', ')', 'part', 'speech', 'semant', 'relationship', 'http', ':', '//www.lexalytics.com/', 'http', ':', '//www.lexalytics.com/', 'w', 'h', 'e', 'p', 'p', 'e', 'r', '8|', '|', 'lexalyt', ',', 'inc.', ',', '48', 'north', 'pleasant', 'st.', 'unit', '301', ',', 'amherst', '01002', 'usa', '|', '1-800-377-8036', '|', 'www.lexalytics.com', 'need', 'least', 'four', 'model', 'solv', 'task', ',', 'model', 'interdepend', 'interact', '.']

 TOTAL PORTER STEM WORDS ==> 58



 ---- SNOWBALL STEMMING ----

['token', 'phrase', 'syntax', 'tree', 'sentenc', 'text', 'pars', '(', 'nlp', ')', 'part', 'speech', 'semant', 'relationship', 'https', ':', '//www.lexalytics.com/', 'https', ':', '//www.lexalytics.com/', 'w', 'h', 'e', 'p', 'p', 'e', 'r', '8|', '|', 'lexalyt', ',', 'inc.', ',', '48', 'north', 'pleasant', 'st.', 'unit', '301', ',', 'amherst', '01002', 'usa', '|', '1-800-377-8036', '|', 'www.lexalytics.com', 'need', 'least', 'four', 'model', 'solv', 'task', ',', 'model', 'interdepend', 'interact', '.']

 TOTAL SNOWBALL STEM WORDS ==> 58



 ---- LEMMATIZATION ----

['TOKENS', 'PHRASES', 'SYNTAX', 'TREES', 'SENTENCES', 'text', 'parsing', '(', 'NLP', ')', 'PARTS', 'SPEECH', 'SEMANTIC', 'RELATIONSHIPS', 'http', ':', '//www.lexalytics.com/', 'http', ':', '//www.lexalytics.com/', 'W', 'H', 'E', 'P', 'P', 'E', 'R', '8|', '|', 'Lexalytics', ',', 'Inc.', ',', '48', 'North', 'Pleasant', 'St.', 'Unit', '301', ',', 'Amherst', '01002', 'USA', '|', '1-800-377-8036', '|', 'www.lexalytics.com', 'need', 'least', 'four', 'model', 'solve', 'task', ',', 'model', 'interdependent', 'interact', '.']

 TOTAL LEMMATIZE WORDS ==> 58

************************************************************************************************************************

73 --> To   create this kind of multi-model solution, we developed proprietary   AI building software. 


 ---- TOKENS ----

 ['To', 'create', 'this', 'kind', 'of', 'multi-model', 'solution', ',', 'we', 'developed', 'proprietary', 'AI', 'building', 'software', '.'] 

 TOTAL TOKENS ==> 15

 ---- POST ----

 [('To', 'TO'), ('create', 'VB'), ('this', 'DT'), ('kind', 'NN'), ('of', 'IN'), ('multi-model', 'JJ'), ('solution', 'NN'), (',', ','), ('we', 'PRP'), ('developed', 'VBD'), ('proprietary', 'JJ'), ('AI', 'NNP'), ('building', 'NN'), ('software', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['create', 'kind', 'multi-model', 'solution', ',', 'developed', 'proprietary', 'AI', 'building', 'software', '.']

 TOTAL FILTERED TOKENS ==>  11

 ---- POST FOR FILTERED TOKENS ----

 [('create', 'NN'), ('kind', 'NN'), ('multi-model', 'JJ'), ('solution', 'NN'), (',', ','), ('developed', 'VBD'), ('proprietary', 'JJ'), ('AI', 'NNP'), ('building', 'NN'), ('software', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['create kind', 'kind multi-model', 'multi-model solution', 'solution ,', ', developed', 'developed proprietary', 'proprietary AI', 'AI building', 'building software', 'software .'] 

 TOTAL BIGRAMS --> 10 



 ---- TRI-GRAMS ---- 

 ['create kind multi-model', 'kind multi-model solution', 'multi-model solution ,', 'solution , developed', ', developed proprietary', 'developed proprietary AI', 'proprietary AI building', 'AI building software', 'building software .'] 

 TOTAL TRIGRAMS --> 9 



 ---- NOUN PHRASES ---- 

 ['create', 'kind', 'multi-model solution', 'building', 'software'] 

 TOTAL NOUN PHRASES --> 5 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['creat', 'kind', 'multi-model', 'solut', ',', 'develop', 'proprietari', 'ai', 'build', 'softwar', '.']

 TOTAL PORTER STEM WORDS ==> 11



 ---- SNOWBALL STEMMING ----

['creat', 'kind', 'multi-model', 'solut', ',', 'develop', 'proprietari', 'ai', 'build', 'softwar', '.']

 TOTAL SNOWBALL STEM WORDS ==> 11



 ---- LEMMATIZATION ----

['create', 'kind', 'multi-model', 'solution', ',', 'developed', 'proprietary', 'AI', 'building', 'software', '.']

 TOTAL LEMMATIZE WORDS ==> 11

************************************************************************************************************************

74 --> This tool, “AI Assembler,” is used to build our features like sentiment,   named entity extraction, intention analysis and more. 


 ---- TOKENS ----

 ['This', 'tool', ',', '“', 'AI', 'Assembler', ',', '”', 'is', 'used', 'to', 'build', 'our', 'features', 'like', 'sentiment', ',', 'named', 'entity', 'extraction', ',', 'intention', 'analysis', 'and', 'more', '.'] 

 TOTAL TOKENS ==> 26

 ---- POST ----

 [('This', 'DT'), ('tool', 'NN'), (',', ','), ('“', 'NNP'), ('AI', 'NNP'), ('Assembler', 'NNP'), (',', ','), ('”', 'NNP'), ('is', 'VBZ'), ('used', 'VBN'), ('to', 'TO'), ('build', 'VB'), ('our', 'PRP$'), ('features', 'NNS'), ('like', 'IN'), ('sentiment', 'NN'), (',', ','), ('named', 'VBN'), ('entity', 'NN'), ('extraction', 'NN'), (',', ','), ('intention', 'NN'), ('analysis', 'NN'), ('and', 'CC'), ('more', 'JJR'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['tool', ',', '“', 'AI', 'Assembler', ',', '”', 'used', 'build', 'features', 'like', 'sentiment', ',', 'named', 'entity', 'extraction', ',', 'intention', 'analysis', '.']

 TOTAL FILTERED TOKENS ==>  20

 ---- POST FOR FILTERED TOKENS ----

 [('tool', 'NN'), (',', ','), ('“', 'NNP'), ('AI', 'NNP'), ('Assembler', 'NNP'), (',', ','), ('”', 'NNP'), ('used', 'VBD'), ('build', 'JJ'), ('features', 'NNS'), ('like', 'IN'), ('sentiment', 'NN'), (',', ','), ('named', 'VBN'), ('entity', 'NN'), ('extraction', 'NN'), (',', ','), ('intention', 'NN'), ('analysis', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['tool ,', ', “', '“ AI', 'AI Assembler', 'Assembler ,', ', ”', '” used', 'used build', 'build features', 'features like', 'like sentiment', 'sentiment ,', ', named', 'named entity', 'entity extraction', 'extraction ,', ', intention', 'intention analysis', 'analysis .'] 

 TOTAL BIGRAMS --> 19 



 ---- TRI-GRAMS ---- 

 ['tool , “', ', “ AI', '“ AI Assembler', 'AI Assembler ,', 'Assembler , ”', ', ” used', '” used build', 'used build features', 'build features like', 'features like sentiment', 'like sentiment ,', 'sentiment , named', ', named entity', 'named entity extraction', 'entity extraction ,', 'extraction , intention', ', intention analysis', 'intention analysis .'] 

 TOTAL TRIGRAMS --> 18 



 ---- NOUN PHRASES ---- 

 ['tool', 'sentiment', 'entity', 'extraction', 'intention', 'analysis'] 

 TOTAL NOUN PHRASES --> 6 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> ['Assembler']
 TOTAL PERSON ENTITY --> 1 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['tool', ',', '“', 'ai', 'assembl', ',', '”', 'use', 'build', 'featur', 'like', 'sentiment', ',', 'name', 'entiti', 'extract', ',', 'intent', 'analysi', '.']

 TOTAL PORTER STEM WORDS ==> 20



 ---- SNOWBALL STEMMING ----

['tool', ',', '“', 'ai', 'assembl', ',', '”', 'use', 'build', 'featur', 'like', 'sentiment', ',', 'name', 'entiti', 'extract', ',', 'intent', 'analysi', '.']

 TOTAL SNOWBALL STEM WORDS ==> 20



 ---- LEMMATIZATION ----

['tool', ',', '“', 'AI', 'Assembler', ',', '”', 'used', 'build', 'feature', 'like', 'sentiment', ',', 'named', 'entity', 'extraction', ',', 'intention', 'analysis', '.']

 TOTAL LEMMATIZE WORDS ==> 20

************************************************************************************************************************

75 --> We also use AI  Assembler to build custom machine learning models used by our customers  and partners. 


 ---- TOKENS ----

 ['We', 'also', 'use', 'AI', 'Assembler', 'to', 'build', 'custom', 'machine', 'learning', 'models', 'used', 'by', 'our', 'customers', 'and', 'partners', '.'] 

 TOTAL TOKENS ==> 18

 ---- POST ----

 [('We', 'PRP'), ('also', 'RB'), ('use', 'VBP'), ('AI', 'NNP'), ('Assembler', 'NNP'), ('to', 'TO'), ('build', 'VB'), ('custom', 'JJ'), ('machine', 'NN'), ('learning', 'NN'), ('models', 'NNS'), ('used', 'VBN'), ('by', 'IN'), ('our', 'PRP$'), ('customers', 'NNS'), ('and', 'CC'), ('partners', 'NNS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['also', 'use', 'AI', 'Assembler', 'build', 'custom', 'machine', 'learning', 'models', 'used', 'customers', 'partners', '.']

 TOTAL FILTERED TOKENS ==>  13

 ---- POST FOR FILTERED TOKENS ----

 [('also', 'RB'), ('use', 'NN'), ('AI', 'NNP'), ('Assembler', 'NNP'), ('build', 'VB'), ('custom', 'NN'), ('machine', 'NN'), ('learning', 'NN'), ('models', 'NNS'), ('used', 'VBN'), ('customers', 'NNS'), ('partners', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['also use', 'use AI', 'AI Assembler', 'Assembler build', 'build custom', 'custom machine', 'machine learning', 'learning models', 'models used', 'used customers', 'customers partners', 'partners .'] 

 TOTAL BIGRAMS --> 12 



 ---- TRI-GRAMS ---- 

 ['also use AI', 'use AI Assembler', 'AI Assembler build', 'Assembler build custom', 'build custom machine', 'custom machine learning', 'machine learning models', 'learning models used', 'models used customers', 'used customers partners', 'customers partners .'] 

 TOTAL TRIGRAMS --> 11 



 ---- NOUN PHRASES ---- 

 ['use', 'custom', 'machine', 'learning'] 

 TOTAL NOUN PHRASES --> 4 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['also', 'use', 'ai', 'assembl', 'build', 'custom', 'machin', 'learn', 'model', 'use', 'custom', 'partner', '.']

 TOTAL PORTER STEM WORDS ==> 13



 ---- SNOWBALL STEMMING ----

['also', 'use', 'ai', 'assembl', 'build', 'custom', 'machin', 'learn', 'model', 'use', 'custom', 'partner', '.']

 TOTAL SNOWBALL STEM WORDS ==> 13



 ---- LEMMATIZATION ----

['also', 'use', 'AI', 'Assembler', 'build', 'custom', 'machine', 'learning', 'model', 'used', 'customer', 'partner', '.']

 TOTAL LEMMATIZE WORDS ==> 13

************************************************************************************************************************

76 --> Among other things, AI Assembler manages dependencies  between models, allowing us to easily upgrade one model and then   re-build other models as necessary. 


 ---- TOKENS ----

 ['Among', 'other', 'things', ',', 'AI', 'Assembler', 'manages', 'dependencies', 'between', 'models', ',', 'allowing', 'us', 'to', 'easily', 'upgrade', 'one', 'model', 'and', 'then', 're-build', 'other', 'models', 'as', 'necessary', '.'] 

 TOTAL TOKENS ==> 26

 ---- POST ----

 [('Among', 'IN'), ('other', 'JJ'), ('things', 'NNS'), (',', ','), ('AI', 'NNP'), ('Assembler', 'NNP'), ('manages', 'VBZ'), ('dependencies', 'NNS'), ('between', 'IN'), ('models', 'NNS'), (',', ','), ('allowing', 'VBG'), ('us', 'PRP'), ('to', 'TO'), ('easily', 'RB'), ('upgrade', 'VB'), ('one', 'CD'), ('model', 'NN'), ('and', 'CC'), ('then', 'RB'), ('re-build', 'VB'), ('other', 'JJ'), ('models', 'NNS'), ('as', 'IN'), ('necessary', 'JJ'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Among', 'things', ',', 'AI', 'Assembler', 'manages', 'dependencies', 'models', ',', 'allowing', 'us', 'easily', 'upgrade', 'one', 'model', 're-build', 'models', 'necessary', '.']

 TOTAL FILTERED TOKENS ==>  19

 ---- POST FOR FILTERED TOKENS ----

 [('Among', 'IN'), ('things', 'NNS'), (',', ','), ('AI', 'NNP'), ('Assembler', 'NNP'), ('manages', 'VBZ'), ('dependencies', 'NNS'), ('models', 'NNS'), (',', ','), ('allowing', 'VBG'), ('us', 'PRP'), ('easily', 'RB'), ('upgrade', 'VBD'), ('one', 'CD'), ('model', 'NN'), ('re-build', 'JJ'), ('models', 'NNS'), ('necessary', 'JJ'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Among things', 'things ,', ', AI', 'AI Assembler', 'Assembler manages', 'manages dependencies', 'dependencies models', 'models ,', ', allowing', 'allowing us', 'us easily', 'easily upgrade', 'upgrade one', 'one model', 'model re-build', 're-build models', 'models necessary', 'necessary .'] 

 TOTAL BIGRAMS --> 18 



 ---- TRI-GRAMS ---- 

 ['Among things ,', 'things , AI', ', AI Assembler', 'AI Assembler manages', 'Assembler manages dependencies', 'manages dependencies models', 'dependencies models ,', 'models , allowing', ', allowing us', 'allowing us easily', 'us easily upgrade', 'easily upgrade one', 'upgrade one model', 'one model re-build', 'model re-build models', 're-build models necessary', 'models necessary .'] 

 TOTAL TRIGRAMS --> 17 



 ---- NOUN PHRASES ---- 

 ['model'] 

 TOTAL NOUN PHRASES --> 1 



 ---- NER ----

 
 ORGANIZATION ---> ['AI Assembler']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['among', 'thing', ',', 'ai', 'assembl', 'manag', 'depend', 'model', ',', 'allow', 'us', 'easili', 'upgrad', 'one', 'model', 're-build', 'model', 'necessari', '.']

 TOTAL PORTER STEM WORDS ==> 19



 ---- SNOWBALL STEMMING ----

['among', 'thing', ',', 'ai', 'assembl', 'manag', 'depend', 'model', ',', 'allow', 'us', 'easili', 'upgrad', 'one', 'model', 're-build', 'model', 'necessari', '.']

 TOTAL SNOWBALL STEM WORDS ==> 19



 ---- LEMMATIZATION ----

['Among', 'thing', ',', 'AI', 'Assembler', 'manages', 'dependency', 'model', ',', 'allowing', 'u', 'easily', 'upgrade', 'one', 'model', 're-build', 'model', 'necessary', '.']

 TOTAL LEMMATIZE WORDS ==> 19

************************************************************************************************************************

77 --> Multi-level granular sentiment analysis is difficult due to the model complexity   and dependencies. 


 ---- TOKENS ----

 ['Multi-level', 'granular', 'sentiment', 'analysis', 'is', 'difficult', 'due', 'to', 'the', 'model', 'complexity', 'and', 'dependencies', '.'] 

 TOTAL TOKENS ==> 14

 ---- POST ----

 [('Multi-level', 'NNP'), ('granular', 'JJ'), ('sentiment', 'NN'), ('analysis', 'NN'), ('is', 'VBZ'), ('difficult', 'JJ'), ('due', 'JJ'), ('to', 'TO'), ('the', 'DT'), ('model', 'NN'), ('complexity', 'NN'), ('and', 'CC'), ('dependencies', 'NNS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Multi-level', 'granular', 'sentiment', 'analysis', 'difficult', 'due', 'model', 'complexity', 'dependencies', '.']

 TOTAL FILTERED TOKENS ==>  10

 ---- POST FOR FILTERED TOKENS ----

 [('Multi-level', 'NNP'), ('granular', 'JJ'), ('sentiment', 'NN'), ('analysis', 'NN'), ('difficult', 'JJ'), ('due', 'JJ'), ('model', 'NN'), ('complexity', 'NN'), ('dependencies', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Multi-level granular', 'granular sentiment', 'sentiment analysis', 'analysis difficult', 'difficult due', 'due model', 'model complexity', 'complexity dependencies', 'dependencies .'] 

 TOTAL BIGRAMS --> 9 



 ---- TRI-GRAMS ---- 

 ['Multi-level granular sentiment', 'granular sentiment analysis', 'sentiment analysis difficult', 'analysis difficult due', 'difficult due model', 'due model complexity', 'model complexity dependencies', 'complexity dependencies .'] 

 TOTAL TRIGRAMS --> 8 



 ---- NOUN PHRASES ---- 

 ['granular sentiment', 'analysis', 'difficult due model', 'complexity'] 

 TOTAL NOUN PHRASES --> 4 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['multi-level', 'granular', 'sentiment', 'analysi', 'difficult', 'due', 'model', 'complex', 'depend', '.']

 TOTAL PORTER STEM WORDS ==> 10



 ---- SNOWBALL STEMMING ----

['multi-level', 'granular', 'sentiment', 'analysi', 'difficult', 'due', 'model', 'complex', 'depend', '.']

 TOTAL SNOWBALL STEM WORDS ==> 10



 ---- LEMMATIZATION ----

['Multi-level', 'granular', 'sentiment', 'analysis', 'difficult', 'due', 'model', 'complexity', 'dependency', '.']

 TOTAL LEMMATIZE WORDS ==> 10

************************************************************************************************************************

78 --> Lexalytics is one of the few companies that actually  provides this service – most companies simply provide document sentiment  and call it done. 


 ---- TOKENS ----

 ['Lexalytics', 'is', 'one', 'of', 'the', 'few', 'companies', 'that', 'actually', 'provides', 'this', 'service', '–', 'most', 'companies', 'simply', 'provide', 'document', 'sentiment', 'and', 'call', 'it', 'done', '.'] 

 TOTAL TOKENS ==> 24

 ---- POST ----

 [('Lexalytics', 'NNS'), ('is', 'VBZ'), ('one', 'CD'), ('of', 'IN'), ('the', 'DT'), ('few', 'JJ'), ('companies', 'NNS'), ('that', 'WDT'), ('actually', 'RB'), ('provides', 'VBZ'), ('this', 'DT'), ('service', 'NN'), ('–', 'VBZ'), ('most', 'RBS'), ('companies', 'NNS'), ('simply', 'RB'), ('provide', 'VB'), ('document', 'NN'), ('sentiment', 'NN'), ('and', 'CC'), ('call', 'VB'), ('it', 'PRP'), ('done', 'VBN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Lexalytics', 'one', 'companies', 'actually', 'provides', 'service', '–', 'companies', 'simply', 'provide', 'document', 'sentiment', 'call', 'done', '.']

 TOTAL FILTERED TOKENS ==>  15

 ---- POST FOR FILTERED TOKENS ----

 [('Lexalytics', 'NNS'), ('one', 'CD'), ('companies', 'NNS'), ('actually', 'RB'), ('provides', 'VBZ'), ('service', 'NN'), ('–', 'NN'), ('companies', 'NNS'), ('simply', 'RB'), ('provide', 'VB'), ('document', 'NN'), ('sentiment', 'NN'), ('call', 'NN'), ('done', 'VBN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Lexalytics one', 'one companies', 'companies actually', 'actually provides', 'provides service', 'service –', '– companies', 'companies simply', 'simply provide', 'provide document', 'document sentiment', 'sentiment call', 'call done', 'done .'] 

 TOTAL BIGRAMS --> 14 



 ---- TRI-GRAMS ---- 

 ['Lexalytics one companies', 'one companies actually', 'companies actually provides', 'actually provides service', 'provides service –', 'service – companies', '– companies simply', 'companies simply provide', 'simply provide document', 'provide document sentiment', 'document sentiment call', 'sentiment call done', 'call done .'] 

 TOTAL TRIGRAMS --> 13 



 ---- NOUN PHRASES ---- 

 ['service', '–', 'document', 'sentiment', 'call'] 

 TOTAL NOUN PHRASES --> 5 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['lexalyt', 'one', 'compani', 'actual', 'provid', 'servic', '–', 'compani', 'simpli', 'provid', 'document', 'sentiment', 'call', 'done', '.']

 TOTAL PORTER STEM WORDS ==> 15



 ---- SNOWBALL STEMMING ----

['lexalyt', 'one', 'compani', 'actual', 'provid', 'servic', '–', 'compani', 'simpli', 'provid', 'document', 'sentiment', 'call', 'done', '.']

 TOTAL SNOWBALL STEM WORDS ==> 15



 ---- LEMMATIZATION ----

['Lexalytics', 'one', 'company', 'actually', 'provides', 'service', '–', 'company', 'simply', 'provide', 'document', 'sentiment', 'call', 'done', '.']

 TOTAL LEMMATIZE WORDS ==> 15

************************************************************************************************************************

79 --> Truth is, solving for entity and category sentiment is very  difficult, and multiplies the amount of work required. 


 ---- TOKENS ----

 ['Truth', 'is', ',', 'solving', 'for', 'entity', 'and', 'category', 'sentiment', 'is', 'very', 'difficult', ',', 'and', 'multiplies', 'the', 'amount', 'of', 'work', 'required', '.'] 

 TOTAL TOKENS ==> 21

 ---- POST ----

 [('Truth', 'NN'), ('is', 'VBZ'), (',', ','), ('solving', 'VBG'), ('for', 'IN'), ('entity', 'NN'), ('and', 'CC'), ('category', 'JJ'), ('sentiment', 'NN'), ('is', 'VBZ'), ('very', 'RB'), ('difficult', 'JJ'), (',', ','), ('and', 'CC'), ('multiplies', 'VBZ'), ('the', 'DT'), ('amount', 'NN'), ('of', 'IN'), ('work', 'NN'), ('required', 'VBN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Truth', ',', 'solving', 'entity', 'category', 'sentiment', 'difficult', ',', 'multiplies', 'amount', 'work', 'required', '.']

 TOTAL FILTERED TOKENS ==>  13

 ---- POST FOR FILTERED TOKENS ----

 [('Truth', 'NN'), (',', ','), ('solving', 'VBG'), ('entity', 'NN'), ('category', 'JJ'), ('sentiment', 'NN'), ('difficult', 'JJ'), (',', ','), ('multiplies', 'NNS'), ('amount', 'VBP'), ('work', 'NN'), ('required', 'VBN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Truth ,', ', solving', 'solving entity', 'entity category', 'category sentiment', 'sentiment difficult', 'difficult ,', ', multiplies', 'multiplies amount', 'amount work', 'work required', 'required .'] 

 TOTAL BIGRAMS --> 12 



 ---- TRI-GRAMS ---- 

 ['Truth , solving', ', solving entity', 'solving entity category', 'entity category sentiment', 'category sentiment difficult', 'sentiment difficult ,', 'difficult , multiplies', ', multiplies amount', 'multiplies amount work', 'amount work required', 'work required .'] 

 TOTAL TRIGRAMS --> 11 



 ---- NOUN PHRASES ---- 

 ['Truth', 'entity', 'category sentiment', 'work'] 

 TOTAL NOUN PHRASES --> 4 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> ['Truth']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['truth', ',', 'solv', 'entiti', 'categori', 'sentiment', 'difficult', ',', 'multipli', 'amount', 'work', 'requir', '.']

 TOTAL PORTER STEM WORDS ==> 13



 ---- SNOWBALL STEMMING ----

['truth', ',', 'solv', 'entiti', 'categori', 'sentiment', 'difficult', ',', 'multipli', 'amount', 'work', 'requir', '.']

 TOTAL SNOWBALL STEM WORDS ==> 13



 ---- LEMMATIZATION ----

['Truth', ',', 'solving', 'entity', 'category', 'sentiment', 'difficult', ',', 'multiplies', 'amount', 'work', 'required', '.']

 TOTAL LEMMATIZE WORDS ==> 13

************************************************************************************************************************

80 --> We do it because our  customers are making business critical decisions, and they need context-rich  insights to make informed decisions that drive business growth. 


 ---- TOKENS ----

 ['We', 'do', 'it', 'because', 'our', 'customers', 'are', 'making', 'business', 'critical', 'decisions', ',', 'and', 'they', 'need', 'context-rich', 'insights', 'to', 'make', 'informed', 'decisions', 'that', 'drive', 'business', 'growth', '.'] 

 TOTAL TOKENS ==> 26

 ---- POST ----

 [('We', 'PRP'), ('do', 'VBP'), ('it', 'PRP'), ('because', 'IN'), ('our', 'PRP$'), ('customers', 'NNS'), ('are', 'VBP'), ('making', 'VBG'), ('business', 'NN'), ('critical', 'JJ'), ('decisions', 'NNS'), (',', ','), ('and', 'CC'), ('they', 'PRP'), ('need', 'VBP'), ('context-rich', 'JJ'), ('insights', 'NNS'), ('to', 'TO'), ('make', 'VB'), ('informed', 'JJ'), ('decisions', 'NNS'), ('that', 'WDT'), ('drive', 'VBP'), ('business', 'NN'), ('growth', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['customers', 'making', 'business', 'critical', 'decisions', ',', 'need', 'context-rich', 'insights', 'make', 'informed', 'decisions', 'drive', 'business', 'growth', '.']

 TOTAL FILTERED TOKENS ==>  16

 ---- POST FOR FILTERED TOKENS ----

 [('customers', 'NNS'), ('making', 'VBG'), ('business', 'NN'), ('critical', 'JJ'), ('decisions', 'NNS'), (',', ','), ('need', 'VBP'), ('context-rich', 'JJ'), ('insights', 'NNS'), ('make', 'VBP'), ('informed', 'JJ'), ('decisions', 'NNS'), ('drive', 'VBP'), ('business', 'NN'), ('growth', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['customers making', 'making business', 'business critical', 'critical decisions', 'decisions ,', ', need', 'need context-rich', 'context-rich insights', 'insights make', 'make informed', 'informed decisions', 'decisions drive', 'drive business', 'business growth', 'growth .'] 

 TOTAL BIGRAMS --> 15 



 ---- TRI-GRAMS ---- 

 ['customers making business', 'making business critical', 'business critical decisions', 'critical decisions ,', 'decisions , need', ', need context-rich', 'need context-rich insights', 'context-rich insights make', 'insights make informed', 'make informed decisions', 'informed decisions drive', 'decisions drive business', 'drive business growth', 'business growth .'] 

 TOTAL TRIGRAMS --> 14 



 ---- NOUN PHRASES ---- 

 ['business', 'business', 'growth'] 

 TOTAL NOUN PHRASES --> 3 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['custom', 'make', 'busi', 'critic', 'decis', ',', 'need', 'context-rich', 'insight', 'make', 'inform', 'decis', 'drive', 'busi', 'growth', '.']

 TOTAL PORTER STEM WORDS ==> 16



 ---- SNOWBALL STEMMING ----

['custom', 'make', 'busi', 'critic', 'decis', ',', 'need', 'context-rich', 'insight', 'make', 'inform', 'decis', 'drive', 'busi', 'growth', '.']

 TOTAL SNOWBALL STEM WORDS ==> 16



 ---- LEMMATIZATION ----

['customer', 'making', 'business', 'critical', 'decision', ',', 'need', 'context-rich', 'insight', 'make', 'informed', 'decision', 'drive', 'business', 'growth', '.']

 TOTAL LEMMATIZE WORDS ==> 16

************************************************************************************************************************

81 --> To borrow from another industry, imagine if you have a wonderful spy  satellite. 


 ---- TOKENS ----

 ['To', 'borrow', 'from', 'another', 'industry', ',', 'imagine', 'if', 'you', 'have', 'a', 'wonderful', 'spy', 'satellite', '.'] 

 TOTAL TOKENS ==> 15

 ---- POST ----

 [('To', 'TO'), ('borrow', 'VB'), ('from', 'IN'), ('another', 'DT'), ('industry', 'NN'), (',', ','), ('imagine', 'VB'), ('if', 'IN'), ('you', 'PRP'), ('have', 'VBP'), ('a', 'DT'), ('wonderful', 'JJ'), ('spy', 'NN'), ('satellite', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['borrow', 'another', 'industry', ',', 'imagine', 'wonderful', 'spy', 'satellite', '.']

 TOTAL FILTERED TOKENS ==>  9

 ---- POST FOR FILTERED TOKENS ----

 [('borrow', 'VB'), ('another', 'DT'), ('industry', 'NN'), (',', ','), ('imagine', 'JJ'), ('wonderful', 'JJ'), ('spy', 'NN'), ('satellite', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['borrow another', 'another industry', 'industry ,', ', imagine', 'imagine wonderful', 'wonderful spy', 'spy satellite', 'satellite .'] 

 TOTAL BIGRAMS --> 8 



 ---- TRI-GRAMS ---- 

 ['borrow another industry', 'another industry ,', 'industry , imagine', ', imagine wonderful', 'imagine wonderful spy', 'wonderful spy satellite', 'spy satellite .'] 

 TOTAL TRIGRAMS --> 7 



 ---- NOUN PHRASES ---- 

 ['another industry', 'imagine wonderful spy', 'satellite'] 

 TOTAL NOUN PHRASES --> 3 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['borrow', 'anoth', 'industri', ',', 'imagin', 'wonder', 'spi', 'satellit', '.']

 TOTAL PORTER STEM WORDS ==> 9



 ---- SNOWBALL STEMMING ----

['borrow', 'anoth', 'industri', ',', 'imagin', 'wonder', 'spi', 'satellit', '.']

 TOTAL SNOWBALL STEM WORDS ==> 9



 ---- LEMMATIZATION ----

['borrow', 'another', 'industry', ',', 'imagine', 'wonderful', 'spy', 'satellite', '.']

 TOTAL LEMMATIZE WORDS ==> 9

************************************************************************************************************************

82 --> Do you want to just be able to say, “There are a bunch of people  there” or, “There’s a known terrorist there, and he’s holding a gun?” builds NLP machine   learning models and manages   dependencies between them. 


 ---- TOKENS ----

 ['Do', 'you', 'want', 'to', 'just', 'be', 'able', 'to', 'say', ',', '“', 'There', 'are', 'a', 'bunch', 'of', 'people', 'there', '”', 'or', ',', '“', 'There', '’', 's', 'a', 'known', 'terrorist', 'there', ',', 'and', 'he', '’', 's', 'holding', 'a', 'gun', '?', '”', 'builds', 'NLP', 'machine', 'learning', 'models', 'and', 'manages', 'dependencies', 'between', 'them', '.'] 

 TOTAL TOKENS ==> 50

 ---- POST ----

 [('Do', 'VBP'), ('you', 'PRP'), ('want', 'VB'), ('to', 'TO'), ('just', 'RB'), ('be', 'VB'), ('able', 'JJ'), ('to', 'TO'), ('say', 'VB'), (',', ','), ('“', 'VB'), ('There', 'EX'), ('are', 'VBP'), ('a', 'DT'), ('bunch', 'NN'), ('of', 'IN'), ('people', 'NNS'), ('there', 'EX'), ('”', 'CD'), ('or', 'CC'), (',', ','), ('“', 'CC'), ('There', 'EX'), ('’', 'NNP'), ('s', 'VBD'), ('a', 'DT'), ('known', 'JJ'), ('terrorist', 'NN'), ('there', 'RB'), (',', ','), ('and', 'CC'), ('he', 'PRP'), ('’', 'VBZ'), ('s', 'JJ'), ('holding', 'VBG'), ('a', 'DT'), ('gun', 'NN'), ('?', '.'), ('”', 'JJ'), ('builds', 'NNS'), ('NLP', 'NNP'), ('machine', 'NN'), ('learning', 'NN'), ('models', 'NNS'), ('and', 'CC'), ('manages', 'VBZ'), ('dependencies', 'NNS'), ('between', 'IN'), ('them', 'PRP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['want', 'able', 'say', ',', '“', 'bunch', 'people', '”', ',', '“', '’', 'known', 'terrorist', ',', '’', 'holding', 'gun', '?', '”', 'builds', 'NLP', 'machine', 'learning', 'models', 'manages', 'dependencies', '.']

 TOTAL FILTERED TOKENS ==>  27

 ---- POST FOR FILTERED TOKENS ----

 [('want', 'NN'), ('able', 'JJ'), ('say', 'NN'), (',', ','), ('“', 'JJ'), ('bunch', 'NN'), ('people', 'NNS'), ('”', 'VBP'), (',', ','), ('“', 'JJ'), ('’', 'NNP'), ('known', 'VBN'), ('terrorist', 'NN'), (',', ','), ('’', 'NNP'), ('holding', 'VBG'), ('gun', 'NN'), ('?', '.'), ('”', 'JJ'), ('builds', 'NNS'), ('NLP', 'NNP'), ('machine', 'NN'), ('learning', 'VBG'), ('models', 'NNS'), ('manages', 'VBZ'), ('dependencies', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['want able', 'able say', 'say ,', ', “', '“ bunch', 'bunch people', 'people ”', '” ,', ', “', '“ ’', '’ known', 'known terrorist', 'terrorist ,', ', ’', '’ holding', 'holding gun', 'gun ?', '? ”', '” builds', 'builds NLP', 'NLP machine', 'machine learning', 'learning models', 'models manages', 'manages dependencies', 'dependencies .'] 

 TOTAL BIGRAMS --> 26 



 ---- TRI-GRAMS ---- 

 ['want able say', 'able say ,', 'say , “', ', “ bunch', '“ bunch people', 'bunch people ”', 'people ” ,', '” , “', ', “ ’', '“ ’ known', '’ known terrorist', 'known terrorist ,', 'terrorist , ’', ', ’ holding', '’ holding gun', 'holding gun ?', 'gun ? ”', '? ” builds', '” builds NLP', 'builds NLP machine', 'NLP machine learning', 'machine learning models', 'learning models manages', 'models manages dependencies', 'manages dependencies .'] 

 TOTAL TRIGRAMS --> 25 



 ---- NOUN PHRASES ---- 

 ['want', 'able say', '“ bunch', 'terrorist', 'gun', 'machine'] 

 TOTAL NOUN PHRASES --> 6 



 ---- NER ----

 
 ORGANIZATION ---> ['NLP']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['want', 'abl', 'say', ',', '“', 'bunch', 'peopl', '”', ',', '“', '’', 'known', 'terrorist', ',', '’', 'hold', 'gun', '?', '”', 'build', 'nlp', 'machin', 'learn', 'model', 'manag', 'depend', '.']

 TOTAL PORTER STEM WORDS ==> 27



 ---- SNOWBALL STEMMING ----

['want', 'abl', 'say', ',', '“', 'bunch', 'peopl', '”', ',', '“', '’', 'known', 'terrorist', ',', '’', 'hold', 'gun', '?', '”', 'build', 'nlp', 'machin', 'learn', 'model', 'manag', 'depend', '.']

 TOTAL SNOWBALL STEM WORDS ==> 27



 ---- LEMMATIZATION ----

['want', 'able', 'say', ',', '“', 'bunch', 'people', '”', ',', '“', '’', 'known', 'terrorist', ',', '’', 'holding', 'gun', '?', '”', 'build', 'NLP', 'machine', 'learning', 'model', 'manages', 'dependency', '.']

 TOTAL LEMMATIZE WORDS ==> 27

************************************************************************************************************************

83 --> AI   Assembler  (our proprietary   software) https://www.lexalytics.com/ https://www.lexalytics.com/  W H I T E  P A P E R 9|       |   Lexalytics, Inc., 48 North Pleasant St. Unit 301, Amherst MA 01002 USA   |   1-800-377-8036 |   www.lexalytics.com C O D I N G  V S . 


 ---- TOKENS ----

 ['AI', 'Assembler', '(', 'our', 'proprietary', 'software', ')', 'https', ':', '//www.lexalytics.com/', 'https', ':', '//www.lexalytics.com/', 'W', 'H', 'I', 'T', 'E', 'P', 'A', 'P', 'E', 'R', '9|', '|', 'Lexalytics', ',', 'Inc.', ',', '48', 'North', 'Pleasant', 'St.', 'Unit', '301', ',', 'Amherst', 'MA', '01002', 'USA', '|', '1-800-377-8036', '|', 'www.lexalytics.com', 'C', 'O', 'D', 'I', 'N', 'G', 'V', 'S', '.'] 

 TOTAL TOKENS ==> 53

 ---- POST ----

 [('AI', 'NNP'), ('Assembler', 'NNP'), ('(', '('), ('our', 'PRP$'), ('proprietary', 'NN'), ('software', 'NN'), (')', ')'), ('https', 'NN'), (':', ':'), ('//www.lexalytics.com/', 'JJ'), ('https', 'NN'), (':', ':'), ('//www.lexalytics.com/', 'JJ'), ('W', 'NNP'), ('H', 'NNP'), ('I', 'PRP'), ('T', 'NNP'), ('E', 'NNP'), ('P', 'NNP'), ('A', 'NNP'), ('P', 'NNP'), ('E', 'NNP'), ('R', 'NNP'), ('9|', 'CD'), ('|', 'NNP'), ('Lexalytics', 'NNP'), (',', ','), ('Inc.', 'NNP'), (',', ','), ('48', 'CD'), ('North', 'NNP'), ('Pleasant', 'NNP'), ('St.', 'NNP'), ('Unit', 'NNP'), ('301', 'CD'), (',', ','), ('Amherst', 'NNP'), ('MA', 'NNP'), ('01002', 'CD'), ('USA', 'NNP'), ('|', 'NNP'), ('1-800-377-8036', 'JJ'), ('|', 'NNP'), ('www.lexalytics.com', 'NN'), ('C', 'NNP'), ('O', 'NNP'), ('D', 'NNP'), ('I', 'PRP'), ('N', 'NNP'), ('G', 'NNP'), ('V', 'NNP'), ('S', 'NNP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['AI', 'Assembler', '(', 'proprietary', 'software', ')', 'https', ':', '//www.lexalytics.com/', 'https', ':', '//www.lexalytics.com/', 'W', 'H', 'E', 'P', 'P', 'E', 'R', '9|', '|', 'Lexalytics', ',', 'Inc.', ',', '48', 'North', 'Pleasant', 'St.', 'Unit', '301', ',', 'Amherst', '01002', 'USA', '|', '1-800-377-8036', '|', 'www.lexalytics.com', 'C', 'N', 'G', 'V', '.']

 TOTAL FILTERED TOKENS ==>  44

 ---- POST FOR FILTERED TOKENS ----

 [('AI', 'NNP'), ('Assembler', 'NNP'), ('(', '('), ('proprietary', 'JJ'), ('software', 'NN'), (')', ')'), ('https', 'NN'), (':', ':'), ('//www.lexalytics.com/', 'JJ'), ('https', 'NN'), (':', ':'), ('//www.lexalytics.com/', 'JJ'), ('W', 'NNP'), ('H', 'NNP'), ('E', 'NNP'), ('P', 'NNP'), ('P', 'NNP'), ('E', 'NNP'), ('R', 'NNP'), ('9|', 'CD'), ('|', 'NNP'), ('Lexalytics', 'NNP'), (',', ','), ('Inc.', 'NNP'), (',', ','), ('48', 'CD'), ('North', 'NNP'), ('Pleasant', 'NNP'), ('St.', 'NNP'), ('Unit', 'NNP'), ('301', 'CD'), (',', ','), ('Amherst', 'NNP'), ('01002', 'CD'), ('USA', 'NNP'), ('|', 'NNP'), ('1-800-377-8036', 'JJ'), ('|', 'NNP'), ('www.lexalytics.com', 'NN'), ('C', 'NNP'), ('N', 'NNP'), ('G', 'NNP'), ('V', 'NNP'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['AI Assembler', 'Assembler (', '( proprietary', 'proprietary software', 'software )', ') https', 'https :', ': //www.lexalytics.com/', '//www.lexalytics.com/ https', 'https :', ': //www.lexalytics.com/', '//www.lexalytics.com/ W', 'W H', 'H E', 'E P', 'P P', 'P E', 'E R', 'R 9|', '9| |', '| Lexalytics', 'Lexalytics ,', ', Inc.', 'Inc. ,', ', 48', '48 North', 'North Pleasant', 'Pleasant St.', 'St. Unit', 'Unit 301', '301 ,', ', Amherst', 'Amherst 01002', '01002 USA', 'USA |', '| 1-800-377-8036', '1-800-377-8036 |', '| www.lexalytics.com', 'www.lexalytics.com C', 'C N', 'N G', 'G V', 'V .'] 

 TOTAL BIGRAMS --> 43 



 ---- TRI-GRAMS ---- 

 ['AI Assembler (', 'Assembler ( proprietary', '( proprietary software', 'proprietary software )', 'software ) https', ') https :', 'https : //www.lexalytics.com/', ': //www.lexalytics.com/ https', '//www.lexalytics.com/ https :', 'https : //www.lexalytics.com/', ': //www.lexalytics.com/ W', '//www.lexalytics.com/ W H', 'W H E', 'H E P', 'E P P', 'P P E', 'P E R', 'E R 9|', 'R 9| |', '9| | Lexalytics', '| Lexalytics ,', 'Lexalytics , Inc.', ', Inc. ,', 'Inc. , 48', ', 48 North', '48 North Pleasant', 'North Pleasant St.', 'Pleasant St. Unit', 'St. Unit 301', 'Unit 301 ,', '301 , Amherst', ', Amherst 01002', 'Amherst 01002 USA', '01002 USA |', 'USA | 1-800-377-8036', '| 1-800-377-8036 |', '1-800-377-8036 | www.lexalytics.com', '| www.lexalytics.com C', 'www.lexalytics.com C N', 'C N G', 'N G V', 'G V .'] 

 TOTAL TRIGRAMS --> 42 



 ---- NOUN PHRASES ---- 

 ['https', ' https', 'www.lexalytics.com'] 

 TOTAL NOUN PHRASES --> 3 



 ---- NER ----

 
 ORGANIZATION ---> ['USA']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> ['North']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['ai', 'assembl', '(', 'proprietari', 'softwar', ')', 'http', ':', '//www.lexalytics.com/', 'http', ':', '//www.lexalytics.com/', 'w', 'h', 'e', 'p', 'p', 'e', 'r', '9|', '|', 'lexalyt', ',', 'inc.', ',', '48', 'north', 'pleasant', 'st.', 'unit', '301', ',', 'amherst', '01002', 'usa', '|', '1-800-377-8036', '|', 'www.lexalytics.com', 'c', 'n', 'g', 'v', '.']

 TOTAL PORTER STEM WORDS ==> 44



 ---- SNOWBALL STEMMING ----

['ai', 'assembl', '(', 'proprietari', 'softwar', ')', 'https', ':', '//www.lexalytics.com/', 'https', ':', '//www.lexalytics.com/', 'w', 'h', 'e', 'p', 'p', 'e', 'r', '9|', '|', 'lexalyt', ',', 'inc.', ',', '48', 'north', 'pleasant', 'st.', 'unit', '301', ',', 'amherst', '01002', 'usa', '|', '1-800-377-8036', '|', 'www.lexalytics.com', 'c', 'n', 'g', 'v', '.']

 TOTAL SNOWBALL STEM WORDS ==> 44



 ---- LEMMATIZATION ----

['AI', 'Assembler', '(', 'proprietary', 'software', ')', 'http', ':', '//www.lexalytics.com/', 'http', ':', '//www.lexalytics.com/', 'W', 'H', 'E', 'P', 'P', 'E', 'R', '9|', '|', 'Lexalytics', ',', 'Inc.', ',', '48', 'North', 'Pleasant', 'St.', 'Unit', '301', ',', 'Amherst', '01002', 'USA', '|', '1-800-377-8036', '|', 'www.lexalytics.com', 'C', 'N', 'G', 'V', '.']

 TOTAL LEMMATIZE WORDS ==> 44

************************************************************************************************************************

84 --> L E A R N I N G :   M A K I N G  T H E  C A S E  F O R  E A C H  Let’s use the same sentence for this next example. 


 ---- TOKENS ----

 ['L', 'E', 'A', 'R', 'N', 'I', 'N', 'G', ':', 'M', 'A', 'K', 'I', 'N', 'G', 'T', 'H', 'E', 'C', 'A', 'S', 'E', 'F', 'O', 'R', 'E', 'A', 'C', 'H', 'Let', '’', 's', 'use', 'the', 'same', 'sentence', 'for', 'this', 'next', 'example', '.'] 

 TOTAL TOKENS ==> 41

 ---- POST ----

 [('L', 'NNP'), ('E', 'NNP'), ('A', 'NNP'), ('R', 'NNP'), ('N', 'NNP'), ('I', 'PRP'), ('N', 'NNP'), ('G', 'NNP'), (':', ':'), ('M', 'NNP'), ('A', 'NNP'), ('K', 'NNP'), ('I', 'PRP'), ('N', 'NNP'), ('G', 'NNP'), ('T', 'NNP'), ('H', 'NNP'), ('E', 'NNP'), ('C', 'NNP'), ('A', 'NNP'), ('S', 'NNP'), ('E', 'NNP'), ('F', 'NNP'), ('O', 'NNP'), ('R', 'NNP'), ('E', 'NNP'), ('A', 'NNP'), ('C', 'NNP'), ('H', 'NNP'), ('Let', 'NNP'), ('’', 'NNP'), ('s', 'VB'), ('use', 'VBP'), ('the', 'DT'), ('same', 'JJ'), ('sentence', 'NN'), ('for', 'IN'), ('this', 'DT'), ('next', 'JJ'), ('example', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['L', 'E', 'R', 'N', 'N', 'G', ':', 'K', 'N', 'G', 'H', 'E', 'C', 'E', 'F', 'R', 'E', 'C', 'H', 'Let', '’', 'use', 'sentence', 'next', 'example', '.']

 TOTAL FILTERED TOKENS ==>  26

 ---- POST FOR FILTERED TOKENS ----

 [('L', 'NNP'), ('E', 'NNP'), ('R', 'NNP'), ('N', 'NNP'), ('N', 'NNP'), ('G', 'NNP'), (':', ':'), ('K', 'NNP'), ('N', 'NNP'), ('G', 'NNP'), ('H', 'NNP'), ('E', 'NNP'), ('C', 'NNP'), ('E', 'NNP'), ('F', 'NNP'), ('R', 'NNP'), ('E', 'NNP'), ('C', 'NNP'), ('H', 'NNP'), ('Let', 'NNP'), ('’', 'NNP'), ('use', 'VB'), ('sentence', 'NN'), ('next', 'JJ'), ('example', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['L E', 'E R', 'R N', 'N N', 'N G', 'G :', ': K', 'K N', 'N G', 'G H', 'H E', 'E C', 'C E', 'E F', 'F R', 'R E', 'E C', 'C H', 'H Let', 'Let ’', '’ use', 'use sentence', 'sentence next', 'next example', 'example .'] 

 TOTAL BIGRAMS --> 25 



 ---- TRI-GRAMS ---- 

 ['L E R', 'E R N', 'R N N', 'N N G', 'N G :', 'G : K', ': K N', 'K N G', 'N G H', 'G H E', 'H E C', 'E C E', 'C E F', 'E F R', 'F R E', 'R E C', 'E C H', 'C H Let', 'H Let ’', 'Let ’ use', '’ use sentence', 'use sentence next', 'sentence next example', 'next example .'] 

 TOTAL TRIGRAMS --> 24 



 ---- NOUN PHRASES ---- 

 ['sentence', 'next example'] 

 TOTAL NOUN PHRASES --> 2 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['l', 'e', 'r', 'n', 'n', 'g', ':', 'k', 'n', 'g', 'h', 'e', 'c', 'e', 'f', 'r', 'e', 'c', 'h', 'let', '’', 'use', 'sentenc', 'next', 'exampl', '.']

 TOTAL PORTER STEM WORDS ==> 26



 ---- SNOWBALL STEMMING ----

['l', 'e', 'r', 'n', 'n', 'g', ':', 'k', 'n', 'g', 'h', 'e', 'c', 'e', 'f', 'r', 'e', 'c', 'h', 'let', '’', 'use', 'sentenc', 'next', 'exampl', '.']

 TOTAL SNOWBALL STEM WORDS ==> 26



 ---- LEMMATIZATION ----

['L', 'E', 'R', 'N', 'N', 'G', ':', 'K', 'N', 'G', 'H', 'E', 'C', 'E', 'F', 'R', 'E', 'C', 'H', 'Let', '’', 'use', 'sentence', 'next', 'example', '.']

 TOTAL LEMMATIZE WORDS ==> 26

************************************************************************************************************************

85 --> Let me hear you say: “Lexalytics is the best text analytics company ever.” Now look at the period at the end of the sentence. 


 ---- TOKENS ----

 ['Let', 'me', 'hear', 'you', 'say', ':', '“', 'Lexalytics', 'is', 'the', 'best', 'text', 'analytics', 'company', 'ever.', '”', 'Now', 'look', 'at', 'the', 'period', 'at', 'the', 'end', 'of', 'the', 'sentence', '.'] 

 TOTAL TOKENS ==> 28

 ---- POST ----

 [('Let', 'VB'), ('me', 'PRP'), ('hear', 'VB'), ('you', 'PRP'), ('say', 'VBP'), (':', ':'), ('“', 'JJ'), ('Lexalytics', 'NNP'), ('is', 'VBZ'), ('the', 'DT'), ('best', 'JJS'), ('text', 'NN'), ('analytics', 'NNS'), ('company', 'NN'), ('ever.', 'VBZ'), ('”', 'RB'), ('Now', 'RB'), ('look', 'VBP'), ('at', 'IN'), ('the', 'DT'), ('period', 'NN'), ('at', 'IN'), ('the', 'DT'), ('end', 'NN'), ('of', 'IN'), ('the', 'DT'), ('sentence', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Let', 'hear', 'say', ':', '“', 'Lexalytics', 'best', 'text', 'analytics', 'company', 'ever.', '”', 'look', 'period', 'end', 'sentence', '.']

 TOTAL FILTERED TOKENS ==>  17

 ---- POST FOR FILTERED TOKENS ----

 [('Let', 'VB'), ('hear', 'JJ'), ('say', 'VB'), (':', ':'), ('“', 'JJ'), ('Lexalytics', 'NNP'), ('best', 'JJS'), ('text', 'NN'), ('analytics', 'NNS'), ('company', 'NN'), ('ever.', 'VBP'), ('”', 'JJ'), ('look', 'NN'), ('period', 'NN'), ('end', 'NN'), ('sentence', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Let hear', 'hear say', 'say :', ': “', '“ Lexalytics', 'Lexalytics best', 'best text', 'text analytics', 'analytics company', 'company ever.', 'ever. ”', '” look', 'look period', 'period end', 'end sentence', 'sentence .'] 

 TOTAL BIGRAMS --> 16 



 ---- TRI-GRAMS ---- 

 ['Let hear say', 'hear say :', 'say : “', ': “ Lexalytics', '“ Lexalytics best', 'Lexalytics best text', 'best text analytics', 'text analytics company', 'analytics company ever.', 'company ever. ”', 'ever. ” look', '” look period', 'look period end', 'period end sentence', 'end sentence .'] 

 TOTAL TRIGRAMS --> 15 



 ---- NOUN PHRASES ---- 

 ['text', 'company', '” look', 'period', 'end', 'sentence'] 

 TOTAL NOUN PHRASES --> 6 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['let', 'hear', 'say', ':', '“', 'lexalyt', 'best', 'text', 'analyt', 'compani', 'ever.', '”', 'look', 'period', 'end', 'sentenc', '.']

 TOTAL PORTER STEM WORDS ==> 17



 ---- SNOWBALL STEMMING ----

['let', 'hear', 'say', ':', '“', 'lexalyt', 'best', 'text', 'analyt', 'compani', 'ever.', '”', 'look', 'period', 'end', 'sentenc', '.']

 TOTAL SNOWBALL STEM WORDS ==> 17



 ---- LEMMATIZATION ----

['Let', 'hear', 'say', ':', '“', 'Lexalytics', 'best', 'text', 'analytics', 'company', 'ever.', '”', 'look', 'period', 'end', 'sentence', '.']

 TOTAL LEMMATIZE WORDS ==> 17

************************************************************************************************************************

86 --> Periods are important   in English because they frequently denote the end of a sentence. 


 ---- TOKENS ----

 ['Periods', 'are', 'important', 'in', 'English', 'because', 'they', 'frequently', 'denote', 'the', 'end', 'of', 'a', 'sentence', '.'] 

 TOTAL TOKENS ==> 15

 ---- POST ----

 [('Periods', 'NNS'), ('are', 'VBP'), ('important', 'JJ'), ('in', 'IN'), ('English', 'NNP'), ('because', 'IN'), ('they', 'PRP'), ('frequently', 'RB'), ('denote', 'VBP'), ('the', 'DT'), ('end', 'NN'), ('of', 'IN'), ('a', 'DT'), ('sentence', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Periods', 'important', 'English', 'frequently', 'denote', 'end', 'sentence', '.']

 TOTAL FILTERED TOKENS ==>  8

 ---- POST FOR FILTERED TOKENS ----

 [('Periods', 'NNS'), ('important', 'JJ'), ('English', 'NNP'), ('frequently', 'RB'), ('denote', 'VBP'), ('end', 'JJ'), ('sentence', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Periods important', 'important English', 'English frequently', 'frequently denote', 'denote end', 'end sentence', 'sentence .'] 

 TOTAL BIGRAMS --> 7 



 ---- TRI-GRAMS ---- 

 ['Periods important English', 'important English frequently', 'English frequently denote', 'frequently denote end', 'denote end sentence', 'end sentence .'] 

 TOTAL TRIGRAMS --> 6 



 ---- NOUN PHRASES ---- 

 ['end sentence'] 

 TOTAL NOUN PHRASES --> 1 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['period', 'import', 'english', 'frequent', 'denot', 'end', 'sentenc', '.']

 TOTAL PORTER STEM WORDS ==> 8



 ---- SNOWBALL STEMMING ----

['period', 'import', 'english', 'frequent', 'denot', 'end', 'sentenc', '.']

 TOTAL SNOWBALL STEM WORDS ==> 8



 ---- LEMMATIZATION ----

['Periods', 'important', 'English', 'frequently', 'denote', 'end', 'sentence', '.']

 TOTAL LEMMATIZE WORDS ==> 8

************************************************************************************************************************

87 --> It’s  important to be able to break sentences apart so that you can figure out  which statements go together. 


 ---- TOKENS ----

 ['It', '’', 's', 'important', 'to', 'be', 'able', 'to', 'break', 'sentences', 'apart', 'so', 'that', 'you', 'can', 'figure', 'out', 'which', 'statements', 'go', 'together', '.'] 

 TOTAL TOKENS ==> 22

 ---- POST ----

 [('It', 'PRP'), ('’', 'VBZ'), ('s', 'JJ'), ('important', 'JJ'), ('to', 'TO'), ('be', 'VB'), ('able', 'JJ'), ('to', 'TO'), ('break', 'VB'), ('sentences', 'NNS'), ('apart', 'RB'), ('so', 'IN'), ('that', 'IN'), ('you', 'PRP'), ('can', 'MD'), ('figure', 'VB'), ('out', 'RP'), ('which', 'WDT'), ('statements', 'NNS'), ('go', 'VB'), ('together', 'RB'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['’', 'important', 'able', 'break', 'sentences', 'apart', 'figure', 'statements', 'go', 'together', '.']

 TOTAL FILTERED TOKENS ==>  11

 ---- POST FOR FILTERED TOKENS ----

 [('’', 'RBS'), ('important', 'JJ'), ('able', 'JJ'), ('break', 'NN'), ('sentences', 'NNS'), ('apart', 'RB'), ('figure', 'NN'), ('statements', 'NNS'), ('go', 'VB'), ('together', 'RB'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['’ important', 'important able', 'able break', 'break sentences', 'sentences apart', 'apart figure', 'figure statements', 'statements go', 'go together', 'together .'] 

 TOTAL BIGRAMS --> 10 



 ---- TRI-GRAMS ---- 

 ['’ important able', 'important able break', 'able break sentences', 'break sentences apart', 'sentences apart figure', 'apart figure statements', 'figure statements go', 'statements go together', 'go together .'] 

 TOTAL TRIGRAMS --> 9 



 ---- NOUN PHRASES ---- 

 ['important able break', 'figure'] 

 TOTAL NOUN PHRASES --> 2 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['’', 'import', 'abl', 'break', 'sentenc', 'apart', 'figur', 'statement', 'go', 'togeth', '.']

 TOTAL PORTER STEM WORDS ==> 11



 ---- SNOWBALL STEMMING ----

['’', 'import', 'abl', 'break', 'sentenc', 'apart', 'figur', 'statement', 'go', 'togeth', '.']

 TOTAL SNOWBALL STEM WORDS ==> 11



 ---- LEMMATIZATION ----

['’', 'important', 'able', 'break', 'sentence', 'apart', 'figure', 'statement', 'go', 'together', '.']

 TOTAL LEMMATIZE WORDS ==> 11

************************************************************************************************************************

88 --> However, periods also denote other things, like “Dr.” for doctor, or “Mr.”   for mister. 


 ---- TOKENS ----

 ['However', ',', 'periods', 'also', 'denote', 'other', 'things', ',', 'like', '“', 'Dr.', '”', 'for', 'doctor', ',', 'or', '“', 'Mr.', '”', 'for', 'mister', '.'] 

 TOTAL TOKENS ==> 22

 ---- POST ----

 [('However', 'RB'), (',', ','), ('periods', 'NNS'), ('also', 'RB'), ('denote', 'VBP'), ('other', 'JJ'), ('things', 'NNS'), (',', ','), ('like', 'IN'), ('“', 'NNP'), ('Dr.', 'NNP'), ('”', 'NNP'), ('for', 'IN'), ('doctor', 'NN'), (',', ','), ('or', 'CC'), ('“', 'VBZ'), ('Mr.', 'NNP'), ('”', 'NNP'), ('for', 'IN'), ('mister', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['However', ',', 'periods', 'also', 'denote', 'things', ',', 'like', '“', 'Dr.', '”', 'doctor', ',', '“', 'Mr.', '”', 'mister', '.']

 TOTAL FILTERED TOKENS ==>  18

 ---- POST FOR FILTERED TOKENS ----

 [('However', 'RB'), (',', ','), ('periods', 'NNS'), ('also', 'RB'), ('denote', 'VBP'), ('things', 'NNS'), (',', ','), ('like', 'IN'), ('“', 'NNP'), ('Dr.', 'NNP'), ('”', 'NNP'), ('doctor', 'NN'), (',', ','), ('“', 'VBZ'), ('Mr.', 'NNP'), ('”', 'NNP'), ('mister', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['However ,', ', periods', 'periods also', 'also denote', 'denote things', 'things ,', ', like', 'like “', '“ Dr.', 'Dr. ”', '” doctor', 'doctor ,', ', “', '“ Mr.', 'Mr. ”', '” mister', 'mister .'] 

 TOTAL BIGRAMS --> 17 



 ---- TRI-GRAMS ---- 

 ['However , periods', ', periods also', 'periods also denote', 'also denote things', 'denote things ,', 'things , like', ', like “', 'like “ Dr.', '“ Dr. ”', 'Dr. ” doctor', '” doctor ,', 'doctor , “', ', “ Mr.', '“ Mr. ”', 'Mr. ” mister', '” mister .'] 

 TOTAL TRIGRAMS --> 16 



 ---- NOUN PHRASES ---- 

 ['doctor', 'mister'] 

 TOTAL NOUN PHRASES --> 2 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> ['Mr.']
 TOTAL PERSON ENTITY --> 1 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['howev', ',', 'period', 'also', 'denot', 'thing', ',', 'like', '“', 'dr.', '”', 'doctor', ',', '“', 'mr.', '”', 'mister', '.']

 TOTAL PORTER STEM WORDS ==> 18



 ---- SNOWBALL STEMMING ----

['howev', ',', 'period', 'also', 'denot', 'thing', ',', 'like', '“', 'dr.', '”', 'doctor', ',', '“', 'mr.', '”', 'mister', '.']

 TOTAL SNOWBALL STEM WORDS ==> 18



 ---- LEMMATIZATION ----

['However', ',', 'period', 'also', 'denote', 'thing', ',', 'like', '“', 'Dr.', '”', 'doctor', ',', '“', 'Mr.', '”', 'mister', '.']

 TOTAL LEMMATIZE WORDS ==> 18

************************************************************************************************************************

89 --> How can a machine tell whether the period is denoting the end   of a sentence or a form of address? 


 ---- TOKENS ----

 ['How', 'can', 'a', 'machine', 'tell', 'whether', 'the', 'period', 'is', 'denoting', 'the', 'end', 'of', 'a', 'sentence', 'or', 'a', 'form', 'of', 'address', '?'] 

 TOTAL TOKENS ==> 21

 ---- POST ----

 [('How', 'WRB'), ('can', 'MD'), ('a', 'DT'), ('machine', 'NN'), ('tell', 'NN'), ('whether', 'IN'), ('the', 'DT'), ('period', 'NN'), ('is', 'VBZ'), ('denoting', 'VBG'), ('the', 'DT'), ('end', 'NN'), ('of', 'IN'), ('a', 'DT'), ('sentence', 'NN'), ('or', 'CC'), ('a', 'DT'), ('form', 'NN'), ('of', 'IN'), ('address', 'NN'), ('?', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['machine', 'tell', 'whether', 'period', 'denoting', 'end', 'sentence', 'form', 'address', '?']

 TOTAL FILTERED TOKENS ==>  10

 ---- POST FOR FILTERED TOKENS ----

 [('machine', 'NN'), ('tell', 'VBP'), ('whether', 'IN'), ('period', 'NN'), ('denoting', 'VBG'), ('end', 'JJ'), ('sentence', 'NN'), ('form', 'NN'), ('address', 'NN'), ('?', '.')] 



 ---- BI-GRAMS ---- 

 ['machine tell', 'tell whether', 'whether period', 'period denoting', 'denoting end', 'end sentence', 'sentence form', 'form address', 'address ?'] 

 TOTAL BIGRAMS --> 9 



 ---- TRI-GRAMS ---- 

 ['machine tell whether', 'tell whether period', 'whether period denoting', 'period denoting end', 'denoting end sentence', 'end sentence form', 'sentence form address', 'form address ?'] 

 TOTAL TRIGRAMS --> 8 



 ---- NOUN PHRASES ---- 

 ['machine', 'period', 'end sentence', 'form', 'address'] 

 TOTAL NOUN PHRASES --> 5 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['machin', 'tell', 'whether', 'period', 'denot', 'end', 'sentenc', 'form', 'address', '?']

 TOTAL PORTER STEM WORDS ==> 10



 ---- SNOWBALL STEMMING ----

['machin', 'tell', 'whether', 'period', 'denot', 'end', 'sentenc', 'form', 'address', '?']

 TOTAL SNOWBALL STEM WORDS ==> 10



 ---- LEMMATIZATION ----

['machine', 'tell', 'whether', 'period', 'denoting', 'end', 'sentence', 'form', 'address', '?']

 TOTAL LEMMATIZE WORDS ==> 10

************************************************************************************************************************

90 --> We could train a machine learning model  for this task by marking up examples of each. 


 ---- TOKENS ----

 ['We', 'could', 'train', 'a', 'machine', 'learning', 'model', 'for', 'this', 'task', 'by', 'marking', 'up', 'examples', 'of', 'each', '.'] 

 TOTAL TOKENS ==> 17

 ---- POST ----

 [('We', 'PRP'), ('could', 'MD'), ('train', 'VB'), ('a', 'DT'), ('machine', 'NN'), ('learning', 'VBG'), ('model', 'NN'), ('for', 'IN'), ('this', 'DT'), ('task', 'NN'), ('by', 'IN'), ('marking', 'VBG'), ('up', 'RP'), ('examples', 'NNS'), ('of', 'IN'), ('each', 'DT'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['could', 'train', 'machine', 'learning', 'model', 'task', 'marking', 'examples', '.']

 TOTAL FILTERED TOKENS ==>  9

 ---- POST FOR FILTERED TOKENS ----

 [('could', 'MD'), ('train', 'VB'), ('machine', 'NN'), ('learning', 'VBG'), ('model', 'NN'), ('task', 'NN'), ('marking', 'VBG'), ('examples', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['could train', 'train machine', 'machine learning', 'learning model', 'model task', 'task marking', 'marking examples', 'examples .'] 

 TOTAL BIGRAMS --> 8 



 ---- TRI-GRAMS ---- 

 ['could train machine', 'train machine learning', 'machine learning model', 'learning model task', 'model task marking', 'task marking examples', 'marking examples .'] 

 TOTAL TRIGRAMS --> 7 



 ---- NOUN PHRASES ---- 

 ['machine', 'model', 'task'] 

 TOTAL NOUN PHRASES --> 3 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['could', 'train', 'machin', 'learn', 'model', 'task', 'mark', 'exampl', '.']

 TOTAL PORTER STEM WORDS ==> 9



 ---- SNOWBALL STEMMING ----

['could', 'train', 'machin', 'learn', 'model', 'task', 'mark', 'exampl', '.']

 TOTAL SNOWBALL STEM WORDS ==> 9



 ---- LEMMATIZATION ----

['could', 'train', 'machine', 'learning', 'model', 'task', 'marking', 'example', '.']

 TOTAL LEMMATIZE WORDS ==> 9

************************************************************************************************************************

91 --> But this isn’t necessarily the  most efficient approach. 


 ---- TOKENS ----

 ['But', 'this', 'isn', '’', 't', 'necessarily', 'the', 'most', 'efficient', 'approach', '.'] 

 TOTAL TOKENS ==> 11

 ---- POST ----

 [('But', 'CC'), ('this', 'DT'), ('isn', 'JJ'), ('’', 'NNP'), ('t', 'NN'), ('necessarily', 'RB'), ('the', 'DT'), ('most', 'RBS'), ('efficient', 'JJ'), ('approach', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['’', 'necessarily', 'efficient', 'approach', '.']

 TOTAL FILTERED TOKENS ==>  5

 ---- POST FOR FILTERED TOKENS ----

 [('’', 'JJ'), ('necessarily', 'RB'), ('efficient', 'JJ'), ('approach', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['’ necessarily', 'necessarily efficient', 'efficient approach', 'approach .'] 

 TOTAL BIGRAMS --> 4 



 ---- TRI-GRAMS ---- 

 ['’ necessarily efficient', 'necessarily efficient approach', 'efficient approach .'] 

 TOTAL TRIGRAMS --> 3 



 ---- NOUN PHRASES ---- 

 ['efficient approach'] 

 TOTAL NOUN PHRASES --> 1 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['’', 'necessarili', 'effici', 'approach', '.']

 TOTAL PORTER STEM WORDS ==> 5



 ---- SNOWBALL STEMMING ----

['’', 'necessarili', 'effici', 'approach', '.']

 TOTAL SNOWBALL STEM WORDS ==> 5



 ---- LEMMATIZATION ----

['’', 'necessarily', 'efficient', 'approach', '.']

 TOTAL LEMMATIZE WORDS ==> 5

************************************************************************************************************************

92 --> After all, there are only a few cases in the English  language where the period is used for anything other than denoting the end  of a sentence. 


 ---- TOKENS ----

 ['After', 'all', ',', 'there', 'are', 'only', 'a', 'few', 'cases', 'in', 'the', 'English', 'language', 'where', 'the', 'period', 'is', 'used', 'for', 'anything', 'other', 'than', 'denoting', 'the', 'end', 'of', 'a', 'sentence', '.'] 

 TOTAL TOKENS ==> 29

 ---- POST ----

 [('After', 'IN'), ('all', 'DT'), (',', ','), ('there', 'EX'), ('are', 'VBP'), ('only', 'RB'), ('a', 'DT'), ('few', 'JJ'), ('cases', 'NNS'), ('in', 'IN'), ('the', 'DT'), ('English', 'JJ'), ('language', 'NN'), ('where', 'WRB'), ('the', 'DT'), ('period', 'NN'), ('is', 'VBZ'), ('used', 'VBN'), ('for', 'IN'), ('anything', 'NN'), ('other', 'JJ'), ('than', 'IN'), ('denoting', 'VBG'), ('the', 'DT'), ('end', 'NN'), ('of', 'IN'), ('a', 'DT'), ('sentence', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 [',', 'cases', 'English', 'language', 'period', 'used', 'anything', 'denoting', 'end', 'sentence', '.']

 TOTAL FILTERED TOKENS ==>  11

 ---- POST FOR FILTERED TOKENS ----

 [(',', ','), ('cases', 'NNS'), ('English', 'JJ'), ('language', 'NN'), ('period', 'NN'), ('used', 'VBD'), ('anything', 'NN'), ('denoting', 'VBG'), ('end', 'JJ'), ('sentence', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 [', cases', 'cases English', 'English language', 'language period', 'period used', 'used anything', 'anything denoting', 'denoting end', 'end sentence', 'sentence .'] 

 TOTAL BIGRAMS --> 10 



 ---- TRI-GRAMS ---- 

 [', cases English', 'cases English language', 'English language period', 'language period used', 'period used anything', 'used anything denoting', 'anything denoting end', 'denoting end sentence', 'end sentence .'] 

 TOTAL TRIGRAMS --> 9 



 ---- NOUN PHRASES ---- 

 ['English language', 'period', 'anything', 'end sentence'] 

 TOTAL NOUN PHRASES --> 4 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> ['English']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

[',', 'case', 'english', 'languag', 'period', 'use', 'anyth', 'denot', 'end', 'sentenc', '.']

 TOTAL PORTER STEM WORDS ==> 11



 ---- SNOWBALL STEMMING ----

[',', 'case', 'english', 'languag', 'period', 'use', 'anyth', 'denot', 'end', 'sentenc', '.']

 TOTAL SNOWBALL STEM WORDS ==> 11



 ---- LEMMATIZATION ----

[',', 'case', 'English', 'language', 'period', 'used', 'anything', 'denoting', 'end', 'sentence', '.']

 TOTAL LEMMATIZE WORDS ==> 11

************************************************************************************************************************

93 --> It is more efficient, faster computationally, and more precise  to just hard-code these cases using NLP code or other algorithms. 


 ---- TOKENS ----

 ['It', 'is', 'more', 'efficient', ',', 'faster', 'computationally', ',', 'and', 'more', 'precise', 'to', 'just', 'hard-code', 'these', 'cases', 'using', 'NLP', 'code', 'or', 'other', 'algorithms', '.'] 

 TOTAL TOKENS ==> 23

 ---- POST ----

 [('It', 'PRP'), ('is', 'VBZ'), ('more', 'RBR'), ('efficient', 'JJ'), (',', ','), ('faster', 'RBR'), ('computationally', 'RB'), (',', ','), ('and', 'CC'), ('more', 'RBR'), ('precise', 'JJ'), ('to', 'TO'), ('just', 'RB'), ('hard-code', 'VB'), ('these', 'DT'), ('cases', 'NNS'), ('using', 'VBG'), ('NLP', 'NNP'), ('code', 'NN'), ('or', 'CC'), ('other', 'JJ'), ('algorithms', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['efficient', ',', 'faster', 'computationally', ',', 'precise', 'hard-code', 'cases', 'using', 'NLP', 'code', 'algorithms', '.']

 TOTAL FILTERED TOKENS ==>  13

 ---- POST FOR FILTERED TOKENS ----

 [('efficient', 'NN'), (',', ','), ('faster', 'RBR'), ('computationally', 'RB'), (',', ','), ('precise', 'JJ'), ('hard-code', 'NN'), ('cases', 'NNS'), ('using', 'VBG'), ('NLP', 'NNP'), ('code', 'NN'), ('algorithms', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['efficient ,', ', faster', 'faster computationally', 'computationally ,', ', precise', 'precise hard-code', 'hard-code cases', 'cases using', 'using NLP', 'NLP code', 'code algorithms', 'algorithms .'] 

 TOTAL BIGRAMS --> 12 



 ---- TRI-GRAMS ---- 

 ['efficient , faster', ', faster computationally', 'faster computationally ,', 'computationally , precise', ', precise hard-code', 'precise hard-code cases', 'hard-code cases using', 'cases using NLP', 'using NLP code', 'NLP code algorithms', 'code algorithms .'] 

 TOTAL TRIGRAMS --> 11 



 ---- NOUN PHRASES ---- 

 ['efficient', 'precise hard-code', 'code', 'algorithms'] 

 TOTAL NOUN PHRASES --> 4 



 ---- NER ----

 
 ORGANIZATION ---> ['NLP']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['effici', ',', 'faster', 'comput', ',', 'precis', 'hard-cod', 'case', 'use', 'nlp', 'code', 'algorithm', '.']

 TOTAL PORTER STEM WORDS ==> 13



 ---- SNOWBALL STEMMING ----

['effici', ',', 'faster', 'comput', ',', 'precis', 'hard-cod', 'case', 'use', 'nlp', 'code', 'algorithm', '.']

 TOTAL SNOWBALL STEM WORDS ==> 13



 ---- LEMMATIZATION ----

['efficient', ',', 'faster', 'computationally', ',', 'precise', 'hard-code', 'case', 'using', 'NLP', 'code', 'algorithm', '.']

 TOTAL LEMMATIZE WORDS ==> 13

************************************************************************************************************************

94 --> Where there are cases that are best handled by NLP code, we write NLP  code or use rules. 


 ---- TOKENS ----

 ['Where', 'there', 'are', 'cases', 'that', 'are', 'best', 'handled', 'by', 'NLP', 'code', ',', 'we', 'write', 'NLP', 'code', 'or', 'use', 'rules', '.'] 

 TOTAL TOKENS ==> 20

 ---- POST ----

 [('Where', 'WRB'), ('there', 'EX'), ('are', 'VBP'), ('cases', 'NNS'), ('that', 'WDT'), ('are', 'VBP'), ('best', 'RBS'), ('handled', 'VBN'), ('by', 'IN'), ('NLP', 'NNP'), ('code', 'NN'), (',', ','), ('we', 'PRP'), ('write', 'VBP'), ('NLP', 'JJ'), ('code', 'NN'), ('or', 'CC'), ('use', 'NN'), ('rules', 'NNS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['cases', 'best', 'handled', 'NLP', 'code', ',', 'write', 'NLP', 'code', 'use', 'rules', '.']

 TOTAL FILTERED TOKENS ==>  12

 ---- POST FOR FILTERED TOKENS ----

 [('cases', 'NNS'), ('best', 'RBS'), ('handled', 'VBD'), ('NLP', 'NNP'), ('code', 'NN'), (',', ','), ('write', 'JJ'), ('NLP', 'NNP'), ('code', 'NN'), ('use', 'NN'), ('rules', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['cases best', 'best handled', 'handled NLP', 'NLP code', 'code ,', ', write', 'write NLP', 'NLP code', 'code use', 'use rules', 'rules .'] 

 TOTAL BIGRAMS --> 11 



 ---- TRI-GRAMS ---- 

 ['cases best handled', 'best handled NLP', 'handled NLP code', 'NLP code ,', 'code , write', ', write NLP', 'write NLP code', 'NLP code use', 'code use rules', 'use rules .'] 

 TOTAL TRIGRAMS --> 10 



 ---- NOUN PHRASES ---- 

 ['code', 'code', 'use'] 

 TOTAL NOUN PHRASES --> 3 



 ---- NER ----

 
 ORGANIZATION ---> ['NLP', 'NLP']
 TOTAL ORGANIZATION ENTITY --> 2 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['case', 'best', 'handl', 'nlp', 'code', ',', 'write', 'nlp', 'code', 'use', 'rule', '.']

 TOTAL PORTER STEM WORDS ==> 12



 ---- SNOWBALL STEMMING ----

['case', 'best', 'handl', 'nlp', 'code', ',', 'write', 'nlp', 'code', 'use', 'rule', '.']

 TOTAL SNOWBALL STEM WORDS ==> 12



 ---- LEMMATIZATION ----

['case', 'best', 'handled', 'NLP', 'code', ',', 'write', 'NLP', 'code', 'use', 'rule', '.']

 TOTAL LEMMATIZE WORDS ==> 12

************************************************************************************************************************

95 --> When we need to build models, we build models. 


 ---- TOKENS ----

 ['When', 'we', 'need', 'to', 'build', 'models', ',', 'we', 'build', 'models', '.'] 

 TOTAL TOKENS ==> 11

 ---- POST ----

 [('When', 'WRB'), ('we', 'PRP'), ('need', 'VBP'), ('to', 'TO'), ('build', 'VB'), ('models', 'NNS'), (',', ','), ('we', 'PRP'), ('build', 'VBP'), ('models', 'NNS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['need', 'build', 'models', ',', 'build', 'models', '.']

 TOTAL FILTERED TOKENS ==>  7

 ---- POST FOR FILTERED TOKENS ----

 [('need', 'NN'), ('build', 'NN'), ('models', 'NNS'), (',', ','), ('build', 'JJ'), ('models', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['need build', 'build models', 'models ,', ', build', 'build models', 'models .'] 

 TOTAL BIGRAMS --> 6 



 ---- TRI-GRAMS ---- 

 ['need build models', 'build models ,', 'models , build', ', build models', 'build models .'] 

 TOTAL TRIGRAMS --> 5 



 ---- NOUN PHRASES ---- 

 ['need', 'build'] 

 TOTAL NOUN PHRASES --> 2 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['need', 'build', 'model', ',', 'build', 'model', '.']

 TOTAL PORTER STEM WORDS ==> 7



 ---- SNOWBALL STEMMING ----

['need', 'build', 'model', ',', 'build', 'model', '.']

 TOTAL SNOWBALL STEM WORDS ==> 7



 ---- LEMMATIZATION ----

['need', 'build', 'model', ',', 'build', 'model', '.']

 TOTAL LEMMATIZE WORDS ==> 7

************************************************************************************************************************

96 --> When we need NLP code or rules,   we write them; when we need   models, we build them. 


 ---- TOKENS ----

 ['When', 'we', 'need', 'NLP', 'code', 'or', 'rules', ',', 'we', 'write', 'them', ';', 'when', 'we', 'need', 'models', ',', 'we', 'build', 'them', '.'] 

 TOTAL TOKENS ==> 21

 ---- POST ----

 [('When', 'WRB'), ('we', 'PRP'), ('need', 'VBP'), ('NLP', 'JJ'), ('code', 'NN'), ('or', 'CC'), ('rules', 'NNS'), (',', ','), ('we', 'PRP'), ('write', 'VBP'), ('them', 'PRP'), (';', ':'), ('when', 'WRB'), ('we', 'PRP'), ('need', 'VBP'), ('models', 'NNS'), (',', ','), ('we', 'PRP'), ('build', 'VBP'), ('them', 'PRP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['need', 'NLP', 'code', 'rules', ',', 'write', ';', 'need', 'models', ',', 'build', '.']

 TOTAL FILTERED TOKENS ==>  12

 ---- POST FOR FILTERED TOKENS ----

 [('need', 'NN'), ('NLP', 'NNP'), ('code', 'NN'), ('rules', 'NNS'), (',', ','), ('write', 'VBP'), (';', ':'), ('need', 'NN'), ('models', 'NNS'), (',', ','), ('build', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['need NLP', 'NLP code', 'code rules', 'rules ,', ', write', 'write ;', '; need', 'need models', 'models ,', ', build', 'build .'] 

 TOTAL BIGRAMS --> 11 



 ---- TRI-GRAMS ---- 

 ['need NLP code', 'NLP code rules', 'code rules ,', 'rules , write', ', write ;', 'write ; need', '; need models', 'need models ,', 'models , build', ', build .'] 

 TOTAL TRIGRAMS --> 10 



 ---- NOUN PHRASES ---- 

 ['need', 'code', 'need', 'build'] 

 TOTAL NOUN PHRASES --> 4 



 ---- NER ----

 
 ORGANIZATION ---> ['NLP']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['need', 'nlp', 'code', 'rule', ',', 'write', ';', 'need', 'model', ',', 'build', '.']

 TOTAL PORTER STEM WORDS ==> 12



 ---- SNOWBALL STEMMING ----

['need', 'nlp', 'code', 'rule', ',', 'write', ';', 'need', 'model', ',', 'build', '.']

 TOTAL SNOWBALL STEM WORDS ==> 12



 ---- LEMMATIZATION ----

['need', 'NLP', 'code', 'rule', ',', 'write', ';', 'need', 'model', ',', 'build', '.']

 TOTAL LEMMATIZE WORDS ==> 12

************************************************************************************************************************

97 --> Practical   efficiencies: https://www.lexalytics.com/ https://www.lexalytics.com/  W H I T E  P A P E R 10|       |   Lexalytics, Inc., 48 North Pleasant St. Unit 301, Amherst MA 01002 USA   |   1-800-377-8036 |   www.lexalytics.com B L A C K  B O X / C L E A R  B O X :   L O O K I N G  I N S I D E  T H E  D A T A  It is important to understand not just what decision a model has made,   but why it has made that decision. 


 ---- TOKENS ----

 ['Practical', 'efficiencies', ':', 'https', ':', '//www.lexalytics.com/', 'https', ':', '//www.lexalytics.com/', 'W', 'H', 'I', 'T', 'E', 'P', 'A', 'P', 'E', 'R', '10|', '|', 'Lexalytics', ',', 'Inc.', ',', '48', 'North', 'Pleasant', 'St.', 'Unit', '301', ',', 'Amherst', 'MA', '01002', 'USA', '|', '1-800-377-8036', '|', 'www.lexalytics.com', 'B', 'L', 'A', 'C', 'K', 'B', 'O', 'X', '/', 'C', 'L', 'E', 'A', 'R', 'B', 'O', 'X', ':', 'L', 'O', 'O', 'K', 'I', 'N', 'G', 'I', 'N', 'S', 'I', 'D', 'E', 'T', 'H', 'E', 'D', 'A', 'T', 'A', 'It', 'is', 'important', 'to', 'understand', 'not', 'just', 'what', 'decision', 'a', 'model', 'has', 'made', ',', 'but', 'why', 'it', 'has', 'made', 'that', 'decision', '.'] 

 TOTAL TOKENS ==> 100

 ---- POST ----

 [('Practical', 'JJ'), ('efficiencies', 'NNS'), (':', ':'), ('https', 'NN'), (':', ':'), ('//www.lexalytics.com/', 'JJ'), ('https', 'NN'), (':', ':'), ('//www.lexalytics.com/', 'JJ'), ('W', 'NNP'), ('H', 'NNP'), ('I', 'PRP'), ('T', 'NNP'), ('E', 'NNP'), ('P', 'NNP'), ('A', 'NNP'), ('P', 'NNP'), ('E', 'NNP'), ('R', 'NNP'), ('10|', 'CD'), ('|', 'NNP'), ('Lexalytics', 'NNP'), (',', ','), ('Inc.', 'NNP'), (',', ','), ('48', 'CD'), ('North', 'NNP'), ('Pleasant', 'NNP'), ('St.', 'NNP'), ('Unit', 'NNP'), ('301', 'CD'), (',', ','), ('Amherst', 'NNP'), ('MA', 'NNP'), ('01002', 'CD'), ('USA', 'NNP'), ('|', 'NNP'), ('1-800-377-8036', 'JJ'), ('|', 'NNP'), ('www.lexalytics.com', 'NN'), ('B', 'NNP'), ('L', 'NNP'), ('A', 'NNP'), ('C', 'NNP'), ('K', 'NNP'), ('B', 'NNP'), ('O', 'NNP'), ('X', 'NNP'), ('/', 'NNP'), ('C', 'NNP'), ('L', 'NNP'), ('E', 'NNP'), ('A', 'NNP'), ('R', 'NNP'), ('B', 'NNP'), ('O', 'NNP'), ('X', 'NN'), (':', ':'), ('L', 'NNP'), ('O', 'NNP'), ('O', 'NNP'), ('K', 'NNP'), ('I', 'PRP'), ('N', 'NNP'), ('G', 'NNP'), ('I', 'PRP'), ('N', 'NNP'), ('S', 'NNP'), ('I', 'PRP'), ('D', 'NNP'), ('E', 'NNP'), ('T', 'NNP'), ('H', 'NNP'), ('E', 'NNP'), ('D', 'NNP'), ('A', 'NNP'), ('T', 'NNP'), ('A', 'NNP'), ('It', 'PRP'), ('is', 'VBZ'), ('important', 'JJ'), ('to', 'TO'), ('understand', 'VB'), ('not', 'RB'), ('just', 'RB'), ('what', 'WP'), ('decision', 'NN'), ('a', 'DT'), ('model', 'NN'), ('has', 'VBZ'), ('made', 'VBN'), (',', ','), ('but', 'CC'), ('why', 'WRB'), ('it', 'PRP'), ('has', 'VBZ'), ('made', 'VBN'), ('that', 'IN'), ('decision', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Practical', 'efficiencies', ':', 'https', ':', '//www.lexalytics.com/', 'https', ':', '//www.lexalytics.com/', 'W', 'H', 'E', 'P', 'P', 'E', 'R', '10|', '|', 'Lexalytics', ',', 'Inc.', ',', '48', 'North', 'Pleasant', 'St.', 'Unit', '301', ',', 'Amherst', '01002', 'USA', '|', '1-800-377-8036', '|', 'www.lexalytics.com', 'B', 'L', 'C', 'K', 'B', 'X', '/', 'C', 'L', 'E', 'R', 'B', 'X', ':', 'L', 'K', 'N', 'G', 'N', 'E', 'H', 'E', 'important', 'understand', 'decision', 'model', 'made', ',', 'made', 'decision', '.']

 TOTAL FILTERED TOKENS ==>  67

 ---- POST FOR FILTERED TOKENS ----

 [('Practical', 'JJ'), ('efficiencies', 'NNS'), (':', ':'), ('https', 'NN'), (':', ':'), ('//www.lexalytics.com/', 'JJ'), ('https', 'NN'), (':', ':'), ('//www.lexalytics.com/', 'JJ'), ('W', 'NNP'), ('H', 'NNP'), ('E', 'NNP'), ('P', 'NNP'), ('P', 'NNP'), ('E', 'NNP'), ('R', 'NNP'), ('10|', 'CD'), ('|', 'NNP'), ('Lexalytics', 'NNP'), (',', ','), ('Inc.', 'NNP'), (',', ','), ('48', 'CD'), ('North', 'NNP'), ('Pleasant', 'NNP'), ('St.', 'NNP'), ('Unit', 'NNP'), ('301', 'CD'), (',', ','), ('Amherst', 'NNP'), ('01002', 'CD'), ('USA', 'NNP'), ('|', 'NNP'), ('1-800-377-8036', 'JJ'), ('|', 'NNP'), ('www.lexalytics.com', 'NN'), ('B', 'NNP'), ('L', 'NNP'), ('C', 'NNP'), ('K', 'NNP'), ('B', 'NNP'), ('X', 'NNP'), ('/', 'NNP'), ('C', 'NNP'), ('L', 'NNP'), ('E', 'NNP'), ('R', 'NNP'), ('B', 'NNP'), ('X', 'NN'), (':', ':'), ('L', 'NNP'), ('K', 'NNP'), ('N', 'NNP'), ('G', 'NNP'), ('N', 'NNP'), ('E', 'NNP'), ('H', 'NNP'), ('E', 'NNP'), ('important', 'JJ'), ('understand', 'NN'), ('decision', 'NN'), ('model', 'NN'), ('made', 'VBD'), (',', ','), ('made', 'VBN'), ('decision', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Practical efficiencies', 'efficiencies :', ': https', 'https :', ': //www.lexalytics.com/', '//www.lexalytics.com/ https', 'https :', ': //www.lexalytics.com/', '//www.lexalytics.com/ W', 'W H', 'H E', 'E P', 'P P', 'P E', 'E R', 'R 10|', '10| |', '| Lexalytics', 'Lexalytics ,', ', Inc.', 'Inc. ,', ', 48', '48 North', 'North Pleasant', 'Pleasant St.', 'St. Unit', 'Unit 301', '301 ,', ', Amherst', 'Amherst 01002', '01002 USA', 'USA |', '| 1-800-377-8036', '1-800-377-8036 |', '| www.lexalytics.com', 'www.lexalytics.com B', 'B L', 'L C', 'C K', 'K B', 'B X', 'X /', '/ C', 'C L', 'L E', 'E R', 'R B', 'B X', 'X :', ': L', 'L K', 'K N', 'N G', 'G N', 'N E', 'E H', 'H E', 'E important', 'important understand', 'understand decision', 'decision model', 'model made', 'made ,', ', made', 'made decision', 'decision .'] 

 TOTAL BIGRAMS --> 66 



 ---- TRI-GRAMS ---- 

 ['Practical efficiencies :', 'efficiencies : https', ': https :', 'https : //www.lexalytics.com/', ': //www.lexalytics.com/ https', '//www.lexalytics.com/ https :', 'https : //www.lexalytics.com/', ': //www.lexalytics.com/ W', '//www.lexalytics.com/ W H', 'W H E', 'H E P', 'E P P', 'P P E', 'P E R', 'E R 10|', 'R 10| |', '10| | Lexalytics', '| Lexalytics ,', 'Lexalytics , Inc.', ', Inc. ,', 'Inc. , 48', ', 48 North', '48 North Pleasant', 'North Pleasant St.', 'Pleasant St. Unit', 'St. Unit 301', 'Unit 301 ,', '301 , Amherst', ', Amherst 01002', 'Amherst 01002 USA', '01002 USA |', 'USA | 1-800-377-8036', '| 1-800-377-8036 |', '1-800-377-8036 | www.lexalytics.com', '| www.lexalytics.com B', 'www.lexalytics.com B L', 'B L C', 'L C K', 'C K B', 'K B X', 'B X /', 'X / C', '/ C L', 'C L E', 'L E R', 'E R B', 'R B X', 'B X :', 'X : L', ': L K', 'L K N', 'K N G', 'N G N', 'G N E', 'N E H', 'E H E', 'H E important', 'E important understand', 'important understand decision', 'understand decision model', 'decision model made', 'model made ,', 'made , made', ', made decision', 'made decision .'] 

 TOTAL TRIGRAMS --> 65 



 ---- NOUN PHRASES ---- 

 ['https', ' https', 'www.lexalytics.com', 'X', 'important understand', 'decision', 'model', 'decision'] 

 TOTAL NOUN PHRASES --> 8 



 ---- NER ----

 
 ORGANIZATION ---> ['USA']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> ['Practical', 'North']
 TOTAL GPE ENTITY --> 2 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['practic', 'effici', ':', 'http', ':', '//www.lexalytics.com/', 'http', ':', '//www.lexalytics.com/', 'w', 'h', 'e', 'p', 'p', 'e', 'r', '10|', '|', 'lexalyt', ',', 'inc.', ',', '48', 'north', 'pleasant', 'st.', 'unit', '301', ',', 'amherst', '01002', 'usa', '|', '1-800-377-8036', '|', 'www.lexalytics.com', 'b', 'l', 'c', 'k', 'b', 'x', '/', 'c', 'l', 'e', 'r', 'b', 'x', ':', 'l', 'k', 'n', 'g', 'n', 'e', 'h', 'e', 'import', 'understand', 'decis', 'model', 'made', ',', 'made', 'decis', '.']

 TOTAL PORTER STEM WORDS ==> 67



 ---- SNOWBALL STEMMING ----

['practic', 'effici', ':', 'https', ':', '//www.lexalytics.com/', 'https', ':', '//www.lexalytics.com/', 'w', 'h', 'e', 'p', 'p', 'e', 'r', '10|', '|', 'lexalyt', ',', 'inc.', ',', '48', 'north', 'pleasant', 'st.', 'unit', '301', ',', 'amherst', '01002', 'usa', '|', '1-800-377-8036', '|', 'www.lexalytics.com', 'b', 'l', 'c', 'k', 'b', 'x', '/', 'c', 'l', 'e', 'r', 'b', 'x', ':', 'l', 'k', 'n', 'g', 'n', 'e', 'h', 'e', 'import', 'understand', 'decis', 'model', 'made', ',', 'made', 'decis', '.']

 TOTAL SNOWBALL STEM WORDS ==> 67



 ---- LEMMATIZATION ----

['Practical', 'efficiency', ':', 'http', ':', '//www.lexalytics.com/', 'http', ':', '//www.lexalytics.com/', 'W', 'H', 'E', 'P', 'P', 'E', 'R', '10|', '|', 'Lexalytics', ',', 'Inc.', ',', '48', 'North', 'Pleasant', 'St.', 'Unit', '301', ',', 'Amherst', '01002', 'USA', '|', '1-800-377-8036', '|', 'www.lexalytics.com', 'B', 'L', 'C', 'K', 'B', 'X', '/', 'C', 'L', 'E', 'R', 'B', 'X', ':', 'L', 'K', 'N', 'G', 'N', 'E', 'H', 'E', 'important', 'understand', 'decision', 'model', 'made', ',', 'made', 'decision', '.']

 TOTAL LEMMATIZE WORDS ==> 67

************************************************************************************************************************

98 --> There are two reasons for this:  There’s often other business-relevant information encoded   in the “why.” For example, knowing simply that certain survey  results are full of negative feedback is not particularly useful. 


 ---- TOKENS ----

 ['There', 'are', 'two', 'reasons', 'for', 'this', ':', 'There', '’', 's', 'often', 'other', 'business-relevant', 'information', 'encoded', 'in', 'the', '“', 'why.', '”', 'For', 'example', ',', 'knowing', 'simply', 'that', 'certain', 'survey', 'results', 'are', 'full', 'of', 'negative', 'feedback', 'is', 'not', 'particularly', 'useful', '.'] 

 TOTAL TOKENS ==> 39

 ---- POST ----

 [('There', 'EX'), ('are', 'VBP'), ('two', 'CD'), ('reasons', 'NNS'), ('for', 'IN'), ('this', 'DT'), (':', ':'), ('There', 'EX'), ('’', 'VBZ'), ('s', 'NNS'), ('often', 'RB'), ('other', 'JJ'), ('business-relevant', 'JJ'), ('information', 'NN'), ('encoded', 'VBN'), ('in', 'IN'), ('the', 'DT'), ('“', 'NNP'), ('why.', 'NN'), ('”', 'NN'), ('For', 'IN'), ('example', 'NN'), (',', ','), ('knowing', 'VBG'), ('simply', 'RB'), ('that', 'DT'), ('certain', 'JJ'), ('survey', 'NN'), ('results', 'NNS'), ('are', 'VBP'), ('full', 'JJ'), ('of', 'IN'), ('negative', 'JJ'), ('feedback', 'NN'), ('is', 'VBZ'), ('not', 'RB'), ('particularly', 'RB'), ('useful', 'JJ'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['two', 'reasons', ':', '’', 'often', 'business-relevant', 'information', 'encoded', '“', 'why.', '”', 'example', ',', 'knowing', 'simply', 'certain', 'survey', 'results', 'full', 'negative', 'feedback', 'particularly', 'useful', '.']

 TOTAL FILTERED TOKENS ==>  24

 ---- POST FOR FILTERED TOKENS ----

 [('two', 'CD'), ('reasons', 'NNS'), (':', ':'), ('’', 'RB'), ('often', 'RB'), ('business-relevant', 'JJ'), ('information', 'NN'), ('encoded', 'VBD'), ('“', 'NNP'), ('why.', 'NN'), ('”', 'NNP'), ('example', 'NN'), (',', ','), ('knowing', 'VBG'), ('simply', 'RB'), ('certain', 'JJ'), ('survey', 'NN'), ('results', 'NNS'), ('full', 'JJ'), ('negative', 'JJ'), ('feedback', 'NN'), ('particularly', 'RB'), ('useful', 'JJ'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['two reasons', 'reasons :', ': ’', '’ often', 'often business-relevant', 'business-relevant information', 'information encoded', 'encoded “', '“ why.', 'why. ”', '” example', 'example ,', ', knowing', 'knowing simply', 'simply certain', 'certain survey', 'survey results', 'results full', 'full negative', 'negative feedback', 'feedback particularly', 'particularly useful', 'useful .'] 

 TOTAL BIGRAMS --> 23 



 ---- TRI-GRAMS ---- 

 ['two reasons :', 'reasons : ’', ': ’ often', '’ often business-relevant', 'often business-relevant information', 'business-relevant information encoded', 'information encoded “', 'encoded “ why.', '“ why. ”', 'why. ” example', '” example ,', 'example , knowing', ', knowing simply', 'knowing simply certain', 'simply certain survey', 'certain survey results', 'survey results full', 'results full negative', 'full negative feedback', 'negative feedback particularly', 'feedback particularly useful', 'particularly useful .'] 

 TOTAL TRIGRAMS --> 22 



 ---- NOUN PHRASES ---- 

 ['business-relevant information', 'why.', 'example', 'certain survey', 'full negative feedback'] 

 TOTAL NOUN PHRASES --> 5 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['two', 'reason', ':', '’', 'often', 'business-relev', 'inform', 'encod', '“', 'why.', '”', 'exampl', ',', 'know', 'simpli', 'certain', 'survey', 'result', 'full', 'neg', 'feedback', 'particularli', 'use', '.']

 TOTAL PORTER STEM WORDS ==> 24



 ---- SNOWBALL STEMMING ----

['two', 'reason', ':', '’', 'often', 'business-relev', 'inform', 'encod', '“', 'why.', '”', 'exampl', ',', 'know', 'simpli', 'certain', 'survey', 'result', 'full', 'negat', 'feedback', 'particular', 'use', '.']

 TOTAL SNOWBALL STEM WORDS ==> 24



 ---- LEMMATIZATION ----

['two', 'reason', ':', '’', 'often', 'business-relevant', 'information', 'encoded', '“', 'why.', '”', 'example', ',', 'knowing', 'simply', 'certain', 'survey', 'result', 'full', 'negative', 'feedback', 'particularly', 'useful', '.']

 TOTAL LEMMATIZE WORDS ==> 24

************************************************************************************************************************

99 --> We want to know which phrases were scored negatively. 


 ---- TOKENS ----

 ['We', 'want', 'to', 'know', 'which', 'phrases', 'were', 'scored', 'negatively', '.'] 

 TOTAL TOKENS ==> 10

 ---- POST ----

 [('We', 'PRP'), ('want', 'VBP'), ('to', 'TO'), ('know', 'VB'), ('which', 'WDT'), ('phrases', 'NNS'), ('were', 'VBD'), ('scored', 'VBN'), ('negatively', 'RB'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['want', 'know', 'phrases', 'scored', 'negatively', '.']

 TOTAL FILTERED TOKENS ==>  6

 ---- POST FOR FILTERED TOKENS ----

 [('want', 'NN'), ('know', 'VBP'), ('phrases', 'NNS'), ('scored', 'VBD'), ('negatively', 'RB'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['want know', 'know phrases', 'phrases scored', 'scored negatively', 'negatively .'] 

 TOTAL BIGRAMS --> 5 



 ---- TRI-GRAMS ---- 

 ['want know phrases', 'know phrases scored', 'phrases scored negatively', 'scored negatively .'] 

 TOTAL TRIGRAMS --> 4 



 ---- NOUN PHRASES ---- 

 ['want'] 

 TOTAL NOUN PHRASES --> 1 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['want', 'know', 'phrase', 'score', 'neg', '.']

 TOTAL PORTER STEM WORDS ==> 6



 ---- SNOWBALL STEMMING ----

['want', 'know', 'phrase', 'score', 'negat', '.']

 TOTAL SNOWBALL STEM WORDS ==> 6



 ---- LEMMATIZATION ----

['want', 'know', 'phrase', 'scored', 'negatively', '.']

 TOTAL LEMMATIZE WORDS ==> 6

************************************************************************************************************************

100 --> We might want to adjust the scoring somehow. 


 ---- TOKENS ----

 ['We', 'might', 'want', 'to', 'adjust', 'the', 'scoring', 'somehow', '.'] 

 TOTAL TOKENS ==> 9

 ---- POST ----

 [('We', 'PRP'), ('might', 'MD'), ('want', 'VB'), ('to', 'TO'), ('adjust', 'VB'), ('the', 'DT'), ('scoring', 'NN'), ('somehow', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['might', 'want', 'adjust', 'scoring', 'somehow', '.']

 TOTAL FILTERED TOKENS ==>  6

 ---- POST FOR FILTERED TOKENS ----

 [('might', 'MD'), ('want', 'VB'), ('adjust', 'JJ'), ('scoring', 'VBG'), ('somehow', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['might want', 'want adjust', 'adjust scoring', 'scoring somehow', 'somehow .'] 

 TOTAL BIGRAMS --> 5 



 ---- TRI-GRAMS ---- 

 ['might want adjust', 'want adjust scoring', 'adjust scoring somehow', 'scoring somehow .'] 

 TOTAL TRIGRAMS --> 4 



 ---- NOUN PHRASES ---- 

 ['somehow'] 

 TOTAL NOUN PHRASES --> 1 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['might', 'want', 'adjust', 'score', 'somehow', '.']

 TOTAL PORTER STEM WORDS ==> 6



 ---- SNOWBALL STEMMING ----

['might', 'want', 'adjust', 'score', 'somehow', '.']

 TOTAL SNOWBALL STEM WORDS ==> 6



 ---- LEMMATIZATION ----

['might', 'want', 'adjust', 'scoring', 'somehow', '.']

 TOTAL LEMMATIZE WORDS ==> 6

************************************************************************************************************************

101 --> If we can’t   see why the model is making a decision, we can’t really affect   the decision and it can be hard to figure out what to do next. 


 ---- TOKENS ----

 ['If', 'we', 'can', '’', 't', 'see', 'why', 'the', 'model', 'is', 'making', 'a', 'decision', ',', 'we', 'can', '’', 't', 'really', 'affect', 'the', 'decision', 'and', 'it', 'can', 'be', 'hard', 'to', 'figure', 'out', 'what', 'to', 'do', 'next', '.'] 

 TOTAL TOKENS ==> 35

 ---- POST ----

 [('If', 'IN'), ('we', 'PRP'), ('can', 'MD'), ('’', 'VB'), ('t', 'JJ'), ('see', 'VB'), ('why', 'WRB'), ('the', 'DT'), ('model', 'NN'), ('is', 'VBZ'), ('making', 'VBG'), ('a', 'DT'), ('decision', 'NN'), (',', ','), ('we', 'PRP'), ('can', 'MD'), ('’', 'VB'), ('t', 'JJ'), ('really', 'RB'), ('affect', 'VBP'), ('the', 'DT'), ('decision', 'NN'), ('and', 'CC'), ('it', 'PRP'), ('can', 'MD'), ('be', 'VB'), ('hard', 'JJ'), ('to', 'TO'), ('figure', 'VB'), ('out', 'RP'), ('what', 'WP'), ('to', 'TO'), ('do', 'VB'), ('next', 'RB'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['’', 'see', 'model', 'making', 'decision', ',', '’', 'really', 'affect', 'decision', 'hard', 'figure', 'next', '.']

 TOTAL FILTERED TOKENS ==>  14

 ---- POST FOR FILTERED TOKENS ----

 [('’', 'NNS'), ('see', 'VBP'), ('model', 'NN'), ('making', 'VBG'), ('decision', 'NN'), (',', ','), ('’', 'NNP'), ('really', 'RB'), ('affect', 'JJ'), ('decision', 'NN'), ('hard', 'JJ'), ('figure', 'NN'), ('next', 'JJ'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['’ see', 'see model', 'model making', 'making decision', 'decision ,', ', ’', '’ really', 'really affect', 'affect decision', 'decision hard', 'hard figure', 'figure next', 'next .'] 

 TOTAL BIGRAMS --> 13 



 ---- TRI-GRAMS ---- 

 ['’ see model', 'see model making', 'model making decision', 'making decision ,', 'decision , ’', ', ’ really', '’ really affect', 'really affect decision', 'affect decision hard', 'decision hard figure', 'hard figure next', 'figure next .'] 

 TOTAL TRIGRAMS --> 12 



 ---- NOUN PHRASES ---- 

 ['model', 'decision', 'affect decision', 'hard figure'] 

 TOTAL NOUN PHRASES --> 4 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['’', 'see', 'model', 'make', 'decis', ',', '’', 'realli', 'affect', 'decis', 'hard', 'figur', 'next', '.']

 TOTAL PORTER STEM WORDS ==> 14



 ---- SNOWBALL STEMMING ----

['’', 'see', 'model', 'make', 'decis', ',', '’', 'realli', 'affect', 'decis', 'hard', 'figur', 'next', '.']

 TOTAL SNOWBALL STEM WORDS ==> 14



 ---- LEMMATIZATION ----

['’', 'see', 'model', 'making', 'decision', ',', '’', 'really', 'affect', 'decision', 'hard', 'figure', 'next', '.']

 TOTAL LEMMATIZE WORDS ==> 14

************************************************************************************************************************

102 --> It’s often difficult to see how or why a model is making a decision. 


 ---- TOKENS ----

 ['It', '’', 's', 'often', 'difficult', 'to', 'see', 'how', 'or', 'why', 'a', 'model', 'is', 'making', 'a', 'decision', '.'] 

 TOTAL TOKENS ==> 17

 ---- POST ----

 [('It', 'PRP'), ('’', 'VBZ'), ('s', 'RB'), ('often', 'RB'), ('difficult', 'JJ'), ('to', 'TO'), ('see', 'VB'), ('how', 'WRB'), ('or', 'CC'), ('why', 'WRB'), ('a', 'DT'), ('model', 'NN'), ('is', 'VBZ'), ('making', 'VBG'), ('a', 'DT'), ('decision', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['’', 'often', 'difficult', 'see', 'model', 'making', 'decision', '.']

 TOTAL FILTERED TOKENS ==>  8

 ---- POST FOR FILTERED TOKENS ----

 [('’', 'RB'), ('often', 'RB'), ('difficult', 'JJ'), ('see', 'VBP'), ('model', 'JJ'), ('making', 'VBG'), ('decision', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['’ often', 'often difficult', 'difficult see', 'see model', 'model making', 'making decision', 'decision .'] 

 TOTAL BIGRAMS --> 7 



 ---- TRI-GRAMS ---- 

 ['’ often difficult', 'often difficult see', 'difficult see model', 'see model making', 'model making decision', 'making decision .'] 

 TOTAL TRIGRAMS --> 6 



 ---- NOUN PHRASES ---- 

 ['decision'] 

 TOTAL NOUN PHRASES --> 1 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['’', 'often', 'difficult', 'see', 'model', 'make', 'decis', '.']

 TOTAL PORTER STEM WORDS ==> 8



 ---- SNOWBALL STEMMING ----

['’', 'often', 'difficult', 'see', 'model', 'make', 'decis', '.']

 TOTAL SNOWBALL STEM WORDS ==> 8



 ---- LEMMATIZATION ----

['’', 'often', 'difficult', 'see', 'model', 'making', 'decision', '.']

 TOTAL LEMMATIZE WORDS ==> 8

************************************************************************************************************************

103 --> This is  particularly true of deep learning models. 


 ---- TOKENS ----

 ['This', 'is', 'particularly', 'true', 'of', 'deep', 'learning', 'models', '.'] 

 TOTAL TOKENS ==> 9

 ---- POST ----

 [('This', 'DT'), ('is', 'VBZ'), ('particularly', 'RB'), ('true', 'JJ'), ('of', 'IN'), ('deep', 'JJ'), ('learning', 'NN'), ('models', 'NNS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['particularly', 'true', 'deep', 'learning', 'models', '.']

 TOTAL FILTERED TOKENS ==>  6

 ---- POST FOR FILTERED TOKENS ----

 [('particularly', 'RB'), ('true', 'JJ'), ('deep', 'RB'), ('learning', 'NN'), ('models', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['particularly true', 'true deep', 'deep learning', 'learning models', 'models .'] 

 TOTAL BIGRAMS --> 5 



 ---- TRI-GRAMS ---- 

 ['particularly true deep', 'true deep learning', 'deep learning models', 'learning models .'] 

 TOTAL TRIGRAMS --> 4 



 ---- NOUN PHRASES ---- 

 ['learning'] 

 TOTAL NOUN PHRASES --> 1 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['particularli', 'true', 'deep', 'learn', 'model', '.']

 TOTAL PORTER STEM WORDS ==> 6



 ---- SNOWBALL STEMMING ----

['particular', 'true', 'deep', 'learn', 'model', '.']

 TOTAL SNOWBALL STEM WORDS ==> 6



 ---- LEMMATIZATION ----

['particularly', 'true', 'deep', 'learning', 'model', '.']

 TOTAL LEMMATIZE WORDS ==> 6

************************************************************************************************************************

104 --> Despite their popularity, they   are profoundly black box algorithms. 


 ---- TOKENS ----

 ['Despite', 'their', 'popularity', ',', 'they', 'are', 'profoundly', 'black', 'box', 'algorithms', '.'] 

 TOTAL TOKENS ==> 11

 ---- POST ----

 [('Despite', 'IN'), ('their', 'PRP$'), ('popularity', 'NN'), (',', ','), ('they', 'PRP'), ('are', 'VBP'), ('profoundly', 'RB'), ('black', 'JJ'), ('box', 'NN'), ('algorithms', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Despite', 'popularity', ',', 'profoundly', 'black', 'box', 'algorithms', '.']

 TOTAL FILTERED TOKENS ==>  8

 ---- POST FOR FILTERED TOKENS ----

 [('Despite', 'IN'), ('popularity', 'NN'), (',', ','), ('profoundly', 'RB'), ('black', 'JJ'), ('box', 'NN'), ('algorithms', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Despite popularity', 'popularity ,', ', profoundly', 'profoundly black', 'black box', 'box algorithms', 'algorithms .'] 

 TOTAL BIGRAMS --> 7 



 ---- TRI-GRAMS ---- 

 ['Despite popularity ,', 'popularity , profoundly', ', profoundly black', 'profoundly black box', 'black box algorithms', 'box algorithms .'] 

 TOTAL TRIGRAMS --> 6 



 ---- NOUN PHRASES ---- 

 ['popularity', 'black box', 'algorithms'] 

 TOTAL NOUN PHRASES --> 3 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['despit', 'popular', ',', 'profoundli', 'black', 'box', 'algorithm', '.']

 TOTAL PORTER STEM WORDS ==> 8



 ---- SNOWBALL STEMMING ----

['despit', 'popular', ',', 'profound', 'black', 'box', 'algorithm', '.']

 TOTAL SNOWBALL STEM WORDS ==> 8



 ---- LEMMATIZATION ----

['Despite', 'popularity', ',', 'profoundly', 'black', 'box', 'algorithm', '.']

 TOTAL LEMMATIZE WORDS ==> 8

************************************************************************************************************************

105 --> https://www.lexalytics.com/ https://www.lexalytics.com/  W H I T E  P A P E R 1 1|       |   Lexalytics, Inc., 48 North Pleasant St. Unit 301, Amherst MA 01002 USA   |   1-800-377-8036 |   www.lexalytics.com One solution to this black box problem is to break big, general models   into smaller, targeted models. 


 ---- TOKENS ----

 ['https', ':', '//www.lexalytics.com/', 'https', ':', '//www.lexalytics.com/', 'W', 'H', 'I', 'T', 'E', 'P', 'A', 'P', 'E', 'R', '1', '1|', '|', 'Lexalytics', ',', 'Inc.', ',', '48', 'North', 'Pleasant', 'St.', 'Unit', '301', ',', 'Amherst', 'MA', '01002', 'USA', '|', '1-800-377-8036', '|', 'www.lexalytics.com', 'One', 'solution', 'to', 'this', 'black', 'box', 'problem', 'is', 'to', 'break', 'big', ',', 'general', 'models', 'into', 'smaller', ',', 'targeted', 'models', '.'] 

 TOTAL TOKENS ==> 58

 ---- POST ----

 [('https', 'NN'), (':', ':'), ('//www.lexalytics.com/', 'JJ'), ('https', 'NN'), (':', ':'), ('//www.lexalytics.com/', 'JJ'), ('W', 'NNP'), ('H', 'NNP'), ('I', 'PRP'), ('T', 'NNP'), ('E', 'NNP'), ('P', 'NNP'), ('A', 'NNP'), ('P', 'NNP'), ('E', 'NNP'), ('R', 'NNP'), ('1', 'CD'), ('1|', 'CD'), ('|', 'NN'), ('Lexalytics', 'NNP'), (',', ','), ('Inc.', 'NNP'), (',', ','), ('48', 'CD'), ('North', 'NNP'), ('Pleasant', 'NNP'), ('St.', 'NNP'), ('Unit', 'NNP'), ('301', 'CD'), (',', ','), ('Amherst', 'NNP'), ('MA', 'NNP'), ('01002', 'CD'), ('USA', 'NNP'), ('|', 'NNP'), ('1-800-377-8036', 'JJ'), ('|', 'NNP'), ('www.lexalytics.com', 'NN'), ('One', 'CD'), ('solution', 'NN'), ('to', 'TO'), ('this', 'DT'), ('black', 'JJ'), ('box', 'NN'), ('problem', 'NN'), ('is', 'VBZ'), ('to', 'TO'), ('break', 'VB'), ('big', 'JJ'), (',', ','), ('general', 'JJ'), ('models', 'NNS'), ('into', 'IN'), ('smaller', 'JJR'), (',', ','), ('targeted', 'JJ'), ('models', 'NNS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['https', ':', '//www.lexalytics.com/', 'https', ':', '//www.lexalytics.com/', 'W', 'H', 'E', 'P', 'P', 'E', 'R', '1', '1|', '|', 'Lexalytics', ',', 'Inc.', ',', '48', 'North', 'Pleasant', 'St.', 'Unit', '301', ',', 'Amherst', '01002', 'USA', '|', '1-800-377-8036', '|', 'www.lexalytics.com', 'One', 'solution', 'black', 'box', 'problem', 'break', 'big', ',', 'general', 'models', 'smaller', ',', 'targeted', 'models', '.']

 TOTAL FILTERED TOKENS ==>  49

 ---- POST FOR FILTERED TOKENS ----

 [('https', 'NN'), (':', ':'), ('//www.lexalytics.com/', 'JJ'), ('https', 'NN'), (':', ':'), ('//www.lexalytics.com/', 'JJ'), ('W', 'NNP'), ('H', 'NNP'), ('E', 'NNP'), ('P', 'NNP'), ('P', 'NNP'), ('E', 'NNP'), ('R', 'NNP'), ('1', 'CD'), ('1|', 'CD'), ('|', 'NN'), ('Lexalytics', 'NNP'), (',', ','), ('Inc.', 'NNP'), (',', ','), ('48', 'CD'), ('North', 'NNP'), ('Pleasant', 'NNP'), ('St.', 'NNP'), ('Unit', 'NNP'), ('301', 'CD'), (',', ','), ('Amherst', 'NNP'), ('01002', 'CD'), ('USA', 'NNP'), ('|', 'NNP'), ('1-800-377-8036', 'JJ'), ('|', 'NNP'), ('www.lexalytics.com', 'NN'), ('One', 'CD'), ('solution', 'NN'), ('black', 'JJ'), ('box', 'NN'), ('problem', 'NN'), ('break', 'NN'), ('big', 'JJ'), (',', ','), ('general', 'JJ'), ('models', 'NNS'), ('smaller', 'JJR'), (',', ','), ('targeted', 'JJ'), ('models', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['https :', ': //www.lexalytics.com/', '//www.lexalytics.com/ https', 'https :', ': //www.lexalytics.com/', '//www.lexalytics.com/ W', 'W H', 'H E', 'E P', 'P P', 'P E', 'E R', 'R 1', '1 1|', '1| |', '| Lexalytics', 'Lexalytics ,', ', Inc.', 'Inc. ,', ', 48', '48 North', 'North Pleasant', 'Pleasant St.', 'St. Unit', 'Unit 301', '301 ,', ', Amherst', 'Amherst 01002', '01002 USA', 'USA |', '| 1-800-377-8036', '1-800-377-8036 |', '| www.lexalytics.com', 'www.lexalytics.com One', 'One solution', 'solution black', 'black box', 'box problem', 'problem break', 'break big', 'big ,', ', general', 'general models', 'models smaller', 'smaller ,', ', targeted', 'targeted models', 'models .'] 

 TOTAL BIGRAMS --> 48 



 ---- TRI-GRAMS ---- 

 ['https : //www.lexalytics.com/', ': //www.lexalytics.com/ https', '//www.lexalytics.com/ https :', 'https : //www.lexalytics.com/', ': //www.lexalytics.com/ W', '//www.lexalytics.com/ W H', 'W H E', 'H E P', 'E P P', 'P P E', 'P E R', 'E R 1', 'R 1 1|', '1 1| |', '1| | Lexalytics', '| Lexalytics ,', 'Lexalytics , Inc.', ', Inc. ,', 'Inc. , 48', ', 48 North', '48 North Pleasant', 'North Pleasant St.', 'Pleasant St. Unit', 'St. Unit 301', 'Unit 301 ,', '301 , Amherst', ', Amherst 01002', 'Amherst 01002 USA', '01002 USA |', 'USA | 1-800-377-8036', '| 1-800-377-8036 |', '1-800-377-8036 | www.lexalytics.com', '| www.lexalytics.com One', 'www.lexalytics.com One solution', 'One solution black', 'solution black box', 'black box problem', 'box problem break', 'problem break big', 'break big ,', 'big , general', ', general models', 'general models smaller', 'models smaller ,', 'smaller , targeted', ', targeted models', 'targeted models .'] 

 TOTAL TRIGRAMS --> 47 



 ---- NOUN PHRASES ---- 

 ['https', ' https', '|', 'www.lexalytics.com', 'solution', 'black box', 'problem', 'break'] 

 TOTAL NOUN PHRASES --> 8 



 ---- NER ----

 
 ORGANIZATION ---> ['USA']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> ['Lexalytics']
 TOTAL PERSON ENTITY --> 1 


 GPE ---> ['North']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['http', ':', '//www.lexalytics.com/', 'http', ':', '//www.lexalytics.com/', 'w', 'h', 'e', 'p', 'p', 'e', 'r', '1', '1|', '|', 'lexalyt', ',', 'inc.', ',', '48', 'north', 'pleasant', 'st.', 'unit', '301', ',', 'amherst', '01002', 'usa', '|', '1-800-377-8036', '|', 'www.lexalytics.com', 'one', 'solut', 'black', 'box', 'problem', 'break', 'big', ',', 'gener', 'model', 'smaller', ',', 'target', 'model', '.']

 TOTAL PORTER STEM WORDS ==> 49



 ---- SNOWBALL STEMMING ----

['https', ':', '//www.lexalytics.com/', 'https', ':', '//www.lexalytics.com/', 'w', 'h', 'e', 'p', 'p', 'e', 'r', '1', '1|', '|', 'lexalyt', ',', 'inc.', ',', '48', 'north', 'pleasant', 'st.', 'unit', '301', ',', 'amherst', '01002', 'usa', '|', '1-800-377-8036', '|', 'www.lexalytics.com', 'one', 'solut', 'black', 'box', 'problem', 'break', 'big', ',', 'general', 'model', 'smaller', ',', 'target', 'model', '.']

 TOTAL SNOWBALL STEM WORDS ==> 49



 ---- LEMMATIZATION ----

['http', ':', '//www.lexalytics.com/', 'http', ':', '//www.lexalytics.com/', 'W', 'H', 'E', 'P', 'P', 'E', 'R', '1', '1|', '|', 'Lexalytics', ',', 'Inc.', ',', '48', 'North', 'Pleasant', 'St.', 'Unit', '301', ',', 'Amherst', '01002', 'USA', '|', '1-800-377-8036', '|', 'www.lexalytics.com', 'One', 'solution', 'black', 'box', 'problem', 'break', 'big', ',', 'general', 'model', 'smaller', ',', 'targeted', 'model', '.']

 TOTAL LEMMATIZE WORDS ==> 49

************************************************************************************************************************

106 --> A model’s internal decisions are often hidden from view, so you should   make sure that the model isn’t doing too much in the first place. 


 ---- TOKENS ----

 ['A', 'model', '’', 's', 'internal', 'decisions', 'are', 'often', 'hidden', 'from', 'view', ',', 'so', 'you', 'should', 'make', 'sure', 'that', 'the', 'model', 'isn', '’', 't', 'doing', 'too', 'much', 'in', 'the', 'first', 'place', '.'] 

 TOTAL TOKENS ==> 31

 ---- POST ----

 [('A', 'DT'), ('model', 'NN'), ('’', 'NNP'), ('s', 'VBZ'), ('internal', 'JJ'), ('decisions', 'NNS'), ('are', 'VBP'), ('often', 'RB'), ('hidden', 'VBN'), ('from', 'IN'), ('view', 'NN'), (',', ','), ('so', 'IN'), ('you', 'PRP'), ('should', 'MD'), ('make', 'VB'), ('sure', 'JJ'), ('that', 'IN'), ('the', 'DT'), ('model', 'NN'), ('isn', 'NN'), ('’', 'NNP'), ('t', 'NN'), ('doing', 'VBG'), ('too', 'RB'), ('much', 'RB'), ('in', 'IN'), ('the', 'DT'), ('first', 'JJ'), ('place', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['model', '’', 'internal', 'decisions', 'often', 'hidden', 'view', ',', 'make', 'sure', 'model', '’', 'much', 'first', 'place', '.']

 TOTAL FILTERED TOKENS ==>  16

 ---- POST FOR FILTERED TOKENS ----

 [('model', 'NN'), ('’', 'NNP'), ('internal', 'JJ'), ('decisions', 'NNS'), ('often', 'RB'), ('hidden', 'JJ'), ('view', 'NN'), (',', ','), ('make', 'VBP'), ('sure', 'JJ'), ('model', 'NN'), ('’', 'NN'), ('much', 'JJ'), ('first', 'JJ'), ('place', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['model ’', '’ internal', 'internal decisions', 'decisions often', 'often hidden', 'hidden view', 'view ,', ', make', 'make sure', 'sure model', 'model ’', '’ much', 'much first', 'first place', 'place .'] 

 TOTAL BIGRAMS --> 15 



 ---- TRI-GRAMS ---- 

 ['model ’ internal', '’ internal decisions', 'internal decisions often', 'decisions often hidden', 'often hidden view', 'hidden view ,', 'view , make', ', make sure', 'make sure model', 'sure model ’', 'model ’ much', '’ much first', 'much first place', 'first place .'] 

 TOTAL TRIGRAMS --> 14 



 ---- NOUN PHRASES ---- 

 ['model', 'hidden view', 'sure model', '’', 'much first place'] 

 TOTAL NOUN PHRASES --> 5 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['model', '’', 'intern', 'decis', 'often', 'hidden', 'view', ',', 'make', 'sure', 'model', '’', 'much', 'first', 'place', '.']

 TOTAL PORTER STEM WORDS ==> 16



 ---- SNOWBALL STEMMING ----

['model', '’', 'intern', 'decis', 'often', 'hidden', 'view', ',', 'make', 'sure', 'model', '’', 'much', 'first', 'place', '.']

 TOTAL SNOWBALL STEM WORDS ==> 16



 ---- LEMMATIZATION ----

['model', '’', 'internal', 'decision', 'often', 'hidden', 'view', ',', 'make', 'sure', 'model', '’', 'much', 'first', 'place', '.']

 TOTAL LEMMATIZE WORDS ==> 16

************************************************************************************************************************

107 --> For example, if we were using one big model that analyzes an entire  document at once, we’d only be able to work at the document level. 


 ---- TOKENS ----

 ['For', 'example', ',', 'if', 'we', 'were', 'using', 'one', 'big', 'model', 'that', 'analyzes', 'an', 'entire', 'document', 'at', 'once', ',', 'we', '’', 'd', 'only', 'be', 'able', 'to', 'work', 'at', 'the', 'document', 'level', '.'] 

 TOTAL TOKENS ==> 31

 ---- POST ----

 [('For', 'IN'), ('example', 'NN'), (',', ','), ('if', 'IN'), ('we', 'PRP'), ('were', 'VBD'), ('using', 'VBG'), ('one', 'CD'), ('big', 'JJ'), ('model', 'NN'), ('that', 'WDT'), ('analyzes', 'VBZ'), ('an', 'DT'), ('entire', 'JJ'), ('document', 'NN'), ('at', 'IN'), ('once', 'RB'), (',', ','), ('we', 'PRP'), ('’', 'VBP'), ('d', 'EX'), ('only', 'RB'), ('be', 'VB'), ('able', 'JJ'), ('to', 'TO'), ('work', 'VB'), ('at', 'IN'), ('the', 'DT'), ('document', 'NN'), ('level', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['example', ',', 'using', 'one', 'big', 'model', 'analyzes', 'entire', 'document', ',', '’', 'able', 'work', 'document', 'level', '.']

 TOTAL FILTERED TOKENS ==>  16

 ---- POST FOR FILTERED TOKENS ----

 [('example', 'NN'), (',', ','), ('using', 'VBG'), ('one', 'CD'), ('big', 'JJ'), ('model', 'NN'), ('analyzes', 'VBZ'), ('entire', 'JJ'), ('document', 'NN'), (',', ','), ('’', 'NNP'), ('able', 'JJ'), ('work', 'NN'), ('document', 'NN'), ('level', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['example ,', ', using', 'using one', 'one big', 'big model', 'model analyzes', 'analyzes entire', 'entire document', 'document ,', ', ’', '’ able', 'able work', 'work document', 'document level', 'level .'] 

 TOTAL BIGRAMS --> 15 



 ---- TRI-GRAMS ---- 

 ['example , using', ', using one', 'using one big', 'one big model', 'big model analyzes', 'model analyzes entire', 'analyzes entire document', 'entire document ,', 'document , ’', ', ’ able', '’ able work', 'able work document', 'work document level', 'document level .'] 

 TOTAL TRIGRAMS --> 14 



 ---- NOUN PHRASES ---- 

 ['example', 'big model', 'entire document', 'able work', 'document', 'level'] 

 TOTAL NOUN PHRASES --> 6 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['exampl', ',', 'use', 'one', 'big', 'model', 'analyz', 'entir', 'document', ',', '’', 'abl', 'work', 'document', 'level', '.']

 TOTAL PORTER STEM WORDS ==> 16



 ---- SNOWBALL STEMMING ----

['exampl', ',', 'use', 'one', 'big', 'model', 'analyz', 'entir', 'document', ',', '’', 'abl', 'work', 'document', 'level', '.']

 TOTAL SNOWBALL STEM WORDS ==> 16



 ---- LEMMATIZATION ----

['example', ',', 'using', 'one', 'big', 'model', 'analyzes', 'entire', 'document', ',', '’', 'able', 'work', 'document', 'level', '.']

 TOTAL LEMMATIZE WORDS ==> 16

************************************************************************************************************************

108 --> Instead, Lexalytics utilizes a pipeline interaction between our models. 


 ---- TOKENS ----

 ['Instead', ',', 'Lexalytics', 'utilizes', 'a', 'pipeline', 'interaction', 'between', 'our', 'models', '.'] 

 TOTAL TOKENS ==> 11

 ---- POST ----

 [('Instead', 'RB'), (',', ','), ('Lexalytics', 'NNP'), ('utilizes', 'VBZ'), ('a', 'DT'), ('pipeline', 'NN'), ('interaction', 'NN'), ('between', 'IN'), ('our', 'PRP$'), ('models', 'NNS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Instead', ',', 'Lexalytics', 'utilizes', 'pipeline', 'interaction', 'models', '.']

 TOTAL FILTERED TOKENS ==>  8

 ---- POST FOR FILTERED TOKENS ----

 [('Instead', 'RB'), (',', ','), ('Lexalytics', 'NNP'), ('utilizes', 'VBZ'), ('pipeline', 'NN'), ('interaction', 'NN'), ('models', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Instead ,', ', Lexalytics', 'Lexalytics utilizes', 'utilizes pipeline', 'pipeline interaction', 'interaction models', 'models .'] 

 TOTAL BIGRAMS --> 7 



 ---- TRI-GRAMS ---- 

 ['Instead , Lexalytics', ', Lexalytics utilizes', 'Lexalytics utilizes pipeline', 'utilizes pipeline interaction', 'pipeline interaction models', 'interaction models .'] 

 TOTAL TRIGRAMS --> 6 



 ---- NOUN PHRASES ---- 

 ['pipeline', 'interaction'] 

 TOTAL NOUN PHRASES --> 2 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> ['Lexalytics']
 TOTAL PERSON ENTITY --> 1 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['instead', ',', 'lexalyt', 'util', 'pipelin', 'interact', 'model', '.']

 TOTAL PORTER STEM WORDS ==> 8



 ---- SNOWBALL STEMMING ----

['instead', ',', 'lexalyt', 'util', 'pipelin', 'interact', 'model', '.']

 TOTAL SNOWBALL STEM WORDS ==> 8



 ---- LEMMATIZATION ----

['Instead', ',', 'Lexalytics', 'utilizes', 'pipeline', 'interaction', 'model', '.']

 TOTAL LEMMATIZE WORDS ==> 8

************************************************************************************************************************

109 --> We start with tokenization, move on to parts of speech, then to phrases,   and all the way up until we’ve deconstructed the document at every   level. 


 ---- TOKENS ----

 ['We', 'start', 'with', 'tokenization', ',', 'move', 'on', 'to', 'parts', 'of', 'speech', ',', 'then', 'to', 'phrases', ',', 'and', 'all', 'the', 'way', 'up', 'until', 'we', '’', 've', 'deconstructed', 'the', 'document', 'at', 'every', 'level', '.'] 

 TOTAL TOKENS ==> 32

 ---- POST ----

 [('We', 'PRP'), ('start', 'VBP'), ('with', 'IN'), ('tokenization', 'NN'), (',', ','), ('move', 'VB'), ('on', 'IN'), ('to', 'TO'), ('parts', 'NNS'), ('of', 'IN'), ('speech', 'NN'), (',', ','), ('then', 'RB'), ('to', 'TO'), ('phrases', 'NNS'), (',', ','), ('and', 'CC'), ('all', 'PDT'), ('the', 'DT'), ('way', 'NN'), ('up', 'RB'), ('until', 'IN'), ('we', 'PRP'), ('’', 'VBP'), ('ve', 'RB'), ('deconstructed', 'VBN'), ('the', 'DT'), ('document', 'NN'), ('at', 'IN'), ('every', 'DT'), ('level', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['start', 'tokenization', ',', 'move', 'parts', 'speech', ',', 'phrases', ',', 'way', '’', 'deconstructed', 'document', 'every', 'level', '.']

 TOTAL FILTERED TOKENS ==>  16

 ---- POST FOR FILTERED TOKENS ----

 [('start', 'NN'), ('tokenization', 'NN'), (',', ','), ('move', 'NN'), ('parts', 'NNS'), ('speech', 'NN'), (',', ','), ('phrases', 'NNS'), (',', ','), ('way', 'NN'), ('’', 'NNP'), ('deconstructed', 'VBD'), ('document', 'JJ'), ('every', 'DT'), ('level', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['start tokenization', 'tokenization ,', ', move', 'move parts', 'parts speech', 'speech ,', ', phrases', 'phrases ,', ', way', 'way ’', '’ deconstructed', 'deconstructed document', 'document every', 'every level', 'level .'] 

 TOTAL BIGRAMS --> 15 



 ---- TRI-GRAMS ---- 

 ['start tokenization ,', 'tokenization , move', ', move parts', 'move parts speech', 'parts speech ,', 'speech , phrases', ', phrases ,', 'phrases , way', ', way ’', 'way ’ deconstructed', '’ deconstructed document', 'deconstructed document every', 'document every level', 'every level .'] 

 TOTAL TRIGRAMS --> 14 



 ---- NOUN PHRASES ---- 

 ['start', 'tokenization', 'move', 'speech', 'way', 'every level'] 

 TOTAL NOUN PHRASES --> 6 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['start', 'token', ',', 'move', 'part', 'speech', ',', 'phrase', ',', 'way', '’', 'deconstruct', 'document', 'everi', 'level', '.']

 TOTAL PORTER STEM WORDS ==> 16



 ---- SNOWBALL STEMMING ----

['start', 'token', ',', 'move', 'part', 'speech', ',', 'phrase', ',', 'way', '’', 'deconstruct', 'document', 'everi', 'level', '.']

 TOTAL SNOWBALL STEM WORDS ==> 16



 ---- LEMMATIZATION ----

['start', 'tokenization', ',', 'move', 'part', 'speech', ',', 'phrase', ',', 'way', '’', 'deconstructed', 'document', 'every', 'level', '.']

 TOTAL LEMMATIZE WORDS ==> 16

************************************************************************************************************************

110 --> When there’s an issue in the pipeline, this approach helps us  see exactly where it is. 


 ---- TOKENS ----

 ['When', 'there', '’', 's', 'an', 'issue', 'in', 'the', 'pipeline', ',', 'this', 'approach', 'helps', 'us', 'see', 'exactly', 'where', 'it', 'is', '.'] 

 TOTAL TOKENS ==> 20

 ---- POST ----

 [('When', 'WRB'), ('there', 'EX'), ('’', 'NNP'), ('s', 'VBD'), ('an', 'DT'), ('issue', 'NN'), ('in', 'IN'), ('the', 'DT'), ('pipeline', 'NN'), (',', ','), ('this', 'DT'), ('approach', 'NN'), ('helps', 'VBZ'), ('us', 'PRP'), ('see', 'VB'), ('exactly', 'RB'), ('where', 'WRB'), ('it', 'PRP'), ('is', 'VBZ'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['’', 'issue', 'pipeline', ',', 'approach', 'helps', 'us', 'see', 'exactly', '.']

 TOTAL FILTERED TOKENS ==>  10

 ---- POST FOR FILTERED TOKENS ----

 [('’', 'JJ'), ('issue', 'NN'), ('pipeline', 'NN'), (',', ','), ('approach', 'NN'), ('helps', 'VBZ'), ('us', 'PRP'), ('see', 'VB'), ('exactly', 'RB'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['’ issue', 'issue pipeline', 'pipeline ,', ', approach', 'approach helps', 'helps us', 'us see', 'see exactly', 'exactly .'] 

 TOTAL BIGRAMS --> 9 



 ---- TRI-GRAMS ---- 

 ['’ issue pipeline', 'issue pipeline ,', 'pipeline , approach', ', approach helps', 'approach helps us', 'helps us see', 'us see exactly', 'see exactly .'] 

 TOTAL TRIGRAMS --> 8 



 ---- NOUN PHRASES ---- 

 ['’ issue', 'pipeline', 'approach'] 

 TOTAL NOUN PHRASES --> 3 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['’', 'issu', 'pipelin', ',', 'approach', 'help', 'us', 'see', 'exactli', '.']

 TOTAL PORTER STEM WORDS ==> 10



 ---- SNOWBALL STEMMING ----

['’', 'issu', 'pipelin', ',', 'approach', 'help', 'us', 'see', 'exact', '.']

 TOTAL SNOWBALL STEM WORDS ==> 10



 ---- LEMMATIZATION ----

['’', 'issue', 'pipeline', ',', 'approach', 'help', 'u', 'see', 'exactly', '.']

 TOTAL LEMMATIZE WORDS ==> 10

************************************************************************************************************************

111 --> Then, we can make adjustments to individual  components, such as retraining the part of speech tagger or tuning  its configuration files. 


 ---- TOKENS ----

 ['Then', ',', 'we', 'can', 'make', 'adjustments', 'to', 'individual', 'components', ',', 'such', 'as', 'retraining', 'the', 'part', 'of', 'speech', 'tagger', 'or', 'tuning', 'its', 'configuration', 'files', '.'] 

 TOTAL TOKENS ==> 24

 ---- POST ----

 [('Then', 'RB'), (',', ','), ('we', 'PRP'), ('can', 'MD'), ('make', 'VB'), ('adjustments', 'NNS'), ('to', 'TO'), ('individual', 'JJ'), ('components', 'NNS'), (',', ','), ('such', 'JJ'), ('as', 'IN'), ('retraining', 'VBG'), ('the', 'DT'), ('part', 'NN'), ('of', 'IN'), ('speech', 'NN'), ('tagger', 'NN'), ('or', 'CC'), ('tuning', 'VBG'), ('its', 'PRP$'), ('configuration', 'NN'), ('files', 'NNS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 [',', 'make', 'adjustments', 'individual', 'components', ',', 'retraining', 'part', 'speech', 'tagger', 'tuning', 'configuration', 'files', '.']

 TOTAL FILTERED TOKENS ==>  14

 ---- POST FOR FILTERED TOKENS ----

 [(',', ','), ('make', 'VBP'), ('adjustments', 'NNS'), ('individual', 'JJ'), ('components', 'NNS'), (',', ','), ('retraining', 'VBG'), ('part', 'NN'), ('speech', 'NN'), ('tagger', 'NN'), ('tuning', 'VBG'), ('configuration', 'NN'), ('files', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 [', make', 'make adjustments', 'adjustments individual', 'individual components', 'components ,', ', retraining', 'retraining part', 'part speech', 'speech tagger', 'tagger tuning', 'tuning configuration', 'configuration files', 'files .'] 

 TOTAL BIGRAMS --> 13 



 ---- TRI-GRAMS ---- 

 [', make adjustments', 'make adjustments individual', 'adjustments individual components', 'individual components ,', 'components , retraining', ', retraining part', 'retraining part speech', 'part speech tagger', 'speech tagger tuning', 'tagger tuning configuration', 'tuning configuration files', 'configuration files .'] 

 TOTAL TRIGRAMS --> 12 



 ---- NOUN PHRASES ---- 

 ['part', 'speech', 'tagger', 'configuration'] 

 TOTAL NOUN PHRASES --> 4 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

[',', 'make', 'adjust', 'individu', 'compon', ',', 'retrain', 'part', 'speech', 'tagger', 'tune', 'configur', 'file', '.']

 TOTAL PORTER STEM WORDS ==> 14



 ---- SNOWBALL STEMMING ----

[',', 'make', 'adjust', 'individu', 'compon', ',', 'retrain', 'part', 'speech', 'tagger', 'tune', 'configur', 'file', '.']

 TOTAL SNOWBALL STEM WORDS ==> 14



 ---- LEMMATIZATION ----

[',', 'make', 'adjustment', 'individual', 'component', ',', 'retraining', 'part', 'speech', 'tagger', 'tuning', 'configuration', 'file', '.']

 TOTAL LEMMATIZE WORDS ==> 14

************************************************************************************************************************

112 --> Using smaller models gives us more flexibility to  determine where an issue is, as well as how to fix it. 


 ---- TOKENS ----

 ['Using', 'smaller', 'models', 'gives', 'us', 'more', 'flexibility', 'to', 'determine', 'where', 'an', 'issue', 'is', ',', 'as', 'well', 'as', 'how', 'to', 'fix', 'it', '.'] 

 TOTAL TOKENS ==> 22

 ---- POST ----

 [('Using', 'VBG'), ('smaller', 'JJR'), ('models', 'NNS'), ('gives', 'VBZ'), ('us', 'PRP'), ('more', 'JJR'), ('flexibility', 'NN'), ('to', 'TO'), ('determine', 'VB'), ('where', 'WRB'), ('an', 'DT'), ('issue', 'NN'), ('is', 'VBZ'), (',', ','), ('as', 'RB'), ('well', 'RB'), ('as', 'IN'), ('how', 'WRB'), ('to', 'TO'), ('fix', 'VB'), ('it', 'PRP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Using', 'smaller', 'models', 'gives', 'us', 'flexibility', 'determine', 'issue', ',', 'well', 'fix', '.']

 TOTAL FILTERED TOKENS ==>  12

 ---- POST FOR FILTERED TOKENS ----

 [('Using', 'VBG'), ('smaller', 'JJR'), ('models', 'NNS'), ('gives', 'VBZ'), ('us', 'PRP'), ('flexibility', 'NN'), ('determine', 'JJ'), ('issue', 'NN'), (',', ','), ('well', 'RB'), ('fix', 'RB'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Using smaller', 'smaller models', 'models gives', 'gives us', 'us flexibility', 'flexibility determine', 'determine issue', 'issue ,', ', well', 'well fix', 'fix .'] 

 TOTAL BIGRAMS --> 11 



 ---- TRI-GRAMS ---- 

 ['Using smaller models', 'smaller models gives', 'models gives us', 'gives us flexibility', 'us flexibility determine', 'flexibility determine issue', 'determine issue ,', 'issue , well', ', well fix', 'well fix .'] 

 TOTAL TRIGRAMS --> 10 



 ---- NOUN PHRASES ---- 

 ['flexibility', 'determine issue'] 

 TOTAL NOUN PHRASES --> 2 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['use', 'smaller', 'model', 'give', 'us', 'flexibl', 'determin', 'issu', ',', 'well', 'fix', '.']

 TOTAL PORTER STEM WORDS ==> 12



 ---- SNOWBALL STEMMING ----

['use', 'smaller', 'model', 'give', 'us', 'flexibl', 'determin', 'issu', ',', 'well', 'fix', '.']

 TOTAL SNOWBALL STEM WORDS ==> 12



 ---- LEMMATIZATION ----

['Using', 'smaller', 'model', 'give', 'u', 'flexibility', 'determine', 'issue', ',', 'well', 'fix', '.']

 TOTAL LEMMATIZE WORDS ==> 12

************************************************************************************************************************

113 --> Take the phrase “Good   Morning America.” It looks  innocuous, but it’s not. 


 ---- TOKENS ----

 ['Take', 'the', 'phrase', '“', 'Good', 'Morning', 'America.', '”', 'It', 'looks', 'innocuous', ',', 'but', 'it', '’', 's', 'not', '.'] 

 TOTAL TOKENS ==> 18

 ---- POST ----

 [('Take', 'VB'), ('the', 'DT'), ('phrase', 'NN'), ('“', 'NNP'), ('Good', 'NNP'), ('Morning', 'NNP'), ('America.', 'NNP'), ('”', 'VBD'), ('It', 'PRP'), ('looks', 'VBZ'), ('innocuous', 'JJ'), (',', ','), ('but', 'CC'), ('it', 'PRP'), ('’', 'NNP'), ('s', 'VBZ'), ('not', 'RB'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Take', 'phrase', '“', 'Good', 'Morning', 'America.', '”', 'looks', 'innocuous', ',', '’', '.']

 TOTAL FILTERED TOKENS ==>  12

 ---- POST FOR FILTERED TOKENS ----

 [('Take', 'VB'), ('phrase', 'NN'), ('“', 'NNP'), ('Good', 'NNP'), ('Morning', 'NNP'), ('America.', 'NNP'), ('”', 'NNP'), ('looks', 'VBZ'), ('innocuous', 'JJ'), (',', ','), ('’', 'NNP'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Take phrase', 'phrase “', '“ Good', 'Good Morning', 'Morning America.', 'America. ”', '” looks', 'looks innocuous', 'innocuous ,', ', ’', '’ .'] 

 TOTAL BIGRAMS --> 11 



 ---- TRI-GRAMS ---- 

 ['Take phrase “', 'phrase “ Good', '“ Good Morning', 'Good Morning America.', 'Morning America. ”', 'America. ” looks', '” looks innocuous', 'looks innocuous ,', 'innocuous , ’', ', ’ .'] 

 TOTAL TRIGRAMS --> 10 



 ---- NOUN PHRASES ---- 

 ['phrase'] 

 TOTAL NOUN PHRASES --> 1 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['take', 'phrase', '“', 'good', 'morn', 'america.', '”', 'look', 'innocu', ',', '’', '.']

 TOTAL PORTER STEM WORDS ==> 12



 ---- SNOWBALL STEMMING ----

['take', 'phrase', '“', 'good', 'morn', 'america.', '”', 'look', 'innocu', ',', '’', '.']

 TOTAL SNOWBALL STEM WORDS ==> 12



 ---- LEMMATIZATION ----

['Take', 'phrase', '“', 'Good', 'Morning', 'America.', '”', 'look', 'innocuous', ',', '’', '.']

 TOTAL LEMMATIZE WORDS ==> 12

************************************************************************************************************************

114 --> If your  part-of-speech tagger fails to  apply “proper noun” to the  phrase “Good Morning America,”  this phrase won’t be denoted   as being an entity. 


 ---- TOKENS ----

 ['If', 'your', 'part-of-speech', 'tagger', 'fails', 'to', 'apply', '“', 'proper', 'noun', '”', 'to', 'the', 'phrase', '“', 'Good', 'Morning', 'America', ',', '”', 'this', 'phrase', 'won', '’', 't', 'be', 'denoted', 'as', 'being', 'an', 'entity', '.'] 

 TOTAL TOKENS ==> 32

 ---- POST ----

 [('If', 'IN'), ('your', 'PRP$'), ('part-of-speech', 'JJ'), ('tagger', 'NN'), ('fails', 'VBZ'), ('to', 'TO'), ('apply', 'VB'), ('“', 'JJ'), ('proper', 'JJ'), ('noun', 'NN'), ('”', 'NN'), ('to', 'TO'), ('the', 'DT'), ('phrase', 'NN'), ('“', 'NNP'), ('Good', 'NNP'), ('Morning', 'NNP'), ('America', 'NNP'), (',', ','), ('”', 'VBZ'), ('this', 'DT'), ('phrase', 'NN'), ('won', 'VBD'), ('’', 'NNP'), ('t', 'NN'), ('be', 'VB'), ('denoted', 'VBN'), ('as', 'IN'), ('being', 'VBG'), ('an', 'DT'), ('entity', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['part-of-speech', 'tagger', 'fails', 'apply', '“', 'proper', 'noun', '”', 'phrase', '“', 'Good', 'Morning', 'America', ',', '”', 'phrase', '’', 'denoted', 'entity', '.']

 TOTAL FILTERED TOKENS ==>  20

 ---- POST FOR FILTERED TOKENS ----

 [('part-of-speech', 'JJ'), ('tagger', 'NN'), ('fails', 'VBZ'), ('apply', 'RB'), ('“', 'JJ'), ('proper', 'JJ'), ('noun', 'NN'), ('”', 'NNP'), ('phrase', 'NN'), ('“', 'NNP'), ('Good', 'NNP'), ('Morning', 'NNP'), ('America', 'NNP'), (',', ','), ('”', 'NNP'), ('phrase', 'NN'), ('’', 'NNP'), ('denoted', 'VBD'), ('entity', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['part-of-speech tagger', 'tagger fails', 'fails apply', 'apply “', '“ proper', 'proper noun', 'noun ”', '” phrase', 'phrase “', '“ Good', 'Good Morning', 'Morning America', 'America ,', ', ”', '” phrase', 'phrase ’', '’ denoted', 'denoted entity', 'entity .'] 

 TOTAL BIGRAMS --> 19 



 ---- TRI-GRAMS ---- 

 ['part-of-speech tagger fails', 'tagger fails apply', 'fails apply “', 'apply “ proper', '“ proper noun', 'proper noun ”', 'noun ” phrase', '” phrase “', 'phrase “ Good', '“ Good Morning', 'Good Morning America', 'Morning America ,', 'America , ”', ', ” phrase', '” phrase ’', 'phrase ’ denoted', '’ denoted entity', 'denoted entity .'] 

 TOTAL TRIGRAMS --> 18 



 ---- NOUN PHRASES ---- 

 ['part-of-speech tagger', '“ proper noun', 'phrase', 'phrase', 'entity'] 

 TOTAL NOUN PHRASES --> 5 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> ['America']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['part-of-speech', 'tagger', 'fail', 'appli', '“', 'proper', 'noun', '”', 'phrase', '“', 'good', 'morn', 'america', ',', '”', 'phrase', '’', 'denot', 'entiti', '.']

 TOTAL PORTER STEM WORDS ==> 20



 ---- SNOWBALL STEMMING ----

['part-of-speech', 'tagger', 'fail', 'appli', '“', 'proper', 'noun', '”', 'phrase', '“', 'good', 'morn', 'america', ',', '”', 'phrase', '’', 'denot', 'entiti', '.']

 TOTAL SNOWBALL STEM WORDS ==> 20



 ---- LEMMATIZATION ----

['part-of-speech', 'tagger', 'fails', 'apply', '“', 'proper', 'noun', '”', 'phrase', '“', 'Good', 'Morning', 'America', ',', '”', 'phrase', '’', 'denoted', 'entity', '.']

 TOTAL LEMMATIZE WORDS ==> 20

************************************************************************************************************************

115 --> You have to know that it’s an entity (TV Show) and not a greeting. 


 ---- TOKENS ----

 ['You', 'have', 'to', 'know', 'that', 'it', '’', 's', 'an', 'entity', '(', 'TV', 'Show', ')', 'and', 'not', 'a', 'greeting', '.'] 

 TOTAL TOKENS ==> 19

 ---- POST ----

 [('You', 'PRP'), ('have', 'VBP'), ('to', 'TO'), ('know', 'VB'), ('that', 'IN'), ('it', 'PRP'), ('’', 'NNP'), ('s', 'VBD'), ('an', 'DT'), ('entity', 'NN'), ('(', '('), ('TV', 'NNP'), ('Show', 'NNP'), (')', ')'), ('and', 'CC'), ('not', 'RB'), ('a', 'DT'), ('greeting', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['know', '’', 'entity', '(', 'TV', 'Show', ')', 'greeting', '.']

 TOTAL FILTERED TOKENS ==>  9

 ---- POST FOR FILTERED TOKENS ----

 [('know', 'VB'), ('’', 'JJ'), ('entity', 'NN'), ('(', '('), ('TV', 'NNP'), ('Show', 'NNP'), (')', ')'), ('greeting', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['know ’', '’ entity', 'entity (', '( TV', 'TV Show', 'Show )', ') greeting', 'greeting .'] 

 TOTAL BIGRAMS --> 8 



 ---- TRI-GRAMS ---- 

 ['know ’ entity', '’ entity (', 'entity ( TV', '( TV Show', 'TV Show )', 'Show ) greeting', ') greeting .'] 

 TOTAL TRIGRAMS --> 7 



 ---- NOUN PHRASES ---- 

 ['’ entity', 'greeting'] 

 TOTAL NOUN PHRASES --> 2 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['know', '’', 'entiti', '(', 'tv', 'show', ')', 'greet', '.']

 TOTAL PORTER STEM WORDS ==> 9



 ---- SNOWBALL STEMMING ----

['know', '’', 'entiti', '(', 'tv', 'show', ')', 'greet', '.']

 TOTAL SNOWBALL STEM WORDS ==> 9



 ---- LEMMATIZATION ----

['know', '’', 'entity', '(', 'TV', 'Show', ')', 'greeting', '.']

 TOTAL LEMMATIZE WORDS ==> 9

************************************************************************************************************************

116 --> If you   don’t, you might interpret “good” as being positive, rather than just part   of the entity name. 


 ---- TOKENS ----

 ['If', 'you', 'don', '’', 't', ',', 'you', 'might', 'interpret', '“', 'good', '”', 'as', 'being', 'positive', ',', 'rather', 'than', 'just', 'part', 'of', 'the', 'entity', 'name', '.'] 

 TOTAL TOKENS ==> 25

 ---- POST ----

 [('If', 'IN'), ('you', 'PRP'), ('don', 'VBP'), ('’', 'JJ'), ('t', 'NN'), (',', ','), ('you', 'PRP'), ('might', 'MD'), ('interpret', 'VB'), ('“', 'RB'), ('good', 'JJ'), ('”', 'NN'), ('as', 'IN'), ('being', 'VBG'), ('positive', 'JJ'), (',', ','), ('rather', 'RB'), ('than', 'IN'), ('just', 'RB'), ('part', 'NN'), ('of', 'IN'), ('the', 'DT'), ('entity', 'NN'), ('name', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['’', ',', 'might', 'interpret', '“', 'good', '”', 'positive', ',', 'rather', 'part', 'entity', 'name', '.']

 TOTAL FILTERED TOKENS ==>  14

 ---- POST FOR FILTERED TOKENS ----

 [('’', 'NNS'), (',', ','), ('might', 'MD'), ('interpret', 'VB'), ('“', 'RB'), ('good', 'JJ'), ('”', 'NNP'), ('positive', 'JJ'), (',', ','), ('rather', 'RB'), ('part', 'NN'), ('entity', 'NN'), ('name', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['’ ,', ', might', 'might interpret', 'interpret “', '“ good', 'good ”', '” positive', 'positive ,', ', rather', 'rather part', 'part entity', 'entity name', 'name .'] 

 TOTAL BIGRAMS --> 13 



 ---- TRI-GRAMS ---- 

 ['’ , might', ', might interpret', 'might interpret “', 'interpret “ good', '“ good ”', 'good ” positive', '” positive ,', 'positive , rather', ', rather part', 'rather part entity', 'part entity name', 'entity name .'] 

 TOTAL TRIGRAMS --> 12 



 ---- NOUN PHRASES ---- 

 ['part', 'entity', 'name'] 

 TOTAL NOUN PHRASES --> 3 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['’', ',', 'might', 'interpret', '“', 'good', '”', 'posit', ',', 'rather', 'part', 'entiti', 'name', '.']

 TOTAL PORTER STEM WORDS ==> 14



 ---- SNOWBALL STEMMING ----

['’', ',', 'might', 'interpret', '“', 'good', '”', 'posit', ',', 'rather', 'part', 'entiti', 'name', '.']

 TOTAL SNOWBALL STEM WORDS ==> 14



 ---- LEMMATIZATION ----

['’', ',', 'might', 'interpret', '“', 'good', '”', 'positive', ',', 'rather', 'part', 'entity', 'name', '.']

 TOTAL LEMMATIZE WORDS ==> 14

************************************************************************************************************************

117 --> GOOD MORNING AMERICA is a registered trademark and brand of American Broadcasting Companies, Inc.  and is not affiliated with Lexalytics, Inc https://www.lexalytics.com/ https://www.lexalytics.com/  W H I T E  P A P E R 12|       |   Lexalytics, Inc., 48 North Pleasant St. Unit 301, Amherst MA 01002 USA   |   1-800-377-8036 |   www.lexalytics.com T U N E  F I R S T ,  T H E N  T R A I N :   E F F I C I E N C Y  B E F O R E  C O M P L E X I T Y  We have a whole white paper devoted to this discussion, but let’s review   the key points. 


 ---- TOKENS ----

 ['GOOD', 'MORNING', 'AMERICA', 'is', 'a', 'registered', 'trademark', 'and', 'brand', 'of', 'American', 'Broadcasting', 'Companies', ',', 'Inc.', 'and', 'is', 'not', 'affiliated', 'with', 'Lexalytics', ',', 'Inc', 'https', ':', '//www.lexalytics.com/', 'https', ':', '//www.lexalytics.com/', 'W', 'H', 'I', 'T', 'E', 'P', 'A', 'P', 'E', 'R', '12|', '|', 'Lexalytics', ',', 'Inc.', ',', '48', 'North', 'Pleasant', 'St.', 'Unit', '301', ',', 'Amherst', 'MA', '01002', 'USA', '|', '1-800-377-8036', '|', 'www.lexalytics.com', 'T', 'U', 'N', 'E', 'F', 'I', 'R', 'S', 'T', ',', 'T', 'H', 'E', 'N', 'T', 'R', 'A', 'I', 'N', ':', 'E', 'F', 'F', 'I', 'C', 'I', 'E', 'N', 'C', 'Y', 'B', 'E', 'F', 'O', 'R', 'E', 'C', 'O', 'M', 'P', 'L', 'E', 'X', 'I', 'T', 'Y', 'We', 'have', 'a', 'whole', 'white', 'paper', 'devoted', 'to', 'this', 'discussion', ',', 'but', 'let', '’', 's', 'review', 'the', 'key', 'points', '.'] 

 TOTAL TOKENS ==> 126

 ---- POST ----

 [('GOOD', 'JJ'), ('MORNING', 'NN'), ('AMERICA', 'NNP'), ('is', 'VBZ'), ('a', 'DT'), ('registered', 'JJ'), ('trademark', 'NN'), ('and', 'CC'), ('brand', 'NN'), ('of', 'IN'), ('American', 'NNP'), ('Broadcasting', 'NNP'), ('Companies', 'NNP'), (',', ','), ('Inc.', 'NNP'), ('and', 'CC'), ('is', 'VBZ'), ('not', 'RB'), ('affiliated', 'VBN'), ('with', 'IN'), ('Lexalytics', 'NNP'), (',', ','), ('Inc', 'NNP'), ('https', 'NN'), (':', ':'), ('//www.lexalytics.com/', 'JJ'), ('https', 'NN'), (':', ':'), ('//www.lexalytics.com/', 'JJ'), ('W', 'NNP'), ('H', 'NNP'), ('I', 'PRP'), ('T', 'NNP'), ('E', 'NNP'), ('P', 'NNP'), ('A', 'NNP'), ('P', 'NNP'), ('E', 'NNP'), ('R', 'NNP'), ('12|', 'CD'), ('|', 'NNP'), ('Lexalytics', 'NNP'), (',', ','), ('Inc.', 'NNP'), (',', ','), ('48', 'CD'), ('North', 'NNP'), ('Pleasant', 'NNP'), ('St.', 'NNP'), ('Unit', 'NNP'), ('301', 'CD'), (',', ','), ('Amherst', 'NNP'), ('MA', 'NNP'), ('01002', 'CD'), ('USA', 'NNP'), ('|', 'NNP'), ('1-800-377-8036', 'JJ'), ('|', 'NNP'), ('www.lexalytics.com', 'NN'), ('T', 'NNP'), ('U', 'NNP'), ('N', 'NNP'), ('E', 'NNP'), ('F', 'NNP'), ('I', 'PRP'), ('R', 'NNP'), ('S', 'NNP'), ('T', 'NNP'), (',', ','), ('T', 'NNP'), ('H', 'NNP'), ('E', 'NNP'), ('N', 'NNP'), ('T', 'NNP'), ('R', 'NNP'), ('A', 'NNP'), ('I', 'PRP'), ('N', 'NNP'), (':', ':'), ('E', 'NN'), ('F', 'NNP'), ('F', 'NNP'), ('I', 'PRP'), ('C', 'VBP'), ('I', 'PRP'), ('E', 'NNP'), ('N', 'NNP'), ('C', 'NNP'), ('Y', 'NNP'), ('B', 'NNP'), ('E', 'NNP'), ('F', 'NNP'), ('O', 'NNP'), ('R', 'NNP'), ('E', 'NNP'), ('C', 'NNP'), ('O', 'NNP'), ('M', 'NNP'), ('P', 'NNP'), ('L', 'NNP'), ('E', 'NNP'), ('X', 'NNP'), ('I', 'PRP'), ('T', 'VBP'), ('Y', 'IN'), ('We', 'PRP'), ('have', 'VBP'), ('a', 'DT'), ('whole', 'JJ'), ('white', 'JJ'), ('paper', 'NN'), ('devoted', 'VBN'), ('to', 'TO'), ('this', 'DT'), ('discussion', 'NN'), (',', ','), ('but', 'CC'), ('let', 'VB'), ('’', 'NNP'), ('s', 'VB'), ('review', 'VB'), ('the', 'DT'), ('key', 'JJ'), ('points', 'NNS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['GOOD', 'MORNING', 'AMERICA', 'registered', 'trademark', 'brand', 'American', 'Broadcasting', 'Companies', ',', 'Inc.', 'affiliated', 'Lexalytics', ',', 'Inc', 'https', ':', '//www.lexalytics.com/', 'https', ':', '//www.lexalytics.com/', 'W', 'H', 'E', 'P', 'P', 'E', 'R', '12|', '|', 'Lexalytics', ',', 'Inc.', ',', '48', 'North', 'Pleasant', 'St.', 'Unit', '301', ',', 'Amherst', '01002', 'USA', '|', '1-800-377-8036', '|', 'www.lexalytics.com', 'U', 'N', 'E', 'F', 'R', ',', 'H', 'E', 'N', 'R', 'N', ':', 'E', 'F', 'F', 'C', 'E', 'N', 'C', 'B', 'E', 'F', 'R', 'E', 'C', 'P', 'L', 'E', 'X', 'whole', 'white', 'paper', 'devoted', 'discussion', ',', 'let', '’', 'review', 'key', 'points', '.']

 TOTAL FILTERED TOKENS ==>  89

 ---- POST FOR FILTERED TOKENS ----

 [('GOOD', 'JJ'), ('MORNING', 'NN'), ('AMERICA', 'NNP'), ('registered', 'VBD'), ('trademark', 'NN'), ('brand', 'NN'), ('American', 'NNP'), ('Broadcasting', 'NNP'), ('Companies', 'NNP'), (',', ','), ('Inc.', 'NNP'), ('affiliated', 'VBD'), ('Lexalytics', 'NNP'), (',', ','), ('Inc', 'NNP'), ('https', 'NN'), (':', ':'), ('//www.lexalytics.com/', 'JJ'), ('https', 'NN'), (':', ':'), ('//www.lexalytics.com/', 'JJ'), ('W', 'NNP'), ('H', 'NNP'), ('E', 'NNP'), ('P', 'NNP'), ('P', 'NNP'), ('E', 'NNP'), ('R', 'NNP'), ('12|', 'CD'), ('|', 'NNP'), ('Lexalytics', 'NNP'), (',', ','), ('Inc.', 'NNP'), (',', ','), ('48', 'CD'), ('North', 'NNP'), ('Pleasant', 'NNP'), ('St.', 'NNP'), ('Unit', 'NNP'), ('301', 'CD'), (',', ','), ('Amherst', 'NNP'), ('01002', 'CD'), ('USA', 'NNP'), ('|', 'NNP'), ('1-800-377-8036', 'JJ'), ('|', 'NNP'), ('www.lexalytics.com', 'NN'), ('U', 'NNP'), ('N', 'NNP'), ('E', 'NNP'), ('F', 'NNP'), ('R', 'NNP'), (',', ','), ('H', 'NNP'), ('E', 'NNP'), ('N', 'NNP'), ('R', 'NNP'), ('N', 'NNP'), (':', ':'), ('E', 'NN'), ('F', 'NNP'), ('F', 'NNP'), ('C', 'NNP'), ('E', 'NNP'), ('N', 'NNP'), ('C', 'NNP'), ('B', 'NNP'), ('E', 'NNP'), ('F', 'NNP'), ('R', 'NNP'), ('E', 'NNP'), ('C', 'NNP'), ('P', 'NNP'), ('L', 'NNP'), ('E', 'NNP'), ('X', 'NNP'), ('whole', 'JJ'), ('white', 'JJ'), ('paper', 'NN'), ('devoted', 'VBN'), ('discussion', 'NN'), (',', ','), ('let', 'VB'), ('’', 'NNP'), ('review', 'VB'), ('key', 'JJ'), ('points', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['GOOD MORNING', 'MORNING AMERICA', 'AMERICA registered', 'registered trademark', 'trademark brand', 'brand American', 'American Broadcasting', 'Broadcasting Companies', 'Companies ,', ', Inc.', 'Inc. affiliated', 'affiliated Lexalytics', 'Lexalytics ,', ', Inc', 'Inc https', 'https :', ': //www.lexalytics.com/', '//www.lexalytics.com/ https', 'https :', ': //www.lexalytics.com/', '//www.lexalytics.com/ W', 'W H', 'H E', 'E P', 'P P', 'P E', 'E R', 'R 12|', '12| |', '| Lexalytics', 'Lexalytics ,', ', Inc.', 'Inc. ,', ', 48', '48 North', 'North Pleasant', 'Pleasant St.', 'St. Unit', 'Unit 301', '301 ,', ', Amherst', 'Amherst 01002', '01002 USA', 'USA |', '| 1-800-377-8036', '1-800-377-8036 |', '| www.lexalytics.com', 'www.lexalytics.com U', 'U N', 'N E', 'E F', 'F R', 'R ,', ', H', 'H E', 'E N', 'N R', 'R N', 'N :', ': E', 'E F', 'F F', 'F C', 'C E', 'E N', 'N C', 'C B', 'B E', 'E F', 'F R', 'R E', 'E C', 'C P', 'P L', 'L E', 'E X', 'X whole', 'whole white', 'white paper', 'paper devoted', 'devoted discussion', 'discussion ,', ', let', 'let ’', '’ review', 'review key', 'key points', 'points .'] 

 TOTAL BIGRAMS --> 88 



 ---- TRI-GRAMS ---- 

 ['GOOD MORNING AMERICA', 'MORNING AMERICA registered', 'AMERICA registered trademark', 'registered trademark brand', 'trademark brand American', 'brand American Broadcasting', 'American Broadcasting Companies', 'Broadcasting Companies ,', 'Companies , Inc.', ', Inc. affiliated', 'Inc. affiliated Lexalytics', 'affiliated Lexalytics ,', 'Lexalytics , Inc', ', Inc https', 'Inc https :', 'https : //www.lexalytics.com/', ': //www.lexalytics.com/ https', '//www.lexalytics.com/ https :', 'https : //www.lexalytics.com/', ': //www.lexalytics.com/ W', '//www.lexalytics.com/ W H', 'W H E', 'H E P', 'E P P', 'P P E', 'P E R', 'E R 12|', 'R 12| |', '12| | Lexalytics', '| Lexalytics ,', 'Lexalytics , Inc.', ', Inc. ,', 'Inc. , 48', ', 48 North', '48 North Pleasant', 'North Pleasant St.', 'Pleasant St. Unit', 'St. Unit 301', 'Unit 301 ,', '301 , Amherst', ', Amherst 01002', 'Amherst 01002 USA', '01002 USA |', 'USA | 1-800-377-8036', '| 1-800-377-8036 |', '1-800-377-8036 | www.lexalytics.com', '| www.lexalytics.com U', 'www.lexalytics.com U N', 'U N E', 'N E F', 'E F R', 'F R ,', 'R , H', ', H E', 'H E N', 'E N R', 'N R N', 'R N :', 'N : E', ': E F', 'E F F', 'F F C', 'F C E', 'C E N', 'E N C', 'N C B', 'C B E', 'B E F', 'E F R', 'F R E', 'R E C', 'E C P', 'C P L', 'P L E', 'L E X', 'E X whole', 'X whole white', 'whole white paper', 'white paper devoted', 'paper devoted discussion', 'devoted discussion ,', 'discussion , let', ', let ’', 'let ’ review', '’ review key', 'review key points', 'key points .'] 

 TOTAL TRIGRAMS --> 87 



 ---- NOUN PHRASES ---- 

 ['GOOD MORNING', 'trademark', 'brand', 'https', ' https', 'www.lexalytics.com', 'E', 'whole white paper', 'discussion'] 

 TOTAL NOUN PHRASES --> 9 



 ---- NER ----

 
 ORGANIZATION ---> ['AMERICA', 'American Broadcasting Companies', 'USA']
 TOTAL ORGANIZATION ENTITY --> 3 


 PERSON ---> ['Inc.', 'Lexalytics', 'Inc', 'H E']
 TOTAL PERSON ENTITY --> 4 


 GPE ---> ['North']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['good', 'morn', 'america', 'regist', 'trademark', 'brand', 'american', 'broadcast', 'compani', ',', 'inc.', 'affili', 'lexalyt', ',', 'inc', 'http', ':', '//www.lexalytics.com/', 'http', ':', '//www.lexalytics.com/', 'w', 'h', 'e', 'p', 'p', 'e', 'r', '12|', '|', 'lexalyt', ',', 'inc.', ',', '48', 'north', 'pleasant', 'st.', 'unit', '301', ',', 'amherst', '01002', 'usa', '|', '1-800-377-8036', '|', 'www.lexalytics.com', 'u', 'n', 'e', 'f', 'r', ',', 'h', 'e', 'n', 'r', 'n', ':', 'e', 'f', 'f', 'c', 'e', 'n', 'c', 'b', 'e', 'f', 'r', 'e', 'c', 'p', 'l', 'e', 'x', 'whole', 'white', 'paper', 'devot', 'discuss', ',', 'let', '’', 'review', 'key', 'point', '.']

 TOTAL PORTER STEM WORDS ==> 89



 ---- SNOWBALL STEMMING ----

['good', 'morn', 'america', 'regist', 'trademark', 'brand', 'american', 'broadcast', 'compani', ',', 'inc.', 'affili', 'lexalyt', ',', 'inc', 'https', ':', '//www.lexalytics.com/', 'https', ':', '//www.lexalytics.com/', 'w', 'h', 'e', 'p', 'p', 'e', 'r', '12|', '|', 'lexalyt', ',', 'inc.', ',', '48', 'north', 'pleasant', 'st.', 'unit', '301', ',', 'amherst', '01002', 'usa', '|', '1-800-377-8036', '|', 'www.lexalytics.com', 'u', 'n', 'e', 'f', 'r', ',', 'h', 'e', 'n', 'r', 'n', ':', 'e', 'f', 'f', 'c', 'e', 'n', 'c', 'b', 'e', 'f', 'r', 'e', 'c', 'p', 'l', 'e', 'x', 'whole', 'white', 'paper', 'devot', 'discuss', ',', 'let', '’', 'review', 'key', 'point', '.']

 TOTAL SNOWBALL STEM WORDS ==> 89



 ---- LEMMATIZATION ----

['GOOD', 'MORNING', 'AMERICA', 'registered', 'trademark', 'brand', 'American', 'Broadcasting', 'Companies', ',', 'Inc.', 'affiliated', 'Lexalytics', ',', 'Inc', 'http', ':', '//www.lexalytics.com/', 'http', ':', '//www.lexalytics.com/', 'W', 'H', 'E', 'P', 'P', 'E', 'R', '12|', '|', 'Lexalytics', ',', 'Inc.', ',', '48', 'North', 'Pleasant', 'St.', 'Unit', '301', ',', 'Amherst', '01002', 'USA', '|', '1-800-377-8036', '|', 'www.lexalytics.com', 'U', 'N', 'E', 'F', 'R', ',', 'H', 'E', 'N', 'R', 'N', ':', 'E', 'F', 'F', 'C', 'E', 'N', 'C', 'B', 'E', 'F', 'R', 'E', 'C', 'P', 'L', 'E', 'X', 'whole', 'white', 'paper', 'devoted', 'discussion', ',', 'let', '’', 'review', 'key', 'point', '.']

 TOTAL LEMMATIZE WORDS ==> 89

************************************************************************************************************************

118 --> We talked above about how machine learning is really  machine teaching, and how changing how a model interprets something  means having to convince it to do that. 


 ---- TOKENS ----

 ['We', 'talked', 'above', 'about', 'how', 'machine', 'learning', 'is', 'really', 'machine', 'teaching', ',', 'and', 'how', 'changing', 'how', 'a', 'model', 'interprets', 'something', 'means', 'having', 'to', 'convince', 'it', 'to', 'do', 'that', '.'] 

 TOTAL TOKENS ==> 29

 ---- POST ----

 [('We', 'PRP'), ('talked', 'VBD'), ('above', 'IN'), ('about', 'IN'), ('how', 'WRB'), ('machine', 'NN'), ('learning', 'NN'), ('is', 'VBZ'), ('really', 'RB'), ('machine', 'NN'), ('teaching', 'NN'), (',', ','), ('and', 'CC'), ('how', 'WRB'), ('changing', 'JJ'), ('how', 'WRB'), ('a', 'DT'), ('model', 'NN'), ('interprets', 'VBZ'), ('something', 'NN'), ('means', 'VBZ'), ('having', 'VBG'), ('to', 'TO'), ('convince', 'VB'), ('it', 'PRP'), ('to', 'TO'), ('do', 'VB'), ('that', 'DT'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['talked', 'machine', 'learning', 'really', 'machine', 'teaching', ',', 'changing', 'model', 'interprets', 'something', 'means', 'convince', '.']

 TOTAL FILTERED TOKENS ==>  14

 ---- POST FOR FILTERED TOKENS ----

 [('talked', 'JJ'), ('machine', 'NN'), ('learning', 'VBG'), ('really', 'RB'), ('machine', 'NN'), ('teaching', 'NN'), (',', ','), ('changing', 'VBG'), ('model', 'NN'), ('interprets', 'NNS'), ('something', 'NN'), ('means', 'VBZ'), ('convince', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['talked machine', 'machine learning', 'learning really', 'really machine', 'machine teaching', 'teaching ,', ', changing', 'changing model', 'model interprets', 'interprets something', 'something means', 'means convince', 'convince .'] 

 TOTAL BIGRAMS --> 13 



 ---- TRI-GRAMS ---- 

 ['talked machine learning', 'machine learning really', 'learning really machine', 'really machine teaching', 'machine teaching ,', 'teaching , changing', ', changing model', 'changing model interprets', 'model interprets something', 'interprets something means', 'something means convince', 'means convince .'] 

 TOTAL TRIGRAMS --> 12 



 ---- NOUN PHRASES ---- 

 ['talked machine', 'machine', 'teaching', 'model', 'something', 'convince'] 

 TOTAL NOUN PHRASES --> 6 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['talk', 'machin', 'learn', 'realli', 'machin', 'teach', ',', 'chang', 'model', 'interpret', 'someth', 'mean', 'convinc', '.']

 TOTAL PORTER STEM WORDS ==> 14



 ---- SNOWBALL STEMMING ----

['talk', 'machin', 'learn', 'realli', 'machin', 'teach', ',', 'chang', 'model', 'interpret', 'someth', 'mean', 'convinc', '.']

 TOTAL SNOWBALL STEM WORDS ==> 14



 ---- LEMMATIZATION ----

['talked', 'machine', 'learning', 'really', 'machine', 'teaching', ',', 'changing', 'model', 'interprets', 'something', 'mean', 'convince', '.']

 TOTAL LEMMATIZE WORDS ==> 14

************************************************************************************************************************

119 --> To achieve this, you have to have   data, and enough of it, that supports the changes needed to make the  model behave differently. 


 ---- TOKENS ----

 ['To', 'achieve', 'this', ',', 'you', 'have', 'to', 'have', 'data', ',', 'and', 'enough', 'of', 'it', ',', 'that', 'supports', 'the', 'changes', 'needed', 'to', 'make', 'the', 'model', 'behave', 'differently', '.'] 

 TOTAL TOKENS ==> 27

 ---- POST ----

 [('To', 'TO'), ('achieve', 'VB'), ('this', 'DT'), (',', ','), ('you', 'PRP'), ('have', 'VBP'), ('to', 'TO'), ('have', 'VB'), ('data', 'NNS'), (',', ','), ('and', 'CC'), ('enough', 'RB'), ('of', 'IN'), ('it', 'PRP'), (',', ','), ('that', 'WDT'), ('supports', 'VBZ'), ('the', 'DT'), ('changes', 'NNS'), ('needed', 'VBN'), ('to', 'TO'), ('make', 'VB'), ('the', 'DT'), ('model', 'NN'), ('behave', 'VBP'), ('differently', 'RB'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['achieve', ',', 'data', ',', 'enough', ',', 'supports', 'changes', 'needed', 'make', 'model', 'behave', 'differently', '.']

 TOTAL FILTERED TOKENS ==>  14

 ---- POST FOR FILTERED TOKENS ----

 [('achieve', 'NN'), (',', ','), ('data', 'NNS'), (',', ','), ('enough', 'RB'), (',', ','), ('supports', 'NNS'), ('changes', 'NNS'), ('needed', 'VBD'), ('make', 'NN'), ('model', 'NN'), ('behave', 'VB'), ('differently', 'RB'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['achieve ,', ', data', 'data ,', ', enough', 'enough ,', ', supports', 'supports changes', 'changes needed', 'needed make', 'make model', 'model behave', 'behave differently', 'differently .'] 

 TOTAL BIGRAMS --> 13 



 ---- TRI-GRAMS ---- 

 ['achieve , data', ', data ,', 'data , enough', ', enough ,', 'enough , supports', ', supports changes', 'supports changes needed', 'changes needed make', 'needed make model', 'make model behave', 'model behave differently', 'behave differently .'] 

 TOTAL TRIGRAMS --> 12 



 ---- NOUN PHRASES ---- 

 ['achieve', 'make', 'model'] 

 TOTAL NOUN PHRASES --> 3 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['achiev', ',', 'data', ',', 'enough', ',', 'support', 'chang', 'need', 'make', 'model', 'behav', 'differ', '.']

 TOTAL PORTER STEM WORDS ==> 14



 ---- SNOWBALL STEMMING ----

['achiev', ',', 'data', ',', 'enough', ',', 'support', 'chang', 'need', 'make', 'model', 'behav', 'differ', '.']

 TOTAL SNOWBALL STEM WORDS ==> 14



 ---- LEMMATIZATION ----

['achieve', ',', 'data', ',', 'enough', ',', 'support', 'change', 'needed', 'make', 'model', 'behave', 'differently', '.']

 TOTAL LEMMATIZE WORDS ==> 14

************************************************************************************************************************

120 --> This is different than tuning. 


 ---- TOKENS ----

 ['This', 'is', 'different', 'than', 'tuning', '.'] 

 TOTAL TOKENS ==> 6

 ---- POST ----

 [('This', 'DT'), ('is', 'VBZ'), ('different', 'JJ'), ('than', 'IN'), ('tuning', 'VBG'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['different', 'tuning', '.']

 TOTAL FILTERED TOKENS ==>  3

 ---- POST FOR FILTERED TOKENS ----

 [('different', 'JJ'), ('tuning', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['different tuning', 'tuning .'] 

 TOTAL BIGRAMS --> 2 



 ---- TRI-GRAMS ---- 

 ['different tuning .'] 

 TOTAL TRIGRAMS --> 1 



 ---- NOUN PHRASES ---- 

 ['different tuning'] 

 TOTAL NOUN PHRASES --> 1 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['differ', 'tune', '.']

 TOTAL PORTER STEM WORDS ==> 3



 ---- SNOWBALL STEMMING ----

['differ', 'tune', '.']

 TOTAL SNOWBALL STEM WORDS ==> 3



 ---- LEMMATIZATION ----

['different', 'tuning', '.']

 TOTAL LEMMATIZE WORDS ==> 3

************************************************************************************************************************

121 --> Tuning is a type of written instruction. 


 ---- TOKENS ----

 ['Tuning', 'is', 'a', 'type', 'of', 'written', 'instruction', '.'] 

 TOTAL TOKENS ==> 8

 ---- POST ----

 [('Tuning', 'NN'), ('is', 'VBZ'), ('a', 'DT'), ('type', 'NN'), ('of', 'IN'), ('written', 'VBN'), ('instruction', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Tuning', 'type', 'written', 'instruction', '.']

 TOTAL FILTERED TOKENS ==>  5

 ---- POST FOR FILTERED TOKENS ----

 [('Tuning', 'VBG'), ('type', 'NN'), ('written', 'VBN'), ('instruction', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Tuning type', 'type written', 'written instruction', 'instruction .'] 

 TOTAL BIGRAMS --> 4 



 ---- TRI-GRAMS ---- 

 ['Tuning type written', 'type written instruction', 'written instruction .'] 

 TOTAL TRIGRAMS --> 3 



 ---- NOUN PHRASES ---- 

 ['type', 'instruction'] 

 TOTAL NOUN PHRASES --> 2 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['tune', 'type', 'written', 'instruct', '.']

 TOTAL PORTER STEM WORDS ==> 5



 ---- SNOWBALL STEMMING ----

['tune', 'type', 'written', 'instruct', '.']

 TOTAL SNOWBALL STEM WORDS ==> 5



 ---- LEMMATIZATION ----

['Tuning', 'type', 'written', 'instruction', '.']

 TOTAL LEMMATIZE WORDS ==> 5

************************************************************************************************************************

122 --> With  tuning, you might tell a model that the airport term “gate change” carries   -0.5 sentiment points, and that this value should be used every time the  model sees the phrase. 


 ---- TOKENS ----

 ['With', 'tuning', ',', 'you', 'might', 'tell', 'a', 'model', 'that', 'the', 'airport', 'term', '“', 'gate', 'change', '”', 'carries', '-0.5', 'sentiment', 'points', ',', 'and', 'that', 'this', 'value', 'should', 'be', 'used', 'every', 'time', 'the', 'model', 'sees', 'the', 'phrase', '.'] 

 TOTAL TOKENS ==> 36

 ---- POST ----

 [('With', 'IN'), ('tuning', 'NN'), (',', ','), ('you', 'PRP'), ('might', 'MD'), ('tell', 'VB'), ('a', 'DT'), ('model', 'NN'), ('that', 'IN'), ('the', 'DT'), ('airport', 'JJ'), ('term', 'NN'), ('“', 'NNP'), ('gate', 'NN'), ('change', 'NN'), ('”', 'NNP'), ('carries', 'VBZ'), ('-0.5', 'NNP'), ('sentiment', 'NN'), ('points', 'NNS'), (',', ','), ('and', 'CC'), ('that', 'IN'), ('this', 'DT'), ('value', 'NN'), ('should', 'MD'), ('be', 'VB'), ('used', 'VBN'), ('every', 'DT'), ('time', 'NN'), ('the', 'DT'), ('model', 'NN'), ('sees', 'VBZ'), ('the', 'DT'), ('phrase', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['tuning', ',', 'might', 'tell', 'model', 'airport', 'term', '“', 'gate', 'change', '”', 'carries', '-0.5', 'sentiment', 'points', ',', 'value', 'used', 'every', 'time', 'model', 'sees', 'phrase', '.']

 TOTAL FILTERED TOKENS ==>  24

 ---- POST FOR FILTERED TOKENS ----

 [('tuning', 'NN'), (',', ','), ('might', 'MD'), ('tell', 'VB'), ('model', 'FW'), ('airport', 'JJ'), ('term', 'NN'), ('“', 'NNP'), ('gate', 'NN'), ('change', 'NN'), ('”', 'NNP'), ('carries', 'VBZ'), ('-0.5', 'NNP'), ('sentiment', 'NN'), ('points', 'NNS'), (',', ','), ('value', 'NN'), ('used', 'VBN'), ('every', 'DT'), ('time', 'NN'), ('model', 'NN'), ('sees', 'NNS'), ('phrase', 'VBP'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['tuning ,', ', might', 'might tell', 'tell model', 'model airport', 'airport term', 'term “', '“ gate', 'gate change', 'change ”', '” carries', 'carries -0.5', '-0.5 sentiment', 'sentiment points', 'points ,', ', value', 'value used', 'used every', 'every time', 'time model', 'model sees', 'sees phrase', 'phrase .'] 

 TOTAL BIGRAMS --> 23 



 ---- TRI-GRAMS ---- 

 ['tuning , might', ', might tell', 'might tell model', 'tell model airport', 'model airport term', 'airport term “', 'term “ gate', '“ gate change', 'gate change ”', 'change ” carries', '” carries -0.5', 'carries -0.5 sentiment', '-0.5 sentiment points', 'sentiment points ,', 'points , value', ', value used', 'value used every', 'used every time', 'every time model', 'time model sees', 'model sees phrase', 'sees phrase .'] 

 TOTAL TRIGRAMS --> 22 



 ---- NOUN PHRASES ---- 

 ['tuning', 'airport term', 'gate', 'change', 'sentiment', 'value', 'every time', 'model'] 

 TOTAL NOUN PHRASES --> 8 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['tune', ',', 'might', 'tell', 'model', 'airport', 'term', '“', 'gate', 'chang', '”', 'carri', '-0.5', 'sentiment', 'point', ',', 'valu', 'use', 'everi', 'time', 'model', 'see', 'phrase', '.']

 TOTAL PORTER STEM WORDS ==> 24



 ---- SNOWBALL STEMMING ----

['tune', ',', 'might', 'tell', 'model', 'airport', 'term', '“', 'gate', 'chang', '”', 'carri', '-0.5', 'sentiment', 'point', ',', 'valu', 'use', 'everi', 'time', 'model', 'see', 'phrase', '.']

 TOTAL SNOWBALL STEM WORDS ==> 24



 ---- LEMMATIZATION ----

['tuning', ',', 'might', 'tell', 'model', 'airport', 'term', '“', 'gate', 'change', '”', 'carry', '-0.5', 'sentiment', 'point', ',', 'value', 'used', 'every', 'time', 'model', 'see', 'phrase', '.']

 TOTAL LEMMATIZE WORDS ==> 24

************************************************************************************************************************

123 --> This new command will instantly apply to everything  that matches the entry. 


 ---- TOKENS ----

 ['This', 'new', 'command', 'will', 'instantly', 'apply', 'to', 'everything', 'that', 'matches', 'the', 'entry', '.'] 

 TOTAL TOKENS ==> 13

 ---- POST ----

 [('This', 'DT'), ('new', 'JJ'), ('command', 'NN'), ('will', 'MD'), ('instantly', 'RB'), ('apply', 'VB'), ('to', 'TO'), ('everything', 'NN'), ('that', 'WDT'), ('matches', 'VBZ'), ('the', 'DT'), ('entry', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['new', 'command', 'instantly', 'apply', 'everything', 'matches', 'entry', '.']

 TOTAL FILTERED TOKENS ==>  8

 ---- POST FOR FILTERED TOKENS ----

 [('new', 'JJ'), ('command', 'NN'), ('instantly', 'RB'), ('apply', 'VB'), ('everything', 'NN'), ('matches', 'NNS'), ('entry', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['new command', 'command instantly', 'instantly apply', 'apply everything', 'everything matches', 'matches entry', 'entry .'] 

 TOTAL BIGRAMS --> 7 



 ---- TRI-GRAMS ---- 

 ['new command instantly', 'command instantly apply', 'instantly apply everything', 'apply everything matches', 'everything matches entry', 'matches entry .'] 

 TOTAL TRIGRAMS --> 6 



 ---- NOUN PHRASES ---- 

 ['new command', 'everything', 'entry'] 

 TOTAL NOUN PHRASES --> 3 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['new', 'command', 'instantli', 'appli', 'everyth', 'match', 'entri', '.']

 TOTAL PORTER STEM WORDS ==> 8



 ---- SNOWBALL STEMMING ----

['new', 'command', 'instant', 'appli', 'everyth', 'match', 'entri', '.']

 TOTAL SNOWBALL STEM WORDS ==> 8



 ---- LEMMATIZATION ----

['new', 'command', 'instantly', 'apply', 'everything', 'match', 'entry', '.']

 TOTAL LEMMATIZE WORDS ==> 8

************************************************************************************************************************

124 --> Training, on the other hand, requires the model  to parse a significant amount of data before it starts to apply (“learn”) the  change. 


 ---- TOKENS ----

 ['Training', ',', 'on', 'the', 'other', 'hand', ',', 'requires', 'the', 'model', 'to', 'parse', 'a', 'significant', 'amount', 'of', 'data', 'before', 'it', 'starts', 'to', 'apply', '(', '“', 'learn', '”', ')', 'the', 'change', '.'] 

 TOTAL TOKENS ==> 30

 ---- POST ----

 [('Training', 'NN'), (',', ','), ('on', 'IN'), ('the', 'DT'), ('other', 'JJ'), ('hand', 'NN'), (',', ','), ('requires', 'VBZ'), ('the', 'DT'), ('model', 'NN'), ('to', 'TO'), ('parse', 'VB'), ('a', 'DT'), ('significant', 'JJ'), ('amount', 'NN'), ('of', 'IN'), ('data', 'NNS'), ('before', 'IN'), ('it', 'PRP'), ('starts', 'VBZ'), ('to', 'TO'), ('apply', 'VB'), ('(', '('), ('“', 'NNP'), ('learn', 'VBP'), ('”', 'NNP'), (')', ')'), ('the', 'DT'), ('change', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Training', ',', 'hand', ',', 'requires', 'model', 'parse', 'significant', 'amount', 'data', 'starts', 'apply', '(', '“', 'learn', '”', ')', 'change', '.']

 TOTAL FILTERED TOKENS ==>  19

 ---- POST FOR FILTERED TOKENS ----

 [('Training', 'NN'), (',', ','), ('hand', 'NN'), (',', ','), ('requires', 'VBZ'), ('model', 'JJ'), ('parse', 'JJ'), ('significant', 'JJ'), ('amount', 'NN'), ('data', 'NNS'), ('starts', 'NNS'), ('apply', 'RB'), ('(', '('), ('“', 'UH'), ('learn', 'VB'), ('”', 'NNP'), (')', ')'), ('change', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Training ,', ', hand', 'hand ,', ', requires', 'requires model', 'model parse', 'parse significant', 'significant amount', 'amount data', 'data starts', 'starts apply', 'apply (', '( “', '“ learn', 'learn ”', '” )', ') change', 'change .'] 

 TOTAL BIGRAMS --> 18 



 ---- TRI-GRAMS ---- 

 ['Training , hand', ', hand ,', 'hand , requires', ', requires model', 'requires model parse', 'model parse significant', 'parse significant amount', 'significant amount data', 'amount data starts', 'data starts apply', 'starts apply (', 'apply ( “', '( “ learn', '“ learn ”', 'learn ” )', '” ) change', ') change .'] 

 TOTAL TRIGRAMS --> 17 



 ---- NOUN PHRASES ---- 

 ['Training', 'hand', 'model parse significant amount', 'change'] 

 TOTAL NOUN PHRASES --> 4 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> ['Training']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['train', ',', 'hand', ',', 'requir', 'model', 'pars', 'signific', 'amount', 'data', 'start', 'appli', '(', '“', 'learn', '”', ')', 'chang', '.']

 TOTAL PORTER STEM WORDS ==> 19



 ---- SNOWBALL STEMMING ----

['train', ',', 'hand', ',', 'requir', 'model', 'pars', 'signific', 'amount', 'data', 'start', 'appli', '(', '“', 'learn', '”', ')', 'chang', '.']

 TOTAL SNOWBALL STEM WORDS ==> 19



 ---- LEMMATIZATION ----

['Training', ',', 'hand', ',', 'requires', 'model', 'parse', 'significant', 'amount', 'data', 'start', 'apply', '(', '“', 'learn', '”', ')', 'change', '.']

 TOTAL LEMMATIZE WORDS ==> 19

************************************************************************************************************************

125 --> Additionally, the more the model “wants” to score something a  particular way, the more that you’re going to have to work to change it. 


 ---- TOKENS ----

 ['Additionally', ',', 'the', 'more', 'the', 'model', '“', 'wants', '”', 'to', 'score', 'something', 'a', 'particular', 'way', ',', 'the', 'more', 'that', 'you', '’', 're', 'going', 'to', 'have', 'to', 'work', 'to', 'change', 'it', '.'] 

 TOTAL TOKENS ==> 31

 ---- POST ----

 [('Additionally', 'RB'), (',', ','), ('the', 'DT'), ('more', 'JJR'), ('the', 'DT'), ('model', 'NN'), ('“', 'NN'), ('wants', 'VBZ'), ('”', 'VB'), ('to', 'TO'), ('score', 'VB'), ('something', 'NN'), ('a', 'DT'), ('particular', 'JJ'), ('way', 'NN'), (',', ','), ('the', 'DT'), ('more', 'JJR'), ('that', 'IN'), ('you', 'PRP'), ('’', 'VBP'), ('re', 'VB'), ('going', 'VBG'), ('to', 'TO'), ('have', 'VB'), ('to', 'TO'), ('work', 'VB'), ('to', 'TO'), ('change', 'VB'), ('it', 'PRP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Additionally', ',', 'model', '“', 'wants', '”', 'score', 'something', 'particular', 'way', ',', '’', 'going', 'work', 'change', '.']

 TOTAL FILTERED TOKENS ==>  16

 ---- POST FOR FILTERED TOKENS ----

 [('Additionally', 'RB'), (',', ','), ('model', 'NN'), ('“', 'NN'), ('wants', 'VBZ'), ('”', 'JJ'), ('score', 'NN'), ('something', 'NN'), ('particular', 'JJ'), ('way', 'NN'), (',', ','), ('’', 'NNP'), ('going', 'VBG'), ('work', 'NN'), ('change', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Additionally ,', ', model', 'model “', '“ wants', 'wants ”', '” score', 'score something', 'something particular', 'particular way', 'way ,', ', ’', '’ going', 'going work', 'work change', 'change .'] 

 TOTAL BIGRAMS --> 15 



 ---- TRI-GRAMS ---- 

 ['Additionally , model', ', model “', 'model “ wants', '“ wants ”', 'wants ” score', '” score something', 'score something particular', 'something particular way', 'particular way ,', 'way , ’', ', ’ going', '’ going work', 'going work change', 'work change .'] 

 TOTAL TRIGRAMS --> 14 



 ---- NOUN PHRASES ---- 

 ['model', '“', '” score', 'something', 'particular way', 'work', 'change'] 

 TOTAL NOUN PHRASES --> 7 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['addit', ',', 'model', '“', 'want', '”', 'score', 'someth', 'particular', 'way', ',', '’', 'go', 'work', 'chang', '.']

 TOTAL PORTER STEM WORDS ==> 16



 ---- SNOWBALL STEMMING ----

['addit', ',', 'model', '“', 'want', '”', 'score', 'someth', 'particular', 'way', ',', '’', 'go', 'work', 'chang', '.']

 TOTAL SNOWBALL STEM WORDS ==> 16



 ---- LEMMATIZATION ----

['Additionally', ',', 'model', '“', 'want', '”', 'score', 'something', 'particular', 'way', ',', '’', 'going', 'work', 'change', '.']

 TOTAL LEMMATIZE WORDS ==> 16

************************************************************************************************************************

126 --> Old habits are hard to unlearn for machine learning systems, too. 


 ---- TOKENS ----

 ['Old', 'habits', 'are', 'hard', 'to', 'unlearn', 'for', 'machine', 'learning', 'systems', ',', 'too', '.'] 

 TOTAL TOKENS ==> 13

 ---- POST ----

 [('Old', 'NNP'), ('habits', 'NNS'), ('are', 'VBP'), ('hard', 'JJ'), ('to', 'TO'), ('unlearn', 'VB'), ('for', 'IN'), ('machine', 'NN'), ('learning', 'VBG'), ('systems', 'NNS'), (',', ','), ('too', 'RB'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Old', 'habits', 'hard', 'unlearn', 'machine', 'learning', 'systems', ',', '.']

 TOTAL FILTERED TOKENS ==>  9

 ---- POST FOR FILTERED TOKENS ----

 [('Old', 'NNP'), ('habits', 'VBZ'), ('hard', 'JJ'), ('unlearn', 'JJ'), ('machine', 'NN'), ('learning', 'VBG'), ('systems', 'NNS'), (',', ','), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Old habits', 'habits hard', 'hard unlearn', 'unlearn machine', 'machine learning', 'learning systems', 'systems ,', ', .'] 

 TOTAL BIGRAMS --> 8 



 ---- TRI-GRAMS ---- 

 ['Old habits hard', 'habits hard unlearn', 'hard unlearn machine', 'unlearn machine learning', 'machine learning systems', 'learning systems ,', 'systems , .'] 

 TOTAL TRIGRAMS --> 7 



 ---- NOUN PHRASES ---- 

 ['hard unlearn machine'] 

 TOTAL NOUN PHRASES --> 1 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> ['Old']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['old', 'habit', 'hard', 'unlearn', 'machin', 'learn', 'system', ',', '.']

 TOTAL PORTER STEM WORDS ==> 9



 ---- SNOWBALL STEMMING ----

['old', 'habit', 'hard', 'unlearn', 'machin', 'learn', 'system', ',', '.']

 TOTAL SNOWBALL STEM WORDS ==> 9



 ---- LEMMATIZATION ----

['Old', 'habit', 'hard', 'unlearn', 'machine', 'learning', 'system', ',', '.']

 TOTAL LEMMATIZE WORDS ==> 9

************************************************************************************************************************

127 --> Assuming the right use case, tuning will always be faster. 


 ---- TOKENS ----

 ['Assuming', 'the', 'right', 'use', 'case', ',', 'tuning', 'will', 'always', 'be', 'faster', '.'] 

 TOTAL TOKENS ==> 12

 ---- POST ----

 [('Assuming', 'VBG'), ('the', 'DT'), ('right', 'NN'), ('use', 'NN'), ('case', 'NN'), (',', ','), ('tuning', 'VBG'), ('will', 'MD'), ('always', 'RB'), ('be', 'VB'), ('faster', 'RBR'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Assuming', 'right', 'use', 'case', ',', 'tuning', 'always', 'faster', '.']

 TOTAL FILTERED TOKENS ==>  9

 ---- POST FOR FILTERED TOKENS ----

 [('Assuming', 'VBG'), ('right', 'RB'), ('use', 'NN'), ('case', 'NN'), (',', ','), ('tuning', 'VBG'), ('always', 'RB'), ('faster', 'RBR'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Assuming right', 'right use', 'use case', 'case ,', ', tuning', 'tuning always', 'always faster', 'faster .'] 

 TOTAL BIGRAMS --> 8 



 ---- TRI-GRAMS ---- 

 ['Assuming right use', 'right use case', 'use case ,', 'case , tuning', ', tuning always', 'tuning always faster', 'always faster .'] 

 TOTAL TRIGRAMS --> 7 



 ---- NOUN PHRASES ---- 

 ['use', 'case'] 

 TOTAL NOUN PHRASES --> 2 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['assum', 'right', 'use', 'case', ',', 'tune', 'alway', 'faster', '.']

 TOTAL PORTER STEM WORDS ==> 9



 ---- SNOWBALL STEMMING ----

['assum', 'right', 'use', 'case', ',', 'tune', 'alway', 'faster', '.']

 TOTAL SNOWBALL STEM WORDS ==> 9



 ---- LEMMATIZATION ----

['Assuming', 'right', 'use', 'case', ',', 'tuning', 'always', 'faster', '.']

 TOTAL LEMMATIZE WORDS ==> 9

************************************************************************************************************************

128 --> But there are many cases when the use of a particular word    is so multifaceted or ambiguous that the number of tuning     rules we’d have to put into place is prohibitive. 


 ---- TOKENS ----

 ['But', 'there', 'are', 'many', 'cases', 'when', 'the', 'use', 'of', 'a', 'particular', 'word', 'is', 'so', 'multifaceted', 'or', 'ambiguous', 'that', 'the', 'number', 'of', 'tuning', 'rules', 'we', '’', 'd', 'have', 'to', 'put', 'into', 'place', 'is', 'prohibitive', '.'] 

 TOTAL TOKENS ==> 34

 ---- POST ----

 [('But', 'CC'), ('there', 'EX'), ('are', 'VBP'), ('many', 'JJ'), ('cases', 'NNS'), ('when', 'WRB'), ('the', 'DT'), ('use', 'NN'), ('of', 'IN'), ('a', 'DT'), ('particular', 'JJ'), ('word', 'NN'), ('is', 'VBZ'), ('so', 'RB'), ('multifaceted', 'JJ'), ('or', 'CC'), ('ambiguous', 'JJ'), ('that', 'IN'), ('the', 'DT'), ('number', 'NN'), ('of', 'IN'), ('tuning', 'VBG'), ('rules', 'NNS'), ('we', 'PRP'), ('’', 'VBP'), ('d', 'RB'), ('have', 'VBP'), ('to', 'TO'), ('put', 'VB'), ('into', 'IN'), ('place', 'NN'), ('is', 'VBZ'), ('prohibitive', 'JJ'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['many', 'cases', 'use', 'particular', 'word', 'multifaceted', 'ambiguous', 'number', 'tuning', 'rules', '’', 'put', 'place', 'prohibitive', '.']

 TOTAL FILTERED TOKENS ==>  15

 ---- POST FOR FILTERED TOKENS ----

 [('many', 'JJ'), ('cases', 'NNS'), ('use', 'VBP'), ('particular', 'JJ'), ('word', 'NN'), ('multifaceted', 'VBD'), ('ambiguous', 'JJ'), ('number', 'NN'), ('tuning', 'VBG'), ('rules', 'NNS'), ('’', 'NNP'), ('put', 'VBD'), ('place', 'NN'), ('prohibitive', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['many cases', 'cases use', 'use particular', 'particular word', 'word multifaceted', 'multifaceted ambiguous', 'ambiguous number', 'number tuning', 'tuning rules', 'rules ’', '’ put', 'put place', 'place prohibitive', 'prohibitive .'] 

 TOTAL BIGRAMS --> 14 



 ---- TRI-GRAMS ---- 

 ['many cases use', 'cases use particular', 'use particular word', 'particular word multifaceted', 'word multifaceted ambiguous', 'multifaceted ambiguous number', 'ambiguous number tuning', 'number tuning rules', 'tuning rules ’', 'rules ’ put', '’ put place', 'put place prohibitive', 'place prohibitive .'] 

 TOTAL TRIGRAMS --> 13 



 ---- NOUN PHRASES ---- 

 ['particular word', 'ambiguous number', 'place', 'prohibitive'] 

 TOTAL NOUN PHRASES --> 4 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['mani', 'case', 'use', 'particular', 'word', 'multifacet', 'ambigu', 'number', 'tune', 'rule', '’', 'put', 'place', 'prohibit', '.']

 TOTAL PORTER STEM WORDS ==> 15



 ---- SNOWBALL STEMMING ----

['mani', 'case', 'use', 'particular', 'word', 'multifacet', 'ambigu', 'number', 'tune', 'rule', '’', 'put', 'place', 'prohibit', '.']

 TOTAL SNOWBALL STEM WORDS ==> 15



 ---- LEMMATIZATION ----

['many', 'case', 'use', 'particular', 'word', 'multifaceted', 'ambiguous', 'number', 'tuning', 'rule', '’', 'put', 'place', 'prohibitive', '.']

 TOTAL LEMMATIZE WORDS ==> 15

************************************************************************************************************************

129 --> This is where       machine learning shines. 


 ---- TOKENS ----

 ['This', 'is', 'where', 'machine', 'learning', 'shines', '.'] 

 TOTAL TOKENS ==> 7

 ---- POST ----

 [('This', 'DT'), ('is', 'VBZ'), ('where', 'WRB'), ('machine', 'NN'), ('learning', 'NN'), ('shines', 'NNS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['machine', 'learning', 'shines', '.']

 TOTAL FILTERED TOKENS ==>  4

 ---- POST FOR FILTERED TOKENS ----

 [('machine', 'NN'), ('learning', 'VBG'), ('shines', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['machine learning', 'learning shines', 'shines .'] 

 TOTAL BIGRAMS --> 3 



 ---- TRI-GRAMS ---- 

 ['machine learning shines', 'learning shines .'] 

 TOTAL TRIGRAMS --> 2 



 ---- NOUN PHRASES ---- 

 ['machine'] 

 TOTAL NOUN PHRASES --> 1 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['machin', 'learn', 'shine', '.']

 TOTAL PORTER STEM WORDS ==> 4



 ---- SNOWBALL STEMMING ----

['machin', 'learn', 'shine', '.']

 TOTAL SNOWBALL STEM WORDS ==> 4



 ---- LEMMATIZATION ----

['machine', 'learning', 'shine', '.']

 TOTAL LEMMATIZE WORDS ==> 4

************************************************************************************************************************

130 --> Give the model enough examples,         and it figures out the rules for itself. 


 ---- TOKENS ----

 ['Give', 'the', 'model', 'enough', 'examples', ',', 'and', 'it', 'figures', 'out', 'the', 'rules', 'for', 'itself', '.'] 

 TOTAL TOKENS ==> 15

 ---- POST ----

 [('Give', 'VB'), ('the', 'DT'), ('model', 'NN'), ('enough', 'JJ'), ('examples', 'NNS'), (',', ','), ('and', 'CC'), ('it', 'PRP'), ('figures', 'VBZ'), ('out', 'RP'), ('the', 'DT'), ('rules', 'NNS'), ('for', 'IN'), ('itself', 'PRP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Give', 'model', 'enough', 'examples', ',', 'figures', 'rules', '.']

 TOTAL FILTERED TOKENS ==>  8

 ---- POST FOR FILTERED TOKENS ----

 [('Give', 'VB'), ('model', 'NN'), ('enough', 'JJ'), ('examples', 'NNS'), (',', ','), ('figures', 'NNS'), ('rules', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Give model', 'model enough', 'enough examples', 'examples ,', ', figures', 'figures rules', 'rules .'] 

 TOTAL BIGRAMS --> 7 



 ---- TRI-GRAMS ---- 

 ['Give model enough', 'model enough examples', 'enough examples ,', 'examples , figures', ', figures rules', 'figures rules .'] 

 TOTAL TRIGRAMS --> 6 



 ---- NOUN PHRASES ---- 

 ['model'] 

 TOTAL NOUN PHRASES --> 1 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['give', 'model', 'enough', 'exampl', ',', 'figur', 'rule', '.']

 TOTAL PORTER STEM WORDS ==> 8



 ---- SNOWBALL STEMMING ----

['give', 'model', 'enough', 'exampl', ',', 'figur', 'rule', '.']

 TOTAL SNOWBALL STEM WORDS ==> 8



 ---- LEMMATIZATION ----

['Give', 'model', 'enough', 'example', ',', 'figure', 'rule', '.']

 TOTAL LEMMATIZE WORDS ==> 8

************************************************************************************************************************

131 --> https://www.lexalytics.com/ https://www.lexalytics.com/  W H I T E  P A P E R 13|       |   Lexalytics, Inc., 48 North Pleasant St. Unit 301, Amherst MA 01002 USA   |   1-800-377-8036 |   www.lexalytics.com There are more potential side-effects of training that you must be aware of. 


 ---- TOKENS ----

 ['https', ':', '//www.lexalytics.com/', 'https', ':', '//www.lexalytics.com/', 'W', 'H', 'I', 'T', 'E', 'P', 'A', 'P', 'E', 'R', '13|', '|', 'Lexalytics', ',', 'Inc.', ',', '48', 'North', 'Pleasant', 'St.', 'Unit', '301', ',', 'Amherst', 'MA', '01002', 'USA', '|', '1-800-377-8036', '|', 'www.lexalytics.com', 'There', 'are', 'more', 'potential', 'side-effects', 'of', 'training', 'that', 'you', 'must', 'be', 'aware', 'of', '.'] 

 TOTAL TOKENS ==> 51

 ---- POST ----

 [('https', 'NN'), (':', ':'), ('//www.lexalytics.com/', 'JJ'), ('https', 'NN'), (':', ':'), ('//www.lexalytics.com/', 'JJ'), ('W', 'NNP'), ('H', 'NNP'), ('I', 'PRP'), ('T', 'NNP'), ('E', 'NNP'), ('P', 'NNP'), ('A', 'NNP'), ('P', 'NNP'), ('E', 'NNP'), ('R', 'NNP'), ('13|', 'CD'), ('|', 'NNP'), ('Lexalytics', 'NNP'), (',', ','), ('Inc.', 'NNP'), (',', ','), ('48', 'CD'), ('North', 'NNP'), ('Pleasant', 'NNP'), ('St.', 'NNP'), ('Unit', 'NNP'), ('301', 'CD'), (',', ','), ('Amherst', 'NNP'), ('MA', 'NNP'), ('01002', 'CD'), ('USA', 'NNP'), ('|', 'NNP'), ('1-800-377-8036', 'JJ'), ('|', 'NNP'), ('www.lexalytics.com', 'NN'), ('There', 'EX'), ('are', 'VBP'), ('more', 'JJR'), ('potential', 'JJ'), ('side-effects', 'NNS'), ('of', 'IN'), ('training', 'NN'), ('that', 'IN'), ('you', 'PRP'), ('must', 'MD'), ('be', 'VB'), ('aware', 'JJ'), ('of', 'IN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['https', ':', '//www.lexalytics.com/', 'https', ':', '//www.lexalytics.com/', 'W', 'H', 'E', 'P', 'P', 'E', 'R', '13|', '|', 'Lexalytics', ',', 'Inc.', ',', '48', 'North', 'Pleasant', 'St.', 'Unit', '301', ',', 'Amherst', '01002', 'USA', '|', '1-800-377-8036', '|', 'www.lexalytics.com', 'potential', 'side-effects', 'training', 'must', 'aware', '.']

 TOTAL FILTERED TOKENS ==>  39

 ---- POST FOR FILTERED TOKENS ----

 [('https', 'NN'), (':', ':'), ('//www.lexalytics.com/', 'JJ'), ('https', 'NN'), (':', ':'), ('//www.lexalytics.com/', 'JJ'), ('W', 'NNP'), ('H', 'NNP'), ('E', 'NNP'), ('P', 'NNP'), ('P', 'NNP'), ('E', 'NNP'), ('R', 'NNP'), ('13|', 'CD'), ('|', 'NNP'), ('Lexalytics', 'NNP'), (',', ','), ('Inc.', 'NNP'), (',', ','), ('48', 'CD'), ('North', 'NNP'), ('Pleasant', 'NNP'), ('St.', 'NNP'), ('Unit', 'NNP'), ('301', 'CD'), (',', ','), ('Amherst', 'NNP'), ('01002', 'CD'), ('USA', 'NNP'), ('|', 'NNP'), ('1-800-377-8036', 'JJ'), ('|', 'NNP'), ('www.lexalytics.com', 'NN'), ('potential', 'JJ'), ('side-effects', 'NNS'), ('training', 'VBG'), ('must', 'MD'), ('aware', 'VB'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['https :', ': //www.lexalytics.com/', '//www.lexalytics.com/ https', 'https :', ': //www.lexalytics.com/', '//www.lexalytics.com/ W', 'W H', 'H E', 'E P', 'P P', 'P E', 'E R', 'R 13|', '13| |', '| Lexalytics', 'Lexalytics ,', ', Inc.', 'Inc. ,', ', 48', '48 North', 'North Pleasant', 'Pleasant St.', 'St. Unit', 'Unit 301', '301 ,', ', Amherst', 'Amherst 01002', '01002 USA', 'USA |', '| 1-800-377-8036', '1-800-377-8036 |', '| www.lexalytics.com', 'www.lexalytics.com potential', 'potential side-effects', 'side-effects training', 'training must', 'must aware', 'aware .'] 

 TOTAL BIGRAMS --> 38 



 ---- TRI-GRAMS ---- 

 ['https : //www.lexalytics.com/', ': //www.lexalytics.com/ https', '//www.lexalytics.com/ https :', 'https : //www.lexalytics.com/', ': //www.lexalytics.com/ W', '//www.lexalytics.com/ W H', 'W H E', 'H E P', 'E P P', 'P P E', 'P E R', 'E R 13|', 'R 13| |', '13| | Lexalytics', '| Lexalytics ,', 'Lexalytics , Inc.', ', Inc. ,', 'Inc. , 48', ', 48 North', '48 North Pleasant', 'North Pleasant St.', 'Pleasant St. Unit', 'St. Unit 301', 'Unit 301 ,', '301 , Amherst', ', Amherst 01002', 'Amherst 01002 USA', '01002 USA |', 'USA | 1-800-377-8036', '| 1-800-377-8036 |', '1-800-377-8036 | www.lexalytics.com', '| www.lexalytics.com potential', 'www.lexalytics.com potential side-effects', 'potential side-effects training', 'side-effects training must', 'training must aware', 'must aware .'] 

 TOTAL TRIGRAMS --> 37 



 ---- NOUN PHRASES ---- 

 ['https', ' https', 'www.lexalytics.com'] 

 TOTAL NOUN PHRASES --> 3 



 ---- NER ----

 
 ORGANIZATION ---> ['USA']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> ['North']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['http', ':', '//www.lexalytics.com/', 'http', ':', '//www.lexalytics.com/', 'w', 'h', 'e', 'p', 'p', 'e', 'r', '13|', '|', 'lexalyt', ',', 'inc.', ',', '48', 'north', 'pleasant', 'st.', 'unit', '301', ',', 'amherst', '01002', 'usa', '|', '1-800-377-8036', '|', 'www.lexalytics.com', 'potenti', 'side-effect', 'train', 'must', 'awar', '.']

 TOTAL PORTER STEM WORDS ==> 39



 ---- SNOWBALL STEMMING ----

['https', ':', '//www.lexalytics.com/', 'https', ':', '//www.lexalytics.com/', 'w', 'h', 'e', 'p', 'p', 'e', 'r', '13|', '|', 'lexalyt', ',', 'inc.', ',', '48', 'north', 'pleasant', 'st.', 'unit', '301', ',', 'amherst', '01002', 'usa', '|', '1-800-377-8036', '|', 'www.lexalytics.com', 'potenti', 'side-effect', 'train', 'must', 'awar', '.']

 TOTAL SNOWBALL STEM WORDS ==> 39



 ---- LEMMATIZATION ----

['http', ':', '//www.lexalytics.com/', 'http', ':', '//www.lexalytics.com/', 'W', 'H', 'E', 'P', 'P', 'E', 'R', '13|', '|', 'Lexalytics', ',', 'Inc.', ',', '48', 'North', 'Pleasant', 'St.', 'Unit', '301', ',', 'Amherst', '01002', 'USA', '|', '1-800-377-8036', '|', 'www.lexalytics.com', 'potential', 'side-effects', 'training', 'must', 'aware', '.']

 TOTAL LEMMATIZE WORDS ==> 39

************************************************************************************************************************

132 --> Say we’re scoring a bunch of documents to try to effect change on a  particular phrase. 


 ---- TOKENS ----

 ['Say', 'we', '’', 're', 'scoring', 'a', 'bunch', 'of', 'documents', 'to', 'try', 'to', 'effect', 'change', 'on', 'a', 'particular', 'phrase', '.'] 

 TOTAL TOKENS ==> 19

 ---- POST ----

 [('Say', 'NNP'), ('we', 'PRP'), ('’', 'VBP'), ('re', 'JJ'), ('scoring', 'VBG'), ('a', 'DT'), ('bunch', 'NN'), ('of', 'IN'), ('documents', 'NNS'), ('to', 'TO'), ('try', 'VB'), ('to', 'TO'), ('effect', 'NN'), ('change', 'NN'), ('on', 'IN'), ('a', 'DT'), ('particular', 'JJ'), ('phrase', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Say', '’', 'scoring', 'bunch', 'documents', 'try', 'effect', 'change', 'particular', 'phrase', '.']

 TOTAL FILTERED TOKENS ==>  11

 ---- POST FOR FILTERED TOKENS ----

 [('Say', 'NNP'), ('’', 'NNP'), ('scoring', 'VBG'), ('bunch', 'NN'), ('documents', 'NNS'), ('try', 'VBP'), ('effect', 'NN'), ('change', 'NN'), ('particular', 'JJ'), ('phrase', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Say ’', '’ scoring', 'scoring bunch', 'bunch documents', 'documents try', 'try effect', 'effect change', 'change particular', 'particular phrase', 'phrase .'] 

 TOTAL BIGRAMS --> 10 



 ---- TRI-GRAMS ---- 

 ['Say ’ scoring', '’ scoring bunch', 'scoring bunch documents', 'bunch documents try', 'documents try effect', 'try effect change', 'effect change particular', 'change particular phrase', 'particular phrase .'] 

 TOTAL TRIGRAMS --> 9 



 ---- NOUN PHRASES ---- 

 ['bunch', 'effect', 'change', 'particular phrase'] 

 TOTAL NOUN PHRASES --> 4 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> ['Say']
 TOTAL PERSON ENTITY --> 1 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['say', '’', 'score', 'bunch', 'document', 'tri', 'effect', 'chang', 'particular', 'phrase', '.']

 TOTAL PORTER STEM WORDS ==> 11



 ---- SNOWBALL STEMMING ----

['say', '’', 'score', 'bunch', 'document', 'tri', 'effect', 'chang', 'particular', 'phrase', '.']

 TOTAL SNOWBALL STEM WORDS ==> 11



 ---- LEMMATIZATION ----

['Say', '’', 'scoring', 'bunch', 'document', 'try', 'effect', 'change', 'particular', 'phrase', '.']

 TOTAL LEMMATIZE WORDS ==> 11

************************************************************************************************************************

133 --> Each of those documents contains more than that one  phrase, and the other phrases in each document will also be affected by our  scoring and re-tuning. 


 ---- TOKENS ----

 ['Each', 'of', 'those', 'documents', 'contains', 'more', 'than', 'that', 'one', 'phrase', ',', 'and', 'the', 'other', 'phrases', 'in', 'each', 'document', 'will', 'also', 'be', 'affected', 'by', 'our', 'scoring', 'and', 're-tuning', '.'] 

 TOTAL TOKENS ==> 28

 ---- POST ----

 [('Each', 'DT'), ('of', 'IN'), ('those', 'DT'), ('documents', 'NNS'), ('contains', 'VBZ'), ('more', 'JJR'), ('than', 'IN'), ('that', 'DT'), ('one', 'CD'), ('phrase', 'NN'), (',', ','), ('and', 'CC'), ('the', 'DT'), ('other', 'JJ'), ('phrases', 'NNS'), ('in', 'IN'), ('each', 'DT'), ('document', 'NN'), ('will', 'MD'), ('also', 'RB'), ('be', 'VB'), ('affected', 'VBN'), ('by', 'IN'), ('our', 'PRP$'), ('scoring', 'NN'), ('and', 'CC'), ('re-tuning', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['documents', 'contains', 'one', 'phrase', ',', 'phrases', 'document', 'also', 'affected', 'scoring', 're-tuning', '.']

 TOTAL FILTERED TOKENS ==>  12

 ---- POST FOR FILTERED TOKENS ----

 [('documents', 'NNS'), ('contains', 'VBZ'), ('one', 'CD'), ('phrase', 'NN'), (',', ','), ('phrases', 'VBZ'), ('document', 'NN'), ('also', 'RB'), ('affected', 'VBD'), ('scoring', 'VBG'), ('re-tuning', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['documents contains', 'contains one', 'one phrase', 'phrase ,', ', phrases', 'phrases document', 'document also', 'also affected', 'affected scoring', 'scoring re-tuning', 're-tuning .'] 

 TOTAL BIGRAMS --> 11 



 ---- TRI-GRAMS ---- 

 ['documents contains one', 'contains one phrase', 'one phrase ,', 'phrase , phrases', ', phrases document', 'phrases document also', 'document also affected', 'also affected scoring', 'affected scoring re-tuning', 'scoring re-tuning .'] 

 TOTAL TRIGRAMS --> 10 



 ---- NOUN PHRASES ---- 

 ['phrase', 'document', 're-tuning'] 

 TOTAL NOUN PHRASES --> 3 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['document', 'contain', 'one', 'phrase', ',', 'phrase', 'document', 'also', 'affect', 'score', 're-tun', '.']

 TOTAL PORTER STEM WORDS ==> 12



 ---- SNOWBALL STEMMING ----

['document', 'contain', 'one', 'phrase', ',', 'phrase', 'document', 'also', 'affect', 'score', 're-tun', '.']

 TOTAL SNOWBALL STEM WORDS ==> 12



 ---- LEMMATIZATION ----

['document', 'contains', 'one', 'phrase', ',', 'phrase', 'document', 'also', 'affected', 'scoring', 're-tuning', '.']

 TOTAL LEMMATIZE WORDS ==> 12

************************************************************************************************************************

134 --> This is particularly true of common phrases, which  appear often enough that they end up influencing the model. 


 ---- TOKENS ----

 ['This', 'is', 'particularly', 'true', 'of', 'common', 'phrases', ',', 'which', 'appear', 'often', 'enough', 'that', 'they', 'end', 'up', 'influencing', 'the', 'model', '.'] 

 TOTAL TOKENS ==> 20

 ---- POST ----

 [('This', 'DT'), ('is', 'VBZ'), ('particularly', 'RB'), ('true', 'JJ'), ('of', 'IN'), ('common', 'JJ'), ('phrases', 'NNS'), (',', ','), ('which', 'WDT'), ('appear', 'VBP'), ('often', 'RB'), ('enough', 'RB'), ('that', 'IN'), ('they', 'PRP'), ('end', 'VBP'), ('up', 'RP'), ('influencing', 'VBG'), ('the', 'DT'), ('model', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['particularly', 'true', 'common', 'phrases', ',', 'appear', 'often', 'enough', 'end', 'influencing', 'model', '.']

 TOTAL FILTERED TOKENS ==>  12

 ---- POST FOR FILTERED TOKENS ----

 [('particularly', 'RB'), ('true', 'JJ'), ('common', 'JJ'), ('phrases', 'NNS'), (',', ','), ('appear', 'VBP'), ('often', 'RB'), ('enough', 'JJ'), ('end', 'NN'), ('influencing', 'VBG'), ('model', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['particularly true', 'true common', 'common phrases', 'phrases ,', ', appear', 'appear often', 'often enough', 'enough end', 'end influencing', 'influencing model', 'model .'] 

 TOTAL BIGRAMS --> 11 



 ---- TRI-GRAMS ---- 

 ['particularly true common', 'true common phrases', 'common phrases ,', 'phrases , appear', ', appear often', 'appear often enough', 'often enough end', 'enough end influencing', 'end influencing model', 'influencing model .'] 

 TOTAL TRIGRAMS --> 10 



 ---- NOUN PHRASES ---- 

 ['enough end', 'model'] 

 TOTAL NOUN PHRASES --> 2 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['particularli', 'true', 'common', 'phrase', ',', 'appear', 'often', 'enough', 'end', 'influenc', 'model', '.']

 TOTAL PORTER STEM WORDS ==> 12



 ---- SNOWBALL STEMMING ----

['particular', 'true', 'common', 'phrase', ',', 'appear', 'often', 'enough', 'end', 'influenc', 'model', '.']

 TOTAL SNOWBALL STEM WORDS ==> 12



 ---- LEMMATIZATION ----

['particularly', 'true', 'common', 'phrase', ',', 'appear', 'often', 'enough', 'end', 'influencing', 'model', '.']

 TOTAL LEMMATIZE WORDS ==> 12

************************************************************************************************************************

135 --> Imagine that you’re scoring news stories from 2008. 


 ---- TOKENS ----

 ['Imagine', 'that', 'you', '’', 're', 'scoring', 'news', 'stories', 'from', '2008', '.'] 

 TOTAL TOKENS ==> 11

 ---- POST ----

 [('Imagine', 'NN'), ('that', 'IN'), ('you', 'PRP'), ('’', 'VBP'), ('re', 'JJ'), ('scoring', 'VBG'), ('news', 'NN'), ('stories', 'NNS'), ('from', 'IN'), ('2008', 'CD'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Imagine', '’', 'scoring', 'news', 'stories', '2008', '.']

 TOTAL FILTERED TOKENS ==>  7

 ---- POST FOR FILTERED TOKENS ----

 [('Imagine', 'NNP'), ('’', 'NNP'), ('scoring', 'VBG'), ('news', 'NN'), ('stories', 'NNS'), ('2008', 'CD'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Imagine ’', '’ scoring', 'scoring news', 'news stories', 'stories 2008', '2008 .'] 

 TOTAL BIGRAMS --> 6 



 ---- TRI-GRAMS ---- 

 ['Imagine ’ scoring', '’ scoring news', 'scoring news stories', 'news stories 2008', 'stories 2008 .'] 

 TOTAL TRIGRAMS --> 5 



 ---- NOUN PHRASES ---- 

 ['news'] 

 TOTAL NOUN PHRASES --> 1 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> ['Imagine']
 TOTAL PERSON ENTITY --> 1 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['imagin', '’', 'score', 'news', 'stori', '2008', '.']

 TOTAL PORTER STEM WORDS ==> 7



 ---- SNOWBALL STEMMING ----

['imagin', '’', 'score', 'news', 'stori', '2008', '.']

 TOTAL SNOWBALL STEM WORDS ==> 7



 ---- LEMMATIZATION ----

['Imagine', '’', 'scoring', 'news', 'story', '2008', '.']

 TOTAL LEMMATIZE WORDS ==> 7

************************************************************************************************************************

136 --> 2008 was truly awful for  business and economics as a whole. 


 ---- TOKENS ----

 ['2008', 'was', 'truly', 'awful', 'for', 'business', 'and', 'economics', 'as', 'a', 'whole', '.'] 

 TOTAL TOKENS ==> 12

 ---- POST ----

 [('2008', 'CD'), ('was', 'VBD'), ('truly', 'RB'), ('awful', 'JJ'), ('for', 'IN'), ('business', 'NN'), ('and', 'CC'), ('economics', 'NNS'), ('as', 'IN'), ('a', 'DT'), ('whole', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['2008', 'truly', 'awful', 'business', 'economics', 'whole', '.']

 TOTAL FILTERED TOKENS ==>  7

 ---- POST FOR FILTERED TOKENS ----

 [('2008', 'CD'), ('truly', 'RB'), ('awful', 'JJ'), ('business', 'NN'), ('economics', 'NNS'), ('whole', 'JJ'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['2008 truly', 'truly awful', 'awful business', 'business economics', 'economics whole', 'whole .'] 

 TOTAL BIGRAMS --> 6 



 ---- TRI-GRAMS ---- 

 ['2008 truly awful', 'truly awful business', 'awful business economics', 'business economics whole', 'economics whole .'] 

 TOTAL TRIGRAMS --> 5 



 ---- NOUN PHRASES ---- 

 ['awful business'] 

 TOTAL NOUN PHRASES --> 1 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['2008', 'truli', 'aw', 'busi', 'econom', 'whole', '.']

 TOTAL PORTER STEM WORDS ==> 7



 ---- SNOWBALL STEMMING ----

['2008', 'truli', 'aw', 'busi', 'econom', 'whole', '.']

 TOTAL SNOWBALL STEM WORDS ==> 7



 ---- LEMMATIZATION ----

['2008', 'truly', 'awful', 'business', 'economics', 'whole', '.']

 TOTAL LEMMATIZE WORDS ==> 7

************************************************************************************************************************

137 --> If you are focused on scoring financial  results from businesses, you’ll be marking a lot of content as negative. 


 ---- TOKENS ----

 ['If', 'you', 'are', 'focused', 'on', 'scoring', 'financial', 'results', 'from', 'businesses', ',', 'you', '’', 'll', 'be', 'marking', 'a', 'lot', 'of', 'content', 'as', 'negative', '.'] 

 TOTAL TOKENS ==> 23

 ---- POST ----

 [('If', 'IN'), ('you', 'PRP'), ('are', 'VBP'), ('focused', 'VBN'), ('on', 'IN'), ('scoring', 'VBG'), ('financial', 'JJ'), ('results', 'NNS'), ('from', 'IN'), ('businesses', 'NNS'), (',', ','), ('you', 'PRP'), ('’', 'VBP'), ('ll', 'JJ'), ('be', 'VB'), ('marking', 'VBG'), ('a', 'DT'), ('lot', 'NN'), ('of', 'IN'), ('content', 'NN'), ('as', 'IN'), ('negative', 'JJ'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['focused', 'scoring', 'financial', 'results', 'businesses', ',', '’', 'marking', 'lot', 'content', 'negative', '.']

 TOTAL FILTERED TOKENS ==>  12

 ---- POST FOR FILTERED TOKENS ----

 [('focused', 'JJ'), ('scoring', 'VBG'), ('financial', 'JJ'), ('results', 'NNS'), ('businesses', 'NNS'), (',', ','), ('’', 'VBP'), ('marking', 'VBG'), ('lot', 'NN'), ('content', 'JJ'), ('negative', 'JJ'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['focused scoring', 'scoring financial', 'financial results', 'results businesses', 'businesses ,', ', ’', '’ marking', 'marking lot', 'lot content', 'content negative', 'negative .'] 

 TOTAL BIGRAMS --> 11 



 ---- TRI-GRAMS ---- 

 ['focused scoring financial', 'scoring financial results', 'financial results businesses', 'results businesses ,', 'businesses , ’', ', ’ marking', '’ marking lot', 'marking lot content', 'lot content negative', 'content negative .'] 

 TOTAL TRIGRAMS --> 10 



 ---- NOUN PHRASES ---- 

 ['lot'] 

 TOTAL NOUN PHRASES --> 1 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['focus', 'score', 'financi', 'result', 'busi', ',', '’', 'mark', 'lot', 'content', 'neg', '.']

 TOTAL PORTER STEM WORDS ==> 12



 ---- SNOWBALL STEMMING ----

['focus', 'score', 'financi', 'result', 'busi', ',', '’', 'mark', 'lot', 'content', 'negat', '.']

 TOTAL SNOWBALL STEM WORDS ==> 12



 ---- LEMMATIZATION ----

['focused', 'scoring', 'financial', 'result', 'business', ',', '’', 'marking', 'lot', 'content', 'negative', '.']

 TOTAL LEMMATIZE WORDS ==> 12

************************************************************************************************************************

138 --> Then, machine learning algorithms will weigh the phrases in the content in  proportion to their occurrence. 


 ---- TOKENS ----

 ['Then', ',', 'machine', 'learning', 'algorithms', 'will', 'weigh', 'the', 'phrases', 'in', 'the', 'content', 'in', 'proportion', 'to', 'their', 'occurrence', '.'] 

 TOTAL TOKENS ==> 18

 ---- POST ----

 [('Then', 'RB'), (',', ','), ('machine', 'NN'), ('learning', 'VBG'), ('algorithms', 'NNS'), ('will', 'MD'), ('weigh', 'VB'), ('the', 'DT'), ('phrases', 'NNS'), ('in', 'IN'), ('the', 'DT'), ('content', 'NN'), ('in', 'IN'), ('proportion', 'NN'), ('to', 'TO'), ('their', 'PRP$'), ('occurrence', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 [',', 'machine', 'learning', 'algorithms', 'weigh', 'phrases', 'content', 'proportion', 'occurrence', '.']

 TOTAL FILTERED TOKENS ==>  10

 ---- POST FOR FILTERED TOKENS ----

 [(',', ','), ('machine', 'NN'), ('learning', 'VBG'), ('algorithms', 'JJ'), ('weigh', 'JJ'), ('phrases', 'NNS'), ('content', 'JJ'), ('proportion', 'NN'), ('occurrence', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 [', machine', 'machine learning', 'learning algorithms', 'algorithms weigh', 'weigh phrases', 'phrases content', 'content proportion', 'proportion occurrence', 'occurrence .'] 

 TOTAL BIGRAMS --> 9 



 ---- TRI-GRAMS ---- 

 [', machine learning', 'machine learning algorithms', 'learning algorithms weigh', 'algorithms weigh phrases', 'weigh phrases content', 'phrases content proportion', 'content proportion occurrence', 'proportion occurrence .'] 

 TOTAL TRIGRAMS --> 8 



 ---- NOUN PHRASES ---- 

 ['machine', 'content proportion', 'occurrence'] 

 TOTAL NOUN PHRASES --> 3 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

[',', 'machin', 'learn', 'algorithm', 'weigh', 'phrase', 'content', 'proport', 'occurr', '.']

 TOTAL PORTER STEM WORDS ==> 10



 ---- SNOWBALL STEMMING ----

[',', 'machin', 'learn', 'algorithm', 'weigh', 'phrase', 'content', 'proport', 'occurr', '.']

 TOTAL SNOWBALL STEM WORDS ==> 10



 ---- LEMMATIZATION ----

[',', 'machine', 'learning', 'algorithm', 'weigh', 'phrase', 'content', 'proportion', 'occurrence', '.']

 TOTAL LEMMATIZE WORDS ==> 10

************************************************************************************************************************

139 --> Unfortunately, that leaves some collateral damage: “first quarter,” “second  quarter,” “third quarter,” and “fourth quarter.” These are neutral terms, but  they occurred in frequent conjunction with negative financial news. 


 ---- TOKENS ----

 ['Unfortunately', ',', 'that', 'leaves', 'some', 'collateral', 'damage', ':', '“', 'first', 'quarter', ',', '”', '“', 'second', 'quarter', ',', '”', '“', 'third', 'quarter', ',', '”', 'and', '“', 'fourth', 'quarter.', '”', 'These', 'are', 'neutral', 'terms', ',', 'but', 'they', 'occurred', 'in', 'frequent', 'conjunction', 'with', 'negative', 'financial', 'news', '.'] 

 TOTAL TOKENS ==> 44

 ---- POST ----

 [('Unfortunately', 'RB'), (',', ','), ('that', 'DT'), ('leaves', 'VBZ'), ('some', 'DT'), ('collateral', 'JJ'), ('damage', 'NN'), (':', ':'), ('“', 'NN'), ('first', 'JJ'), ('quarter', 'NN'), (',', ','), ('”', 'NNP'), ('“', 'NNP'), ('second', 'JJ'), ('quarter', 'NN'), (',', ','), ('”', 'NNP'), ('“', 'NNP'), ('third', 'JJ'), ('quarter', 'NN'), (',', ','), ('”', 'NNP'), ('and', 'CC'), ('“', 'NNP'), ('fourth', 'JJ'), ('quarter.', 'NN'), ('”', 'IN'), ('These', 'DT'), ('are', 'VBP'), ('neutral', 'JJ'), ('terms', 'NNS'), (',', ','), ('but', 'CC'), ('they', 'PRP'), ('occurred', 'VBD'), ('in', 'IN'), ('frequent', 'JJ'), ('conjunction', 'NN'), ('with', 'IN'), ('negative', 'JJ'), ('financial', 'JJ'), ('news', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Unfortunately', ',', 'leaves', 'collateral', 'damage', ':', '“', 'first', 'quarter', ',', '”', '“', 'second', 'quarter', ',', '”', '“', 'third', 'quarter', ',', '”', '“', 'fourth', 'quarter.', '”', 'neutral', 'terms', ',', 'occurred', 'frequent', 'conjunction', 'negative', 'financial', 'news', '.']

 TOTAL FILTERED TOKENS ==>  35

 ---- POST FOR FILTERED TOKENS ----

 [('Unfortunately', 'RB'), (',', ','), ('leaves', 'VBZ'), ('collateral', 'JJ'), ('damage', 'NN'), (':', ':'), ('“', 'NN'), ('first', 'JJ'), ('quarter', 'NN'), (',', ','), ('”', 'NNP'), ('“', 'NNP'), ('second', 'JJ'), ('quarter', 'NN'), (',', ','), ('”', 'NNP'), ('“', 'NNP'), ('third', 'JJ'), ('quarter', 'NN'), (',', ','), ('”', 'NNP'), ('“', 'NNP'), ('fourth', 'JJ'), ('quarter.', 'NN'), ('”', 'NNP'), ('neutral', 'JJ'), ('terms', 'NNS'), (',', ','), ('occurred', 'VBD'), ('frequent', 'JJ'), ('conjunction', 'NN'), ('negative', 'JJ'), ('financial', 'JJ'), ('news', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Unfortunately ,', ', leaves', 'leaves collateral', 'collateral damage', 'damage :', ': “', '“ first', 'first quarter', 'quarter ,', ', ”', '” “', '“ second', 'second quarter', 'quarter ,', ', ”', '” “', '“ third', 'third quarter', 'quarter ,', ', ”', '” “', '“ fourth', 'fourth quarter.', 'quarter. ”', '” neutral', 'neutral terms', 'terms ,', ', occurred', 'occurred frequent', 'frequent conjunction', 'conjunction negative', 'negative financial', 'financial news', 'news .'] 

 TOTAL BIGRAMS --> 34 



 ---- TRI-GRAMS ---- 

 ['Unfortunately , leaves', ', leaves collateral', 'leaves collateral damage', 'collateral damage :', 'damage : “', ': “ first', '“ first quarter', 'first quarter ,', 'quarter , ”', ', ” “', '” “ second', '“ second quarter', 'second quarter ,', 'quarter , ”', ', ” “', '” “ third', '“ third quarter', 'third quarter ,', 'quarter , ”', ', ” “', '” “ fourth', '“ fourth quarter.', 'fourth quarter. ”', 'quarter. ” neutral', '” neutral terms', 'neutral terms ,', 'terms , occurred', ', occurred frequent', 'occurred frequent conjunction', 'frequent conjunction negative', 'conjunction negative financial', 'negative financial news', 'financial news .'] 

 TOTAL TRIGRAMS --> 33 



 ---- NOUN PHRASES ---- 

 ['collateral damage', '“', 'first quarter', 'second quarter', 'third quarter', 'fourth quarter.', 'frequent conjunction', 'negative financial news'] 

 TOTAL NOUN PHRASES --> 8 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['unfortun', ',', 'leav', 'collater', 'damag', ':', '“', 'first', 'quarter', ',', '”', '“', 'second', 'quarter', ',', '”', '“', 'third', 'quarter', ',', '”', '“', 'fourth', 'quarter.', '”', 'neutral', 'term', ',', 'occur', 'frequent', 'conjunct', 'neg', 'financi', 'news', '.']

 TOTAL PORTER STEM WORDS ==> 35



 ---- SNOWBALL STEMMING ----

['unfortun', ',', 'leav', 'collater', 'damag', ':', '“', 'first', 'quarter', ',', '”', '“', 'second', 'quarter', ',', '”', '“', 'third', 'quarter', ',', '”', '“', 'fourth', 'quarter.', '”', 'neutral', 'term', ',', 'occur', 'frequent', 'conjunct', 'negat', 'financi', 'news', '.']

 TOTAL SNOWBALL STEM WORDS ==> 35



 ---- LEMMATIZATION ----

['Unfortunately', ',', 'leaf', 'collateral', 'damage', ':', '“', 'first', 'quarter', ',', '”', '“', 'second', 'quarter', ',', '”', '“', 'third', 'quarter', ',', '”', '“', 'fourth', 'quarter.', '”', 'neutral', 'term', ',', 'occurred', 'frequent', 'conjunction', 'negative', 'financial', 'news', '.']

 TOTAL LEMMATIZE WORDS ==> 35

************************************************************************************************************************

140 --> So, the  machine learning algorithm will weight those phrases as being negative. 


 ---- TOKENS ----

 ['So', ',', 'the', 'machine', 'learning', 'algorithm', 'will', 'weight', 'those', 'phrases', 'as', 'being', 'negative', '.'] 

 TOTAL TOKENS ==> 14

 ---- POST ----

 [('So', 'RB'), (',', ','), ('the', 'DT'), ('machine', 'NN'), ('learning', 'VBG'), ('algorithm', 'NN'), ('will', 'MD'), ('weight', 'VB'), ('those', 'DT'), ('phrases', 'NNS'), ('as', 'IN'), ('being', 'VBG'), ('negative', 'JJ'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 [',', 'machine', 'learning', 'algorithm', 'weight', 'phrases', 'negative', '.']

 TOTAL FILTERED TOKENS ==>  8

 ---- POST FOR FILTERED TOKENS ----

 [(',', ','), ('machine', 'NN'), ('learning', 'VBG'), ('algorithm', 'JJ'), ('weight', 'NN'), ('phrases', 'NNS'), ('negative', 'JJ'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 [', machine', 'machine learning', 'learning algorithm', 'algorithm weight', 'weight phrases', 'phrases negative', 'negative .'] 

 TOTAL BIGRAMS --> 7 



 ---- TRI-GRAMS ---- 

 [', machine learning', 'machine learning algorithm', 'learning algorithm weight', 'algorithm weight phrases', 'weight phrases negative', 'phrases negative .'] 

 TOTAL TRIGRAMS --> 6 



 ---- NOUN PHRASES ---- 

 ['machine', 'algorithm weight'] 

 TOTAL NOUN PHRASES --> 2 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

[',', 'machin', 'learn', 'algorithm', 'weight', 'phrase', 'neg', '.']

 TOTAL PORTER STEM WORDS ==> 8



 ---- SNOWBALL STEMMING ----

[',', 'machin', 'learn', 'algorithm', 'weight', 'phrase', 'negat', '.']

 TOTAL SNOWBALL STEM WORDS ==> 8



 ---- LEMMATIZATION ----

[',', 'machine', 'learning', 'algorithm', 'weight', 'phrase', 'negative', '.']

 TOTAL LEMMATIZE WORDS ==> 8

************************************************************************************************************************

141 --> That will end up negatively impacting your results for years to come. 


 ---- TOKENS ----

 ['That', 'will', 'end', 'up', 'negatively', 'impacting', 'your', 'results', 'for', 'years', 'to', 'come', '.'] 

 TOTAL TOKENS ==> 13

 ---- POST ----

 [('That', 'DT'), ('will', 'MD'), ('end', 'VB'), ('up', 'RP'), ('negatively', 'RB'), ('impacting', 'VBG'), ('your', 'PRP$'), ('results', 'NNS'), ('for', 'IN'), ('years', 'NNS'), ('to', 'TO'), ('come', 'VB'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['end', 'negatively', 'impacting', 'results', 'years', 'come', '.']

 TOTAL FILTERED TOKENS ==>  7

 ---- POST FOR FILTERED TOKENS ----

 [('end', 'NN'), ('negatively', 'RB'), ('impacting', 'JJ'), ('results', 'NNS'), ('years', 'NNS'), ('come', 'VBP'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['end negatively', 'negatively impacting', 'impacting results', 'results years', 'years come', 'come .'] 

 TOTAL BIGRAMS --> 6 



 ---- TRI-GRAMS ---- 

 ['end negatively impacting', 'negatively impacting results', 'impacting results years', 'results years come', 'years come .'] 

 TOTAL TRIGRAMS --> 5 



 ---- NOUN PHRASES ---- 

 ['end'] 

 TOTAL NOUN PHRASES --> 1 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['end', 'neg', 'impact', 'result', 'year', 'come', '.']

 TOTAL PORTER STEM WORDS ==> 7



 ---- SNOWBALL STEMMING ----

['end', 'negat', 'impact', 'result', 'year', 'come', '.']

 TOTAL SNOWBALL STEM WORDS ==> 7



 ---- LEMMATIZATION ----

['end', 'negatively', 'impacting', 'result', 'year', 'come', '.']

 TOTAL LEMMATIZE WORDS ==> 7

************************************************************************************************************************

142 --> Lexalytics has put a number of checks and balances into our text analytics  system to handle situations like this. 


 ---- TOKENS ----

 ['Lexalytics', 'has', 'put', 'a', 'number', 'of', 'checks', 'and', 'balances', 'into', 'our', 'text', 'analytics', 'system', 'to', 'handle', 'situations', 'like', 'this', '.'] 

 TOTAL TOKENS ==> 20

 ---- POST ----

 [('Lexalytics', 'NNS'), ('has', 'VBZ'), ('put', 'VBN'), ('a', 'DT'), ('number', 'NN'), ('of', 'IN'), ('checks', 'NNS'), ('and', 'CC'), ('balances', 'NNS'), ('into', 'IN'), ('our', 'PRP$'), ('text', 'NN'), ('analytics', 'NNS'), ('system', 'NN'), ('to', 'TO'), ('handle', 'VB'), ('situations', 'NNS'), ('like', 'IN'), ('this', 'DT'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Lexalytics', 'put', 'number', 'checks', 'balances', 'text', 'analytics', 'system', 'handle', 'situations', 'like', '.']

 TOTAL FILTERED TOKENS ==>  12

 ---- POST FOR FILTERED TOKENS ----

 [('Lexalytics', 'NNS'), ('put', 'VBD'), ('number', 'NN'), ('checks', 'NNS'), ('balances', 'NNS'), ('text', 'VBP'), ('analytics', 'NNS'), ('system', 'NN'), ('handle', 'JJ'), ('situations', 'NNS'), ('like', 'IN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Lexalytics put', 'put number', 'number checks', 'checks balances', 'balances text', 'text analytics', 'analytics system', 'system handle', 'handle situations', 'situations like', 'like .'] 

 TOTAL BIGRAMS --> 11 



 ---- TRI-GRAMS ---- 

 ['Lexalytics put number', 'put number checks', 'number checks balances', 'checks balances text', 'balances text analytics', 'text analytics system', 'analytics system handle', 'system handle situations', 'handle situations like', 'situations like .'] 

 TOTAL TRIGRAMS --> 10 



 ---- NOUN PHRASES ---- 

 ['number', 'system'] 

 TOTAL NOUN PHRASES --> 2 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['lexalyt', 'put', 'number', 'check', 'balanc', 'text', 'analyt', 'system', 'handl', 'situat', 'like', '.']

 TOTAL PORTER STEM WORDS ==> 12



 ---- SNOWBALL STEMMING ----

['lexalyt', 'put', 'number', 'check', 'balanc', 'text', 'analyt', 'system', 'handl', 'situat', 'like', '.']

 TOTAL SNOWBALL STEM WORDS ==> 12



 ---- LEMMATIZATION ----

['Lexalytics', 'put', 'number', 'check', 'balance', 'text', 'analytics', 'system', 'handle', 'situation', 'like', '.']

 TOTAL LEMMATIZE WORDS ==> 12

************************************************************************************************************************

143 --> Sometimes you just need to be able   to reach in and tell the software that “first quarter” is really just neutral,   despite what it might think. 


 ---- TOKENS ----

 ['Sometimes', 'you', 'just', 'need', 'to', 'be', 'able', 'to', 'reach', 'in', 'and', 'tell', 'the', 'software', 'that', '“', 'first', 'quarter', '”', 'is', 'really', 'just', 'neutral', ',', 'despite', 'what', 'it', 'might', 'think', '.'] 

 TOTAL TOKENS ==> 30

 ---- POST ----

 [('Sometimes', 'RB'), ('you', 'PRP'), ('just', 'RB'), ('need', 'VB'), ('to', 'TO'), ('be', 'VB'), ('able', 'JJ'), ('to', 'TO'), ('reach', 'VB'), ('in', 'IN'), ('and', 'CC'), ('tell', 'VB'), ('the', 'DT'), ('software', 'NN'), ('that', 'WDT'), ('“', 'VBZ'), ('first', 'JJ'), ('quarter', 'NN'), ('”', 'NN'), ('is', 'VBZ'), ('really', 'RB'), ('just', 'RB'), ('neutral', 'JJ'), (',', ','), ('despite', 'IN'), ('what', 'WP'), ('it', 'PRP'), ('might', 'MD'), ('think', 'VB'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Sometimes', 'need', 'able', 'reach', 'tell', 'software', '“', 'first', 'quarter', '”', 'really', 'neutral', ',', 'despite', 'might', 'think', '.']

 TOTAL FILTERED TOKENS ==>  17

 ---- POST FOR FILTERED TOKENS ----

 [('Sometimes', 'RB'), ('need', 'MD'), ('able', 'JJ'), ('reach', 'VB'), ('tell', 'NN'), ('software', 'NN'), ('“', 'NNP'), ('first', 'RB'), ('quarter', 'NN'), ('”', 'NNP'), ('really', 'RB'), ('neutral', 'JJ'), (',', ','), ('despite', 'IN'), ('might', 'MD'), ('think', 'VB'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Sometimes need', 'need able', 'able reach', 'reach tell', 'tell software', 'software “', '“ first', 'first quarter', 'quarter ”', '” really', 'really neutral', 'neutral ,', ', despite', 'despite might', 'might think', 'think .'] 

 TOTAL BIGRAMS --> 16 



 ---- TRI-GRAMS ---- 

 ['Sometimes need able', 'need able reach', 'able reach tell', 'reach tell software', 'tell software “', 'software “ first', '“ first quarter', 'first quarter ”', 'quarter ” really', '” really neutral', 'really neutral ,', 'neutral , despite', ', despite might', 'despite might think', 'might think .'] 

 TOTAL TRIGRAMS --> 15 



 ---- NOUN PHRASES ---- 

 ['tell', 'software', 'quarter'] 

 TOTAL NOUN PHRASES --> 3 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['sometim', 'need', 'abl', 'reach', 'tell', 'softwar', '“', 'first', 'quarter', '”', 'realli', 'neutral', ',', 'despit', 'might', 'think', '.']

 TOTAL PORTER STEM WORDS ==> 17



 ---- SNOWBALL STEMMING ----

['sometim', 'need', 'abl', 'reach', 'tell', 'softwar', '“', 'first', 'quarter', '”', 'realli', 'neutral', ',', 'despit', 'might', 'think', '.']

 TOTAL SNOWBALL STEM WORDS ==> 17



 ---- LEMMATIZATION ----

['Sometimes', 'need', 'able', 'reach', 'tell', 'software', '“', 'first', 'quarter', '”', 'really', 'neutral', ',', 'despite', 'might', 'think', '.']

 TOTAL LEMMATIZE WORDS ==> 17

************************************************************************************************************************

144 --> CHART SOURCE: ThomsonOne; Bullion Management Group Inc.,   http://bmg-group.com/2008-financial-crisis/ can have many   unforeseen side-effects. 


 ---- TOKENS ----

 ['CHART', 'SOURCE', ':', 'ThomsonOne', ';', 'Bullion', 'Management', 'Group', 'Inc.', ',', 'http', ':', '//bmg-group.com/2008-financial-crisis/', 'can', 'have', 'many', 'unforeseen', 'side-effects', '.'] 

 TOTAL TOKENS ==> 19

 ---- POST ----

 [('CHART', 'NNP'), ('SOURCE', 'NNP'), (':', ':'), ('ThomsonOne', 'NN'), (';', ':'), ('Bullion', 'NNP'), ('Management', 'NNP'), ('Group', 'NNP'), ('Inc.', 'NNP'), (',', ','), ('http', 'NN'), (':', ':'), ('//bmg-group.com/2008-financial-crisis/', 'JJ'), ('can', 'MD'), ('have', 'VB'), ('many', 'JJ'), ('unforeseen', 'JJ'), ('side-effects', 'NNS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['CHART', 'SOURCE', ':', 'ThomsonOne', ';', 'Bullion', 'Management', 'Group', 'Inc.', ',', 'http', ':', '//bmg-group.com/2008-financial-crisis/', 'many', 'unforeseen', 'side-effects', '.']

 TOTAL FILTERED TOKENS ==>  17

 ---- POST FOR FILTERED TOKENS ----

 [('CHART', 'NNP'), ('SOURCE', 'NNP'), (':', ':'), ('ThomsonOne', 'NN'), (';', ':'), ('Bullion', 'NNP'), ('Management', 'NNP'), ('Group', 'NNP'), ('Inc.', 'NNP'), (',', ','), ('http', 'NN'), (':', ':'), ('//bmg-group.com/2008-financial-crisis/', 'JJ'), ('many', 'JJ'), ('unforeseen', 'JJ'), ('side-effects', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['CHART SOURCE', 'SOURCE :', ': ThomsonOne', 'ThomsonOne ;', '; Bullion', 'Bullion Management', 'Management Group', 'Group Inc.', 'Inc. ,', ', http', 'http :', ': //bmg-group.com/2008-financial-crisis/', '//bmg-group.com/2008-financial-crisis/ many', 'many unforeseen', 'unforeseen side-effects', 'side-effects .'] 

 TOTAL BIGRAMS --> 16 



 ---- TRI-GRAMS ---- 

 ['CHART SOURCE :', 'SOURCE : ThomsonOne', ': ThomsonOne ;', 'ThomsonOne ; Bullion', '; Bullion Management', 'Bullion Management Group', 'Management Group Inc.', 'Group Inc. ,', 'Inc. , http', ', http :', 'http : //bmg-group.com/2008-financial-crisis/', ': //bmg-group.com/2008-financial-crisis/ many', '//bmg-group.com/2008-financial-crisis/ many unforeseen', 'many unforeseen side-effects', 'unforeseen side-effects .'] 

 TOTAL TRIGRAMS --> 15 



 ---- NOUN PHRASES ---- 

 ['ThomsonOne', 'http'] 

 TOTAL NOUN PHRASES --> 2 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> ['Bullion Management Group Inc.']
 TOTAL PERSON ENTITY --> 1 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['chart', 'sourc', ':', 'thomsonon', ';', 'bullion', 'manag', 'group', 'inc.', ',', 'http', ':', '//bmg-group.com/2008-financial-crisis/', 'mani', 'unforeseen', 'side-effect', '.']

 TOTAL PORTER STEM WORDS ==> 17



 ---- SNOWBALL STEMMING ----

['chart', 'sourc', ':', 'thomsonon', ';', 'bullion', 'manag', 'group', 'inc.', ',', 'http', ':', '//bmg-group.com/2008-financial-crisis/', 'mani', 'unforeseen', 'side-effect', '.']

 TOTAL SNOWBALL STEM WORDS ==> 17



 ---- LEMMATIZATION ----

['CHART', 'SOURCE', ':', 'ThomsonOne', ';', 'Bullion', 'Management', 'Group', 'Inc.', ',', 'http', ':', '//bmg-group.com/2008-financial-crisis/', 'many', 'unforeseen', 'side-effects', '.']

 TOTAL LEMMATIZE WORDS ==> 17

************************************************************************************************************************

145 --> Training  a model https://www.lexalytics.com/ https://www.lexalytics.com/  W H I T E  P A P E R 14|       |   Lexalytics, Inc., 48 North Pleasant St. Unit 301, Amherst MA 01002 USA   |   1-800-377-8036 |   www.lexalytics.com S U M M A R Y  /  C O N C L U S I O N  Text analytics is arguably one of the most complex tasks for an AI. 


 ---- TOKENS ----

 ['Training', 'a', 'model', 'https', ':', '//www.lexalytics.com/', 'https', ':', '//www.lexalytics.com/', 'W', 'H', 'I', 'T', 'E', 'P', 'A', 'P', 'E', 'R', '14|', '|', 'Lexalytics', ',', 'Inc.', ',', '48', 'North', 'Pleasant', 'St.', 'Unit', '301', ',', 'Amherst', 'MA', '01002', 'USA', '|', '1-800-377-8036', '|', 'www.lexalytics.com', 'S', 'U', 'M', 'M', 'A', 'R', 'Y', '/', 'C', 'O', 'N', 'C', 'L', 'U', 'S', 'I', 'O', 'N', 'Text', 'analytics', 'is', 'arguably', 'one', 'of', 'the', 'most', 'complex', 'tasks', 'for', 'an', 'AI', '.'] 

 TOTAL TOKENS ==> 72

 ---- POST ----

 [('Training', 'VBG'), ('a', 'DT'), ('model', 'NN'), ('https', 'NN'), (':', ':'), ('//www.lexalytics.com/', 'JJ'), ('https', 'NN'), (':', ':'), ('//www.lexalytics.com/', 'JJ'), ('W', 'NNP'), ('H', 'NNP'), ('I', 'PRP'), ('T', 'NNP'), ('E', 'NNP'), ('P', 'NNP'), ('A', 'NNP'), ('P', 'NNP'), ('E', 'NNP'), ('R', 'NNP'), ('14|', 'CD'), ('|', 'NNP'), ('Lexalytics', 'NNP'), (',', ','), ('Inc.', 'NNP'), (',', ','), ('48', 'CD'), ('North', 'NNP'), ('Pleasant', 'NNP'), ('St.', 'NNP'), ('Unit', 'NNP'), ('301', 'CD'), (',', ','), ('Amherst', 'NNP'), ('MA', 'NNP'), ('01002', 'CD'), ('USA', 'NNP'), ('|', 'NNP'), ('1-800-377-8036', 'JJ'), ('|', 'NNP'), ('www.lexalytics.com', 'NN'), ('S', 'NNP'), ('U', 'NNP'), ('M', 'NNP'), ('M', 'NNP'), ('A', 'NNP'), ('R', 'NNP'), ('Y', 'NNP'), ('/', 'NNP'), ('C', 'NNP'), ('O', 'NNP'), ('N', 'NNP'), ('C', 'NNP'), ('L', 'NNP'), ('U', 'NNP'), ('S', 'NNP'), ('I', 'PRP'), ('O', 'NNP'), ('N', 'NNP'), ('Text', 'NNP'), ('analytics', 'NNS'), ('is', 'VBZ'), ('arguably', 'RB'), ('one', 'CD'), ('of', 'IN'), ('the', 'DT'), ('most', 'RBS'), ('complex', 'JJ'), ('tasks', 'NNS'), ('for', 'IN'), ('an', 'DT'), ('AI', 'NNP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Training', 'model', 'https', ':', '//www.lexalytics.com/', 'https', ':', '//www.lexalytics.com/', 'W', 'H', 'E', 'P', 'P', 'E', 'R', '14|', '|', 'Lexalytics', ',', 'Inc.', ',', '48', 'North', 'Pleasant', 'St.', 'Unit', '301', ',', 'Amherst', '01002', 'USA', '|', '1-800-377-8036', '|', 'www.lexalytics.com', 'U', 'R', '/', 'C', 'N', 'C', 'L', 'U', 'N', 'Text', 'analytics', 'arguably', 'one', 'complex', 'tasks', 'AI', '.']

 TOTAL FILTERED TOKENS ==>  52

 ---- POST FOR FILTERED TOKENS ----

 [('Training', 'VBG'), ('model', 'NN'), ('https', 'NN'), (':', ':'), ('//www.lexalytics.com/', 'JJ'), ('https', 'NN'), (':', ':'), ('//www.lexalytics.com/', 'JJ'), ('W', 'NNP'), ('H', 'NNP'), ('E', 'NNP'), ('P', 'NNP'), ('P', 'NNP'), ('E', 'NNP'), ('R', 'NNP'), ('14|', 'CD'), ('|', 'NNP'), ('Lexalytics', 'NNP'), (',', ','), ('Inc.', 'NNP'), (',', ','), ('48', 'CD'), ('North', 'NNP'), ('Pleasant', 'NNP'), ('St.', 'NNP'), ('Unit', 'NNP'), ('301', 'CD'), (',', ','), ('Amherst', 'NNP'), ('01002', 'CD'), ('USA', 'NNP'), ('|', 'NNP'), ('1-800-377-8036', 'JJ'), ('|', 'NNP'), ('www.lexalytics.com', 'NN'), ('U', 'NNP'), ('R', 'NNP'), ('/', 'NNP'), ('C', 'NNP'), ('N', 'NNP'), ('C', 'NNP'), ('L', 'NNP'), ('U', 'NNP'), ('N', 'NNP'), ('Text', 'NNP'), ('analytics', 'NNS'), ('arguably', 'RB'), ('one', 'CD'), ('complex', 'JJ'), ('tasks', 'NNS'), ('AI', 'NNP'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Training model', 'model https', 'https :', ': //www.lexalytics.com/', '//www.lexalytics.com/ https', 'https :', ': //www.lexalytics.com/', '//www.lexalytics.com/ W', 'W H', 'H E', 'E P', 'P P', 'P E', 'E R', 'R 14|', '14| |', '| Lexalytics', 'Lexalytics ,', ', Inc.', 'Inc. ,', ', 48', '48 North', 'North Pleasant', 'Pleasant St.', 'St. Unit', 'Unit 301', '301 ,', ', Amherst', 'Amherst 01002', '01002 USA', 'USA |', '| 1-800-377-8036', '1-800-377-8036 |', '| www.lexalytics.com', 'www.lexalytics.com U', 'U R', 'R /', '/ C', 'C N', 'N C', 'C L', 'L U', 'U N', 'N Text', 'Text analytics', 'analytics arguably', 'arguably one', 'one complex', 'complex tasks', 'tasks AI', 'AI .'] 

 TOTAL BIGRAMS --> 51 



 ---- TRI-GRAMS ---- 

 ['Training model https', 'model https :', 'https : //www.lexalytics.com/', ': //www.lexalytics.com/ https', '//www.lexalytics.com/ https :', 'https : //www.lexalytics.com/', ': //www.lexalytics.com/ W', '//www.lexalytics.com/ W H', 'W H E', 'H E P', 'E P P', 'P P E', 'P E R', 'E R 14|', 'R 14| |', '14| | Lexalytics', '| Lexalytics ,', 'Lexalytics , Inc.', ', Inc. ,', 'Inc. , 48', ', 48 North', '48 North Pleasant', 'North Pleasant St.', 'Pleasant St. Unit', 'St. Unit 301', 'Unit 301 ,', '301 , Amherst', ', Amherst 01002', 'Amherst 01002 USA', '01002 USA |', 'USA | 1-800-377-8036', '| 1-800-377-8036 |', '1-800-377-8036 | www.lexalytics.com', '| www.lexalytics.com U', 'www.lexalytics.com U R', 'U R /', 'R / C', '/ C N', 'C N C', 'N C L', 'C L U', 'L U N', 'U N Text', 'N Text analytics', 'Text analytics arguably', 'analytics arguably one', 'arguably one complex', 'one complex tasks', 'complex tasks AI', 'tasks AI .'] 

 TOTAL TRIGRAMS --> 50 



 ---- NOUN PHRASES ---- 

 ['model', 'https', ' https', 'www.lexalytics.com'] 

 TOTAL NOUN PHRASES --> 4 



 ---- NER ----

 
 ORGANIZATION ---> ['USA']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> ['North']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['train', 'model', 'http', ':', '//www.lexalytics.com/', 'http', ':', '//www.lexalytics.com/', 'w', 'h', 'e', 'p', 'p', 'e', 'r', '14|', '|', 'lexalyt', ',', 'inc.', ',', '48', 'north', 'pleasant', 'st.', 'unit', '301', ',', 'amherst', '01002', 'usa', '|', '1-800-377-8036', '|', 'www.lexalytics.com', 'u', 'r', '/', 'c', 'n', 'c', 'l', 'u', 'n', 'text', 'analyt', 'arguabl', 'one', 'complex', 'task', 'ai', '.']

 TOTAL PORTER STEM WORDS ==> 52



 ---- SNOWBALL STEMMING ----

['train', 'model', 'https', ':', '//www.lexalytics.com/', 'https', ':', '//www.lexalytics.com/', 'w', 'h', 'e', 'p', 'p', 'e', 'r', '14|', '|', 'lexalyt', ',', 'inc.', ',', '48', 'north', 'pleasant', 'st.', 'unit', '301', ',', 'amherst', '01002', 'usa', '|', '1-800-377-8036', '|', 'www.lexalytics.com', 'u', 'r', '/', 'c', 'n', 'c', 'l', 'u', 'n', 'text', 'analyt', 'arguabl', 'one', 'complex', 'task', 'ai', '.']

 TOTAL SNOWBALL STEM WORDS ==> 52



 ---- LEMMATIZATION ----

['Training', 'model', 'http', ':', '//www.lexalytics.com/', 'http', ':', '//www.lexalytics.com/', 'W', 'H', 'E', 'P', 'P', 'E', 'R', '14|', '|', 'Lexalytics', ',', 'Inc.', ',', '48', 'North', 'Pleasant', 'St.', 'Unit', '301', ',', 'Amherst', '01002', 'USA', '|', '1-800-377-8036', '|', 'www.lexalytics.com', 'U', 'R', '/', 'C', 'N', 'C', 'L', 'U', 'N', 'Text', 'analytics', 'arguably', 'one', 'complex', 'task', 'AI', '.']

 TOTAL LEMMATIZE WORDS ==> 52

************************************************************************************************************************

146 --> Language is messy and complex. 


 ---- TOKENS ----

 ['Language', 'is', 'messy', 'and', 'complex', '.'] 

 TOTAL TOKENS ==> 6

 ---- POST ----

 [('Language', 'NN'), ('is', 'VBZ'), ('messy', 'JJ'), ('and', 'CC'), ('complex', 'JJ'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Language', 'messy', 'complex', '.']

 TOTAL FILTERED TOKENS ==>  4

 ---- POST FOR FILTERED TOKENS ----

 [('Language', 'JJ'), ('messy', 'NN'), ('complex', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Language messy', 'messy complex', 'complex .'] 

 TOTAL BIGRAMS --> 3 



 ---- TRI-GRAMS ---- 

 ['Language messy complex', 'messy complex .'] 

 TOTAL TRIGRAMS --> 2 



 ---- NOUN PHRASES ---- 

 ['Language messy', 'complex'] 

 TOTAL NOUN PHRASES --> 2 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> ['Language']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['languag', 'messi', 'complex', '.']

 TOTAL PORTER STEM WORDS ==> 4



 ---- SNOWBALL STEMMING ----

['languag', 'messi', 'complex', '.']

 TOTAL SNOWBALL STEM WORDS ==> 4



 ---- LEMMATIZATION ----

['Language', 'messy', 'complex', '.']

 TOTAL LEMMATIZE WORDS ==> 4

************************************************************************************************************************

147 --> Meaning varies from speaker to speaker  and listener to listener. 


 ---- TOKENS ----

 ['Meaning', 'varies', 'from', 'speaker', 'to', 'speaker', 'and', 'listener', 'to', 'listener', '.'] 

 TOTAL TOKENS ==> 11

 ---- POST ----

 [('Meaning', 'VBG'), ('varies', 'NNS'), ('from', 'IN'), ('speaker', 'NN'), ('to', 'TO'), ('speaker', 'NN'), ('and', 'CC'), ('listener', 'NN'), ('to', 'TO'), ('listener', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Meaning', 'varies', 'speaker', 'speaker', 'listener', 'listener', '.']

 TOTAL FILTERED TOKENS ==>  7

 ---- POST FOR FILTERED TOKENS ----

 [('Meaning', 'VBG'), ('varies', 'NNS'), ('speaker', 'NN'), ('speaker', 'NN'), ('listener', 'NN'), ('listener', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Meaning varies', 'varies speaker', 'speaker speaker', 'speaker listener', 'listener listener', 'listener .'] 

 TOTAL BIGRAMS --> 6 



 ---- TRI-GRAMS ---- 

 ['Meaning varies speaker', 'varies speaker speaker', 'speaker speaker listener', 'speaker listener listener', 'listener listener .'] 

 TOTAL TRIGRAMS --> 5 



 ---- NOUN PHRASES ---- 

 ['speaker', 'speaker', 'listener', 'listener'] 

 TOTAL NOUN PHRASES --> 4 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['mean', 'vari', 'speaker', 'speaker', 'listen', 'listen', '.']

 TOTAL PORTER STEM WORDS ==> 7



 ---- SNOWBALL STEMMING ----

['mean', 'vari', 'speaker', 'speaker', 'listen', 'listen', '.']

 TOTAL SNOWBALL STEM WORDS ==> 7



 ---- LEMMATIZATION ----

['Meaning', 'varies', 'speaker', 'speaker', 'listener', 'listener', '.']

 TOTAL LEMMATIZE WORDS ==> 7

************************************************************************************************************************

148 --> Machine learning provides a rich solution set for  handling this complexity, but must be implemented in a way that’s relevant  to the problem – and hand-in-hand with natural language processing code. 


 ---- TOKENS ----

 ['Machine', 'learning', 'provides', 'a', 'rich', 'solution', 'set', 'for', 'handling', 'this', 'complexity', ',', 'but', 'must', 'be', 'implemented', 'in', 'a', 'way', 'that', '’', 's', 'relevant', 'to', 'the', 'problem', '–', 'and', 'hand-in-hand', 'with', 'natural', 'language', 'processing', 'code', '.'] 

 TOTAL TOKENS ==> 35

 ---- POST ----

 [('Machine', 'NN'), ('learning', 'VBG'), ('provides', 'VBZ'), ('a', 'DT'), ('rich', 'JJ'), ('solution', 'NN'), ('set', 'VBN'), ('for', 'IN'), ('handling', 'VBG'), ('this', 'DT'), ('complexity', 'NN'), (',', ','), ('but', 'CC'), ('must', 'MD'), ('be', 'VB'), ('implemented', 'VBN'), ('in', 'IN'), ('a', 'DT'), ('way', 'NN'), ('that', 'IN'), ('’', 'NNP'), ('s', 'VBD'), ('relevant', 'JJ'), ('to', 'TO'), ('the', 'DT'), ('problem', 'NN'), ('–', 'NN'), ('and', 'CC'), ('hand-in-hand', 'NN'), ('with', 'IN'), ('natural', 'JJ'), ('language', 'NN'), ('processing', 'NN'), ('code', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Machine', 'learning', 'provides', 'rich', 'solution', 'set', 'handling', 'complexity', ',', 'must', 'implemented', 'way', '’', 'relevant', 'problem', '–', 'hand-in-hand', 'natural', 'language', 'processing', 'code', '.']

 TOTAL FILTERED TOKENS ==>  22

 ---- POST FOR FILTERED TOKENS ----

 [('Machine', 'NN'), ('learning', 'VBG'), ('provides', 'VBZ'), ('rich', 'JJ'), ('solution', 'NN'), ('set', 'VBN'), ('handling', 'VBG'), ('complexity', 'NN'), (',', ','), ('must', 'MD'), ('implemented', 'VB'), ('way', 'NN'), ('’', 'NNP'), ('relevant', 'NN'), ('problem', 'NN'), ('–', 'NNP'), ('hand-in-hand', 'NN'), ('natural', 'JJ'), ('language', 'NN'), ('processing', 'NN'), ('code', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Machine learning', 'learning provides', 'provides rich', 'rich solution', 'solution set', 'set handling', 'handling complexity', 'complexity ,', ', must', 'must implemented', 'implemented way', 'way ’', '’ relevant', 'relevant problem', 'problem –', '– hand-in-hand', 'hand-in-hand natural', 'natural language', 'language processing', 'processing code', 'code .'] 

 TOTAL BIGRAMS --> 21 



 ---- TRI-GRAMS ---- 

 ['Machine learning provides', 'learning provides rich', 'provides rich solution', 'rich solution set', 'solution set handling', 'set handling complexity', 'handling complexity ,', 'complexity , must', ', must implemented', 'must implemented way', 'implemented way ’', 'way ’ relevant', '’ relevant problem', 'relevant problem –', 'problem – hand-in-hand', '– hand-in-hand natural', 'hand-in-hand natural language', 'natural language processing', 'language processing code', 'processing code .'] 

 TOTAL TRIGRAMS --> 20 



 ---- NOUN PHRASES ---- 

 ['Machine', 'rich solution', 'complexity', 'way', 'relevant', 'problem', 'hand-in-hand', 'natural language', 'processing', 'code'] 

 TOTAL NOUN PHRASES --> 10 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> ['Machine']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['machin', 'learn', 'provid', 'rich', 'solut', 'set', 'handl', 'complex', ',', 'must', 'implement', 'way', '’', 'relev', 'problem', '–', 'hand-in-hand', 'natur', 'languag', 'process', 'code', '.']

 TOTAL PORTER STEM WORDS ==> 22



 ---- SNOWBALL STEMMING ----

['machin', 'learn', 'provid', 'rich', 'solut', 'set', 'handl', 'complex', ',', 'must', 'implement', 'way', '’', 'relev', 'problem', '–', 'hand-in-hand', 'natur', 'languag', 'process', 'code', '.']

 TOTAL SNOWBALL STEM WORDS ==> 22



 ---- LEMMATIZATION ----

['Machine', 'learning', 'provides', 'rich', 'solution', 'set', 'handling', 'complexity', ',', 'must', 'implemented', 'way', '’', 'relevant', 'problem', '–', 'hand-in-hand', 'natural', 'language', 'processing', 'code', '.']

 TOTAL LEMMATIZE WORDS ==> 22

************************************************************************************************************************

149 --> Moreover, although it’s necessary to use machine learning, it’s not sufficient  to use a single type of model, like a big “unsupervised learning” system. 


 ---- TOKENS ----

 ['Moreover', ',', 'although', 'it', '’', 's', 'necessary', 'to', 'use', 'machine', 'learning', ',', 'it', '’', 's', 'not', 'sufficient', 'to', 'use', 'a', 'single', 'type', 'of', 'model', ',', 'like', 'a', 'big', '“', 'unsupervised', 'learning', '”', 'system', '.'] 

 TOTAL TOKENS ==> 34

 ---- POST ----

 [('Moreover', 'RB'), (',', ','), ('although', 'IN'), ('it', 'PRP'), ('’', 'VBZ'), ('s', 'JJ'), ('necessary', 'JJ'), ('to', 'TO'), ('use', 'VB'), ('machine', 'NN'), ('learning', 'NN'), (',', ','), ('it', 'PRP'), ('’', 'VBD'), ('s', 'PRP'), ('not', 'RB'), ('sufficient', 'JJ'), ('to', 'TO'), ('use', 'VB'), ('a', 'DT'), ('single', 'JJ'), ('type', 'NN'), ('of', 'IN'), ('model', 'NN'), (',', ','), ('like', 'IN'), ('a', 'DT'), ('big', 'JJ'), ('“', 'NN'), ('unsupervised', 'VBD'), ('learning', 'VBG'), ('”', 'NN'), ('system', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Moreover', ',', 'although', '’', 'necessary', 'use', 'machine', 'learning', ',', '’', 'sufficient', 'use', 'single', 'type', 'model', ',', 'like', 'big', '“', 'unsupervised', 'learning', '”', 'system', '.']

 TOTAL FILTERED TOKENS ==>  24

 ---- POST FOR FILTERED TOKENS ----

 [('Moreover', 'RB'), (',', ','), ('although', 'IN'), ('’', 'NNP'), ('necessary', 'JJ'), ('use', 'NN'), ('machine', 'NN'), ('learning', 'NN'), (',', ','), ('’', 'NNP'), ('sufficient', 'NN'), ('use', 'NN'), ('single', 'JJ'), ('type', 'NN'), ('model', 'NN'), (',', ','), ('like', 'IN'), ('big', 'JJ'), ('“', 'NNS'), ('unsupervised', 'VBD'), ('learning', 'VBG'), ('”', 'NN'), ('system', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Moreover ,', ', although', 'although ’', '’ necessary', 'necessary use', 'use machine', 'machine learning', 'learning ,', ', ’', '’ sufficient', 'sufficient use', 'use single', 'single type', 'type model', 'model ,', ', like', 'like big', 'big “', '“ unsupervised', 'unsupervised learning', 'learning ”', '” system', 'system .'] 

 TOTAL BIGRAMS --> 23 



 ---- TRI-GRAMS ---- 

 ['Moreover , although', ', although ’', 'although ’ necessary', '’ necessary use', 'necessary use machine', 'use machine learning', 'machine learning ,', 'learning , ’', ', ’ sufficient', '’ sufficient use', 'sufficient use single', 'use single type', 'single type model', 'type model ,', 'model , like', ', like big', 'like big “', 'big “ unsupervised', '“ unsupervised learning', 'unsupervised learning ”', 'learning ” system', '” system .'] 

 TOTAL TRIGRAMS --> 22 



 ---- NOUN PHRASES ---- 

 ['necessary use', 'machine', 'learning', 'sufficient', 'use', 'single type', 'model', '”', 'system'] 

 TOTAL NOUN PHRASES --> 9 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['moreov', ',', 'although', '’', 'necessari', 'use', 'machin', 'learn', ',', '’', 'suffici', 'use', 'singl', 'type', 'model', ',', 'like', 'big', '“', 'unsupervis', 'learn', '”', 'system', '.']

 TOTAL PORTER STEM WORDS ==> 24



 ---- SNOWBALL STEMMING ----

['moreov', ',', 'although', '’', 'necessari', 'use', 'machin', 'learn', ',', '’', 'suffici', 'use', 'singl', 'type', 'model', ',', 'like', 'big', '“', 'unsupervis', 'learn', '”', 'system', '.']

 TOTAL SNOWBALL STEM WORDS ==> 24



 ---- LEMMATIZATION ----

['Moreover', ',', 'although', '’', 'necessary', 'use', 'machine', 'learning', ',', '’', 'sufficient', 'use', 'single', 'type', 'model', ',', 'like', 'big', '“', 'unsupervised', 'learning', '”', 'system', '.']

 TOTAL LEMMATIZE WORDS ==> 24

************************************************************************************************************************

150 --> Certain aspects of machine learning are very subjective, and need to be  trained or tuned to match your perspective. 


 ---- TOKENS ----

 ['Certain', 'aspects', 'of', 'machine', 'learning', 'are', 'very', 'subjective', ',', 'and', 'need', 'to', 'be', 'trained', 'or', 'tuned', 'to', 'match', 'your', 'perspective', '.'] 

 TOTAL TOKENS ==> 21

 ---- POST ----

 [('Certain', 'NNP'), ('aspects', 'NNS'), ('of', 'IN'), ('machine', 'NN'), ('learning', 'NN'), ('are', 'VBP'), ('very', 'RB'), ('subjective', 'JJ'), (',', ','), ('and', 'CC'), ('need', 'MD'), ('to', 'TO'), ('be', 'VB'), ('trained', 'VBN'), ('or', 'CC'), ('tuned', 'VBN'), ('to', 'TO'), ('match', 'VB'), ('your', 'PRP$'), ('perspective', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Certain', 'aspects', 'machine', 'learning', 'subjective', ',', 'need', 'trained', 'tuned', 'match', 'perspective', '.']

 TOTAL FILTERED TOKENS ==>  12

 ---- POST FOR FILTERED TOKENS ----

 [('Certain', 'NN'), ('aspects', 'NNS'), ('machine', 'NN'), ('learning', 'VBG'), ('subjective', 'JJ'), (',', ','), ('need', 'VBP'), ('trained', 'VBN'), ('tuned', 'JJ'), ('match', 'NN'), ('perspective', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Certain aspects', 'aspects machine', 'machine learning', 'learning subjective', 'subjective ,', ', need', 'need trained', 'trained tuned', 'tuned match', 'match perspective', 'perspective .'] 

 TOTAL BIGRAMS --> 11 



 ---- TRI-GRAMS ---- 

 ['Certain aspects machine', 'aspects machine learning', 'machine learning subjective', 'learning subjective ,', 'subjective , need', ', need trained', 'need trained tuned', 'trained tuned match', 'tuned match perspective', 'match perspective .'] 

 TOTAL TRIGRAMS --> 10 



 ---- NOUN PHRASES ---- 

 ['Certain', 'machine', 'tuned match', 'perspective'] 

 TOTAL NOUN PHRASES --> 4 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> ['Certain']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['certain', 'aspect', 'machin', 'learn', 'subject', ',', 'need', 'train', 'tune', 'match', 'perspect', '.']

 TOTAL PORTER STEM WORDS ==> 12



 ---- SNOWBALL STEMMING ----

['certain', 'aspect', 'machin', 'learn', 'subject', ',', 'need', 'train', 'tune', 'match', 'perspect', '.']

 TOTAL SNOWBALL STEM WORDS ==> 12



 ---- LEMMATIZATION ----

['Certain', 'aspect', 'machine', 'learning', 'subjective', ',', 'need', 'trained', 'tuned', 'match', 'perspective', '.']

 TOTAL LEMMATIZE WORDS ==> 12

************************************************************************************************************************

151 --> Lexalytics combines many   types of machine learning along with pure natural language processing code. 


 ---- TOKENS ----

 ['Lexalytics', 'combines', 'many', 'types', 'of', 'machine', 'learning', 'along', 'with', 'pure', 'natural', 'language', 'processing', 'code', '.'] 

 TOTAL TOKENS ==> 15

 ---- POST ----

 [('Lexalytics', 'NNP'), ('combines', 'NNS'), ('many', 'JJ'), ('types', 'NNS'), ('of', 'IN'), ('machine', 'NN'), ('learning', 'VBG'), ('along', 'IN'), ('with', 'IN'), ('pure', 'JJ'), ('natural', 'JJ'), ('language', 'NN'), ('processing', 'NN'), ('code', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Lexalytics', 'combines', 'many', 'types', 'machine', 'learning', 'along', 'pure', 'natural', 'language', 'processing', 'code', '.']

 TOTAL FILTERED TOKENS ==>  13

 ---- POST FOR FILTERED TOKENS ----

 [('Lexalytics', 'NNP'), ('combines', 'NNS'), ('many', 'JJ'), ('types', 'NNS'), ('machine', 'NN'), ('learning', 'NN'), ('along', 'IN'), ('pure', 'JJ'), ('natural', 'JJ'), ('language', 'NN'), ('processing', 'NN'), ('code', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Lexalytics combines', 'combines many', 'many types', 'types machine', 'machine learning', 'learning along', 'along pure', 'pure natural', 'natural language', 'language processing', 'processing code', 'code .'] 

 TOTAL BIGRAMS --> 12 



 ---- TRI-GRAMS ---- 

 ['Lexalytics combines many', 'combines many types', 'many types machine', 'types machine learning', 'machine learning along', 'learning along pure', 'along pure natural', 'pure natural language', 'natural language processing', 'language processing code', 'processing code .'] 

 TOTAL TRIGRAMS --> 11 



 ---- NOUN PHRASES ---- 

 ['machine', 'learning', 'pure natural language', 'processing', 'code'] 

 TOTAL NOUN PHRASES --> 5 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> ['Lexalytics']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['lexalyt', 'combin', 'mani', 'type', 'machin', 'learn', 'along', 'pure', 'natur', 'languag', 'process', 'code', '.']

 TOTAL PORTER STEM WORDS ==> 13



 ---- SNOWBALL STEMMING ----

['lexalyt', 'combin', 'mani', 'type', 'machin', 'learn', 'along', 'pure', 'natur', 'languag', 'process', 'code', '.']

 TOTAL SNOWBALL STEM WORDS ==> 13



 ---- LEMMATIZATION ----

['Lexalytics', 'combine', 'many', 'type', 'machine', 'learning', 'along', 'pure', 'natural', 'language', 'processing', 'code', '.']

 TOTAL LEMMATIZE WORDS ==> 13

************************************************************************************************************************

152 --> We have no prejudice for one algorithm over another except in how they  help us provide the best possible text analytics system to our customers. 


 ---- TOKENS ----

 ['We', 'have', 'no', 'prejudice', 'for', 'one', 'algorithm', 'over', 'another', 'except', 'in', 'how', 'they', 'help', 'us', 'provide', 'the', 'best', 'possible', 'text', 'analytics', 'system', 'to', 'our', 'customers', '.'] 

 TOTAL TOKENS ==> 26

 ---- POST ----

 [('We', 'PRP'), ('have', 'VBP'), ('no', 'DT'), ('prejudice', 'NN'), ('for', 'IN'), ('one', 'CD'), ('algorithm', 'NN'), ('over', 'IN'), ('another', 'DT'), ('except', 'NN'), ('in', 'IN'), ('how', 'WRB'), ('they', 'PRP'), ('help', 'VBP'), ('us', 'PRP'), ('provide', 'VB'), ('the', 'DT'), ('best', 'JJS'), ('possible', 'JJ'), ('text', 'NN'), ('analytics', 'NNS'), ('system', 'NN'), ('to', 'TO'), ('our', 'PRP$'), ('customers', 'NNS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['prejudice', 'one', 'algorithm', 'another', 'except', 'help', 'us', 'provide', 'best', 'possible', 'text', 'analytics', 'system', 'customers', '.']

 TOTAL FILTERED TOKENS ==>  15

 ---- POST FOR FILTERED TOKENS ----

 [('prejudice', 'NN'), ('one', 'CD'), ('algorithm', 'NN'), ('another', 'DT'), ('except', 'IN'), ('help', 'NN'), ('us', 'PRP'), ('provide', 'VBP'), ('best', 'JJS'), ('possible', 'JJ'), ('text', 'NN'), ('analytics', 'NNS'), ('system', 'NN'), ('customers', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['prejudice one', 'one algorithm', 'algorithm another', 'another except', 'except help', 'help us', 'us provide', 'provide best', 'best possible', 'possible text', 'text analytics', 'analytics system', 'system customers', 'customers .'] 

 TOTAL BIGRAMS --> 14 



 ---- TRI-GRAMS ---- 

 ['prejudice one algorithm', 'one algorithm another', 'algorithm another except', 'another except help', 'except help us', 'help us provide', 'us provide best', 'provide best possible', 'best possible text', 'possible text analytics', 'text analytics system', 'analytics system customers', 'system customers .'] 

 TOTAL TRIGRAMS --> 13 



 ---- NOUN PHRASES ---- 

 ['prejudice', 'algorithm', 'help', 'possible text', 'system'] 

 TOTAL NOUN PHRASES --> 5 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['prejudic', 'one', 'algorithm', 'anoth', 'except', 'help', 'us', 'provid', 'best', 'possibl', 'text', 'analyt', 'system', 'custom', '.']

 TOTAL PORTER STEM WORDS ==> 15



 ---- SNOWBALL STEMMING ----

['prejudic', 'one', 'algorithm', 'anoth', 'except', 'help', 'us', 'provid', 'best', 'possibl', 'text', 'analyt', 'system', 'custom', '.']

 TOTAL SNOWBALL STEM WORDS ==> 15



 ---- LEMMATIZATION ----

['prejudice', 'one', 'algorithm', 'another', 'except', 'help', 'u', 'provide', 'best', 'possible', 'text', 'analytics', 'system', 'customer', '.']

 TOTAL LEMMATIZE WORDS ==> 15

************************************************************************************************************************

153 --> to explore how Lexalytics   can help your business at  lexalytics.com/contact Contact  us https://www.lexalytics.com/ https://www.lexalytics.com/ https://www.lexalytics.com/contact https://www.lexalytics.com/contact  ©  2019 Lexalytics, Inc. | M achine Learning W hite Paper | v3c Lexalytics processes BILLIONS of   unstructured documents every day, GLOBALLY. 


 ---- TOKENS ----

 ['to', 'explore', 'how', 'Lexalytics', 'can', 'help', 'your', 'business', 'at', 'lexalytics.com/contact', 'Contact', 'us', 'https', ':', '//www.lexalytics.com/', 'https', ':', '//www.lexalytics.com/', 'https', ':', '//www.lexalytics.com/contact', 'https', ':', '//www.lexalytics.com/contact', '©', '2019', 'Lexalytics', ',', 'Inc.', '|', 'M', 'achine', 'Learning', 'W', 'hite', 'Paper', '|', 'v3c', 'Lexalytics', 'processes', 'BILLIONS', 'of', 'unstructured', 'documents', 'every', 'day', ',', 'GLOBALLY', '.'] 

 TOTAL TOKENS ==> 49

 ---- POST ----

 [('to', 'TO'), ('explore', 'VB'), ('how', 'WRB'), ('Lexalytics', 'NNS'), ('can', 'MD'), ('help', 'VB'), ('your', 'PRP$'), ('business', 'NN'), ('at', 'IN'), ('lexalytics.com/contact', 'JJ'), ('Contact', 'NNP'), ('us', 'PRP'), ('https', 'NN'), (':', ':'), ('//www.lexalytics.com/', 'JJ'), ('https', 'NN'), (':', ':'), ('//www.lexalytics.com/', 'JJ'), ('https', 'NN'), (':', ':'), ('//www.lexalytics.com/contact', 'JJ'), ('https', 'NN'), (':', ':'), ('//www.lexalytics.com/contact', 'NN'), ('©', 'IN'), ('2019', 'CD'), ('Lexalytics', 'NNP'), (',', ','), ('Inc.', 'NNP'), ('|', 'NNP'), ('M', 'NNP'), ('achine', 'NN'), ('Learning', 'NNP'), ('W', 'NNP'), ('hite', 'JJ'), ('Paper', 'NNP'), ('|', 'NNP'), ('v3c', 'IN'), ('Lexalytics', 'NNP'), ('processes', 'VBZ'), ('BILLIONS', 'NNP'), ('of', 'IN'), ('unstructured', 'JJ'), ('documents', 'NNS'), ('every', 'DT'), ('day', 'NN'), (',', ','), ('GLOBALLY', 'NNP'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['explore', 'Lexalytics', 'help', 'business', 'lexalytics.com/contact', 'Contact', 'us', 'https', ':', '//www.lexalytics.com/', 'https', ':', '//www.lexalytics.com/', 'https', ':', '//www.lexalytics.com/contact', 'https', ':', '//www.lexalytics.com/contact', '©', '2019', 'Lexalytics', ',', 'Inc.', '|', 'achine', 'Learning', 'W', 'hite', 'Paper', '|', 'v3c', 'Lexalytics', 'processes', 'BILLIONS', 'unstructured', 'documents', 'every', 'day', ',', 'GLOBALLY', '.']

 TOTAL FILTERED TOKENS ==>  42

 ---- POST FOR FILTERED TOKENS ----

 [('explore', 'NN'), ('Lexalytics', 'NNP'), ('help', 'NN'), ('business', 'NN'), ('lexalytics.com/contact', 'NN'), ('Contact', 'NNP'), ('us', 'PRP'), ('https', 'NN'), (':', ':'), ('//www.lexalytics.com/', 'JJ'), ('https', 'NN'), (':', ':'), ('//www.lexalytics.com/', 'JJ'), ('https', 'NN'), (':', ':'), ('//www.lexalytics.com/contact', 'JJ'), ('https', 'NN'), (':', ':'), ('//www.lexalytics.com/contact', 'NN'), ('©', 'IN'), ('2019', 'CD'), ('Lexalytics', 'NNP'), (',', ','), ('Inc.', 'NNP'), ('|', 'NNP'), ('achine', 'VBP'), ('Learning', 'NNP'), ('W', 'NNP'), ('hite', 'JJ'), ('Paper', 'NNP'), ('|', 'NNP'), ('v3c', 'IN'), ('Lexalytics', 'NNP'), ('processes', 'VBZ'), ('BILLIONS', 'NNP'), ('unstructured', 'JJ'), ('documents', 'NNS'), ('every', 'DT'), ('day', 'NN'), (',', ','), ('GLOBALLY', 'NNP'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['explore Lexalytics', 'Lexalytics help', 'help business', 'business lexalytics.com/contact', 'lexalytics.com/contact Contact', 'Contact us', 'us https', 'https :', ': //www.lexalytics.com/', '//www.lexalytics.com/ https', 'https :', ': //www.lexalytics.com/', '//www.lexalytics.com/ https', 'https :', ': //www.lexalytics.com/contact', '//www.lexalytics.com/contact https', 'https :', ': //www.lexalytics.com/contact', '//www.lexalytics.com/contact ©', '© 2019', '2019 Lexalytics', 'Lexalytics ,', ', Inc.', 'Inc. |', '| achine', 'achine Learning', 'Learning W', 'W hite', 'hite Paper', 'Paper |', '| v3c', 'v3c Lexalytics', 'Lexalytics processes', 'processes BILLIONS', 'BILLIONS unstructured', 'unstructured documents', 'documents every', 'every day', 'day ,', ', GLOBALLY', 'GLOBALLY .'] 

 TOTAL BIGRAMS --> 41 



 ---- TRI-GRAMS ---- 

 ['explore Lexalytics help', 'Lexalytics help business', 'help business lexalytics.com/contact', 'business lexalytics.com/contact Contact', 'lexalytics.com/contact Contact us', 'Contact us https', 'us https :', 'https : //www.lexalytics.com/', ': //www.lexalytics.com/ https', '//www.lexalytics.com/ https :', 'https : //www.lexalytics.com/', ': //www.lexalytics.com/ https', '//www.lexalytics.com/ https :', 'https : //www.lexalytics.com/contact', ': //www.lexalytics.com/contact https', '//www.lexalytics.com/contact https :', 'https : //www.lexalytics.com/contact', ': //www.lexalytics.com/contact ©', '//www.lexalytics.com/contact © 2019', '© 2019 Lexalytics', '2019 Lexalytics ,', 'Lexalytics , Inc.', ', Inc. |', 'Inc. | achine', '| achine Learning', 'achine Learning W', 'Learning W hite', 'W hite Paper', 'hite Paper |', 'Paper | v3c', '| v3c Lexalytics', 'v3c Lexalytics processes', 'Lexalytics processes BILLIONS', 'processes BILLIONS unstructured', 'BILLIONS unstructured documents', 'unstructured documents every', 'documents every day', 'every day ,', 'day , GLOBALLY', ', GLOBALLY .'] 

 TOTAL TRIGRAMS --> 40 



 ---- NOUN PHRASES ---- 

 ['explore', 'help', 'business', 'lexalytics.com', 'https', ' https', ' https', ' https', '', 'every day'] 

 TOTAL NOUN PHRASES --> 10 



 ---- NER ----

 
 ORGANIZATION ---> ['Lexalytics', 'BILLIONS', 'GLOBALLY']
 TOTAL ORGANIZATION ENTITY --> 3 


 PERSON ---> ['Paper']
 TOTAL PERSON ENTITY --> 1 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['explor', 'lexalyt', 'help', 'busi', 'lexalytics.com/contact', 'contact', 'us', 'http', ':', '//www.lexalytics.com/', 'http', ':', '//www.lexalytics.com/', 'http', ':', '//www.lexalytics.com/contact', 'http', ':', '//www.lexalytics.com/contact', '©', '2019', 'lexalyt', ',', 'inc.', '|', 'achin', 'learn', 'w', 'hite', 'paper', '|', 'v3c', 'lexalyt', 'process', 'billion', 'unstructur', 'document', 'everi', 'day', ',', 'global', '.']

 TOTAL PORTER STEM WORDS ==> 42



 ---- SNOWBALL STEMMING ----

['explor', 'lexalyt', 'help', 'busi', 'lexalytics.com/contact', 'contact', 'us', 'https', ':', '//www.lexalytics.com/', 'https', ':', '//www.lexalytics.com/', 'https', ':', '//www.lexalytics.com/contact', 'https', ':', '//www.lexalytics.com/contact', '©', '2019', 'lexalyt', ',', 'inc.', '|', 'achin', 'learn', 'w', 'hite', 'paper', '|', 'v3c', 'lexalyt', 'process', 'billion', 'unstructur', 'document', 'everi', 'day', ',', 'global', '.']

 TOTAL SNOWBALL STEM WORDS ==> 42



 ---- LEMMATIZATION ----

['explore', 'Lexalytics', 'help', 'business', 'lexalytics.com/contact', 'Contact', 'u', 'http', ':', '//www.lexalytics.com/', 'http', ':', '//www.lexalytics.com/', 'http', ':', '//www.lexalytics.com/contact', 'http', ':', '//www.lexalytics.com/contact', '©', '2019', 'Lexalytics', ',', 'Inc.', '|', 'achine', 'Learning', 'W', 'hite', 'Paper', '|', 'v3c', 'Lexalytics', 'process', 'BILLIONS', 'unstructured', 'document', 'every', 'day', ',', 'GLOBALLY', '.']

 TOTAL LEMMATIZE WORDS ==> 42

************************************************************************************************************************

154 --> We transform unstructured text into usable data and powerful stories. 


 ---- TOKENS ----

 ['We', 'transform', 'unstructured', 'text', 'into', 'usable', 'data', 'and', 'powerful', 'stories', '.'] 

 TOTAL TOKENS ==> 11

 ---- POST ----

 [('We', 'PRP'), ('transform', 'VBP'), ('unstructured', 'JJ'), ('text', 'NN'), ('into', 'IN'), ('usable', 'JJ'), ('data', 'NNS'), ('and', 'CC'), ('powerful', 'JJ'), ('stories', 'NNS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['transform', 'unstructured', 'text', 'usable', 'data', 'powerful', 'stories', '.']

 TOTAL FILTERED TOKENS ==>  8

 ---- POST FOR FILTERED TOKENS ----

 [('transform', 'NN'), ('unstructured', 'VBD'), ('text', 'JJ'), ('usable', 'JJ'), ('data', 'NNS'), ('powerful', 'JJ'), ('stories', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['transform unstructured', 'unstructured text', 'text usable', 'usable data', 'data powerful', 'powerful stories', 'stories .'] 

 TOTAL BIGRAMS --> 7 



 ---- TRI-GRAMS ---- 

 ['transform unstructured text', 'unstructured text usable', 'text usable data', 'usable data powerful', 'data powerful stories', 'powerful stories .'] 

 TOTAL TRIGRAMS --> 6 



 ---- NOUN PHRASES ---- 

 ['transform'] 

 TOTAL NOUN PHRASES --> 1 



 ---- NER ----

 
 ORGANIZATION ---> []
 TOTAL ORGANIZATION ENTITY --> 0 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['transform', 'unstructur', 'text', 'usabl', 'data', 'power', 'stori', '.']

 TOTAL PORTER STEM WORDS ==> 8



 ---- SNOWBALL STEMMING ----

['transform', 'unstructur', 'text', 'usabl', 'data', 'power', 'stori', '.']

 TOTAL SNOWBALL STEM WORDS ==> 8



 ---- LEMMATIZATION ----

['transform', 'unstructured', 'text', 'usable', 'data', 'powerful', 'story', '.']

 TOTAL LEMMATIZE WORDS ==> 8

************************************************************************************************************************

155 --> Our on-premise Salience® engine, SaaS Semantria® API, and end-to-end Lexalytics  Intelligence Platform® combine natural language processing with artificial intelligence  to reveal context-rich patterns and insights within comments, reviews, surveys, and   other text documents. 


 ---- TOKENS ----

 ['Our', 'on-premise', 'Salience®', 'engine', ',', 'SaaS', 'Semantria®', 'API', ',', 'and', 'end-to-end', 'Lexalytics', 'Intelligence', 'Platform®', 'combine', 'natural', 'language', 'processing', 'with', 'artificial', 'intelligence', 'to', 'reveal', 'context-rich', 'patterns', 'and', 'insights', 'within', 'comments', ',', 'reviews', ',', 'surveys', ',', 'and', 'other', 'text', 'documents', '.'] 

 TOTAL TOKENS ==> 39

 ---- POST ----

 [('Our', 'PRP$'), ('on-premise', 'JJ'), ('Salience®', 'NNP'), ('engine', 'NN'), (',', ','), ('SaaS', 'NNP'), ('Semantria®', 'NNP'), ('API', 'NNP'), (',', ','), ('and', 'CC'), ('end-to-end', 'JJ'), ('Lexalytics', 'NNS'), ('Intelligence', 'NNP'), ('Platform®', 'NNP'), ('combine', 'VBP'), ('natural', 'JJ'), ('language', 'NN'), ('processing', 'NN'), ('with', 'IN'), ('artificial', 'JJ'), ('intelligence', 'NN'), ('to', 'TO'), ('reveal', 'VB'), ('context-rich', 'JJ'), ('patterns', 'NNS'), ('and', 'CC'), ('insights', 'NNS'), ('within', 'IN'), ('comments', 'NNS'), (',', ','), ('reviews', 'NNS'), (',', ','), ('surveys', 'NNS'), (',', ','), ('and', 'CC'), ('other', 'JJ'), ('text', 'NN'), ('documents', 'NNS'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['on-premise', 'Salience®', 'engine', ',', 'SaaS', 'Semantria®', 'API', ',', 'end-to-end', 'Lexalytics', 'Intelligence', 'Platform®', 'combine', 'natural', 'language', 'processing', 'artificial', 'intelligence', 'reveal', 'context-rich', 'patterns', 'insights', 'within', 'comments', ',', 'reviews', ',', 'surveys', ',', 'text', 'documents', '.']

 TOTAL FILTERED TOKENS ==>  32

 ---- POST FOR FILTERED TOKENS ----

 [('on-premise', 'JJ'), ('Salience®', 'NNP'), ('engine', 'NN'), (',', ','), ('SaaS', 'NNP'), ('Semantria®', 'NNP'), ('API', 'NNP'), (',', ','), ('end-to-end', 'JJ'), ('Lexalytics', 'NNP'), ('Intelligence', 'NNP'), ('Platform®', 'NNP'), ('combine', 'VBP'), ('natural', 'JJ'), ('language', 'NN'), ('processing', 'VBG'), ('artificial', 'JJ'), ('intelligence', 'NN'), ('reveal', 'JJ'), ('context-rich', 'JJ'), ('patterns', 'NNS'), ('insights', 'NNS'), ('within', 'IN'), ('comments', 'NNS'), (',', ','), ('reviews', 'NNS'), (',', ','), ('surveys', 'NNS'), (',', ','), ('text', 'JJ'), ('documents', 'NNS'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['on-premise Salience®', 'Salience® engine', 'engine ,', ', SaaS', 'SaaS Semantria®', 'Semantria® API', 'API ,', ', end-to-end', 'end-to-end Lexalytics', 'Lexalytics Intelligence', 'Intelligence Platform®', 'Platform® combine', 'combine natural', 'natural language', 'language processing', 'processing artificial', 'artificial intelligence', 'intelligence reveal', 'reveal context-rich', 'context-rich patterns', 'patterns insights', 'insights within', 'within comments', 'comments ,', ', reviews', 'reviews ,', ', surveys', 'surveys ,', ', text', 'text documents', 'documents .'] 

 TOTAL BIGRAMS --> 31 



 ---- TRI-GRAMS ---- 

 ['on-premise Salience® engine', 'Salience® engine ,', 'engine , SaaS', ', SaaS Semantria®', 'SaaS Semantria® API', 'Semantria® API ,', 'API , end-to-end', ', end-to-end Lexalytics', 'end-to-end Lexalytics Intelligence', 'Lexalytics Intelligence Platform®', 'Intelligence Platform® combine', 'Platform® combine natural', 'combine natural language', 'natural language processing', 'language processing artificial', 'processing artificial intelligence', 'artificial intelligence reveal', 'intelligence reveal context-rich', 'reveal context-rich patterns', 'context-rich patterns insights', 'patterns insights within', 'insights within comments', 'within comments ,', 'comments , reviews', ', reviews ,', 'reviews , surveys', ', surveys ,', 'surveys , text', ', text documents', 'text documents .'] 

 TOTAL TRIGRAMS --> 30 



 ---- NOUN PHRASES ---- 

 ['engine', 'natural language', 'artificial intelligence'] 

 TOTAL NOUN PHRASES --> 3 



 ---- NER ----

 
 ORGANIZATION ---> ['SaaS', 'Lexalytics Intelligence']
 TOTAL ORGANIZATION ENTITY --> 2 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> []
 TOTAL GPE ENTITY --> 0 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['on-premis', 'salience®', 'engin', ',', 'saa', 'semantria®', 'api', ',', 'end-to-end', 'lexalyt', 'intellig', 'platform®', 'combin', 'natur', 'languag', 'process', 'artifici', 'intellig', 'reveal', 'context-rich', 'pattern', 'insight', 'within', 'comment', ',', 'review', ',', 'survey', ',', 'text', 'document', '.']

 TOTAL PORTER STEM WORDS ==> 32



 ---- SNOWBALL STEMMING ----

['on-premis', 'salience®', 'engin', ',', 'saa', 'semantria®', 'api', ',', 'end-to-end', 'lexalyt', 'intellig', 'platform®', 'combin', 'natur', 'languag', 'process', 'artifici', 'intellig', 'reveal', 'context-rich', 'pattern', 'insight', 'within', 'comment', ',', 'review', ',', 'survey', ',', 'text', 'document', '.']

 TOTAL SNOWBALL STEM WORDS ==> 32



 ---- LEMMATIZATION ----

['on-premise', 'Salience®', 'engine', ',', 'SaaS', 'Semantria®', 'API', ',', 'end-to-end', 'Lexalytics', 'Intelligence', 'Platform®', 'combine', 'natural', 'language', 'processing', 'artificial', 'intelligence', 'reveal', 'context-rich', 'pattern', 'insight', 'within', 'comment', ',', 'review', ',', 'survey', ',', 'text', 'document', '.']

 TOTAL LEMMATIZE WORDS ==> 32

************************************************************************************************************************

156 --> Data analytics and data analyst companies rely on Lexalytics to build better  products, share insights between engineering, marketing, PR, and support teams,  and drive business growth. 


 ---- TOKENS ----

 ['Data', 'analytics', 'and', 'data', 'analyst', 'companies', 'rely', 'on', 'Lexalytics', 'to', 'build', 'better', 'products', ',', 'share', 'insights', 'between', 'engineering', ',', 'marketing', ',', 'PR', ',', 'and', 'support', 'teams', ',', 'and', 'drive', 'business', 'growth', '.'] 

 TOTAL TOKENS ==> 32

 ---- POST ----

 [('Data', 'NNP'), ('analytics', 'NNS'), ('and', 'CC'), ('data', 'NNS'), ('analyst', 'NN'), ('companies', 'NNS'), ('rely', 'VBP'), ('on', 'IN'), ('Lexalytics', 'NNS'), ('to', 'TO'), ('build', 'VB'), ('better', 'JJR'), ('products', 'NNS'), (',', ','), ('share', 'NN'), ('insights', 'NNS'), ('between', 'IN'), ('engineering', 'NN'), (',', ','), ('marketing', 'NN'), (',', ','), ('PR', 'NNP'), (',', ','), ('and', 'CC'), ('support', 'NN'), ('teams', 'NNS'), (',', ','), ('and', 'CC'), ('drive', 'NN'), ('business', 'NN'), ('growth', 'NN'), ('.', '.')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['Data', 'analytics', 'data', 'analyst', 'companies', 'rely', 'Lexalytics', 'build', 'better', 'products', ',', 'share', 'insights', 'engineering', ',', 'marketing', ',', 'PR', ',', 'support', 'teams', ',', 'drive', 'business', 'growth', '.']

 TOTAL FILTERED TOKENS ==>  26

 ---- POST FOR FILTERED TOKENS ----

 [('Data', 'NNP'), ('analytics', 'NNS'), ('data', 'NNS'), ('analyst', 'NN'), ('companies', 'NNS'), ('rely', 'VBP'), ('Lexalytics', 'NNP'), ('build', 'VBP'), ('better', 'JJR'), ('products', 'NNS'), (',', ','), ('share', 'NN'), ('insights', 'NNS'), ('engineering', 'NN'), (',', ','), ('marketing', 'NN'), (',', ','), ('PR', 'NNP'), (',', ','), ('support', 'NN'), ('teams', 'NNS'), (',', ','), ('drive', 'NN'), ('business', 'NN'), ('growth', 'NN'), ('.', '.')] 



 ---- BI-GRAMS ---- 

 ['Data analytics', 'analytics data', 'data analyst', 'analyst companies', 'companies rely', 'rely Lexalytics', 'Lexalytics build', 'build better', 'better products', 'products ,', ', share', 'share insights', 'insights engineering', 'engineering ,', ', marketing', 'marketing ,', ', PR', 'PR ,', ', support', 'support teams', 'teams ,', ', drive', 'drive business', 'business growth', 'growth .'] 

 TOTAL BIGRAMS --> 25 



 ---- TRI-GRAMS ---- 

 ['Data analytics data', 'analytics data analyst', 'data analyst companies', 'analyst companies rely', 'companies rely Lexalytics', 'rely Lexalytics build', 'Lexalytics build better', 'build better products', 'better products ,', 'products , share', ', share insights', 'share insights engineering', 'insights engineering ,', 'engineering , marketing', ', marketing ,', 'marketing , PR', ', PR ,', 'PR , support', ', support teams', 'support teams ,', 'teams , drive', ', drive business', 'drive business growth', 'business growth .'] 

 TOTAL TRIGRAMS --> 24 



 ---- NOUN PHRASES ---- 

 ['analyst', 'share', 'engineering', 'marketing', 'support', 'drive', 'business', 'growth'] 

 TOTAL NOUN PHRASES --> 8 



 ---- NER ----

 
 ORGANIZATION ---> ['PR']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> ['Data']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['data', 'analyt', 'data', 'analyst', 'compani', 'reli', 'lexalyt', 'build', 'better', 'product', ',', 'share', 'insight', 'engin', ',', 'market', ',', 'pr', ',', 'support', 'team', ',', 'drive', 'busi', 'growth', '.']

 TOTAL PORTER STEM WORDS ==> 26



 ---- SNOWBALL STEMMING ----

['data', 'analyt', 'data', 'analyst', 'compani', 'reli', 'lexalyt', 'build', 'better', 'product', ',', 'share', 'insight', 'engin', ',', 'market', ',', 'pr', ',', 'support', 'team', ',', 'drive', 'busi', 'growth', '.']

 TOTAL SNOWBALL STEM WORDS ==> 26



 ---- LEMMATIZATION ----

['Data', 'analytics', 'data', 'analyst', 'company', 'rely', 'Lexalytics', 'build', 'better', 'product', ',', 'share', 'insight', 'engineering', ',', 'marketing', ',', 'PR', ',', 'support', 'team', ',', 'drive', 'business', 'growth', '.']

 TOTAL LEMMATIZE WORDS ==> 26

************************************************************************************************************************

157 --> For more information, visit www.lexalytics.com or call 1-800-377-8036  W H I T E  P A P E R 15|       | Lexalytics, Inc., 48 North Pleasant St. Unit 301, Amherst MA 01002 USA | 1-800-377-8036 | www.lexalytics.com https://www.lexalytics.com/ https://www.lexalytics.com/ https://www.lexalytics.com/ https://www.facebook.com/lexalytics/ https://twitter.com/Lexalytics https://www.linkedin.com/company/lexalytics-inc/ 


 ---- TOKENS ----

 ['For', 'more', 'information', ',', 'visit', 'www.lexalytics.com', 'or', 'call', '1-800-377-8036', 'W', 'H', 'I', 'T', 'E', 'P', 'A', 'P', 'E', 'R', '15|', '|', 'Lexalytics', ',', 'Inc.', ',', '48', 'North', 'Pleasant', 'St.', 'Unit', '301', ',', 'Amherst', 'MA', '01002', 'USA', '|', '1-800-377-8036', '|', 'www.lexalytics.com', 'https', ':', '//www.lexalytics.com/', 'https', ':', '//www.lexalytics.com/', 'https', ':', '//www.lexalytics.com/', 'https', ':', '//www.facebook.com/lexalytics/', 'https', ':', '//twitter.com/Lexalytics', 'https', ':', '//www.linkedin.com/company/lexalytics-inc/'] 

 TOTAL TOKENS ==> 58

 ---- POST ----

 [('For', 'IN'), ('more', 'JJR'), ('information', 'NN'), (',', ','), ('visit', 'NN'), ('www.lexalytics.com', 'NN'), ('or', 'CC'), ('call', 'VB'), ('1-800-377-8036', 'JJ'), ('W', 'NNP'), ('H', 'NNP'), ('I', 'PRP'), ('T', 'NNP'), ('E', 'NNP'), ('P', 'NNP'), ('A', 'NNP'), ('P', 'NNP'), ('E', 'NNP'), ('R', 'NNP'), ('15|', 'CD'), ('|', 'NNP'), ('Lexalytics', 'NNP'), (',', ','), ('Inc.', 'NNP'), (',', ','), ('48', 'CD'), ('North', 'NNP'), ('Pleasant', 'NNP'), ('St.', 'NNP'), ('Unit', 'NNP'), ('301', 'CD'), (',', ','), ('Amherst', 'NNP'), ('MA', 'NNP'), ('01002', 'CD'), ('USA', 'NNP'), ('|', 'NNP'), ('1-800-377-8036', 'JJ'), ('|', 'NNP'), ('www.lexalytics.com', 'NN'), ('https', 'NN'), (':', ':'), ('//www.lexalytics.com/', 'JJ'), ('https', 'NN'), (':', ':'), ('//www.lexalytics.com/', 'JJ'), ('https', 'NN'), (':', ':'), ('//www.lexalytics.com/', 'JJ'), ('https', 'NN'), (':', ':'), ('//www.facebook.com/lexalytics/', 'JJ'), ('https', 'NN'), (':', ':'), ('//twitter.com/Lexalytics', 'NNS'), ('https', 'NNS'), (':', ':'), ('//www.linkedin.com/company/lexalytics-inc/', 'NN')] 



 ---- TOKENS AFTER STOP-WORDS REMOVAL ---- 

 ['information', ',', 'visit', 'www.lexalytics.com', 'call', '1-800-377-8036', 'W', 'H', 'E', 'P', 'P', 'E', 'R', '15|', '|', 'Lexalytics', ',', 'Inc.', ',', '48', 'North', 'Pleasant', 'St.', 'Unit', '301', ',', 'Amherst', '01002', 'USA', '|', '1-800-377-8036', '|', 'www.lexalytics.com', 'https', ':', '//www.lexalytics.com/', 'https', ':', '//www.lexalytics.com/', 'https', ':', '//www.lexalytics.com/', 'https', ':', '//www.facebook.com/lexalytics/', 'https', ':', '//twitter.com/Lexalytics', 'https', ':', '//www.linkedin.com/company/lexalytics-inc/']

 TOTAL FILTERED TOKENS ==>  51

 ---- POST FOR FILTERED TOKENS ----

 [('information', 'NN'), (',', ','), ('visit', 'NN'), ('www.lexalytics.com', 'NN'), ('call', 'NN'), ('1-800-377-8036', 'JJ'), ('W', 'NNP'), ('H', 'NNP'), ('E', 'NNP'), ('P', 'NNP'), ('P', 'NNP'), ('E', 'NNP'), ('R', 'NNP'), ('15|', 'CD'), ('|', 'NNP'), ('Lexalytics', 'NNP'), (',', ','), ('Inc.', 'NNP'), (',', ','), ('48', 'CD'), ('North', 'NNP'), ('Pleasant', 'NNP'), ('St.', 'NNP'), ('Unit', 'NNP'), ('301', 'CD'), (',', ','), ('Amherst', 'NNP'), ('01002', 'CD'), ('USA', 'NNP'), ('|', 'NNP'), ('1-800-377-8036', 'JJ'), ('|', 'NNP'), ('www.lexalytics.com', 'NN'), ('https', 'NN'), (':', ':'), ('//www.lexalytics.com/', 'JJ'), ('https', 'NN'), (':', ':'), ('//www.lexalytics.com/', 'JJ'), ('https', 'NN'), (':', ':'), ('//www.lexalytics.com/', 'JJ'), ('https', 'NN'), (':', ':'), ('//www.facebook.com/lexalytics/', 'JJ'), ('https', 'NN'), (':', ':'), ('//twitter.com/Lexalytics', 'NNS'), ('https', 'NNS'), (':', ':'), ('//www.linkedin.com/company/lexalytics-inc/', 'NN')] 



 ---- BI-GRAMS ---- 

 ['information ,', ', visit', 'visit www.lexalytics.com', 'www.lexalytics.com call', 'call 1-800-377-8036', '1-800-377-8036 W', 'W H', 'H E', 'E P', 'P P', 'P E', 'E R', 'R 15|', '15| |', '| Lexalytics', 'Lexalytics ,', ', Inc.', 'Inc. ,', ', 48', '48 North', 'North Pleasant', 'Pleasant St.', 'St. Unit', 'Unit 301', '301 ,', ', Amherst', 'Amherst 01002', '01002 USA', 'USA |', '| 1-800-377-8036', '1-800-377-8036 |', '| www.lexalytics.com', 'www.lexalytics.com https', 'https :', ': //www.lexalytics.com/', '//www.lexalytics.com/ https', 'https :', ': //www.lexalytics.com/', '//www.lexalytics.com/ https', 'https :', ': //www.lexalytics.com/', '//www.lexalytics.com/ https', 'https :', ': //www.facebook.com/lexalytics/', '//www.facebook.com/lexalytics/ https', 'https :', ': //twitter.com/Lexalytics', '//twitter.com/Lexalytics https', 'https :', ': //www.linkedin.com/company/lexalytics-inc/'] 

 TOTAL BIGRAMS --> 50 



 ---- TRI-GRAMS ---- 

 ['information , visit', ', visit www.lexalytics.com', 'visit www.lexalytics.com call', 'www.lexalytics.com call 1-800-377-8036', 'call 1-800-377-8036 W', '1-800-377-8036 W H', 'W H E', 'H E P', 'E P P', 'P P E', 'P E R', 'E R 15|', 'R 15| |', '15| | Lexalytics', '| Lexalytics ,', 'Lexalytics , Inc.', ', Inc. ,', 'Inc. , 48', ', 48 North', '48 North Pleasant', 'North Pleasant St.', 'Pleasant St. Unit', 'St. Unit 301', 'Unit 301 ,', '301 , Amherst', ', Amherst 01002', 'Amherst 01002 USA', '01002 USA |', 'USA | 1-800-377-8036', '| 1-800-377-8036 |', '1-800-377-8036 | www.lexalytics.com', '| www.lexalytics.com https', 'www.lexalytics.com https :', 'https : //www.lexalytics.com/', ': //www.lexalytics.com/ https', '//www.lexalytics.com/ https :', 'https : //www.lexalytics.com/', ': //www.lexalytics.com/ https', '//www.lexalytics.com/ https :', 'https : //www.lexalytics.com/', ': //www.lexalytics.com/ https', '//www.lexalytics.com/ https :', 'https : //www.facebook.com/lexalytics/', ': //www.facebook.com/lexalytics/ https', '//www.facebook.com/lexalytics/ https :', 'https : //twitter.com/Lexalytics', ': //twitter.com/Lexalytics https', '//twitter.com/Lexalytics https :', 'https : //www.linkedin.com/company/lexalytics-inc/'] 

 TOTAL TRIGRAMS --> 49 



 ---- NOUN PHRASES ---- 

 ['information', 'visit', 'www.lexalytics.com', 'call', 'www.lexalytics.com', 'https', ' https', ' https', ' https', ' https', ''] 

 TOTAL NOUN PHRASES --> 11 



 ---- NER ----

 
 ORGANIZATION ---> ['USA']
 TOTAL ORGANIZATION ENTITY --> 1 


 PERSON ---> []
 TOTAL PERSON ENTITY --> 0 


 GPE ---> ['North']
 TOTAL GPE ENTITY --> 1 


 DATE ---> []
 TOTAL DATE ENTITY --> 0 



 ---- PORTER STEMMING ----

['inform', ',', 'visit', 'www.lexalytics.com', 'call', '1-800-377-8036', 'w', 'h', 'e', 'p', 'p', 'e', 'r', '15|', '|', 'lexalyt', ',', 'inc.', ',', '48', 'north', 'pleasant', 'st.', 'unit', '301', ',', 'amherst', '01002', 'usa', '|', '1-800-377-8036', '|', 'www.lexalytics.com', 'http', ':', '//www.lexalytics.com/', 'http', ':', '//www.lexalytics.com/', 'http', ':', '//www.lexalytics.com/', 'http', ':', '//www.facebook.com/lexalytics/', 'http', ':', '//twitter.com/lexalyt', 'http', ':', '//www.linkedin.com/company/lexalytics-inc/']

 TOTAL PORTER STEM WORDS ==> 51



 ---- SNOWBALL STEMMING ----

['inform', ',', 'visit', 'www.lexalytics.com', 'call', '1-800-377-8036', 'w', 'h', 'e', 'p', 'p', 'e', 'r', '15|', '|', 'lexalyt', ',', 'inc.', ',', '48', 'north', 'pleasant', 'st.', 'unit', '301', ',', 'amherst', '01002', 'usa', '|', '1-800-377-8036', '|', 'www.lexalytics.com', 'https', ':', '//www.lexalytics.com/', 'https', ':', '//www.lexalytics.com/', 'https', ':', '//www.lexalytics.com/', 'https', ':', '//www.facebook.com/lexalytics/', 'https', ':', '//twitter.com/lexalyt', 'https', ':', '//www.linkedin.com/company/lexalytics-inc/']

 TOTAL SNOWBALL STEM WORDS ==> 51



 ---- LEMMATIZATION ----

['information', ',', 'visit', 'www.lexalytics.com', 'call', '1-800-377-8036', 'W', 'H', 'E', 'P', 'P', 'E', 'R', '15|', '|', 'Lexalytics', ',', 'Inc.', ',', '48', 'North', 'Pleasant', 'St.', 'Unit', '301', ',', 'Amherst', '01002', 'USA', '|', '1-800-377-8036', '|', 'www.lexalytics.com', 'http', ':', '//www.lexalytics.com/', 'http', ':', '//www.lexalytics.com/', 'http', ':', '//www.lexalytics.com/', 'http', ':', '//www.facebook.com/lexalytics/', 'http', ':', '//twitter.com/Lexalytics', 'http', ':', '//www.linkedin.com/company/lexalytics-inc/']

 TOTAL LEMMATIZE WORDS ==> 51

************************************************************************************************************************

